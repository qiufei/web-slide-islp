## Introduction to Deep Learning

::: {layout-ncol=2}
-   Deep Learning is a very active area of research.
-   It is a subfield of machine learning.
-   The cornerstone of deep learning is the **neural network**.

![](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_1_1.png){fig-align="center" width=50%}
:::

## What is Data Mining, Machine Learning and Statistical Learning

::: {layout-ncol=2}
### Data Mining
- Discovering patterns, anomalies, and insights from large datasets.

### Machine Learning
-  Algorithms that improve their performance as they are exposed to more data.
-  Focuses on prediction and decision-making.

### Statistical Learning
- A subfield of statistics. Focuses on statistical models and methods for finding patterns in data.
- Bridges the gap between traditional statistics and machine learning. Focus: *Interpretable Models*.

:::

```{mermaid}
graph LR
    A[Data Mining] --> C(Common Ground)
    B[Machine Learning] --> C
    D[Statistical Learning] --> C
    C --> E[Insights & Predictions]
```

## History and Motivation

-   Neural networks gained prominence in the late 1980s.
-   Initial excitement and hype, followed by a period of stabilization.
-   Declined in popularity due to the rise of SVMs, boosting, and random forests (more "automatic", less "tinkering").
-   Resurfaced after 2010 with the name "Deep Learning" and have achieved many successes in image, video, speech and text modeling.
  -   New architectures.
  -   Larger datasets.
  -   More computing power.

## Neural Networks: The Basics

-   Inspired by the structure of the human brain.
-   Composed of interconnected nodes ("neurons") organized in layers.
-   Learn complex, non-linear relationships between inputs and outputs.
-   Foundation for most deep learning models.

## Single Layer Neural Networks

::: {layout-ncol=2}
-   Takes an input vector $\mathbf{X} = (X_1, X_2, \dots, X_p)$.
-   Builds a non-linear function $f(\mathbf{X})$ to predict a response $Y$.
-   Uses a specific structure:
    -   **Input Layer:** The features $X_1, \dots, X_p$.
    -   **Hidden Layer:**  Computes *activations* $A_k = h_k(\mathbf{X})$. These are non-linear transformations of linear combinations of the inputs.
    -   **Output Layer:**  A linear model that uses the activations as inputs, producing $f(\mathbf{X})$.
-   The functions $h_k(\cdot)$ are *learned* during training.

![](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_1.svg)
:::

## Single Layer Neural Network: Mathematical Formulation

-   The neural network model is:

    $$
    f(\mathbf{X}) = \beta_0 + \sum_{k=1}^K \beta_k h_k(\mathbf{X})
    $$

    where

    $$
    A_k = h_k(\mathbf{X}) = g\left(w_{k0} + \sum_{j=1}^p w_{kj}X_j\right)
    $$

-   $K$ is the number of hidden units (we choose this).
-   $g(\cdot)$ is the *activation function* (explained next).
-   $w_{kj}$ and $\beta_k$ are the *weights* (parameters to be learned).
-  $A_k$ are called *activations*.

## Activation Functions

::: {layout-ncol=2}
-   Introduce non-linearity into the model.  Without them, the neural network would collapse into a simple linear model.
-   Common choices:
    -   **Sigmoid:**  $g(z) = \frac{1}{1 + e^{-z}}$ (Historically popular, squashes values between 0 and 1.)
    -   **ReLU (Rectified Linear Unit):** $g(z) = \max(0, z)$ (Currently very popular, computationally efficient.)

![](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_2.svg)

:::

##  Why Non-linearity Matters

::: nonincremental
-   **Capturing Complexity:**  Real-world relationships are rarely linear. Activation functions allow the model to learn complex patterns.
-   **Interactions:**  Non-linearities enable the model to capture interactions between input variables.
-   **Example:** A simple example with $p=2$ inputs and $K=2$ hidden units, using $g(z) = z^2$, demonstrates how interactions can be modeled:
:::
$$
f(\mathbf{X}) =  X_1X_2
$$


## Multilayer Neural Networks

::: {layout-ncol=2}
- Modern neural networks have multiple hidden layers.
- Each layer builds upon the previous layer's activations, creating increasingly complex representations.
- Figure 10.4 shows a multilayer network for the MNIST digit classification task.
-  MNIST: handwritten digits, 28x28 grayscale images (784 pixels).

![](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_3.svg)
:::

## MNIST Dataset

::: {layout-ncol=2}
-   Goal: Classify images into their correct digit (0-9).
-   **One-hot encoding:** Output represented as a vector $\mathbf{Y} = (Y_0, Y_1, \dots, Y_9)$ where $Y_i = 1$ if the digit is $i$, and 0 otherwise.
-   60,000 training images, 10,000 test images.

![](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_4.svg)

:::

##  Multilayer Neural Networks: Key Differences from Single Layer

::: nonincremental
-   **Multiple Hidden Layers:**  $L_1$ (256 units), $L_2$ (128 units), etc.
-   **Multiple Outputs:**  One for each class (e.g., 10 for MNIST).
-   **Multitask Learning:** Can predict multiple different responses simultaneously.
-   **Loss Function:** Tailored to the task (e.g., cross-entropy for classification).
:::

## Multilayer Neural Networks: Mathematical Formulation

-   First hidden layer (same as before, but with superscript (1)):

    $$
    A_k^{(1)} = h_k^{(1)}(\mathbf{X}) = g\left(w_{k0}^{(1)} + \sum_{j=1}^p w_{kj}^{(1)} X_j\right)
    $$

-   Second hidden layer (takes activations from the first layer as input):

    $$
    A_l^{(2)} = h_l^{(2)}(\mathbf{X}) = g\left(w_{l0}^{(2)} + \sum_{k=1}^{K_1} w_{lk}^{(2)} A_k^{(1)}\right)
    $$

-   Output layer (for multi-class classification, uses *softmax*):

    $$
    f_m(\mathbf{X}) = \Pr(Y = m | \mathbf{X}) = \frac{e^{Z_m}}{\sum_{m'=0}^9 e^{Z_{m'}}}
    $$

    where  $Z_m = \beta_{m0} + \sum_{l=1}^{K_2} \beta_{ml} A_l^{(2)}$.

##  Loss Function and Optimization

-   **Cross-entropy loss** (for multi-class classification):

$$
-\sum_{i=1}^n \sum_{m=0}^9 y_{im} \log(f_m(\mathbf{x}_i))
$$

-   Goal: Find the weights that minimize this loss function.
-   Optimization is done using *gradient descent* (explained later).
- **Weights** refers to all trainable parameters, including coefficients and bias.

## Comparison with Linear Models (MNIST)

| Method                       | Test Error |
| ---------------------------- | ---------- |
| Neural Network + Ridge       | 2.3%       |
| Neural Network + Dropout     | 1.8%       |
| Multinomial Logistic Regression| 7.2%       |
| Linear Discriminant Analysis   | 12.7%      |

::: {.callout-note}
Neural networks significantly outperform linear methods on this task.
:::

## Convolutional Neural Networks (CNNs)

-   Specialized neural networks for image classification (and other tasks with spatial structure).
-   Inspired by how humans process visual information.
-   Key Idea: Learn *local* features and combine them to recognize *global* patterns.

## CNN Architecture: Key Components

::: nonincremental
-   **Convolutional Layers:**
    -   Apply *convolution filters* (small templates) to the input.
    -   Detect local features (edges, textures, etc.).
    -   *Learned* filters, not predefined.
    -   *Weight sharing*: the same filter is applied across the entire image.
-   **Pooling Layers:**
    -   Downsample the feature maps.
    -   Reduce dimensionality and provide some translation invariance.
    -   *Max pooling*: Takes the maximum value within a region.
-  **Flatten Layers:** convert multi-dimension feature maps to vector
-   **Fully Connected Layers:** Standard neural network layers (like those described earlier).
:::

## CNN Example: Tiger Classification

![](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_6.svg)

-   Input: Image of a tiger.
-   Convolutional layers identify low-level features (edges, stripes).
-   Pooling layers downsample and provide invariance.
-   Higher layers combine features (eyes, ears, etc.).
-   Output: Probability of the image being a tiger.

## Convolution Operation: Detailed Explanation

::: {layout-ncol=2}
-   A convolution filter is a small matrix (e.g., 3x3).
-   It's "slid" across the input image.
-   At each position, element-wise multiplication and summation are performed.
-   The result is a single value in the *convolved image*.
-   Different filters detect different features.

![](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_7.svg)

:::

## Convolution Operation: Example
- Input Image:
$$
Original Image = \begin{bmatrix}
a & b & c\\
d & e & f\\
g & h & i\\
j & k & l
\end{bmatrix}
$$

- Filter:
$$
ConvolutionFilter = \begin{bmatrix}
\alpha & \beta\\
\gamma & \delta
\end{bmatrix}
$$

- Convolved Image:
$$
Convolved Image = \begin{bmatrix}
aa + b\beta + d\gamma + e\delta & ba + c\beta + e\gamma + f\delta\\
da + e\beta + g\gamma + h\delta & ea + f\beta + h\gamma + i\delta\\
ga + h\beta + j\gamma + k\delta & ha + i\beta + k\gamma + l\delta
\end{bmatrix}
$$

## Convolutional Layer: Details

-   **Multiple Channels:**  Color images have 3 channels (Red, Green, Blue).  Convolution filters also have multiple channels.
-   **Multiple Filters:**  A convolutional layer uses *many* filters to extract different features.
-   **ReLU Activation:** Typically applied after convolution.
-   **Detector Layer:** The combination of convolution and ReLU is sometimes called a detector layer.

## Pooling Layer

-   **Purpose:** Reduce the spatial dimensions of the feature maps.
-   **Max Pooling:**  Takes the maximum value within a non-overlapping region (e.g., 2x2).
-   **Benefits:**
    -   Reduces computation.
    -   Provides some translation invariance.

## Example of Max Pooling

Input:

$$
\begin{bmatrix}
1 & 2 & 5 & 3 \\
3 & 0 & 1 & 2 \\
2 & 1 & 3 & 4 \\
1 & 1 & 2 & 0 \\
\end{bmatrix}
$$

Output (after max pooling with a 2x2 window):

$$
\begin{bmatrix}
3 & 5 \\
2 & 4
\end{bmatrix}
$$

## CNN Architecture: Putting it Together

![](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_8.svg)

-   Sequence of convolutional and pooling layers.
-   Convolutional layers extract features.
-   Pooling layers downsample.
-   Flatten layer: Converts the 3D feature maps into a 1D vector.
-   Fully connected layers: Perform classification.
-   Softmax output layer: Produces probabilities for each class.

## Data Augmentation

::: {layout-ncol=2}

-   **Purpose:**  Artificially increase the size of the training set.
-   **Method:**  Apply random transformations to training images (rotation, zoom, shift, flip, etc.).
-   **Benefits:**
    -   Reduces overfitting.
    -   Improves generalization.
    -   Acts as a form of regularization.

![](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_9.svg)

:::

## Pretrained Classifiers

::: {layout-ncol=2}

-   Leverage models trained on massive datasets (e.g., ImageNet).
-   Example: ResNet50.
-   **Weight Freezing:**  Use the pretrained convolutional layers as feature extractors and train only the final layers.

![](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_10.svg)

:::

## Pretrained Classifiers: Table 10.10

| Image         | True Label      | Prediction 1   | Prob 1 | Prediction 2     | Prob 2 | Prediction 3   | Prob 3 |
|---------------|-----------------|----------------|--------|-------------------|--------|-----------------|--------|
| (Flamingo)    | Flamingo        | Flamingo       | 0.83   | Spoonbill         | 0.17   | White stork     | 0.00   |
| (Cooper's Hawk)| Cooper's Hawk  | Kite           | 0.60   | Great grey owl    | 0.09   | Nail            | 0.12   |
| (Cooper's Hawk)| Cooper's Hawk   | Fountain    | 0.35   | nail    |   0.12     |    hook      |   0.07     |
|    (Lhasa Apso)           |   Lhasa Apso              |     Tibetan terrier           |    0.56    |      Lhasa         |   0.32     | cocker spaniel             |  0.03      |
|       (Cat)        |        Cat         |    Old English sheepdog            |   0.82     |   Shih-Tzu              |   0.04     |       Persian cat          |   0.04     |
| (Cape weaver)              |       Cape weaver           |      jacamar          |  0.28      |       macaw        |   0.12     |        robin         |     0.12   |

## Document Classification

-   Another important application of deep learning.
-   Goal: Predict attributes of documents (e.g., sentiment, topic).
-   **Featurization:**  Converting text into numerical representations.
-  Example: Sentiment analysis of IMDb movie reviews (positive or negative).

## Bag-of-Words Model

-   Simplest featurization method.
-   Represents a document as a vector indicating the presence/absence of words from a dictionary.
-   Ignores word order and context.
-   Sparse representation (most entries are zero).
-   *Bag-of-n-grams* considers sequences of n words.

## Recurrent Neural Networks (RNNs)

-   Designed for sequential data (text, time series, etc.).
-   Key Idea:  Process the input sequence *one element at a time*, maintaining a "hidden state" that captures information from previous elements.
-   "Unrolling" the RNN reveals its sequential nature.

## RNN Architecture

![](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_12.svg)

-   Input: Sequence of vectors $\mathbf{X} = \{\mathbf{X}_1, \mathbf{X}_2, \dots, \mathbf{X}_L\}$.
-   Hidden state:  $A_l$ (captures information from previous steps).
-   Output:  $O_l$ (often only the final output $O_L$ is used).
-   *Weight sharing*: The same weights ($\mathbf{W, U, B}$) are used at each step.

## RNN: Mathematical Formulation

-   Hidden state update:

    $$
    \mathbf{A}_{lk} = g\left(w_{k0} + \sum_{j=1}^p w_{kj} \mathbf{X}_{lj} + \sum_{s=1}^K u_{ks} \mathbf{A}_{l-1,s}\right)
    $$

-   Output:

    $$
    \mathbf{O}_l = \beta_0 + \sum_{k=1}^K \beta_k \mathbf{A}_{lk}
    $$

- $g(\cdot)$ is the activation function (e.g., ReLU).
- $\mathbf{W, U, B}$ are the shared weight matrices.

## RNNs for Document Classification

-   Instead of bag-of-words, use the *sequence* of words.
-   **Word Embeddings:** Represent words as dense, low-dimensional vectors (e.g., using word2vec or GloVe).
-   Embedding Layer: Maps one-hot encoded words to their embedding vectors.
-   Process the sequence of word embeddings using the RNN.

## Word Embeddings: Example
![](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_13.svg)
- One hot encoding vector length is the vocabulary size.
- Word embeddings provide dense representation.

## RNNs for Time Series Forecasting

-   Example:  Predicting stock market trading volume.
-   Input:  Sequence of past values (volume, return, volatility).
-   Output:  Predicted volume for the next day.
-   Autocorrelation:  Values in time series are often correlated with past values.

## Time Series Data: Example

![](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_14.svg)

-   Daily trading statistics from the NYSE.
-   Log trading volume, Dow Jones return, log volatility.

## Autocorrelation Function

![](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_15.svg)

-   Measures the correlation between a time series and its lagged values.
-   Shows how strongly related values are at different time lags.

## RNN Forecaster: Setup

-   Input Sequence:  $L$ past observations (e.g., $L=5$ days).

$$
\mathbf{X}_1 = \begin{pmatrix} v_{t-L} \\ r_{t-L} \\ z_{t-L} \end{pmatrix}, \mathbf{X}_2 = \begin{pmatrix} v_{t-L+1} \\ r_{t-L+1} \\ z_{t-L+1} \end{pmatrix}, \dots, \mathbf{X}_L = \begin{pmatrix} v_{t-1} \\ r_{t-1} \\ z_{t-1} \end{pmatrix}
$$

-   Output:  $Y = v_t$ (trading volume on day $t$).
-   Create many such (X, Y) pairs from the historical data.

## RNN Forecasting Results

![](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_16.svg)

-   RNN achieves $R^2 = 0.42$ on the test data.
-   Outperforms a simple baseline (using yesterday's volume).

## Autoregression (AR) Model

-   A traditional time series model.
-   Predicts the current value based on a linear combination of past values.
-   AR(L) model:

    $$
    \hat{v}_t = \beta_0 + \beta_1 v_{t-1} + \beta_2 v_{t-2} + \dots + \beta_L v_{t-L}
    $$

-   Can include lagged values of other variables (e.g., return, volatility).
-   RNN can be seen a non-linear extension of autoregression.

## Long Short-Term Memory (LSTM)

-   A more elaborate type of RNN.
-   Addresses the "vanishing gradient" problem in long sequences.
-   Maintains two hidden states: short-term and long-term memory.
-   Often improves performance compared to basic RNNs.

## Fitting Neural Networks: Overview

-   Complex optimization problem (non-convex).
-   Multiple local minima.
-   Key Techniques:
    -   **Gradient Descent:**  Iteratively adjust the weights to minimize the loss function.
    -   **Backpropagation:** Efficiently compute the gradient of the loss function.
    -   **Regularization:** Prevent overfitting (ridge, lasso, dropout).
    -   **Stochastic Gradient Descent (SGD):**  Use small batches of data to update the weights.
    -    **Early Stopping**: Stop training before the model begins over fitting.

## Gradient Descent: Illustration
![](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_17.svg)

## Gradient Descent

-   Iterative optimization algorithm.
-   Goal: Find the values of the parameters ($\theta$) that minimize the loss function $R(\theta)$.
-   Steps:
    1.  Initialize $\theta$ (often randomly).
    2.  Repeatedly update $\theta$ by moving in the *opposite* direction of the gradient:

        $$
        \theta^{(m+1)} \leftarrow \theta^{(m)} - \rho \nabla R(\theta^{(m)})
        $$

    3. $\rho$ is the *learning rate* (controls the step size).

## Backpropagation

-   Efficiently computes the gradient of the loss function with respect to the weights.
-   Uses the chain rule of calculus.
-   "Propagates" the error signal backward through the network.

## Regularization

-   **Purpose:** Prevent overfitting.
-   **Methods:**
    -   **Ridge/Lasso:** Add a penalty term to the loss function.
    -   **Dropout:** Randomly "drop out" units during training.
    -   **Early Stopping:**  Monitor performance on a validation set and stop training when it starts to worsen.

## Stochastic Gradient Descent (SGD)

-   Use small batches of data (minibatches) to update the weights.
-   Faster than using the entire dataset for each update.
-   Introduces randomness, which can help escape local minima.
-   The standard approach for training deep learning models.

##  Training and Validation Errors

![](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_18.svg)

-   Monitor both training and validation error during training.
-   Early stopping: Stop training when validation error starts to increase.

## Dropout Learning
![](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_19.svg)

## Network Tuning

-   Choosing the right architecture and hyperparameters is important.
-   Key Considerations:
    -   Number of hidden layers.
    -   Number of units per layer.
    -   Regularization parameters (dropout rate, ridge/lasso strength).
    -   Learning rate, batch size, number of epochs.
-   Often involves trial and error, and can be time-consuming.

## Interpolation and Double Descent

![](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_20.svg)

-   **Interpolation:**  Fitting a model that perfectly fits the training data (zero training error).
-   **Double Descent:**  A phenomenon where test error *decreases* again after increasing as model complexity increases beyond the interpolation threshold.
-   Observed in some deep learning models.
-   Does *not* contradict the bias-variance tradeoff.

## Double Descent: Explanation
- The phenomenon that the test error has U shape at first, then descent again after the interpolation point.
-  It does not conflict with the bias-variance trade-off.
-   Regularization methods can still get great results without interpolating.

## When to Use Deep Learning

-   **Large Datasets:**  Deep learning excels when you have a *lot* of data.
-   **Complex Relationships:**  When the relationship between inputs and outputs is highly non-linear.
-   **Feature Engineering is Difficult:**  Deep learning can automatically learn features.
-   **Interpretability is Less Important:**  Deep learning models are often "black boxes."
-   **Computational Resources:**  Training deep learning models can be computationally expensive.

## Summary

-   Deep learning is a powerful set of techniques based on neural networks.
-   CNNs are specialized for images, RNNs for sequential data.
-   Fitting involves complex optimization, but software tools simplify the process.
-   Regularization is crucial to prevent overfitting.
-   Deep learning excels with large datasets and complex relationships.
-   Consider simpler models if they perform well and are more interpretable.

## Thoughts and Discussion

-   What are the ethical implications of using deep learning in various applications (e.g., facial recognition, loan applications)?
-   How can we make deep learning models more interpretable?
-   What are the limitations of deep learning, and when might other methods be more appropriate?
-   How might deep learning evolve in the future? What new architectures or applications might emerge?
- How to determine model complexity? What is the impact of using a model that is too simple or too complex?
- What steps can be taken to ensure data quality and address biases in datasets used for training deep learning models?


