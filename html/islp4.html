<!DOCTYPE html>
<html lang="zh-Hans"><head>
<script src="islp4_files/libs/clipboard/clipboard.min.js"></script>
<script src="islp4_files/libs/quarto-html/tabby.min.js"></script>
<script src="islp4_files/libs/quarto-html/popper.min.js"></script>
<script src="islp4_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="islp4_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="islp4_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="islp4_files/libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.39">

  <meta name="author" content="ÈÇ±È£û">
  <title>islp4</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="islp4_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="islp4_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="islp4_files/libs/revealjs/dist/theme/quarto-a527665ed01beaa5bc5d6c6d96bbc18d.css">
  <link rel="stylesheet" href="my.css">
  <link href="islp4_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="islp4_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="islp4_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="islp4_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">


<section id="introduction-to-classification" class="title-slide slide level2 center">
<h2>Introduction to Classification</h2>
<div>
<ul>
<li class="fragment">So far, we‚Äôve focused on <strong>regression</strong> problems, where the response variable is <em>quantitative</em>.</li>
<li class="fragment">Now, we shift gears to <strong>classification</strong> problems, where the response variable is <em>qualitative</em> (categorical).</li>
<li class="fragment"><strong>Goal:</strong> To predict a qualitative response ‚Äì that is, to classify an observation into a category or class.</li>
<li class="fragment">Examples:
<ul>
<li class="fragment">Predicting whether a patient has a specific disease (yes/no).</li>
<li class="fragment">Classifying an email as spam or not spam.</li>
<li class="fragment">Determining whether a credit card transaction is fraudulent.</li>
</ul></li>
</ul>
</div>
</section>

<section id="what-is-classification" class="title-slide slide level2 center">
<h2>What is Classification?</h2>
<ul>
<li><strong>Classification</strong> involves assigning an observation to a category, or class.</li>
<li>It‚Äôs like sorting things into different boxes. üì¶</li>
<li>Many methods first predict the <em>probability</em> of belonging to each category, and classify based on those probabilities.</li>
</ul>
</section>

<section id="classification-vs.-regression" class="title-slide slide level2 center">
<h2>Classification vs.&nbsp;Regression</h2>
<table class="caption-top">
<thead>
<tr class="header">
<th>Feature</th>
<th>Regression</th>
<th>Classification</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Response Variable</td>
<td>Quantitative</td>
<td>Qualitative</td>
</tr>
<tr class="even">
<td>Goal</td>
<td>Predict a numerical value</td>
<td>Predict a category</td>
</tr>
<tr class="odd">
<td>Example</td>
<td>Predict house price</td>
<td>Predict disease presence (yes/no)</td>
</tr>
</tbody>
</table>
</section>

<section id="why-not-linear-regression" class="title-slide slide level2 center">
<h2>Why Not Linear Regression?</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li>Text material pointed out linear regression is not appropriate in the case of a qualitative response.</li>
<li>Suppose there are three possible diagnoses: <em>stroke</em>, <em>drug overdose</em>, and <em>epileptic seizure</em>.</li>
<li>We <em>could</em> try coding them numerically (e.g., 1=stroke, 2=drug overdose, 3=epileptic seizure).</li>
</ul>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2Fcoding.png"></p>
<figcaption>Medical conditions coding</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div>
<ul>
<li class="fragment">But this imposes an <em>order</em> and <em>equal differences</em> that may not make sense!</li>
<li class="fragment">Different codings would give completely different models and predictions!</li>
<li class="fragment">For <em>binary</em> (two-level) responses, linear regression <em>can</em> work (0/1 coding), but probabilities might fall outside [0,1]. Better options exist!</li>
</ul>
</div>
</section>

<section id="example-the-default-data" class="title-slide slide level2 center">
<h2>Example: The Default Data</h2>
<ul>
<li>We‚Äôll use a simulated dataset called ‚ÄúDefault‚Äù.</li>
<li>Goal: Predict whether an individual will default on their credit card payment.</li>
<li>Predictors:
<ul>
<li>Annual income.</li>
<li>Monthly credit card balance.</li>
</ul></li>
<li>Response: <code>default</code> (Yes/No)</li>
</ul>
</section>

<section id="visualizing-the-default-data" class="title-slide slide level2 center">
<h2>Visualizing the Default Data</h2>

<img data-src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_1.svg" width="800" class="r-stretch quarto-figure-center"><p class="caption">The Default data set</p><div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<ul>
<li><strong>Left:</strong> Scatterplot of income and balance, color-coded by default status (orange = defaulted, blue = did not default).</li>
<li><strong>Center:</strong> Boxplots of balance, separated by default status. Defaulters tend to have higher balances.</li>
<li><strong>Right:</strong> Boxplots of income, separated by default status. The relationship is less clear here.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-tip callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Higher credit card balances seem associated with a higher probability of default.</p>
</div>
</div>
</div>
</section>

<section id="logistic-regression-the-core-idea" class="title-slide slide level2 center">
<h2>Logistic Regression: The Core Idea</h2>
<ul>
<li>Instead of modeling the <em>response</em> directly, logistic regression models the <em>probability</em> that Y belongs to a particular category.</li>
<li>For the Default data, we model: Pr(default = Yes | balance, income)</li>
<li>This probability will always be between 0 and 1. üëç</li>
<li>Once we have the probability, we can classify (e.g., predict ‚Äúdefault=Yes‚Äù if the probability is &gt; 0.5).</li>
</ul>
</section>

<section id="the-logistic-model" class="title-slide slide level2 center">
<h2>The Logistic Model</h2>
<ul>
<li>We need a function that outputs values between 0 and 1, for any input. The <strong>logistic function</strong> does this!</li>
</ul>
<p><span class="math display">\[
p(X) = \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}}
\]</span></p>
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<ul>
<li><em>p(X)</em>: Probability of the event (e.g., default) given predictor(s) <em>X</em>.</li>
<li>Œ≤‚ÇÄ and Œ≤‚ÇÅ: Coefficients estimated from the data.</li>
<li><em>e</em>: The base of the natural logarithm (approximately 2.718).</li>
</ul>
</div>
</div>
</div>
<ul>
<li>This produces an S-shaped curve.</li>
</ul>
</section>

<section id="linear-vs.-logistic-regression-on-default-data" class="title-slide slide level2 center">
<h2>Linear vs.&nbsp;Logistic Regression on Default Data</h2>

<img data-src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_2.svg" width="800" class="r-stretch quarto-figure-center"><p class="caption">Classification using the Default data</p><div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<ul>
<li><strong>Left:</strong> Linear regression. Notice the negative probabilities and probabilities &gt; 1! üôÅ</li>
<li><strong>Right:</strong> Logistic regression. Probabilities are always between 0 and 1. üôÇ</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-tip callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<p>Logistic regression provides a more sensible fit for binary outcomes.</p>
</div>
</div>
</div>
</section>

<section id="the-logistic-model-continued" class="title-slide slide level2 center">
<h2>The Logistic Model (Continued)</h2>
<ul>
<li>A little algebra reveals a connection to <em>odds</em>:</li>
</ul>
<p><span class="math display">\[
\frac{p(X)}{1 - p(X)} = e^{\beta_0 + \beta_1X}
\]</span></p>
<ul>
<li><p>The left side is the <strong>odds</strong>, which can range from 0 to ‚àû.</p></li>
<li><p>Taking the logarithm of both sides:</p></li>
</ul>
<p><span class="math display">\[
\log\left(\frac{p(X)}{1 - p(X)}\right) = \beta_0 + \beta_1X
\]</span></p>
<ul>
<li>The left side is the <strong>log-odds</strong> or <strong>logit</strong>. This is <em>linear</em> in X!</li>
</ul>
</section>

<section id="interpreting-the-coefficients" class="title-slide slide level2 center">
<h2>Interpreting the Coefficients</h2>
<ul>
<li>In linear regression, Œ≤‚ÇÅ is the <em>average change in Y</em> for a one-unit increase in X.</li>
<li>In logistic regression, Œ≤‚ÇÅ is the <em>change in the log-odds</em> for a one-unit increase in X.</li>
<li>Equivalently, a one-unit increase in X <em>multiplies the odds</em> by e<sup>Œ≤‚ÇÅ</sup>.</li>
<li>The <em>amount</em> p(X) changes depends on the <em>current value</em> of X, because the relationship is non-linear.</li>
</ul>
</section>

<section id="estimating-the-coefficients" class="title-slide slide level2 center">
<h2>Estimating the Coefficients</h2>
<ul>
<li>We use <strong>maximum likelihood estimation (MLE)</strong>.<br>
</li>
<li>The goal is to find the coefficients (Œ≤‚ÇÄ, Œ≤‚ÇÅ, etc.) that make the <em>observed data</em> most likely.</li>
<li>The likelihood function for logistic regression:</li>
</ul>
<p><span class="math display">\[
l(\beta_0, \beta_1) = \prod_{i:y_i=1} p(x_i) \prod_{i':y_{i'}=0} (1 - p(x_{i'}))
\]</span></p>
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<ul>
<li>We multiply the probabilities of observing each data point, given the coefficients.</li>
<li>For defaulters (y·µ¢=1), we use p(x·µ¢). For non-defaulters (y·µ¢‚Äô=0), we use 1-p(x·µ¢‚Äô).</li>
</ul>
</div>
</div>
</div>
<ul>
<li>Software (like R) handles the maximization for us.</li>
</ul>
</section>

<section id="making-predictions" class="title-slide slide level2 center">
<h2>Making Predictions</h2>
<ul>
<li><p>Once we have the estimated coefficients, we can predict the probability of default for <em>any</em> given values of balance and income.</p></li>
<li><p>Example (using coefficients from Table 4.1):</p>
<ul>
<li>balance = $1,000: p(X) ‚âà 0.00576 (less than 1% chance of default)</li>
<li>balance = $2,000: p(X) ‚âà 0.586 (58.6% chance of default)</li>
</ul></li>
<li><p>We can classify based on a threshold (e.g., classify as ‚Äúdefault‚Äù if p(X) &gt; 0.5).</p></li>
</ul>
</section>

<section id="multiple-logistic-regression" class="title-slide slide level2 center">
<h2>Multiple Logistic Regression</h2>
<ul>
<li>Just like with linear regression, we can include <em>multiple predictors</em>:</li>
</ul>
<p><span class="math display">\[
\log\left(\frac{p(X)}{1 - p(X)}\right) = \beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p
\]</span> <span class="math display">\[
p(X) = \frac{e^{\beta_0 + \beta_1X_1 + \dots + \beta_pX_p}}{1 + e^{\beta_0 + \beta_1X_1 + \dots + \beta_pX_p}}
\]</span> - We estimate the coefficients using MLE. - Interpretation: Œ≤‚±º represents the change in log-odds for a one-unit increase in X‚±º, <em>holding all other predictors constant</em>.</p>
</section>

<section id="example-multiple-logistic-regression-on-default-data" class="title-slide slide level2 center">
<h2>Example: Multiple Logistic Regression on Default Data</h2>
<ul>
<li>Predictors: balance, income, student (dummy variable: 1 if student, 0 if not).</li>
</ul>
<table class="caption-top">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 20%">
<col style="width: 15%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">Coefficient</th>
<th style="text-align: left;">Std. error</th>
<th style="text-align: left;">z-statistic</th>
<th style="text-align: left;">p-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Intercept</td>
<td style="text-align: left;">-10.8690</td>
<td style="text-align: left;">0.4923</td>
<td style="text-align: left;">-22.08</td>
<td style="text-align: left;">&lt; 0.0001</td>
</tr>
<tr class="even">
<td style="text-align: left;">balance</td>
<td style="text-align: left;">0.0057</td>
<td style="text-align: left;">0.0002</td>
<td style="text-align: left;">24.74</td>
<td style="text-align: left;">&lt; 0.0001</td>
</tr>
<tr class="odd">
<td style="text-align: left;">income</td>
<td style="text-align: left;">0.0030</td>
<td style="text-align: left;">0.0082</td>
<td style="text-align: left;">0.37</td>
<td style="text-align: left;">0.7115</td>
</tr>
<tr class="even">
<td style="text-align: left;">student[Yes]</td>
<td style="text-align: left;">-0.6468</td>
<td style="text-align: left;">0.2362</td>
<td style="text-align: left;">-2.74</td>
<td style="text-align: left;">0.0062</td>
</tr>
</tbody>
</table>
<ul>
<li><em>Surprising</em> result: The coefficient for <code>student</code> is <em>negative</em>! This suggests students are <em>less</em> likely to default, holding balance and income constant.</li>
</ul>
</section>

<section id="confounding" class="title-slide slide level2 center">
<h2>Confounding</h2>

<img data-src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_3.svg" class="r-stretch quarto-figure-center"><p class="caption">Confounding in the Default data.</p><div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<ul>
<li><strong>Left:</strong> Default rates for students (orange) and non-students (blue) as a function of balance. Solid lines: default rate at each balance. Dashed lines: <em>overall</em> default rate.</li>
<li><strong>Right:</strong> Boxplots of balance for students and non-students.</li>
</ul>
</div>
</div>
</div>
<ul>
<li>Students have <em>higher</em> overall default rates (dashed lines), but <em>lower</em> default rates at each balance level (solid lines).</li>
<li>Reason: <code>student</code> and <code>balance</code> are <em>correlated</em>. Students tend to have higher balances, which are associated with higher default rates.</li>
<li>This is <strong>confounding</strong>: The effect of one predictor is mixed up with the effect of another.</li>
</ul>
</section>

<section id="multinomial-logistic-regression" class="title-slide slide level2 center">
<h2>Multinomial Logistic Regression</h2>
<ul>
<li>What if the response has <em>more than two</em> categories? (e.g., medical diagnosis: stroke, drug overdose, epileptic seizure)</li>
<li><strong>Multinomial logistic regression</strong> extends the two-class case.</li>
<li>We choose a <em>baseline</em> category (e.g., the Kth category).</li>
<li>We model the log-odds of each category <em>relative to the baseline</em>:</li>
</ul>
<p><span class="math display">\[
\log\left(\frac{\Pr(Y = k|X = x)}{\Pr(Y = K|X = x)}\right) = \beta_{k0} + \beta_{k1}x_1 + \dots + \beta_{kp}x_p
\]</span></p>
<ul>
<li>There are K-1 sets of coefficients.</li>
<li>The choice of baseline is arbitrary; the <em>predictions</em> will be the same regardless.</li>
</ul>
</section>

<section id="multinomial-logistic-regression-softmax-coding" class="title-slide slide level2 center">
<h2>Multinomial Logistic Regression: Softmax Coding</h2>
<ul>
<li>An alternative, equivalent formulation is <strong>softmax coding</strong>.</li>
<li>Instead of a baseline, we treat all <em>K</em> classes symmetrically:</li>
</ul>
<p><span class="math display">\[
\Pr(Y = k|X = x) = \frac{e^{\beta_{k0} + \beta_{k1}x_1 + \dots + \beta_{kp}x_p}}{\sum_{l=1}^K e^{\beta_{l0} + \beta_{l1}x_1 + \dots + \beta_{lp}x_p}}
\]</span></p>
<ul>
<li>This is often used in machine learning.</li>
</ul>
</section>

<section id="generative-models-for-classification" class="title-slide slide level2 center scrollable">
<h2>Generative Models for Classification</h2>
<ul>
<li>Logistic regression <em>directly</em> models Pr(Y = k | X = x).</li>
<li>Now we explore an <em>indirect</em> approach:
<ol type="1">
<li>Model the distribution of the predictors <em>X</em> separately in each response class (i.e., for each value of <em>Y</em>).</li>
<li>Use <strong>Bayes‚Äô theorem</strong> to ‚Äúflip‚Äù these around into estimates of Pr(Y = k | X = x).</li>
</ol></li>
<li>These are called <strong>generative models</strong> because they specify how the data is generated.</li>
</ul>
</section>

<section id="bayes-theorem-for-classification" class="title-slide slide level2 center">
<h2>Bayes‚Äô Theorem for Classification</h2>
<ul>
<li>Let œÄ<sub>k</sub> be the <em>prior probability</em> that an observation comes from class <em>k</em>.</li>
<li>Let f<sub>k</sub>(X) = Pr(X | Y = k) be the <em>density function</em> of X for an observation from class <em>k</em>.</li>
<li><strong>Bayes‚Äô theorem</strong> states:</li>
</ul>
<p><span class="math display">\[
\Pr(Y = k|X = x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^K \pi_l f_l(x)}
\]</span></p>
<ul>
<li>To use this, we need to estimate œÄ<sub>k</sub> and f<sub>k</sub>(x). Estimating œÄ<sub>k</sub> is usually easy (fraction of training observations in class k). Estimating f<sub>k</sub>(x) is harder.</li>
</ul>
</section>

<section id="why-bother-with-generative-models" class="title-slide slide level2 center">
<h2>Why Bother with Generative Models?</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p>If we already have logistic regression, why use this indirect approach?</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li>When there is substantial separation between classes, logistic regression parameters can be unstable. Generative models may be better.</li>
<li>If the distribution of predictors is approximately normal within each class, and the sample size is small, generative models may be more accurate.</li>
<li>Generative models can be easily extended to more than two response classes.</li>
</ul>
</div>
</div>
</div>
</section>

<section id="linear-discriminant-analysis-lda" class="title-slide slide level2 center">
<h2>Linear Discriminant Analysis (LDA)</h2>
<ul>
<li><strong>Assumptions:</strong>
<ul>
<li>f<sub>k</sub>(x) is <em>normal</em> (Gaussian).</li>
<li>We have a <em>common variance</em> across all K classes (œÉ¬≤).</li>
</ul></li>
<li>For a single predictor (p=1):</li>
</ul>
<p><span class="math display">\[
f_k(x) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{1}{2\sigma^2}(x - \mu_k)^2\right)
\]</span></p>
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<ul>
<li>Œº<sub>k</sub>: mean for class k</li>
<li>œÉ¬≤: common variance</li>
</ul>
</div>
</div>
</div>
</section>

<section id="lda-continued" class="title-slide slide level2 center">
<h2>LDA (Continued)</h2>
<ul>
<li>Plugging f<sub>k</sub>(x) into Bayes‚Äô theorem and simplifying, we classify an observation to the class for which this is largest:</li>
</ul>
<p><span class="math display">\[
\delta_k(x) = x \cdot \frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + \log(\pi_k)
\]</span></p>
<ul>
<li>This is the <strong>discriminant function</strong>. It‚Äôs <em>linear</em> in x. That‚Äôs why it‚Äôs called <em>Linear</em> Discriminant Analysis!</li>
<li>In practice, we don‚Äôt know the true parameters (œÄ<sub>k</sub>, Œº<sub>k</sub>, œÉ¬≤). We <em>estimate</em> them from the training data.</li>
</ul>
</section>

<section id="lda-estimating-the-parameters" class="title-slide slide level2 center">
<h2>LDA: Estimating the Parameters</h2>
<ul>
<li><p>ŒºÃÇ<sub>k</sub> = (1/n<sub>k</sub>) Œ£<sub>i:y·µ¢=k</sub> x·µ¢ (sample mean for class k)</p></li>
<li><p>œÉÃÇ¬≤ = (1/(n-K)) Œ£<sub>k=1</sub><sup>K</sup> Œ£<sub>i:y·µ¢=k</sub> (x·µ¢ - ŒºÃÇ<sub>k</sub>)¬≤ (pooled variance estimate)</p></li>
<li><p>œÄÃÇ<sub>k</sub> = n<sub>k</sub>/n (sample proportion for class k)</p></li>
<li><p>We plug these estimates into the discriminant function.</p></li>
</ul>
</section>

<section id="lda-example" class="title-slide slide level2 center">
<h2>LDA: Example</h2>

<img data-src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_4.svg" class="r-stretch quarto-figure-center"><p class="caption">One-dimensional normal density functions and LDA decision boundary</p><div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<ul>
<li><strong>Left:</strong> Two normal density functions. Dashed line: Bayes decision boundary.</li>
<li><strong>Right:</strong> 20 observations from each class (histograms). Dashed line: Bayes boundary. Solid line: LDA boundary.</li>
</ul>
</div>
</div>
</div>
<ul>
<li>LDA approximates the Bayes classifier.</li>
</ul>
</section>

<section id="lda-with-multiple-predictors-p-1" class="title-slide slide level2 center">
<h2>LDA with Multiple Predictors (p &gt; 1)</h2>
<ul>
<li><p>We assume X = (X‚ÇÅ, X‚ÇÇ, ‚Ä¶, X<sub>p</sub>) follows a <em>multivariate Gaussian</em> distribution, with a class-specific mean vector and a <em>common covariance matrix</em>.</p></li>
<li><p>Multivariate Gaussian density:</p></li>
</ul>
<p><span class="math display">\[
f(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu)^T\Sigma^{-1}(x - \mu)\right)
\]</span></p>
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<ul>
<li>Œº: mean vector</li>
<li>Œ£: covariance matrix</li>
<li>|Œ£|: determinant of Œ£</li>
</ul>
</div>
</div>
</div>
</section>

<section id="lda-with-multiple-predictors-continued" class="title-slide slide level2 center">
<h2>LDA with Multiple Predictors (Continued)</h2>
<ul>
<li>Plugging the multivariate Gaussian density into Bayes‚Äô theorem, we classify to the class for which this is largest:</li>
</ul>
<p><span class="math display">\[
\delta_k(x) = x^T \Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + \log \pi_k
\]</span></p>
<ul>
<li>Again, this is <em>linear</em> in x.</li>
<li>We estimate the parameters (Œº<sub>k</sub>, Œ£, œÄ<sub>k</sub>) from the training data.</li>
</ul>
</section>

<section id="lda-example-with-three-classes" class="title-slide slide level2 center">
<h2>LDA: Example with Three Classes</h2>
<p><img data-src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_6.svg" alt="Multivariate Gaussian distribution with three classes, and LDA decision boundaries"> ::: {.callout-note appearance=‚Äúsimple‚Äù} - <strong>Left:</strong> Ellipses contain 95% of the probability for each class. Dashed lines: Bayes decision boundaries. - <strong>Right:</strong> 20 observations from each class. Solid lines: LDA decision boundaries. ::: - LDA approximates the Bayes decision boundaries.</p>
</section>

<section id="lda-on-the-default-data" class="title-slide slide level2 center scrollable">
<h2>LDA on the Default Data</h2>
<ul>
<li>We can apply LDA to the Default data (predicting default based on balance and student status).</li>
<li>Training error rate: 2.75%. Sounds good, but‚Ä¶</li>
<li>Two problems:
<ol type="1">
<li>Training error rates are usually lower than <em>test error rates</em>.</li>
<li>Only 3.33% of individuals in the training data defaulted. A <em>useless</em> classifier that always predicts ‚Äúno default‚Äù would have a 3.33% error rate! (This is the <strong>null classifier</strong>.)</li>
</ol></li>
</ul>
</section>

<section id="confusion-matrix" class="title-slide slide level2 center">
<h2>Confusion Matrix</h2>
<ul>
<li>A <strong>confusion matrix</strong> shows the types of errors being made.</li>
</ul>
<table class="caption-top">
<colgroup>
<col style="width: 25%">
<col style="width: 35%">
<col style="width: 25%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">Predicted No Default</th>
<th style="text-align: left;">Predicted Default</th>
<th style="text-align: left;">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Actual No Default</td>
<td style="text-align: left;">9644</td>
<td style="text-align: left;">23</td>
<td style="text-align: left;">9667</td>
</tr>
<tr class="even">
<td style="text-align: left;">Actual Default</td>
<td style="text-align: left;">252</td>
<td style="text-align: left;">81</td>
<td style="text-align: left;">333</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Total</td>
<td style="text-align: left;">9896</td>
<td style="text-align: left;">104</td>
<td style="text-align: left;">10000</td>
</tr>
</tbody>
</table>
<ul>
<li>LDA misclassifies 252/333 = 75.7% of defaulters! (Low <strong>sensitivity</strong>.)</li>
<li>But it correctly classifies (1 - 23/9667) = 99.8% of non-defaulters. (High <strong>specificity</strong>.)</li>
</ul>
</section>

<section id="modifying-the-threshold" class="title-slide slide level2 center">
<h2>Modifying the Threshold</h2>
<ul>
<li>LDA (and the Bayes classifier) uses a threshold of 0.5 for the posterior probability of default.</li>
<li>If we <em>lower</em> the threshold (e.g., to 0.2), we can increase sensitivity (correctly identify more defaulters), but at the cost of decreased specificity (more false positives).</li>
<li>The best threshold depends on the <em>relative costs</em> of the two types of errors.</li>
</ul>
</section>

<section id="roc-curves" class="title-slide slide level2 center">
<h2>ROC Curves</h2>

<img data-src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_7.svg" class="r-stretch quarto-figure-center"><p class="caption">Error rates as a function of threshold</p><div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<ul>
<li>Shows various error rates as the threshold changes.</li>
<li>Black solid line: overall error rate</li>
<li>Blue dashed line: fraction of defaulters incorrectly classified</li>
<li>Orange dotted line: fraction of errors among non-defaulters</li>
</ul>
</div>
</div>
</div>
<ul>
<li>A <strong>Receiver Operating Characteristic (ROC) curve</strong> plots the <em>true positive rate</em> (sensitivity) versus the <em>false positive rate</em> (1 - specificity) for all possible thresholds.</li>
</ul>
</section>

<section id="roc-curve-continued" class="title-slide slide level2 center">
<h2>ROC Curve (Continued)</h2>

<img data-src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_8.svg" class="r-stretch quarto-figure-center"><p class="caption">ROC curve for LDA classifier on Default data</p><div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<ul>
<li>Ideal curve hugs the top left corner (high true positive rate, low false positive rate).</li>
<li>Dotted line: ‚Äúno information‚Äù classifier.</li>
</ul>
</div>
</div>
</div>
<ul>
<li>The <strong>area under the curve (AUC)</strong> summarizes performance. AUC = 1 is perfect; AUC = 0.5 is no better than chance.</li>
</ul>
</section>

<section id="quadratic-discriminant-analysis-qda" class="title-slide slide level2 center">
<h2>Quadratic Discriminant Analysis (QDA)</h2>
<ul>
<li><strong>QDA</strong> relaxes the assumption of a <em>common</em> covariance matrix.</li>
<li>Each class has its <em>own</em> covariance matrix, Œ£<sub>k</sub>.</li>
<li>The discriminant function becomes <em>quadratic</em> in x:</li>
</ul>
<p><span class="math display">\[
\delta_k(x) = -\frac{1}{2}x^T\Sigma_k^{-1}x + x^T\Sigma_k^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma_k^{-1}\mu_k - \frac{1}{2}\log|\Sigma_k| + \log \pi_k
\]</span></p>
<ul>
<li>We estimate the parameters (Œº<sub>k</sub>, Œ£<sub>k</sub>, œÄ<sub>k</sub>) from the training data.</li>
</ul>
</section>

<section id="lda-vs.-qda-bias-variance-trade-off" class="title-slide slide level2 center">
<h2>LDA vs.&nbsp;QDA: Bias-Variance Trade-Off</h2>
<ul>
<li>QDA is more <em>flexible</em> than LDA (more parameters to estimate).</li>
<li>QDA has <em>higher variance</em> but potentially <em>lower bias</em>.</li>
<li>LDA tends to be better when there are <em>fewer</em> training observations (reducing variance is crucial).</li>
<li>QDA is recommended when the training set is <em>large</em>, or when the assumption of a common covariance matrix is clearly untenable.</li>
</ul>
</section>

<section id="lda-vs.-qda-example" class="title-slide slide level2 center">
<h2>LDA vs.&nbsp;QDA: Example</h2>

<img data-src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_9.svg" class="r-stretch quarto-figure-center"><p class="caption">LDA and QDA decision boundaries</p><div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<ul>
<li><strong>Left:</strong> Classes have <em>common</em> correlation. Bayes boundary (dashed) is linear. LDA (black dotted) is better than QDA (green solid).</li>
<li><strong>Right:</strong> Classes have <em>different</em> correlations. Bayes boundary is quadratic. QDA is better than LDA.</li>
</ul>
</div>
</div>
</div>
</section>

<section id="naive-bayes" class="title-slide slide level2 center">
<h2>Naive Bayes</h2>
<ul>
<li>Makes a <em>very</em> strong assumption: <strong>Within each class, the <em>p</em> predictors are independent.</strong></li>
<li>f<sub>k</sub>(x) = f<sub>k1</sub>(x‚ÇÅ) √ó f<sub>k2</sub>(x‚ÇÇ) √ó ‚Ä¶ √ó f<sub>kp</sub>(x<sub>p</sub>)
<ul>
<li>f<sub>kj</sub> is the density function of the jth predictor in class k.</li>
</ul></li>
<li>This simplifies things <em>a lot</em>! We only need to estimate <em>one-dimensional</em> densities.</li>
<li>We plug this into Bayes‚Äô theorem:</li>
</ul>
<p><span class="math display">\[
\Pr(Y = k|X = x) = \frac{\pi_k \times f_{k1}(x_1) \times f_{k2}(x_2) \times \dots \times f_{kp}(x_p)}{\sum_{l=1}^K \pi_l \times f_{l1}(x_1) \times f_{l2}(x_2) \times \dots \times f_{lp}(x_p)}
\]</span></p>
</section>

<section id="naive-bayes-estimating-the-one-dimensional-densities" class="title-slide slide level2 center">
<h2>Naive Bayes: Estimating the One-Dimensional Densities</h2>
<ul>
<li>If X<sub>j</sub> is <em>quantitative</em>:
<ul>
<li>We can assume X<sub>j</sub> | Y = k ~ N(Œº<sub>jk</sub>, œÉ<sub>j</sub>¬≤) (Gaussian).</li>
<li>Or, we can use a non-parametric estimate (e.g., histogram, kernel density estimator).</li>
</ul></li>
<li>If X<sub>j</sub> is <em>qualitative</em>:
<ul>
<li>We can simply count the proportion of training observations for each level of the predictor within each class.</li>
</ul></li>
</ul>
</section>

<section id="naive-bayes-example" class="title-slide slide level2 center">
<h2>Naive Bayes: Example</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="3">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_10_1.svg"></p>
<figcaption>f11</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_10_2.svg"></p>
<figcaption>f12</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_10_3.svg"></p>
<figcaption>f13</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="3">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_10_4.svg"></p>
<figcaption>f21</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_10_5.svg"></p>
<figcaption>f22</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_10_6.svg"></p>
<figcaption>f23</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<ul>
<li>Two classes (K=2), three predictors (p=3). First two predictors are quantitative, third is qualitative (three levels).</li>
<li>Estimated density functions f<sub>kj</sub> are shown.</li>
</ul>
</div>
</div>
</div>
<ul>
<li>Naive Bayes often works surprisingly well, despite the strong independence assumption. It reduces variance, which can be beneficial.</li>
</ul>
</section>

<section id="comparison-of-classification-methods-an-analytical-perspective" class="title-slide slide level2 center">
<h2>Comparison of Classification Methods: An Analytical Perspective</h2>
<ul>
<li>Logistic regression, LDA, QDA, and naive Bayes can all be expressed in terms of maximizing Pr(Y = k | X = x).</li>
<li>Equivalently, we can maximize the log-odds relative to a baseline class (K):</li>
</ul>
<p><span class="math display">\[
\log\left(\frac{\Pr(Y = k|X = x)}{\Pr(Y = K|X = x)}\right)
\]</span></p>
<ul>
<li>The <em>form</em> of this log-odds expression differs for each method.</li>
</ul>
<table class="caption-top">
<colgroup>
<col style="width: 10%">
<col style="width: 89%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Log-Odds Form</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Logistic Regression</td>
<td style="text-align: left;">Œ≤<sub>k0</sub> + Œ£<sub>j=1</sub><sup>p</sup> Œ≤<sub>kj</sub>x<sub>j</sub> (linear)</td>
</tr>
<tr class="even">
<td style="text-align: left;">LDA</td>
<td style="text-align: left;">a<sub>k</sub> + Œ£<sub>j=1</sub><sup>p</sup> b<sub>kj</sub>x<sub>j</sub> (linear)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">QDA</td>
<td style="text-align: left;">a<sub>k</sub> + Œ£<sub>j=1</sub><sup>p</sup> b<sub>kj</sub>x<sub>j</sub> + Œ£<sub>j=1</sub><sup>p</sup> Œ£<sub>l=1</sub><sup>p</sup> c<sub>kj</sub><sub>l</sub>x<sub>j</sub>x<sub>l</sub> (quadratic)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Naive Bayes</td>
<td style="text-align: left;">a<sub>k</sub> + Œ£<sub>j=1</sub><sup>p</sup> g<sub>kj</sub>(x<sub>j</sub>) (additive, where g<sub>kj</sub> is a function of x<sub>j</sub>)</td>
</tr>
</tbody>
</table>
<ul>
<li>LDA is a special case of QDA.</li>
<li>Any classifier with a <em>linear</em> decision boundary is a special case of naive Bayes.</li>
<li>Logistic regression has the same <em>linear</em> form as LDA, but the coefficients are estimated differently.</li>
<li>None of the method dominate others universally.</li>
</ul>
</section>

<section id="a-comparison-of-classification-methodsan-empirical-comparison" class="title-slide slide level2 center">
<h2>A comparison of classification methodsÔºöAn empirical comparison</h2>
<ul>
<li>Test error rates of different methods are compared through six different scenarios.</li>
</ul>
<style>
.half-width {
  width: 50%;
  float: left;
}
</style>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="half-width quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_11.svg"></p>
<figcaption>FIGURE 4.11</figcaption>
</figure>
</div>
</div>
<div class="half-width quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_12.svg"></p>
<figcaption>FIGURE 4.12</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<ul>
<li>Six different scenarios are involved, scenarios 1-3 Bayes decision boundaries are linear, and scenarios 4-6 Bayes decision boundaries are non-linear.</li>
<li>There were p=2 quantitative predictors in each of the six scenarios.</li>
<li>For each scenario, 100 random training data sets were generated.</li>
<li>Scenario 1 to Scenario 6 are described in the text material page 31 and page 32.</li>
</ul>
</div>
</div>
</div>
<ul>
<li>When the true decision boundaries are <em>linear</em>, LDA and logistic regression perform well.</li>
<li>When boundaries are moderately non-linear, QDA or naive Bayes may be better.</li>
<li>For <em>very</em> complex boundaries, a non-parametric method like KNN can be superior, <em>but</em> the level of smoothness must be chosen carefully.</li>
</ul>
</section>

<section id="generalized-linear-models-glms" class="title-slide slide level2 center">
<h2>Generalized Linear Models (GLMs)</h2>
<ul>
<li>So far, the response Y is either quantitative(use least squares linear regression to predict) or qualitative(use classification methods).</li>
<li>But Y is neither qualitative nor quantitative is also possible, like the number of hourly users of a bike sharing program.</li>
<li>To predict the number of bike users, the least squares linear regression model shown defects(negative fitted values, variance increasing when mean number increasing, response not continuous-valued)</li>
<li>Poisson regression could provide a much more natural and elegant approach.</li>
</ul>
</section>

<section id="poisson-regression" class="title-slide slide level2 center">
<h2>Poisson Regression</h2>
<ul>
<li>Suppose the random variable Y takes on nonnegative integer values and follows the Poisson distribution. The probability could be written as:</li>
</ul>
<p><span class="math display">\[
\Pr(Y = k) = \frac{e^{-\lambda}\lambda^k}{k!}  \quad \text{for } k = 0, 1, 2, \dots
\]</span></p>
<div class="callout callout-note callout-style-simple">
<div class="callout-body">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-content">
<ul>
<li>k! means <code>k factorial</code>, Œª &gt; 0 is the expected value of Y, which also equals the variance of Y.</li>
<li>Larger the mean of Y, larger the variance of Y.</li>
</ul>
</div>
</div>
</div>
<ul>
<li>Poisson distribution is used to model counts.</li>
<li>Number of bike users could be modeled as Poisson distribution with mean value. The mean could vary as a function of the covariates.</li>
</ul>
</section>

<section id="poisson-regression-model" class="title-slide slide level2 center">
<h2>Poisson Regression Model</h2>
<p><span class="math display">\[
\log(\lambda(X_1, \dots, X_p)) = \beta_0 + \beta_1X_1 + \dots + \beta_pX_p
\]</span> or equivalently</p>
<p><span class="math display">\[
\lambda(X_1, \dots, X_p) = e^{\beta_0 + \beta_1X_1 + \dots + \beta_pX_p}
\]</span></p>
<ul>
<li>The log of Œª is linear in predictors.</li>
<li>Œª takes on nonnegative values for all values of covariates.</li>
<li>We use maximum likelihood to estimate parameters.</li>
</ul>
</section>

<section id="comparing-poisson-regression-model-to-the-linear-regression-model" class="title-slide slide level2 center">
<h2>Comparing Poisson Regression Model to the linear regression model</h2>
<ul>
<li>Interpretation of the coefficients</li>
<li>Mean-variance relationship</li>
<li>Nonnegative fitted values</li>
<li>Please read text material for details.</li>
</ul>
</section>

<section id="generalized-linear-models-in-greater-generality" class="title-slide slide level2 center scrollable">
<h2>Generalized Linear Models in Greater Generality</h2>
<ul>
<li>Three types of regression models have been discussed: linear, logistic and Poisson. They share some common characteristics:
<ol type="1">
<li>Predictors are used to predict response variable. Conditional on predictors, Y belongs to a certain family of distributions. We make different assumption on the family of distributions that Y belongs to, correspondingly, we have different regression models.</li>
<li>Modeling the mean of Y. Use different <em>link function</em> Œ∑, these regression models could be expressed as below:</li>
</ol></li>
</ul>
<p><span class="math display">\[
\eta(E(Y|X_1, \dots, X_p)) = \beta_0 + \beta_1X_1 + \dots + \beta_pX_p
\]</span></p>
<ul>
<li>All these regression models are examples of <strong>Generalized Linear Model(GLM)</strong></li>
</ul>
</section>

<section id="summary" class="title-slide slide level2 center">
<h2>Summary</h2>
<ul>
<li>Classification predicts <em>qualitative</em> responses.</li>
<li>Logistic regression, LDA, QDA, and naive Bayes are common classification methods.</li>
<li>Logistic regression models the <em>probability</em> of class membership using the logistic function.</li>
<li>LDA and QDA assume the predictors follow a Gaussian distribution within each class. LDA assumes a <em>common</em> covariance matrix; QDA does not.</li>
<li>Naive Bayes assumes <em>independence</em> of predictors within each class.</li>
<li>The choice of method depends on the data and the bias-variance trade-off.</li>
<li>ROC curves are useful for evaluating classifier performance across a range of thresholds.</li>
<li>Generalized linear models handle responses from non-normal distributions, Poisson regression model is an example.</li>
</ul>
</section>

<section id="thoughts-and-discussion" class="title-slide slide level2 center">
<h2>Thoughts and Discussion ü§î</h2>
<div>
<ul>
<li class="fragment">Can you think of real-world scenarios where each classification method (logistic regression, LDA, QDA, naive Bayes) might be most appropriate?</li>
<li class="fragment">How would you choose the ‚Äúbest‚Äù classification method for a given dataset? What metrics would you consider?</li>
<li class="fragment">What are the limitations of each method? When might they fail?</li>
<li class="fragment">How does the bias-variance trade-off play a role in choosing a classification method?</li>
<li class="fragment">Can we apply the knowledge of mean-variance relationship and fitted values to choose suitable regression model?</li>
</ul>
</div>

</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="https://assets.qiufei.site/zwu/zwu_logo.png" class="slide-logo"></p>
<div class="footer footer-default">
<p>ÈÇ±È£û üíå hfutqiufei@163.com</p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="islp4_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="islp4_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="islp4_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="islp4_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="islp4_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="islp4_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="islp4_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="islp4_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="islp4_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="islp4_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Â∑≤Â§çÂà∂");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Â∑≤Â§çÂà∂");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>