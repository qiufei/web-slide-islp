## Introduction to Linear Regression

::: {layout-ncol=2}
-   Linear regression is a fundamental approach in supervised learning, used primarily for predicting a quantitative response.
-   It's a cornerstone in statistical learning, widely used and extensively studied.

![ ](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F2_1.svg)
:::

::: {.callout-note appearance="simple"}
Linear regression serves as a stepping stone to more complex statistical learning methods. Many advanced techniques can be seen as extensions or generalizations of linear regression.
:::

## Key Questions in Linear Regression

We explore the `Advertising` data to address several key questions:

1.  **Relationship Existence:** Is there a connection between advertising budget and sales? 
2.  **Relationship Strength:** How strong is the link between budget and sales?
3.  **Media Contribution:** Which advertising media (TV, radio, newspaper) contribute to sales?
4.  **Association Size:**  How much does sales increase for each dollar spent on each medium?
5.  **Prediction Accuracy:** Can we accurately predict future sales?
6.  **Linearity Check:** Is the relationship between advertising and sales linear?
7.  **Media Synergy:** Do the advertising media work together synergistically (interaction effect)?

::: {.callout-note appearance="simple"}
Addressing these questions helps determine if a relationship exists, its strength, individual media contributions, prediction accuracy, the linearity of the connection, and potential synergy effects.
:::

## Simple Linear Regression: The Basics

Simple linear regression predicts a quantitative response, $Y$, using a single predictor variable, $X$, assuming a linear relationship:

$$
Y \approx \beta_0 + \beta_1X
$$

-   $\beta_0$: Intercept (value of $Y$ when $X = 0$).
-   $\beta_1$: Slope (change in $Y$ for a one-unit increase in $X$).
-   $\beta_0$ and $\beta_1$ are the model *coefficients* or *parameters*.

::: {.callout-note appearance="simple"}
This equation represents a straight line, where $\beta_0$ is the y-intercept, and $\beta_1$ is the slope.
:::

## Simple Linear Regression: Example

For instance, let's regress `sales` onto `TV` advertising:

$$
\text{sales} \approx \beta_0 + \beta_1 \times \text{TV}
$$

-   $Y$: Sales (in thousands of units).
-   $X$: TV advertising budget (in thousands of dollars).

Once we estimate the coefficients $\beta_0$ and $\beta_1$ (denoted as $\hat{\beta_0}$ and $\hat{\beta_1}$), we can predict sales:

$$
\hat{y} = \hat{\beta_0} + \hat{\beta_1}x
$$

::: {.callout-note appearance="simple"}
The "hat" symbol (^) indicates an estimated value.  $\hat{y}$ is the *predicted* value of sales.
:::

## Estimating the Coefficients: Least Squares

Our goal is to find $\hat{\beta_0}$ and $\hat{\beta_1}$ that best fit the data, meaning the line should be as close as possible to the data points $(x_i, y_i)$. We use the *least squares* method, which minimizes the *residual sum of squares* (RSS):

$$
\text{RSS} = \sum_{i=1}^{n} (y_i - \hat{y_i})^2 = \sum_{i=1}^{n} (y_i - \hat{\beta_0} - \hat{\beta_1}x_i)^2
$$

-   $y_i$: Actual sales for the $i$-th observation.
-   $\hat{y_i}$: Predicted sales for the $i$-th observation.
-   $e_i = y_i - \hat{y_i}$: The *residual* for the $i$-th observation (the difference between the actual and predicted values).

::: {.callout-note appearance="simple"}
Least squares finds the line that minimizes the sum of the squared vertical distances between the data points and the line.
:::
## Estimating the Coefficients: Formulas

Using calculus, we find the values of $\hat{\beta_0}$ and $\hat{\beta_1}$ that minimize RSS:

$$
\hat{\beta_1} = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2}
$$

$$
\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}
$$

-   $\bar{x}$: Sample mean of $X$.
-   $\bar{y}$: Sample mean of $Y$.

::: {.callout-note appearance="simple"}
These formulas provide the best estimates for the slope and intercept based on the available data. The slope ($\hat{\beta_1}$) represents the average change in Y for a one-unit increase in X, and the intercept ($\hat{\beta_0}$) is the predicted value of Y when X is zero.
:::

## Visualizing the Least Squares Fit

![Least Squares Fit](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F3_1.svg)

::: {.callout-note appearance="simple"}
This figure shows the least squares regression line for `sales` versus `TV` advertising.  Each grey line segment represents a *residual*, the difference between the observed `sales` value and the value predicted by the line. The least squares method minimizes the sum of the squares of these residuals.
:::

## Assessing Coefficient Accuracy: Population vs. Sample

-   **Population Regression Line:** The "true" (but usually unknown) relationship: $Y = \beta_0 + \beta_1X + \epsilon$.
-   **Least Squares Line:**  The estimated relationship based on our sample: $\hat{y} = \hat{\beta_0} + \hat{\beta_1}x$.
- $\epsilon$: random error term

![Population vs. Sample Regression Lines](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F3_3.svg)

::: {.callout-note appearance="simple"}
The left panel shows the true population regression line (red) and the estimated least squares line (blue) from a single sample. The right panel shows ten different least squares lines, each estimated from a different sample drawn from the same population. The least squares lines vary, but they cluster around the true population line.
:::

## Assessing Coefficient Accuracy: Unbiasedness

-   $\hat{\beta_0}$ and $\hat{\beta_1}$ are *estimates* of the true (unknown) parameters $\beta_0$ and $\beta_1$.
-   These estimates are *unbiased*: on average, they will equal the true values.  This means if we took many samples and calculated the estimates each time, the average of the estimates would converge to the true values.

::: {.callout-note appearance="simple"}
Unbiasedness means that our estimation method doesn't systematically over- or underestimate the true values.
:::

## Assessing Coefficient Accuracy: Standard Error

-   **Standard Error:** Measures the average amount that an estimate ($\hat{\beta_0}$ or $\hat{\beta_1}$) differs from the true value ($\beta_0$ or $\beta_1$).
-   Formulas for standard errors:

$$
\text{SE}(\hat{\beta_0})^2 = \sigma^2 \left[ \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2} \right]
$$

$$
\text{SE}(\hat{\beta_1})^2 = \frac{\sigma^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}
$$
- $\sigma^2$: Variance of the error term $\epsilon$ (usually unknown, estimated by the *residual standard error*, RSE).

::: {.callout-note appearance="simple"}
Smaller standard errors indicate more precise estimates. Notice that SE($\hat{\beta_1}$) is smaller when the $x_i$ values are more spread out.
:::

## Assessing Coefficient Accuracy: Confidence Intervals

-   **Confidence Interval:** A range of values that is likely to contain the true unknown value of a parameter, with a certain level of confidence (e.g., 95%).
-   Approximate 95% confidence interval for $\beta_1$:

$$
\hat{\beta_1} \pm 2 \cdot \text{SE}(\hat{\beta_1})
$$

::: {.callout-note appearance="simple"}
This means that if we were to repeatedly sample from the population and construct 95% confidence intervals, approximately 95% of those intervals would contain the true value of $\beta_1$.
:::

## Hypothesis Testing

-   **Null Hypothesis (H₀):**  There is no relationship between $X$ and $Y$ ($\beta_1 = 0$).
-   **Alternative Hypothesis (Hₐ):** There is *some* relationship between $X$ and $Y$ ($\beta_1 \neq 0$).
-   **t-statistic:** Measures how many standard deviations $\hat{\beta_1}$ is away from 0:

$$
t = \frac{\hat{\beta_1} - 0}{\text{SE}(\hat{\beta_1})}
$$

-   **p-value:** The probability of observing a t-statistic as extreme as, or more extreme than, the one calculated, *assuming* $H_0$ is true.

::: {.callout-note appearance="simple"}
A small p-value (typically < 0.05) provides evidence *against* the null hypothesis, suggesting a relationship between X and Y.
:::

## Hypothesis Testing: Example (Advertising Data)

| Predictor   | Coefficient | Std. error | t-statistic | p-value  |
| :---------- | :---------- | :--------- | :---------- | :------- |
| Intercept   | 7.0325      | 0.4578     | 15.36       | < 0.0001 |
| TV          | 0.0475      | 0.0027     | 17.67       | < 0.0001 |

::: {.callout-note appearance="simple"}
The table shows the results of regressing `sales` on `TV` advertising.  The very small p-value for `TV` provides strong evidence that $\beta_1 \neq 0$, meaning there *is* a relationship between TV advertising and sales.
:::

## Assessing Model Accuracy: RSE

-   **Residual Standard Error (RSE):** An estimate of the standard deviation of the error term $\epsilon$. It represents the average amount that the response will deviate from the true regression line.

$$
\text{RSE} = \sqrt{\frac{1}{n-2}\text{RSS}} = \sqrt{\frac{1}{n-2}\sum_{i=1}^{n}(y_i - \hat{y_i})^2}
$$

::: {.callout-note appearance="simple"}
Lower RSE values indicate a better fit. The RSE is measured in the units of Y.
:::

## Assessing Model Accuracy: R²

-   **R² Statistic:** Measures the *proportion of variance explained* by the model. It always falls between 0 and 1.

$$
R^2 = \frac{\text{TSS} - \text{RSS}}{\text{TSS}} = 1 - \frac{\text{RSS}}{\text{TSS}}
$$

-   **Total Sum of Squares (TSS):** $\sum(y_i - \bar{y})^2$ - Measures the total variance in the response $Y$.

::: {.callout-note appearance="simple"}
R² closer to 1 indicates that a large proportion of the variability in the response is explained by the regression. R² is the square of the correlation between X and Y in simple linear regression.
:::

## Assessing Model Accuracy: Example (Advertising Data)

| Quantity                | Value  |
| :---------------------- | :----- |
| Residual standard error | 3.26   |
| R²                      | 0.612  |
| F-statistic             | 312.1  |

::: {.callout-note appearance="simple"}
For the regression of `sales` on `TV`, the RSE is 3.26 (thousands of units), and the R² is 0.612.  This means that about 61.2% of the variability in sales is explained by TV advertising.
:::

## Multiple Linear Regression

Extends simple linear regression to handle *multiple* predictors:

$$
Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p + \epsilon
$$

-   $\beta_j$: The average effect on $Y$ of a one-unit increase in $X_j$, *holding all other predictors fixed*.

::: {.callout-note appearance="simple"}
Each predictor now has its own slope coefficient.
:::
## Multiple Linear Regression: Example (Advertising Data)

$$
\text{sales} = \beta_0 + \beta_1 \times \text{TV} + \beta_2 \times \text{radio} + \beta_3 \times \text{newspaper} + \epsilon
$$

| Predictor   | Coefficient | Std. error | t-statistic | p-value  |
| :---------- | :---------- | :--------- | :---------- | :------- |
| Intercept   | 2.939       | 0.3119     | 9.42        | < 0.0001 |
| TV          | 0.046       | 0.0014     | 32.81       | < 0.0001 |
| radio       | 0.189       | 0.0086     | 21.89       | < 0.0001 |
| newspaper   | -0.001      | 0.0059     | -0.18       | 0.8599   |

::: {.callout-note appearance="simple"}
Holding TV and newspaper advertising fixed, spending an additional $1,000 on radio advertising is associated with an increase in sales of approximately 189 units. The newspaper coefficient is not statistically significant. The interpretation changes compared to simple linear regression due to correlations between the predictors.
:::

## Correlation Between Predictors

|           | TV      | radio   | newspaper | sales   |
| :-------- | :------ | :------ | :-------- | :------ |
| TV        | 1.0000  | 0.0548  | 0.0567    | 0.7822  |
| radio     | 0.0548  | 1.0000  | 0.3541    | 0.5762  |
| newspaper | 0.0567  | 0.3541  | 1.0000    | 0.2283  |
| sales     | 0.7822  | 0.5762  | 0.2283    | 1.0000  |

::: {.callout-note appearance="simple"}
This correlation matrix shows the pairwise correlations between the variables in the `Advertising` data.  Notice the moderate correlation (0.35) between `radio` and `newspaper`. This explains the difference in the `newspaper` coefficient between the simple and multiple linear regressions.
:::

## Important Questions in Multiple Linear Regression

1.  **Any Useful Predictors?** Is at least *one* predictor useful in predicting the response? (F-test)
2.  **All or Subset?** Do *all* predictors help explain $Y$, or only a subset? (Variable selection)
3.  **Model Fit:** How well does the model fit the data? (RSE, R²)
4.  **Prediction:** Given predictor values, what should we predict for the response, and how accurate is our prediction? (Prediction intervals, confidence intervals)

## One: Is There a Relationship? (F-test)

-   **Null Hypothesis (H₀):**  *All* coefficients are zero ($\beta_1 = \beta_2 = \dots = \beta_p = 0$).
-   **Alternative Hypothesis (Hₐ):** At least *one* coefficient is non-zero.
-   **F-statistic:**

$$
F = \frac{(\text{TSS} - \text{RSS})/p}{\text{RSS}/(n - p - 1)}
$$

::: {.callout-note appearance="simple"}
If there's *no* relationship between the response and predictors, the F-statistic will be close to 1.  If Hₐ is true, F will be *greater* than 1.
:::

## F-test: Example (Advertising Data)

| Quantity                | Value  |
| :---------------------- | :----- |
| Residual standard error | 1.69   |
| R²                      | 0.897  |
| F-statistic             | 570    |

::: {.callout-note appearance="simple"}
The F-statistic for the multiple regression of `sales` on `TV`, `radio`, and `newspaper` is 570.  This is much larger than 1, providing strong evidence against the null hypothesis. The p-value is essentially zero, indicating that at least one advertising medium is related to sales.
:::

## Two: Deciding on Important Variables (Variable Selection)

-   **Goal:** Identify the subset of predictors that are most strongly related to the response.
-   **Methods:**
    -   *Forward Selection:* Start with the null model (intercept only) and add predictors one by one.
    -   *Backward Selection:* Start with all predictors and remove them one by one.
    -   *Mixed Selection:* Combination of forward and backward selection.

::: {.callout-note appearance="simple"}
We typically can't try all possible subsets of predictors (there are 2ᵖ of them!), so we use these more efficient methods.
:::

## Three: Model Fit (RSE and R²)

-   **RSE and R²:**  Same interpretations as in simple linear regression.
-   **Important Note:** R² will *always* increase when more variables are added to the model, even if those variables are only weakly associated with the response.

::: {.callout-note appearance="simple"}
Adding more variables always reduces the RSS on the *training* data, but may not improve predictions on *new* data.
:::

## Four: Predictions

Three sources of uncertainty in predictions:

1.  **Coefficient Uncertainty:** The least squares plane is only an *estimate* of the true population regression plane. (Reducible error, addressed with confidence intervals)
2.  **Model Bias:** The linear model is likely an approximation. (Reducible error, addressed by considering more complex models)
3.  **Irreducible Error:** Even if we knew the true relationship, we couldn't predict $Y$ perfectly because of the random error $\epsilon$. (Addressed with prediction intervals)

## Confidence vs. Prediction Intervals

-   **Confidence Interval:** Quantifies uncertainty around the *average* response value.
-   **Prediction Interval:** Quantifies uncertainty around a *single* response value.

::: {.callout-note appearance="simple"}
Prediction intervals are *always* wider than confidence intervals because they account for both the uncertainty in estimating the population regression plane *and* the inherent variability of individual data points around that plane.
:::

## Qualitative Predictors

-   **Qualitative Predictor (Factor):** A variable with categorical values (levels).
-   **Dummy Variable:** A numerical variable used to represent a qualitative predictor in a regression model.
    -   For a predictor with two levels: Create one dummy variable.
    -   For a predictor with more than two levels: Create one fewer dummy variable than the number of levels.
    - One level will serve as a baseline.

::: {.callout-note appearance="simple"}
    Each dummy variable is coded as 0 or 1, indicating the absence or presence of a particular level.
:::

## Qualitative Predictors: Example (Credit Data)

We want to predict `balance` using the `own` variable (whether someone owns a house).

- Create a dummy variable:

$$
x_i = \begin{cases}
1 & \text{if person } i \text{ owns a house} \\
0 & \text{if person } i \text{ does not own a house}
\end{cases}
$$

- Regression model:

$$
y_i = \beta_0 + \beta_1x_i + \epsilon_i = \begin{cases}
\beta_0 + \beta_1 + \epsilon_i & \text{if person } i \text{ owns a house} \\
\beta_0 + \epsilon_i & \text{if person } i \text{ does not own a house}
\end{cases}
$$

::: {.callout-note appearance="simple"}
$\beta_0$ represents the average credit card balance for non-owners, and $\beta_0 + \beta_1$ represents the average balance for owners. $\beta_1$ is the average *difference* in balance between owners and non-owners.
:::

## Interactions

-   **Additive Assumption:** The effect of one predictor on the response does *not* depend on the values of other predictors.
-   **Interaction Effect (Synergy):** The effect of one predictor on the response *does* depend on the values of other predictors.
-   **Interaction Term:**  Include the *product* of two predictors in the model.

::: {.callout-note appearance="simple"}
Interactions allow the relationship between a predictor and the response to vary depending on the values of other predictors.
:::

## Interactions: Example (Advertising Data)

$$
\text{sales} = \beta_0 + \beta_1 \times \text{TV} + \beta_2 \times \text{radio} + \beta_3 \times (\text{TV} \times \text{radio}) + \epsilon
$$

This can be rewritten as:

$$
\text{sales} = \beta_0 + (\beta_1 + \beta_3 \times \text{radio}) \times \text{TV} + \beta_2 \times \text{radio} + \epsilon
$$

::: {.callout-note appearance="simple"}
Now, the slope for `TV` depends on the value of `radio`. The interaction term allows for synergy between the advertising media.
:::

## Non-linear Relationships: Polynomial Regression

-   **Linearity Assumption:** The relationship between the predictors and the response is linear.
-   **Polynomial Regression:** Include polynomial terms (e.g., $X^2$, $X^3$) of the predictors in the model to capture non-linear relationships.

$$
\text{mpg} = \beta_0 + \beta_1 \times \text{horsepower} + \beta_2 \times \text{horsepower}^2 + \epsilon
$$

::: {.callout-note appearance="simple"}
This is still a *linear* regression model (linear in the coefficients), but it models a *non-linear* relationship between `mpg` and `horsepower`.

![Polynomial Regression](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F3_8.svg)

:::

## Potential Problems in Linear Regression

1.  **Non-linearity:**  The relationship between response and predictors is not linear. (Use residual plots to detect)
2.  **Correlation of Error Terms:**  Errors are not independent. (Common in time series data)
3.  **Non-constant Variance of Error Terms (Heteroscedasticity):** Variance of errors changes with the response. (Funnel shape in residual plot)
4.  **Outliers:**  Observations with unusual response values.
5.  **High Leverage Points:** Observations with unusual predictor values.
6.  **Collinearity:**  Predictors are highly correlated.

::: {.callout-note appearance="simple"}
These problems can affect the accuracy and interpretability of the regression model.
:::

## Summary

-   Linear regression is a fundamental and versatile tool for predicting a quantitative response.
-   It relies on assumptions about linearity, additivity, and the error terms.
-   We can assess model fit using RSE and R², and assess coefficient significance using t-statistics and p-values.
-   Multiple linear regression allows for multiple predictors, and we can use the F-test to check for any relationship between the predictors and the response.
-   Extensions like qualitative predictors, interaction terms, and polynomial regression increase the flexibility of the linear model.

## Thoughts and Discussion

-   How do we choose the "best" model among a set of possible models (e.g., different combinations of predictors, interactions, polynomial terms)?
-   What are the limitations of linear regression, and when might other, more complex methods be more appropriate?
-   How can we effectively diagnose and address the potential problems in linear regression (non-linearity, collinearity, etc.)?
-   How can the insights from a linear regression model be used to inform real-world decisions (e.g., marketing strategies)?
- How does the size and quality of data affect the model outcome?
