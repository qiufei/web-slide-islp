## Introduction to Classification 

::: {.incremental}
-  So far, we've focused on **regression** problems, where the response variable is *quantitative*.
-  Now, we shift gears to **classification** problems, where the response variable is *qualitative* (categorical).
-  **Goal:** To predict a qualitative response ‚Äì that is, to classify an observation into a category or class.
- Examples:
    -  Predicting whether a patient has a specific disease (yes/no).
    -  Classifying an email as spam or not spam.
    -  Determining whether a credit card transaction is fraudulent.
:::

## What is Classification?

-  **Classification** involves assigning an observation to a category, or class.
-  It's like sorting things into different boxes. üì¶
- Many methods first predict the *probability* of belonging to each category, and classify based on those probabilities.

## Classification vs. Regression

| Feature | Regression  | Classification  |
| ------- | ------------------ | ------------------ |
| Response Variable | Quantitative | Qualitative |
| Goal  | Predict a numerical value | Predict a category |
| Example  | Predict house price  | Predict disease presence (yes/no)  |

## Why Not Linear Regression?

::: {layout-ncol=2}
- Text material pointed out linear regression is not appropriate in the case of a qualitative response.
-  Suppose there are three possible diagnoses: *stroke*, *drug overdose*, and *epileptic seizure*. 
- We *could* try coding them numerically (e.g., 1=stroke, 2=drug overdose, 3=epileptic seizure).

![Medical conditions coding](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2Fcoding.png){fig-align="center"}
:::

::: incremental
- But this imposes an *order* and *equal differences* that may not make sense!
-  Different codings would give completely different models and predictions!
-  For *binary* (two-level) responses, linear regression *can* work (0/1 coding), but probabilities might fall outside [0,1]. Better options exist!
:::

## Example: The Default Data

-   We'll use a simulated dataset called "Default".
-   Goal: Predict whether an individual will default on their credit card payment.
-   Predictors:
    -   Annual income.
    -   Monthly credit card balance.
- Response: `default` (Yes/No)

## Visualizing the Default Data

![The Default data set](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_1.svg){fig-align="center" width="800"}

::: {.callout-note appearance="simple"}
- **Left:** Scatterplot of income and balance, color-coded by default status (orange = defaulted, blue = did not default).
- **Center:** Boxplots of balance, separated by default status. Defaulters tend to have higher balances.
- **Right:** Boxplots of income, separated by default status. The relationship is less clear here.
:::
::: {.callout-tip appearance="simple"}
Higher credit card balances seem associated with a higher probability of default.
:::

## Logistic Regression: The Core Idea

-   Instead of modeling the *response* directly, logistic regression models the *probability* that Y belongs to a particular category.
-   For the Default data, we model:  Pr(default = Yes | balance, income)
-   This probability will always be between 0 and 1. üëç
-   Once we have the probability, we can classify (e.g., predict "default=Yes" if the probability is > 0.5).

## The Logistic Model

-   We need a function that outputs values between 0 and 1, for any input. The **logistic function** does this!

$$
p(X) = \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}}
$$

::: {.callout-note appearance="simple"}
-   *p(X)*: Probability of the event (e.g., default) given predictor(s) *X*.
-   Œ≤‚ÇÄ and Œ≤‚ÇÅ: Coefficients estimated from the data.
-   *e*: The base of the natural logarithm (approximately 2.718).
:::

- This produces an S-shaped curve.

## Linear vs. Logistic Regression on Default Data

![Classification using the Default data](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_2.svg){fig-align="center" width="800"}

::: {.callout-note appearance="simple"}
- **Left:** Linear regression.  Notice the negative probabilities and probabilities > 1! üôÅ
- **Right:** Logistic regression.  Probabilities are always between 0 and 1. üôÇ
:::

::: {.callout-tip appearance="simple"}
Logistic regression provides a more sensible fit for binary outcomes.
:::

## The Logistic Model (Continued)

- A little algebra reveals a connection to *odds*:

$$
\frac{p(X)}{1 - p(X)} = e^{\beta_0 + \beta_1X}
$$

- The left side is the **odds**, which can range from 0 to ‚àû.

- Taking the logarithm of both sides:

$$
\log\left(\frac{p(X)}{1 - p(X)}\right) = \beta_0 + \beta_1X
$$

- The left side is the **log-odds** or **logit**.  This is *linear* in X!

## Interpreting the Coefficients

-   In linear regression, Œ≤‚ÇÅ is the *average change in Y* for a one-unit increase in X.
-   In logistic regression, Œ≤‚ÇÅ is the *change in the log-odds* for a one-unit increase in X.
-   Equivalently, a one-unit increase in X *multiplies the odds* by e<sup>Œ≤‚ÇÅ</sup>.
-   The *amount* p(X) changes depends on the *current value* of X, because the relationship is non-linear.

## Estimating the Coefficients

-   We use **maximum likelihood estimation (MLE)**.  
- The goal is to find the coefficients (Œ≤‚ÇÄ, Œ≤‚ÇÅ, etc.) that make the *observed data* most likely.
- The likelihood function for logistic regression:

$$
l(\beta_0, \beta_1) = \prod_{i:y_i=1} p(x_i) \prod_{i':y_{i'}=0} (1 - p(x_{i'}))
$$

::: {.callout-note appearance="simple"}
- We multiply the probabilities of observing each data point, given the coefficients.
- For defaulters (y·µ¢=1), we use p(x·µ¢).  For non-defaulters (y·µ¢'=0), we use 1-p(x·µ¢').
:::
- Software (like R) handles the maximization for us.

## Making Predictions

- Once we have the estimated coefficients, we can predict the probability of default for *any* given values of balance and income.

-   Example (using coefficients from Table 4.1):
    -   balance = $1,000:  p(X) ‚âà 0.00576 (less than 1% chance of default)
    -   balance = $2,000:  p(X) ‚âà 0.586 (58.6% chance of default)

- We can classify based on a threshold (e.g., classify as "default" if p(X) > 0.5).

## Multiple Logistic Regression

-   Just like with linear regression, we can include *multiple predictors*:

$$
\log\left(\frac{p(X)}{1 - p(X)}\right) = \beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p
$$
$$
p(X) = \frac{e^{\beta_0 + \beta_1X_1 + \dots + \beta_pX_p}}{1 + e^{\beta_0 + \beta_1X_1 + \dots + \beta_pX_p}}
$$
-   We estimate the coefficients using MLE.
- Interpretation:  Œ≤‚±º represents the change in log-odds for a one-unit increase in X‚±º, *holding all other predictors constant*.

## Example: Multiple Logistic Regression on Default Data

-   Predictors: balance, income, student (dummy variable: 1 if student, 0 if not).

|             | Coefficient | Std. error | z-statistic | p-value  |
| :---------- | :---------- | :--------- | :---------- | :------- |
| Intercept   | -10.8690    | 0.4923     | -22.08      | < 0.0001 |
| balance     | 0.0057      | 0.0002     | 24.74       | < 0.0001 |
| income      | 0.0030      | 0.0082     | 0.37        | 0.7115   |
| student[Yes]| -0.6468     | 0.2362     | -2.74       | 0.0062   |
: {tbl-colwidths="[25,25,20,15,15]"}

-   *Surprising* result: The coefficient for `student` is *negative*!  This suggests students are *less* likely to default, holding balance and income constant.

## Confounding

![Confounding in the Default data.](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_3.svg)

::: {.callout-note appearance="simple"}
-   **Left:** Default rates for students (orange) and non-students (blue) as a function of balance.  Solid lines:  default rate at each balance.  Dashed lines:  *overall* default rate.
-   **Right:** Boxplots of balance for students and non-students.
:::

-   Students have *higher* overall default rates (dashed lines), but *lower* default rates at each balance level (solid lines).
-   Reason:  `student` and `balance` are *correlated*.  Students tend to have higher balances, which are associated with higher default rates.
-   This is **confounding**:  The effect of one predictor is mixed up with the effect of another.

## Multinomial Logistic Regression

-   What if the response has *more than two* categories? (e.g., medical diagnosis: stroke, drug overdose, epileptic seizure)
-   **Multinomial logistic regression** extends the two-class case.
-   We choose a *baseline* category (e.g., the Kth category).
-   We model the log-odds of each category *relative to the baseline*:

$$
\log\left(\frac{\Pr(Y = k|X = x)}{\Pr(Y = K|X = x)}\right) = \beta_{k0} + \beta_{k1}x_1 + \dots + \beta_{kp}x_p
$$

-   There are K-1 sets of coefficients.
- The choice of baseline is arbitrary; the *predictions* will be the same regardless.

## Multinomial Logistic Regression: Softmax Coding

- An alternative, equivalent formulation is **softmax coding**.
- Instead of a baseline, we treat all *K* classes symmetrically:

$$
\Pr(Y = k|X = x) = \frac{e^{\beta_{k0} + \beta_{k1}x_1 + \dots + \beta_{kp}x_p}}{\sum_{l=1}^K e^{\beta_{l0} + \beta_{l1}x_1 + \dots + \beta_{lp}x_p}}
$$

- This is often used in machine learning.

## Generative Models for Classification

- Logistic regression *directly* models Pr(Y = k | X = x).
- Now we explore an *indirect* approach:
    1.  Model the distribution of the predictors *X* separately in each response class (i.e., for each value of *Y*).
    2.  Use **Bayes' theorem** to "flip" these around into estimates of Pr(Y = k | X = x).

- These are called **generative models** because they specify how the data is generated.

## Bayes' Theorem for Classification

- Let œÄ<sub>k</sub> be the *prior probability* that an observation comes from class *k*.
- Let f<sub>k</sub>(X) = Pr(X | Y = k) be the *density function* of X for an observation from class *k*.
- **Bayes' theorem** states:

$$
\Pr(Y = k|X = x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^K \pi_l f_l(x)}
$$

- To use this, we need to estimate œÄ<sub>k</sub> and f<sub>k</sub>(x). Estimating œÄ<sub>k</sub> is usually easy (fraction of training observations in class k). Estimating f<sub>k</sub>(x) is harder.

## Why Bother with Generative Models?

::: {layout-ncol=2}
If we already have logistic regression, why use this indirect approach?

- When there is substantial separation between classes, logistic regression parameters can be unstable. Generative models may be better.
- If the distribution of predictors is approximately normal within each class, and the sample size is small, generative models may be more accurate.
- Generative models can be easily extended to more than two response classes.
:::

## Linear Discriminant Analysis (LDA)

- **Assumptions:**
    -   f<sub>k</sub>(x) is *normal* (Gaussian).
    -   We have a *common variance* across all K classes (œÉ¬≤).

- For a single predictor (p=1):

$$
f_k(x) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{1}{2\sigma^2}(x - \mu_k)^2\right)
$$

::: {.callout-note appearance="simple"}
- Œº<sub>k</sub>: mean for class k
- œÉ¬≤:  common variance
:::

## LDA (Continued)

- Plugging f<sub>k</sub>(x) into Bayes' theorem and simplifying, we classify an observation to the class for which this is largest:

$$
\delta_k(x) = x \cdot \frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + \log(\pi_k)
$$

- This is the **discriminant function**.  It's *linear* in x. That's why it's called *Linear* Discriminant Analysis!
- In practice, we don't know the true parameters (œÄ<sub>k</sub>, Œº<sub>k</sub>, œÉ¬≤).  We *estimate* them from the training data.

## LDA: Estimating the Parameters

-   ŒºÃÇ<sub>k</sub> = (1/n<sub>k</sub>) Œ£<sub>i:y·µ¢=k</sub> x·µ¢  (sample mean for class k)
-   œÉÃÇ¬≤ = (1/(n-K)) Œ£<sub>k=1</sub><sup>K</sup> Œ£<sub>i:y·µ¢=k</sub> (x·µ¢ - ŒºÃÇ<sub>k</sub>)¬≤  (pooled variance estimate)
-   œÄÃÇ<sub>k</sub> = n<sub>k</sub>/n (sample proportion for class k)

- We plug these estimates into the discriminant function.

## LDA: Example

![One-dimensional normal density functions and LDA decision boundary](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_4.svg)

::: {.callout-note appearance="simple"}
-   **Left:** Two normal density functions.  Dashed line: Bayes decision boundary.
-   **Right:** 20 observations from each class (histograms).  Dashed line: Bayes boundary. Solid line: LDA boundary.
:::
- LDA approximates the Bayes classifier.

## LDA with Multiple Predictors (p > 1)

- We assume X = (X‚ÇÅ, X‚ÇÇ, ..., X<sub>p</sub>) follows a *multivariate Gaussian* distribution, with a class-specific mean vector and a *common covariance matrix*.

- Multivariate Gaussian density:

$$
f(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu)^T\Sigma^{-1}(x - \mu)\right)
$$

::: {.callout-note appearance="simple"}
- Œº: mean vector
- Œ£: covariance matrix
- |Œ£|: determinant of Œ£
:::

## LDA with Multiple Predictors (Continued)

- Plugging the multivariate Gaussian density into Bayes' theorem, we classify to the class for which this is largest:

$$
\delta_k(x) = x^T \Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + \log \pi_k
$$

- Again, this is *linear* in x.
- We estimate the parameters (Œº<sub>k</sub>, Œ£, œÄ<sub>k</sub>) from the training data.

## LDA: Example with Three Classes

![Multivariate Gaussian distribution with three classes, and LDA decision boundaries](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_6.svg)
::: {.callout-note appearance="simple"}
- **Left:**  Ellipses contain 95% of the probability for each class. Dashed lines: Bayes decision boundaries.
- **Right:** 20 observations from each class. Solid lines: LDA decision boundaries.
:::
- LDA approximates the Bayes decision boundaries.

## LDA on the Default Data

-   We can apply LDA to the Default data (predicting default based on balance and student status).
-   Training error rate: 2.75%.  Sounds good, but...
-   Two problems:
    1.  Training error rates are usually lower than *test error rates*.
    2.  Only 3.33% of individuals in the training data defaulted.  A *useless* classifier that always predicts "no default" would have a 3.33% error rate! (This is the **null classifier**.)

## Confusion Matrix

-   A **confusion matrix** shows the types of errors being made.

|                 | Predicted No Default | Predicted Default | Total |
| :-------------- | :------------------- | :---------------- | :---- |
| Actual No Default | 9644                | 23               | 9667  |
| Actual Default    | 252                 | 81               | 333   |
| Total           | 9896                | 104               | 10000   |
: {tbl-colwidths="[25,35,25,15]"}

-   LDA misclassifies 252/333 = 75.7% of defaulters! (Low **sensitivity**.)
-   But it correctly classifies (1 - 23/9667) = 99.8% of non-defaulters. (High **specificity**.)

## Modifying the Threshold

-   LDA (and the Bayes classifier) uses a threshold of 0.5 for the posterior probability of default.
-   If we *lower* the threshold (e.g., to 0.2), we can increase sensitivity (correctly identify more defaulters), but at the cost of decreased specificity (more false positives).
- The best threshold depends on the *relative costs* of the two types of errors.

## ROC Curves

![Error rates as a function of threshold](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_7.svg)

::: {.callout-note appearance="simple"}
- Shows various error rates as the threshold changes.
- Black solid line: overall error rate
- Blue dashed line: fraction of defaulters incorrectly classified
- Orange dotted line: fraction of errors among non-defaulters
:::

- A **Receiver Operating Characteristic (ROC) curve** plots the *true positive rate* (sensitivity) versus the *false positive rate* (1 - specificity) for all possible thresholds.

## ROC Curve (Continued)

![ROC curve for LDA classifier on Default data](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_8.svg)

::: {.callout-note appearance="simple"}
- Ideal curve hugs the top left corner (high true positive rate, low false positive rate).
- Dotted line: "no information" classifier.
:::

- The **area under the curve (AUC)** summarizes performance.  AUC = 1 is perfect; AUC = 0.5 is no better than chance.

## Quadratic Discriminant Analysis (QDA)

-   **QDA** relaxes the assumption of a *common* covariance matrix.
-   Each class has its *own* covariance matrix, Œ£<sub>k</sub>.
-   The discriminant function becomes *quadratic* in x:

$$
\delta_k(x) = -\frac{1}{2}x^T\Sigma_k^{-1}x + x^T\Sigma_k^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma_k^{-1}\mu_k - \frac{1}{2}\log|\Sigma_k| + \log \pi_k
$$

- We estimate the parameters (Œº<sub>k</sub>, Œ£<sub>k</sub>, œÄ<sub>k</sub>) from the training data.

## LDA vs. QDA: Bias-Variance Trade-Off

-   QDA is more *flexible* than LDA (more parameters to estimate).
-   QDA has *higher variance* but potentially *lower bias*.
-   LDA tends to be better when there are *fewer* training observations (reducing variance is crucial).
-   QDA is recommended when the training set is *large*, or when the assumption of a common covariance matrix is clearly untenable.

## LDA vs. QDA: Example

![LDA and QDA decision boundaries](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_9.svg)

::: {.callout-note appearance="simple"}
-   **Left:**  Classes have *common* correlation.  Bayes boundary (dashed) is linear.  LDA (black dotted) is better than QDA (green solid).
-   **Right:** Classes have *different* correlations. Bayes boundary is quadratic. QDA is better than LDA.
:::

## Naive Bayes

- Makes a *very* strong assumption: **Within each class, the *p* predictors are independent.**
- f<sub>k</sub>(x) = f<sub>k1</sub>(x‚ÇÅ) √ó f<sub>k2</sub>(x‚ÇÇ) √ó ... √ó f<sub>kp</sub>(x<sub>p</sub>)
    - f<sub>kj</sub> is the density function of the jth predictor in class k.
- This simplifies things *a lot*! We only need to estimate *one-dimensional* densities.
- We plug this into Bayes' theorem:

$$
\Pr(Y = k|X = x) = \frac{\pi_k \times f_{k1}(x_1) \times f_{k2}(x_2) \times \dots \times f_{kp}(x_p)}{\sum_{l=1}^K \pi_l \times f_{l1}(x_1) \times f_{l2}(x_2) \times \dots \times f_{lp}(x_p)}
$$

## Naive Bayes: Estimating the One-Dimensional Densities

- If X<sub>j</sub> is *quantitative*:
    - We can assume X<sub>j</sub> | Y = k ~ N(Œº<sub>jk</sub>, œÉ<sub>j</sub>¬≤) (Gaussian).
    - Or, we can use a non-parametric estimate (e.g., histogram, kernel density estimator).
- If X<sub>j</sub> is *qualitative*:
    - We can simply count the proportion of training observations for each level of the predictor within each class.

## Naive Bayes: Example
::: {layout-ncol=3}
![f11](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_10_1.svg)

![f12](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_10_2.svg)

![f13](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_10_3.svg)
:::

::: {layout-ncol=3}
![f21](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_10_4.svg)

![f22](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_10_5.svg)

![f23](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_10_6.svg)
:::

::: {.callout-note appearance="simple"}
-   Two classes (K=2), three predictors (p=3). First two predictors are quantitative, third is qualitative (three levels).
-   Estimated density functions f<sub>kj</sub> are shown.
:::

-   Naive Bayes often works surprisingly well, despite the strong independence assumption.  It reduces variance, which can be beneficial.

## Comparison of Classification Methods: An Analytical Perspective

-   Logistic regression, LDA, QDA, and naive Bayes can all be expressed in terms of maximizing Pr(Y = k | X = x).
-   Equivalently, we can maximize the log-odds relative to a baseline class (K):

$$
\log\left(\frac{\Pr(Y = k|X = x)}{\Pr(Y = K|X = x)}\right)
$$

-   The *form* of this log-odds expression differs for each method.

| Method            | Log-Odds Form                                                                                                                                         |
| :---------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------- |
| Logistic Regression | Œ≤<sub>k0</sub> + Œ£<sub>j=1</sub><sup>p</sup> Œ≤<sub>kj</sub>x<sub>j</sub>  (linear)                                                                                                 |
| LDA               | a<sub>k</sub> + Œ£<sub>j=1</sub><sup>p</sup> b<sub>kj</sub>x<sub>j</sub> (linear)                                                                                                  |
| QDA               | a<sub>k</sub> + Œ£<sub>j=1</sub><sup>p</sup> b<sub>kj</sub>x<sub>j</sub> + Œ£<sub>j=1</sub><sup>p</sup> Œ£<sub>l=1</sub><sup>p</sup> c<sub>kj</sub><sub>l</sub>x<sub>j</sub>x<sub>l</sub> (quadratic)                        |
| Naive Bayes      | a<sub>k</sub> + Œ£<sub>j=1</sub><sup>p</sup> g<sub>kj</sub>(x<sub>j</sub>) (additive, where g<sub>kj</sub> is a function of x<sub>j</sub>)                                                          |

-   LDA is a special case of QDA.
-   Any classifier with a *linear* decision boundary is a special case of naive Bayes.
-   Logistic regression has the same *linear* form as LDA, but the coefficients are estimated differently.
- None of the method dominate others universally.

## A comparison of classification methodsÔºöAn empirical comparison

- Test error rates of different methods are compared through six different scenarios.

```{=html}
<style>
.half-width {
  width: 50%;
  float: left;
}
</style>
```
::: {layout-ncol=2}
::: half-width
![FIGURE 4.11](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_11.svg)
:::

::: half-width
![FIGURE 4.12](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_12.svg)
:::
:::

::: {.callout-note appearance="simple"}
- Six different scenarios are involved, scenarios 1-3 Bayes decision boundaries are linear, and scenarios 4-6 Bayes decision boundaries are non-linear.
- There were p=2 quantitative predictors in each of the six scenarios.
- For each scenario, 100 random training data sets were generated.
- Scenario 1 to Scenario 6 are described in the text material page 31 and page 32.
:::

- When the true decision boundaries are *linear*, LDA and logistic regression perform well.
- When boundaries are moderately non-linear, QDA or naive Bayes may be better.
- For *very* complex boundaries, a non-parametric method like KNN can be superior, *but* the level of smoothness must be chosen carefully.

## Generalized Linear Models (GLMs)

- So far, the response Y is either quantitative(use least squares linear regression to predict) or qualitative(use classification methods).
- But Y is neither qualitative nor quantitative is also possible, like the number of hourly users of a bike sharing program.
- To predict the number of bike users, the least squares linear regression model shown defects(negative fitted values, variance increasing when mean number increasing, response not continuous-valued)
- Poisson regression could provide a much more natural and elegant approach.

## Poisson Regression

- Suppose the random variable Y takes on nonnegative integer values and follows the Poisson distribution. The probability could be written as:

$$
\Pr(Y = k) = \frac{e^{-\lambda}\lambda^k}{k!}  \quad \text{for } k = 0, 1, 2, \dots
$$

::: {.callout-note appearance="simple"}
- k! means `k factorial`, Œª > 0 is the expected value of Y, which also equals the variance of Y.
- Larger the mean of Y, larger the variance of Y.
:::

- Poisson distribution is used to model counts.
- Number of bike users could be modeled as Poisson distribution with mean value. The mean could vary as a function of the covariates.

## Poisson Regression Model

$$
\log(\lambda(X_1, \dots, X_p)) = \beta_0 + \beta_1X_1 + \dots + \beta_pX_p
$$
or equivalently

$$
\lambda(X_1, \dots, X_p) = e^{\beta_0 + \beta_1X_1 + \dots + \beta_pX_p}
$$

- The log of Œª is linear in predictors.
- Œª takes on nonnegative values for all values of covariates.
-  We use maximum likelihood to estimate parameters.

## Comparing Poisson Regression Model to the linear regression model

- Interpretation of the coefficients
- Mean-variance relationship
- Nonnegative fitted values
- Please read text material for details.

## Generalized Linear Models in Greater Generality

- Three types of regression models have been discussed: linear, logistic and Poisson. They share some common characteristics:
    1.  Predictors are used to predict response variable.
        Conditional on predictors, Y belongs to a certain family of distributions.
        We make different assumption on the family of distributions that Y belongs to, correspondingly, we have different regression models.
    2.  Modeling the mean of Y.
        Use different *link function* Œ∑, these regression models could be expressed as below:

$$
\eta(E(Y|X_1, \dots, X_p)) = \beta_0 + \beta_1X_1 + \dots + \beta_pX_p
$$

-  All these regression models are examples of **Generalized Linear Model(GLM)**

## Summary

-   Classification predicts *qualitative* responses.
-   Logistic regression, LDA, QDA, and naive Bayes are common classification methods.
-   Logistic regression models the *probability* of class membership using the logistic function.
-   LDA and QDA assume the predictors follow a Gaussian distribution within each class.  LDA assumes a *common* covariance matrix; QDA does not.
-   Naive Bayes assumes *independence* of predictors within each class.
-   The choice of method depends on the data and the bias-variance trade-off.
-   ROC curves are useful for evaluating classifier performance across a range of thresholds.
-   Generalized linear models handle responses from non-normal distributions, Poisson regression model is an example.

## Thoughts and Discussion ü§î

::: incremental
-   Can you think of real-world scenarios where each classification method (logistic regression, LDA, QDA, naive Bayes) might be most appropriate?
-   How would you choose the "best" classification method for a given dataset? What metrics would you consider?
-   What are the limitations of each method? When might they fail?
-   How does the bias-variance trade-off play a role in choosing a classification method?
- Can we apply the knowledge of mean-variance relationship and fitted values to choose suitable regression model?
:::
