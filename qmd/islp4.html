<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Statistical Learning: Classification</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<link href="../https://assets.qiufei.site/personal/profile.jpg" rel="icon" type="image/jpeg">
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-8ea72dc5fed832574809a9c94082fbbb.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-681fbf911679f9b3dbf9743eb275ba49.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-7e49aeac8059a213a463aa1a739e8272.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://qiufei.github.io"> 
<span class="menu-text">È¶ñÈ°µ</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://qiufei.github.io/web-slide-r"> 
<span class="menu-text">RËØæ‰ª∂</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://qiufei.github.io/web-slide-marketing"> 
<span class="menu-text">Ëê•ÈîÄËØæ‰ª∂</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Statistical Learning: Classification</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="welcome-to-classification" class="level2">
<h2 class="anchored" data-anchor-id="welcome-to-classification">Welcome to Classification! üëã</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<p>So far, we‚Äôve journeyed through the world of <strong>regression</strong>, where we predicted <em>numbers</em>. Now, we‚Äôre switching gears to <strong>classification</strong>, where we predict <em>categories</em>. Think of it like sorting objects into different boxes! üì¶</p>
</div>
</div>
</div>
</section>
<section id="regression-vs.-classification-the-big-picture" class="level2">
<h2 class="anchored" data-anchor-id="regression-vs.-classification-the-big-picture">Regression vs.&nbsp;Classification: The Big Picture</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Regression:</strong> Predicting a <em>quantitative</em> (numerical) response.</p>
</div>
</div>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<ul>
<li>Examples:
<ul>
<li>Predicting house prices üè†</li>
<li>Estimating sales revenue üí∞</li>
<li>Forecasting stock prices üìà</li>
</ul></li>
</ul>
</div>
</div>
</div>
</section>
<section id="regression-vs.-classification-the-big-picture-continued" class="level2">
<h2 class="anchored" data-anchor-id="regression-vs.-classification-the-big-picture-continued">Regression vs.&nbsp;Classification: The Big Picture (Continued)</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Classification:</strong> Predicting a <em>qualitative</em> (categorical) response.</p>
</div>
</div>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<ul>
<li>Examples:
<ul>
<li>Diagnosing a disease (present or absent) ‚öïÔ∏è</li>
<li>Filtering spam emails (spam or not spam) üìß</li>
<li>Detecting fraudulent transactions (fraud or not fraud) üí≥</li>
</ul></li>
</ul>
</div>
</div>
</div>
</section>
<section id="what-is-classification" class="level2">
<h2 class="anchored" data-anchor-id="what-is-classification">What is Classification?</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<ul>
<li><strong>Classification</strong> is like sorting objects into distinct groups based on their features.</li>
<li>We assign each observation to a specific category or class.</li>
</ul>
</div>
</div>
<div class="quarto-layout-row">
<div class="cell quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
    A[Observation 1] --&gt; C(Class A)
    B[Observation 2] --&gt; D(Class B)
    C --&gt; E[Classification]
    D --&gt; E
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="classification-probabilities-first" class="level2">
<h2 class="anchored" data-anchor-id="classification-probabilities-first">Classification: Probabilities First</h2>
<ul>
<li>Many classification methods start by predicting the <em>probability</em> of an observation belonging to each category.</li>
<li>Then, based on these probabilities (and a threshold), we make the final classification.</li>
</ul>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<ul>
<li><strong>Example:</strong>
<ul>
<li>An email: 70% chance of being spam, 30% chance of not being spam.</li>
<li>With a 50% threshold, we‚Äôd classify it as spam! üìß</li>
</ul></li>
</ul>
</div>
</div>
</div>
</section>
<section id="classification-vs.-regression-a-clear-comparison" class="level2">
<h2 class="anchored" data-anchor-id="classification-vs.-regression-a-clear-comparison">Classification vs.&nbsp;Regression: A Clear Comparison</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 40%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Regression</th>
<th style="text-align: left;">Classification</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Response Variable</td>
<td style="text-align: left;">Quantitative (numerical)</td>
<td style="text-align: left;">Qualitative (categorical)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Goal</td>
<td style="text-align: left;">Predict a <em>number</em></td>
<td style="text-align: left;">Predict a <em>category</em></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Example</td>
<td style="text-align: left;">Predict house price</td>
<td style="text-align: left;">Predict disease presence (yes/no)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Output</td>
<td style="text-align: left;">Continuous value</td>
<td style="text-align: left;">Discrete category</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
<section id="why-not-linear-regression-for-categories" class="level2">
<h2 class="anchored" data-anchor-id="why-not-linear-regression-for-categories">Why NOT Linear Regression for Categories? ü§î</h2>
<ul>
<li><p>Linear regression is designed for <em>numbers</em>. What happens if we try to use it for categories? Let‚Äôs consider predicting a medical diagnosis with three possibilities:</p>
<ol type="1">
<li>Stroke</li>
<li>Drug overdose</li>
<li>Epileptic seizure</li>
</ol></li>
</ul>
</section>
<section id="a-problematic-coding-scheme" class="level2">
<h2 class="anchored" data-anchor-id="a-problematic-coding-scheme">A Problematic Coding Scheme ü§ï</h2>
<ul>
<li><p>We <em>could</em> try to code these numerically:</p>
<ul>
<li>1 = Stroke</li>
<li>2 = Drug overdose</li>
<li>3 = Epileptic seizure</li>
</ul></li>
<li><p>But this approach introduces serious problems!</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2Fcoding.png" class="img-fluid figure-img"></p>
<figcaption>Medical conditions coding</figcaption>
</figure>
</div>
</section>
<section id="the-problems-with-numerical-coding" class="level2">
<h2 class="anchored" data-anchor-id="the-problems-with-numerical-coding">The Problems with Numerical Coding ‚ùå</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<ul>
<li><p><strong>Problem 1: Arbitrary Order:</strong> The coding implies an order (stroke &lt; drug overdose &lt; epileptic seizure) that is medically <em>meaningless</em>.</p></li>
<li><p><strong>Problem 2: Equal Differences:</strong> It suggests equal differences between conditions (stroke to drug overdose = drug overdose to seizure), which is also <em>not</em> medically valid.</p></li>
<li><p><strong>Problem 3: Inconsistent Results:</strong> Different coding schemes (e.g., 1=drug overdose, 2=seizure, 3=stroke) would lead to completely <em>different</em> models and predictions!</p></li>
</ul>
</div>
</div>
</div>
</section>
<section id="binary-responses-a-special-case" class="level2">
<h2 class="anchored" data-anchor-id="binary-responses-a-special-case">Binary Responses: A Special Case?</h2>
<ul>
<li><p><strong>Binary Responses (Two Categories):</strong> If we have only <em>two</em> categories (yes/no, true/false), we <em>can</em> code them as 0 and 1. Linear regression <em>can</em> be used in this case.</p></li>
<li><p><strong>However:</strong></p>
<ul>
<li>Linear regression might predict probabilities <em>outside</em> the [0, 1] range. This is nonsensical.</li>
<li>Better methods exist that are specifically designed for binary classification (like logistic regression!).</li>
</ul></li>
</ul>
</section>
<section id="example-the-default-data" class="level2">
<h2 class="anchored" data-anchor-id="example-the-default-data">Example: The Default Data üí≥</h2>
<ul>
<li>We‚Äôll use a simulated dataset called ‚ÄúDefault‚Äù to explore classification.</li>
</ul>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<ul>
<li><strong>Goal:</strong> Predict whether an individual will <em>default</em> on their credit card payment.</li>
<li><strong>Response:</strong> <code>default</code> (Yes/No) ‚Äì a binary, qualitative variable.</li>
<li><strong>Predictors:</strong>
<ul>
<li><code>balance</code>: Credit card balance (quantitative).</li>
<li><code>income</code>: Annual income (quantitative).</li>
</ul></li>
</ul>
</div>
</div>
</div>
</section>
<section id="visualizing-the-default-data" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-the-default-data">Visualizing the Default Data</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_1.svg" class="img-fluid figure-img" width="800"></p>
<figcaption>The Default data set</figcaption>
</figure>
</div>
</section>
<section id="default-data-left-panel---scatterplot" class="level2">
<h2 class="anchored" data-anchor-id="default-data-left-panel---scatterplot">Default Data: Left Panel - Scatterplot</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_1.svg" class="img-fluid figure-img" width="800"></p>
<figcaption>The Default data set</figcaption>
</figure>
</div>
</div>
</div>
</div>
<ul>
<li><strong>Left Panel (Scatterplot):</strong>
<ul>
<li><code>income</code> (x-axis) vs.&nbsp;<code>balance</code> (y-axis).</li>
<li>Orange: Individuals who defaulted.</li>
<li>Blue: Individuals who did not default.</li>
<li>We see <em>some</em> separation: defaulters tend to have higher balances, but there‚Äôs overlap.</li>
</ul></li>
</ul>
</section>
<section id="default-data-center-panel---balance-boxplots" class="level2">
<h2 class="anchored" data-anchor-id="default-data-center-panel---balance-boxplots">Default Data: Center Panel - Balance Boxplots</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_1.svg" class="img-fluid figure-img" width="800"></p>
<figcaption>The Default data set</figcaption>
</figure>
</div>
</div>
</div>
</div>
<ul>
<li><strong>Center Panel (Boxplots of Balance):</strong>
<ul>
<li>Compares <code>balance</code> distribution for defaulters (orange) and non-defaulters (blue).</li>
<li>The orange boxplot is shifted <em>higher</em>, indicating higher balances for defaulters on average.</li>
<li>Boxes show the interquartile range (IQR), and whiskers show the range (excluding outliers).</li>
</ul></li>
</ul>
</section>
<section id="default-data-right-panel---income-boxplots" class="level2">
<h2 class="anchored" data-anchor-id="default-data-right-panel---income-boxplots">Default Data: Right Panel - Income Boxplots</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_1.svg" class="img-fluid figure-img" width="800"></p>
<figcaption>The Default data set</figcaption>
</figure>
</div>
</div>
</div>
</div>
<ul>
<li><strong>Right Panel (Boxplots of Income):</strong>
<ul>
<li>Compares <code>income</code> distribution for defaulters (orange) and non-defaulters (blue).</li>
<li>The relationship is <em>much less clear</em> than with <code>balance</code>. The boxplots largely overlap.</li>
</ul></li>
</ul>
</section>
<section id="default-data-key-takeaway" class="level2">
<h2 class="anchored" data-anchor-id="default-data-key-takeaway">Default Data: Key Takeaway</h2>
<ul>
<li>The visualizations suggest that <code>balance</code> is a <em>stronger predictor</em> of <code>default</code> than <code>income</code>. Higher balances seem associated with a higher probability of default. This guides our model building.</li>
</ul>
</section>
<section id="logistic-regression-modeling-the-probability" class="level2">
<h2 class="anchored" data-anchor-id="logistic-regression-modeling-the-probability">Logistic Regression: Modeling the <em>Probability</em></h2>
<ul>
<li>Logistic regression doesn‚Äôt directly model the <em>response</em> (<code>default</code>). Instead, it models the <em>probability</em> that the response belongs to a specific category.</li>
</ul>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<ul>
<li><strong>Specifically:</strong> We model Pr(default = Yes | balance, income) ‚Äì the probability of defaulting, <em>given</em> balance and income.</li>
<li>This probability will always be between 0 and 1, which makes perfect sense! üëç</li>
</ul>
</div>
</div>
</div>
</section>
<section id="logistic-regression-from-probability-to-classification" class="level2">
<h2 class="anchored" data-anchor-id="logistic-regression-from-probability-to-classification">Logistic Regression: From Probability to Classification</h2>
<ul>
<li>Once we have the estimated probability of default, we can <em>classify</em>.</li>
</ul>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<ul>
<li><strong>Example:</strong>
<ul>
<li>If Pr(default = Yes | balance, income) &gt; 0.5, predict ‚Äúdefault = Yes‚Äù.</li>
<li>If Pr(default = Yes | balance, income) ‚â§ 0.5, predict ‚Äúdefault = No‚Äù.</li>
</ul></li>
<li>The 0.5 threshold is common, but it‚Äôs adjustable.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="the-logistic-function-bending-between-0-and-1" class="level2">
<h2 class="anchored" data-anchor-id="the-logistic-function-bending-between-0-and-1">The Logistic Function: Bending Between 0 and 1</h2>
<ul>
<li>We need a function that always outputs a value between 0 and 1, no matter the input. The <strong>logistic function</strong> is perfect for this:</li>
</ul>
<p><span class="math display">\[
p(X) = \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}}
\]</span></p>
</section>
<section id="logistic-function-breaking-it-down" class="level2">
<h2 class="anchored" data-anchor-id="logistic-function-breaking-it-down">Logistic Function: Breaking it Down</h2>
<ul>
<li><p><em>p(X)</em>: Probability of the event (e.g., default) given predictor(s) <em>X</em>.</p></li>
<li><p>Œ≤‚ÇÄ and Œ≤‚ÇÅ: Coefficients estimated from the data. They control the curve‚Äôs shape and position.</p></li>
<li><p><em>e</em>: The base of the natural logarithm (‚âà 2.718).</p></li>
<li><p><strong>Crucial Feature:</strong> No matter the value of Œ≤‚ÇÄ + Œ≤‚ÇÅX, the output <em>p(X)</em> is <em>always</em> between 0 and 1.</p></li>
</ul>
</section>
<section id="the-logistic-function-an-s-shaped-curve" class="level2">
<h2 class="anchored" data-anchor-id="the-logistic-function-an-s-shaped-curve">The Logistic Function: An S-Shaped Curve</h2>
<ul>
<li>The logistic function creates an S-shaped curve (a sigmoid).</li>
</ul>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
    A[Low X] --&gt; B(Low p(X) ~ 0)
    B --&gt; C{Increasing X}
    C --&gt; D(Increasing p(X))
    D --&gt; E[High X]
    E --&gt; F(High p(X) ~ 1)
    style C fill:#f9f,stroke:#333,stroke-width:2px
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<ul>
<li>As X increases, p(X) smoothly transitions from near 0 to near 1.</li>
</ul>
</section>
<section id="linear-vs.-logistic-a-visual-comparison" class="level2">
<h2 class="anchored" data-anchor-id="linear-vs.-logistic-a-visual-comparison">Linear vs.&nbsp;Logistic: A Visual Comparison</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_2.svg" class="img-fluid figure-img" width="800"></p>
<figcaption>Classification using the Default data</figcaption>
</figure>
</div>
</section>
<section id="linear-regression-the-problem" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression-the-problem">Linear Regression: The Problem</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_2.svg" class="img-fluid figure-img" width="800"></p>
<figcaption>Classification using the Default data</figcaption>
</figure>
</div>
</div>
</div>
</div>
<ul>
<li><strong>Left Panel (Linear Regression):</strong>
<ul>
<li>Blue line: Fit of a linear regression to the <code>default</code> data (0/1 coded).</li>
<li><em>Major Issue:</em> For low/high <code>balance</code>, predicted probabilities are <em>outside</em> [0, 1]! Probabilities can‚Äôt be negative or greater than 1. üôÅ</li>
</ul></li>
</ul>
</section>
<section id="logistic-regression-the-solution" class="level2">
<h2 class="anchored" data-anchor-id="logistic-regression-the-solution">Logistic Regression: The Solution</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_2.svg" class="img-fluid figure-img" width="800"></p>
<figcaption>Classification using the Default data</figcaption>
</figure>
</div>
</div>
</div>
</div>
<ul>
<li><strong>Right Panel (Logistic Regression):</strong>
<ul>
<li>Orange curve: Fit of a logistic regression.</li>
<li>Predicted probabilities are <em>always</em> between 0 and 1, as they should be! üôÇ</li>
<li>The characteristic S-shape.</li>
</ul></li>
</ul>
</section>
<section id="linear-vs.-logistic-the-verdict" class="level2">
<h2 class="anchored" data-anchor-id="linear-vs.-logistic-the-verdict">Linear vs.&nbsp;Logistic: The Verdict</h2>
<ul>
<li>Logistic regression is <em>far</em> more appropriate for modeling probabilities and binary outcomes. It respects the fundamental constraint that probabilities must be between 0 and 1.</li>
</ul>
</section>
<section id="odds-another-way-to-think-about-probability" class="level2">
<h2 class="anchored" data-anchor-id="odds-another-way-to-think-about-probability">Odds: Another Way to Think About Probability</h2>
<ul>
<li>We can rewrite the logistic model to highlight its connection to <em>odds</em>:</li>
</ul>
<p><span class="math display">\[
\frac{p(X)}{1 - p(X)} = e^{\beta_0 + \beta_1X}
\]</span></p>
<ul>
<li><p><strong>Left-hand side:</strong> The <em>odds</em> of the event (e.g., default).</p>
<ul>
<li>Odds range from 0 to ‚àû.</li>
<li>Odds = 1: Event is equally likely to happen or not.</li>
<li>Odds &gt; 1: Event is <em>more</em> likely to happen.</li>
<li>Odds &lt; 1: Event is <em>less</em> likely to happen.</li>
</ul></li>
<li><p><strong>Relationship:</strong> Odds = p / (1 - p)</p></li>
<li><p><strong>Example:</strong> If p(X) = 0.8, odds are 0.8 / (1 - 0.8) = 4. The event is four times more likely to happen than not.</p></li>
</ul>
</section>
<section id="log-odds-logit-the-linear-connection" class="level2">
<h2 class="anchored" data-anchor-id="log-odds-logit-the-linear-connection">Log-Odds (Logit): The Linear Connection</h2>
<ul>
<li>Taking the <em>logarithm</em> of both sides of the odds equation:</li>
</ul>
<p><span class="math display">\[
\log\left(\frac{p(X)}{1 - p(X)}\right) = \beta_0 + \beta_1X
\]</span></p>
<ul>
<li><p><strong>Left-hand side:</strong> The <em>log-odds</em> or <em>logit</em>.</p></li>
<li><p><strong>Right-hand side:</strong> A <em>linear</em> function of X.</p></li>
<li><p><strong>Key Insight:</strong> Logistic regression models the <em>log-odds</em> as a linear function of the predictors. This explains the ‚Äúlogistic‚Äù in its name, even though it‚Äôs for classification!</p></li>
</ul>
</section>
<section id="interpreting-logistic-regression-coefficients" class="level2">
<h2 class="anchored" data-anchor-id="interpreting-logistic-regression-coefficients">Interpreting Logistic Regression Coefficients</h2>
<ul>
<li><p><strong>Linear Regression:</strong> Œ≤‚ÇÅ is the <em>average change in Y</em> for a one-unit increase in X.</p></li>
<li><p><strong>Logistic Regression:</strong> The interpretation is different because we‚Äôre modeling the log-odds.</p></li>
</ul>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<ul>
<li><p>Œ≤‚ÇÅ is the <em>change in the log-odds</em> for a one-unit increase in X.</p></li>
<li><p>Equivalently, a one-unit increase in X <em>multiplies the odds</em> by e<sup>Œ≤‚ÇÅ</sup>.</p></li>
<li><p>Because of the non-linear S-curve, the <em>amount</em> p(X) changes for a one-unit increase in X depends on the <em>current value</em> of X. The change is not probability.</p></li>
</ul>
</div>
</div>
</div>
</section>
<section id="estimating-coefficients-maximum-likelihood-mle" class="level2">
<h2 class="anchored" data-anchor-id="estimating-coefficients-maximum-likelihood-mle">Estimating Coefficients: Maximum Likelihood (MLE)</h2>
<ul>
<li><p>We use <strong>maximum likelihood estimation (MLE)</strong> to find the best estimates for Œ≤‚ÇÄ, Œ≤‚ÇÅ, etc.</p></li>
<li><p><strong>Goal of MLE:</strong> Find the coefficient values that make the <em>observed data</em> most likely. We want to maximize the probability of observing the actual outcomes (defaults/non-defaults) in our training data, <em>given</em> the coefficients.</p></li>
</ul>
</section>
<section id="the-likelihood-function-making-data-likely" class="level2">
<h2 class="anchored" data-anchor-id="the-likelihood-function-making-data-likely">The Likelihood Function: Making Data Likely</h2>
<ul>
<li>The <strong>likelihood function</strong> for logistic regression is:</li>
</ul>
<p><span class="math display">\[
l(\beta_0, \beta_1) = \prod_{i:y_i=1} p(x_i) \prod_{i':y_{i'}=0} (1 - p(x_{i'}))
\]</span></p>
</section>
<section id="likelihood-function-demystified" class="level2">
<h2 class="anchored" data-anchor-id="likelihood-function-demystified">Likelihood Function: Demystified</h2>
<ul>
<li><p>This formula might look complex, but the core idea is simple.</p></li>
<li><p>We multiply the probabilities of observing each data point, <em>given</em> coefficients Œ≤‚ÇÄ and Œ≤‚ÇÅ.</p></li>
<li><p><strong>First Product:</strong> Œ†<sub>i:y·µ¢=1</sub> p(x·µ¢)</p>
<ul>
<li>Considers individuals who <em>did</em> default (y·µ¢ = 1).</li>
<li>For each, p(x·µ¢) is the probability of default, given their predictors (x·µ¢) and the current coefficients.</li>
</ul></li>
<li><p><strong>Second Product:</strong> Œ†<sub>i‚Äô:y·µ¢‚Äô=0</sub> (1 - p(x·µ¢‚Äô))</p>
<ul>
<li>Considers individuals who <em>did not</em> default (y·µ¢‚Äô = 0).</li>
<li>For each, 1 - p(x·µ¢‚Äô) is the probability of <em>not</em> defaulting.</li>
</ul></li>
<li><p><strong>Overall:</strong> Multiplying these probabilities gives the likelihood of the entire dataset, given the coefficients. MLE finds the coefficients that maximize this product.</p></li>
</ul>
</section>
<section id="optimization-letting-the-computer-do-the-work" class="level2">
<h2 class="anchored" data-anchor-id="optimization-letting-the-computer-do-the-work">Optimization: Letting the Computer Do the Work</h2>
<ul>
<li><p>We don‚Äôt maximize the likelihood function by hand! Statistical software (like R) has built-in optimization algorithms.</p></li>
<li><p>These algorithms find the values of Œ≤‚ÇÄ, Œ≤‚ÇÅ, etc., that maximize the likelihood, giving us the <em>maximum likelihood estimates</em>.</p></li>
</ul>
</section>
<section id="making-predictions-from-coefficients-to-probabilities" class="level2">
<h2 class="anchored" data-anchor-id="making-predictions-from-coefficients-to-probabilities">Making Predictions: From Coefficients to Probabilities</h2>
<ul>
<li><p>With the estimated coefficients (Œ≤ÃÇ‚ÇÄ, Œ≤ÃÇ‚ÇÅ, etc.), we can predict the probability of default for <em>any</em> given predictor values.</p></li>
<li><p>Plug the values into the logistic function:</p></li>
</ul>
<p>p(X) = e<sup>(Œ≤ÃÇ‚ÇÄ + Œ≤ÃÇ‚ÇÅX)</sup> / (1 + e<sup>(Œ≤ÃÇ‚ÇÄ + Œ≤ÃÇ‚ÇÅX)</sup>)</p>
</section>
<section id="prediction-example-putting-it-into-practice" class="level2">
<h2 class="anchored" data-anchor-id="prediction-example-putting-it-into-practice">Prediction Example: Putting it into Practice</h2>
<ul>
<li><p>Suppose we get these estimated coefficients (illustrative values):</p>
<ul>
<li>Œ≤ÃÇ‚ÇÄ = -2</li>
<li>Œ≤ÃÇ‚ÇÅ = 0.005 (where X is <code>balance</code>)</li>
</ul></li>
<li><p>Predict default probability for two individuals:</p>
<ol type="1">
<li>Individual A: <code>balance</code> = $1,000</li>
<li>Individual B: <code>balance</code> = $2,000</li>
</ol></li>
<li><p><strong>Calculations:</strong></p>
<ul>
<li>Individual A: p(X) = e<sup>(-2 + 0.005 * 1000)</sup> / (1 + e<sup>(-2 + 0.005 * 1000)</sup>) ‚âà 0.95</li>
<li>Individual B: p(X) = e<sup>(-2 + 0.005 * 2000)</sup> / (1 + e<sup>(-2 + 0.005 * 2000)</sup>) ‚âà 1</li>
</ul></li>
<li><p><strong>Predictions:</strong></p>
<ul>
<li>Individual A: Estimated probability of default ‚âà 95%.</li>
<li>Individual B: Estimated probability of default ‚âà 100%.</li>
</ul></li>
</ul>
</section>
<section id="from-probabilities-to-classes-the-threshold" class="level2">
<h2 class="anchored" data-anchor-id="from-probabilities-to-classes-the-threshold">From Probabilities to Classes: The Threshold</h2>
<ul>
<li><p>After calculating probabilities, we <em>classify</em> using a threshold.</p></li>
<li><p><strong>Common Threshold:</strong> 0.5</p></li>
<li><p><strong>Classification Rule:</strong></p>
<ul>
<li>If p(X) &gt; 0.5, predict ‚Äúdefault = Yes‚Äù.</li>
<li>If p(X) ‚â§ 0.5, predict ‚Äúdefault = No‚Äù.</li>
</ul></li>
<li><p><strong>In our example:</strong></p>
<ul>
<li>Individual A: Predict ‚Äúdefault = Yes‚Äù (0.95 &gt; 0.5).</li>
<li>Individual B: Predict ‚Äúdefault = Yes‚Äù (1 &gt; 0.5).</li>
</ul></li>
</ul>
</section>
<section id="multiple-logistic-regression-more-predictors" class="level2">
<h2 class="anchored" data-anchor-id="multiple-logistic-regression-more-predictors">Multiple Logistic Regression: More Predictors!</h2>
<ul>
<li><p>Like linear regression, we can include <em>multiple</em> predictors.</p></li>
<li><p><strong>The Model:</strong></p></li>
</ul>
<p><span class="math display">\[
\log\left(\frac{p(X)}{1 - p(X)}\right) = \beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p
\]</span></p>
<p><span class="math display">\[
p(X) = \frac{e^{\beta_0 + \beta_1X_1 + \dots + \beta_pX_p}}{1 + e^{\beta_0 + \beta_1X_1 + \dots + \beta_pX_p}}
\]</span></p>
</section>
<section id="multiple-logistic-regression-explained" class="level2">
<h2 class="anchored" data-anchor-id="multiple-logistic-regression-explained">Multiple Logistic Regression: Explained</h2>
<ul>
<li><p>X‚ÇÅ, X‚ÇÇ, ‚Ä¶, X<sub>p</sub>: The <em>p</em> predictors.</p></li>
<li><p>Œ≤‚ÇÄ, Œ≤‚ÇÅ, Œ≤‚ÇÇ, ‚Ä¶, Œ≤<sub>p</sub>: Coefficients estimated from the data.</p></li>
<li><p>The log-odds are now a linear function of <em>all</em> predictors.</p></li>
<li><p>We still use MLE to estimate coefficients.</p></li>
<li><p><strong>Interpretation:</strong></p>
<ul>
<li>Œ≤<sub>j</sub>: Change in log-odds for a one-unit increase in X<sub>j</sub>, <em>holding all other predictors constant</em>.</li>
<li>Equivalently, a one-unit increase in X<sub>j</sub> multiplies the odds by e<sup>Œ≤<sub>j</sub></sup>, holding others constant.</li>
</ul></li>
</ul>
</section>
<section id="example-multiple-logistic-regression-on-default" class="level2">
<h2 class="anchored" data-anchor-id="example-multiple-logistic-regression-on-default">Example: Multiple Logistic Regression on Default</h2>
<ul>
<li>Model: <code>default</code> ~ <code>balance</code> + <code>income</code> + <code>student</code></li>
<li><code>student</code> is a <em>dummy variable</em>:
<ul>
<li><code>student</code> = 1 if student, 0 if not.</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 20%">
<col style="width: 15%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">Coefficient</th>
<th style="text-align: left;">Std. error</th>
<th style="text-align: left;">z-statistic</th>
<th style="text-align: left;">p-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Intercept</td>
<td style="text-align: left;">-10.8690</td>
<td style="text-align: left;">0.4923</td>
<td style="text-align: left;">-22.08</td>
<td style="text-align: left;">&lt; 0.0001</td>
</tr>
<tr class="even">
<td style="text-align: left;">balance</td>
<td style="text-align: left;">0.0057</td>
<td style="text-align: left;">0.0002</td>
<td style="text-align: left;">24.74</td>
<td style="text-align: left;">&lt; 0.0001</td>
</tr>
<tr class="odd">
<td style="text-align: left;">income</td>
<td style="text-align: left;">0.0030</td>
<td style="text-align: left;">0.0082</td>
<td style="text-align: left;">0.37</td>
<td style="text-align: left;">0.7115</td>
</tr>
<tr class="even">
<td style="text-align: left;">student[Yes]</td>
<td style="text-align: left;">-0.6468</td>
<td style="text-align: left;">0.2362</td>
<td style="text-align: left;">-2.74</td>
<td style="text-align: left;">0.0062</td>
</tr>
</tbody>
</table>
</section>
<section id="interpreting-the-coefficients-a-deep-dive" class="level2">
<h2 class="anchored" data-anchor-id="interpreting-the-coefficients-a-deep-dive">Interpreting the Coefficients: A Deep Dive</h2>
<ul>
<li><p><strong>Intercept (Œ≤‚ÇÄ = -10.8690):</strong> Log-odds of default for a non-student with <code>balance</code> = 0 and <code>income</code> = 0. Not directly interpretable.</p></li>
<li><p><strong>balance (Œ≤‚ÇÅ = 0.0057):</strong> For each $1 increase in <code>balance</code>, the log-odds of default increase by 0.0057, <em>holding income and student status constant</em>. Equivalently, odds multiply by e<sup>0.0057</sup> ‚âà 1.0057.</p></li>
<li><p><strong>income (Œ≤‚ÇÇ = 0.0030):</strong> For each $1 increase in <code>income</code>, the log-odds of default <em>increase</em> by 0.003, holding other variables constant. It‚Äôs not significant.</p></li>
<li><p><strong>student[Yes] (Œ≤‚ÇÉ = -0.6468):</strong> Being a student <em>decreases</em> the log-odds of default by 0.6468, holding <code>balance</code> and <code>income</code> constant. Odds multiply by e<sup>-0.6468</sup> ‚âà 0.52. <em>Surprising!</em> We might expect the opposite, but this is <em>controlling for balance and income</em>.</p></li>
</ul>
</section>
<section id="confounding-unmasking-hidden-relationships" class="level2">
<h2 class="anchored" data-anchor-id="confounding-unmasking-hidden-relationships">Confounding: Unmasking Hidden Relationships</h2>
<ul>
<li>The surprising student result is due to <strong>confounding</strong>.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_3.svg" class="img-fluid figure-img"></p>
<figcaption>Confounding in the Default data.</figcaption>
</figure>
</div>
</section>
<section id="confounding-left-panel---default-rates" class="level2">
<h2 class="anchored" data-anchor-id="confounding-left-panel---default-rates">Confounding: Left Panel - Default Rates</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_3.svg" class="img-fluid figure-img"></p>
<figcaption>Confounding in the Default data.</figcaption>
</figure>
</div>
</div>
</div>
</div>
<ul>
<li><strong>Left Panel (Default Rates vs.&nbsp;Balance):</strong>
<ul>
<li>Orange: Students.</li>
<li>Blue: Non-students.</li>
<li>Solid: Default rate at <em>each</em> balance level.</li>
<li>Dashed: <em>Overall</em> (average) default rate.</li>
</ul></li>
<li><strong>Key:</strong> Students have <em>higher overall</em> default rates (dashed), but <em>lower</em> rates at <em>each balance level</em> (solid).</li>
</ul>
</section>
<section id="confounding-right-panel---balance-distribution" class="level2">
<h2 class="anchored" data-anchor-id="confounding-right-panel---balance-distribution">Confounding: Right Panel - Balance Distribution</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_3.svg" class="img-fluid figure-img"></p>
<figcaption>Confounding in the Default data.</figcaption>
</figure>
</div>
</div>
</div>
</div>
<ul>
<li><strong>Right Panel (Boxplots of Balance):</strong>
<ul>
<li>Compares <code>balance</code> distribution for students (orange) and non-students (blue).</li>
<li>Students tend to have <em>higher</em> credit card balances.</li>
</ul></li>
</ul>
</section>
<section id="confounding-the-full-story" class="level2">
<h2 class="anchored" data-anchor-id="confounding-the-full-story">Confounding: The Full Story</h2>
<ol type="1">
<li><strong>Students have higher balances.</strong> (Right panel boxplots)</li>
<li><strong>Higher balances lead to higher default rates.</strong> (Our initial finding)</li>
<li><strong>Thus, students have higher <em>overall</em> default rates due to higher balances.</strong> (Dashed lines in left panel)</li>
<li><strong>But, <em>controlling for balance</em>, students have <em>lower</em> default rates.</strong> (Solid lines in left panel, and the multiple logistic regression coefficient)</li>
</ol>
<ul>
<li><strong>Confounding:</strong> The effect of <code>student</code> status is mixed up with <code>balance</code>. Without controlling for <code>balance</code>, we get a misleading impression.</li>
</ul>
</section>
<section id="multinomial-logistic-regression-beyond-two-categories" class="level2">
<h2 class="anchored" data-anchor-id="multinomial-logistic-regression-beyond-two-categories">Multinomial Logistic Regression: Beyond Two Categories</h2>
<ul>
<li><p>What if we have <em>more than two</em> response categories?</p></li>
<li><p><strong>Example:</strong> Medical diagnosis:</p>
<ol type="1">
<li>Stroke</li>
<li>Drug overdose</li>
<li>Epileptic seizure</li>
</ol></li>
<li><p><strong>Multinomial logistic regression</strong> extends logistic regression to handle this.</p></li>
</ul>
</section>
<section id="multinomial-logistic-regression-the-core-concept" class="level2">
<h2 class="anchored" data-anchor-id="multinomial-logistic-regression-the-core-concept">Multinomial Logistic Regression: The Core Concept</h2>
<ul>
<li><p>We choose one category as a <em>baseline</em> (e.g., ‚Äústroke‚Äù).</p></li>
<li><p>We model the log-odds of <em>each other category relative to the baseline</em>.</p></li>
<li><p><strong>Model:</strong></p></li>
</ul>
<p><span class="math display">\[
\log\left(\frac{\Pr(Y = k|X = x)}{\Pr(Y = K|X = x)}\right) = \beta_{k0} + \beta_{k1}x_1 + \dots + \beta_{kp}x_p
\]</span></p>
</section>
<section id="multinomial-logistic-regression-details" class="level2">
<h2 class="anchored" data-anchor-id="multinomial-logistic-regression-details">Multinomial Logistic Regression: Details</h2>
<ul>
<li><p><em>k</em>: Index for categories (k = 1, 2, ‚Ä¶, K-1). We have K-1 equations.</p></li>
<li><p><em>K</em>: Baseline category.</p></li>
<li><p>Œ≤<sub>k0</sub>, Œ≤<sub>k1</sub>, ‚Ä¶, Œ≤<sub>kp</sub>: Coefficients for category <em>k</em> (relative to baseline). K-1 sets of these.</p></li>
<li><p>X = (x‚ÇÅ, x‚ÇÇ, ‚Ä¶, x<sub>p</sub>): Predictors.</p></li>
<li><p>This models the log-odds of being in category <em>k</em> <em>compared to</em> the baseline <em>K</em>, as a linear function of the predictors.</p></li>
<li><p><strong>Key:</strong> The baseline choice is arbitrary. <em>Predicted probabilities</em> are the same regardless.</p></li>
</ul>
</section>
<section id="multinomial-logistic-regression-softmax-coding-alternative" class="level2">
<h2 class="anchored" data-anchor-id="multinomial-logistic-regression-softmax-coding-alternative">Multinomial Logistic Regression: Softmax Coding (Alternative)</h2>
<ul>
<li><p><strong>Softmax coding</strong> is an equivalent formulation, treating all <em>K</em> categories symmetrically.</p></li>
<li><p><strong>Model:</strong></p></li>
</ul>
<p><span class="math display">\[
\Pr(Y = k|X = x) = \frac{e^{\beta_{k0} + \beta_{k1}x_1 + \dots + \beta_{kp}x_p}}{\sum_{l=1}^K e^{\beta_{l0} + \beta_{l1}x_1 + \dots + \beta_{lp}x_p}}
\]</span></p>
</section>
<section id="softmax-coding-breakdown" class="level2">
<h2 class="anchored" data-anchor-id="softmax-coding-breakdown">Softmax Coding: Breakdown</h2>
<ul>
<li><p>This calculates Pr(Y = k | X).</p></li>
<li><p>Numerator: Exponentiated linear combination for category <em>k</em>.</p></li>
<li><p>Denominator: <em>Sum</em> of exponentiated linear combinations for <em>all</em> categories. Ensures probabilities sum to 1.</p></li>
<li><p><strong>Softmax Function:</strong> Transforms linear combinations into probabilities. Generalizes the logistic function.</p></li>
<li><p><strong>Use in Machine Learning:</strong> Common in neural networks for multi-class classification.</p></li>
</ul>
</section>
<section id="generative-models-a-different-approach" class="level2">
<h2 class="anchored" data-anchor-id="generative-models-a-different-approach">Generative Models: A Different Approach</h2>
<ul>
<li><p>So far, we‚Äôve <em>directly</em> modeled Pr(Y = k | X) (e.g., logistic regression).</p></li>
<li><p><strong>Generative models</strong> take an <em>indirect</em> approach:</p>
<ol type="1">
<li><strong>Model the distribution of predictors <em>X</em> <em>separately for each response class</em>.</strong> This is Pr(X | Y = k).</li>
<li><strong>Use Bayes‚Äô theorem to ‚Äúflip‚Äù this to get Pr(Y = k | X).</strong> This is what we want for classification.</li>
</ol></li>
</ul>
</section>
<section id="bayes-theorem-the-key-to-flipping" class="level2">
<h2 class="anchored" data-anchor-id="bayes-theorem-the-key-to-flipping">Bayes‚Äô Theorem: The Key to ‚ÄúFlipping‚Äù</h2>
<ul>
<li>Definitions:
<ul>
<li>œÄ<sub>k</sub>: <em>Prior probability</em> of an observation coming from class <em>k</em> (overall probability).</li>
<li>f<sub>k</sub>(X) = Pr(X | Y = k): <em>Density function</em> of predictors <em>X</em> for class <em>k</em> (how predictors are distributed <em>within</em> class <em>k</em>).</li>
</ul></li>
<li><strong>Bayes‚Äô Theorem:</strong></li>
</ul>
<p><span class="math display">\[
\Pr(Y = k|X = x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^K \pi_l f_l(x)}
\]</span></p>
</section>
<section id="bayes-theorem-explained" class="level2">
<h2 class="anchored" data-anchor-id="bayes-theorem-explained">Bayes‚Äô Theorem: Explained</h2>
<ul>
<li><p><strong>Left-hand side:</strong> Pr(Y = k | X = x) ‚Äì <em>Posterior probability</em>. Probability of class <em>k</em>, <em>given</em> predictors <em>x</em>. What we want.</p></li>
<li><p><strong>Right-hand side:</strong></p>
<ul>
<li>œÄ<sub>k</sub>: Prior probability of class <em>k</em>.</li>
<li>f<sub>k</sub>(x): Density of predictors <em>X</em> for class <em>k</em>.</li>
<li>Œ£<sub>l=1</sub><sup>K</sup> œÄ<sub>l</sub>f<sub>l</sub>(x): Normalizing constant (sum over all classes). Ensures probabilities sum to 1.</li>
</ul></li>
<li><p><strong>Intuition:</strong> Bayes‚Äô theorem updates our prior belief (œÄ<sub>k</sub>) based on evidence from predictors (f<sub>k</sub>(x)).</p></li>
<li><p>To use Bayes‚Äô theorem, we must estimate œÄk and fk(x). Estimating œÄk is easy, while estimating fk(x) is hard.</p></li>
</ul>
</section>
<section id="why-generative-models-advantages" class="level2">
<h2 class="anchored" data-anchor-id="why-generative-models-advantages">Why Generative Models? Advantages</h2>
<ul>
<li><p>Why this indirect approach when we have direct methods like logistic regression?</p></li>
<li><p><strong>Stability:</strong> With <em>substantial class separation</em> (predictors clearly distinguish classes), logistic regression parameter estimates can be unstable. Generative models can be more stable.</p></li>
<li><p><strong>Small Sample Size + Normality:</strong> If predictors are approximately <em>normal</em> within each class, <em>and</em> the sample size is small, generative models like LDA can be more accurate.</p></li>
<li><p><strong>More Than Two Classes:</strong> Generative models easily handle multiple classes.</p></li>
</ul>
</section>
<section id="linear-discriminant-analysis-lda-a-generative-model" class="level2">
<h2 class="anchored" data-anchor-id="linear-discriminant-analysis-lda-a-generative-model">Linear Discriminant Analysis (LDA): A Generative Model</h2>
<ul>
<li><p><strong>LDA</strong> is a generative model with specific assumptions about predictor distribution.</p></li>
<li><p><strong>LDA Assumptions:</strong></p>
<ol type="1">
<li><strong>Normality:</strong> Density functions f<sub>k</sub>(x) are <em>normal</em> (Gaussian). Predictors follow a bell-shaped distribution within each class.</li>
<li><strong>Common Variance:</strong> <em>Common variance</em> (œÉ¬≤) across all <em>K</em> classes. The spread is the same, though means may differ.</li>
</ol></li>
</ul>
</section>
<section id="lda-the-normal-density-single-predictor" class="level2">
<h2 class="anchored" data-anchor-id="lda-the-normal-density-single-predictor">LDA: The Normal Density (Single Predictor)</h2>
<ul>
<li>For one predictor (p = 1):</li>
</ul>
<p><span class="math display">\[
f_k(x) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{1}{2\sigma^2}(x - \mu_k)^2\right)
\]</span></p>
<ul>
<li>Œº<sub>k</sub>: <em>Mean</em> of predictor <em>X</em> for class <em>k</em>.</li>
<li>œÉ¬≤: <em>Common variance</em> across all classes.</li>
<li>The familiar bell-shaped curve.</li>
</ul>
</section>
<section id="lda-the-discriminant-function" class="level2">
<h2 class="anchored" data-anchor-id="lda-the-discriminant-function">LDA: The Discriminant Function</h2>
<ul>
<li><p>Plugging f<sub>k</sub>(x) into Bayes‚Äô theorem and simplifying gives a simple result.</p></li>
<li><p>Classify to the class with the <em>largest</em>:</p></li>
</ul>
<p><span class="math display">\[
\delta_k(x) = x \cdot \frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + \log(\pi_k)
\]</span></p>
<ul>
<li>This is the <strong>discriminant function</strong>.</li>
</ul>
</section>
<section id="lda-discriminant-function-breakdown" class="level2">
<h2 class="anchored" data-anchor-id="lda-discriminant-function-breakdown">LDA Discriminant Function: Breakdown</h2>
<ul>
<li><p>Œ¥<sub>k</sub>(x): Discriminant score for class <em>k</em>. Classify to the class with the highest score.</p></li>
<li><p><strong>Key:</strong> <em>Linear</em> in <em>x</em>. Hence, <em>Linear</em> Discriminant Analysis! Decision boundaries are straight lines (or hyperplanes).</p></li>
<li><p>Terms:</p>
<ul>
<li>x ‚ãÖ (Œº<sub>k</sub> / œÉ¬≤): Influence of predictor <em>x</em> and class mean Œº<sub>k</sub>, scaled by variance.</li>
<li><ul>
<li>(Œº<sub>k</sub>¬≤ / 2œÉ¬≤): Constant for each class, related to mean.</li>
</ul></li>
<li>log(œÄ<sub>k</sub>): Incorporates prior probability of class <em>k</em>.</li>
</ul></li>
</ul>
</section>
<section id="lda-estimating-the-parameters-from-data" class="level2">
<h2 class="anchored" data-anchor-id="lda-estimating-the-parameters-from-data">LDA: Estimating the Parameters (From Data)</h2>
<ul>
<li><p>We don‚Äôt know true parameters (œÄ<sub>k</sub>, Œº<sub>k</sub>, œÉ¬≤). We <em>estimate</em> them from training data.</p></li>
<li><p><strong>Estimates:</strong></p>
<ul>
<li><p>ŒºÃÇ<sub>k</sub> = (1/n<sub>k</sub>) Œ£<sub>i:y·µ¢=k</sub> x·µ¢: <em>Sample mean</em> of <em>X</em> for class <em>k</em>. Average predictor values for observations in class <em>k</em>.</p></li>
<li><p>œÉÃÇ¬≤ = (1/(n-K)) Œ£<sub>k=1</sub><sup>K</sup> Œ£<sub>i:y·µ¢=k</sub> (x·µ¢ - ŒºÃÇ<sub>k</sub>)¬≤: <em>Pooled variance estimate</em>. Weighted average of within-class variances (reflects common variance assumption).</p></li>
<li><p>œÄÃÇ<sub>k</sub> = n<sub>k</sub> / n: <em>Sample proportion</em> of observations in class <em>k</em>.</p></li>
</ul></li>
<li><p>Plug <em>estimates</em> into the discriminant function for predictions.</p></li>
</ul>
</section>
<section id="lda-a-visual-example-one-predictor" class="level2">
<h2 class="anchored" data-anchor-id="lda-a-visual-example-one-predictor">LDA: A Visual Example (One Predictor)</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_4.svg" class="img-fluid figure-img"></p>
<figcaption>One-dimensional normal density functions and LDA decision boundary</figcaption>
</figure>
</div>
</section>
<section id="lda-example-left-panel---density-functions" class="level2">
<h2 class="anchored" data-anchor-id="lda-example-left-panel---density-functions">LDA Example: Left Panel - Density Functions</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_4.svg" class="img-fluid figure-img"></p>
<figcaption>One-dimensional normal density functions and LDA decision boundary</figcaption>
</figure>
</div>
</div>
</div>
</div>
<ul>
<li><strong>Left Panel:</strong>
<ul>
<li>Two normal density functions (p=1, K=2).</li>
<li>Orange: Class 1 (f‚ÇÅ(x)).</li>
<li>Blue: Class 2 (f‚ÇÇ(x)).</li>
<li>Dashed: <em>Bayes decision boundary</em>. Optimal boundary, given true distributions. Where densities intersect.</li>
</ul></li>
</ul>
</section>
<section id="lda-example-right-panel---histograms-and-boundaries" class="level2">
<h2 class="anchored" data-anchor-id="lda-example-right-panel---histograms-and-boundaries">LDA Example: Right Panel - Histograms and Boundaries</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_4.svg" class="img-fluid figure-img"></p>
<figcaption>One-dimensional normal density functions and LDA decision boundary</figcaption>
</figure>
</div>
</div>
</div>
</div>
<ul>
<li><strong>Right Panel:</strong>
<ul>
<li>Histograms of 20 observations from each class.</li>
<li>Orange: Class 1.</li>
<li>Blue: Class 2.</li>
<li>Dashed: Bayes decision boundary (same as left).</li>
<li>Solid: <em>LDA decision boundary</em>. Found by LDA using <em>estimated</em> parameters.</li>
</ul></li>
<li><strong>Key:</strong> LDA approximates the Bayes boundary. Here, with normal data and equal variances, LDA does well.</li>
</ul>
</section>
<section id="lda-with-multiple-predictors-p-1-going-multivariate" class="level2">
<h2 class="anchored" data-anchor-id="lda-with-multiple-predictors-p-1-going-multivariate">LDA with Multiple Predictors (p &gt; 1): Going Multivariate</h2>
<ul>
<li><p>With multiple predictors, we extend LDA‚Äôs assumptions.</p></li>
<li><p><strong>Assumption:</strong> Predictors X = (X‚ÇÅ, X‚ÇÇ, ‚Ä¶, X<sub>p</sub>) follow a <em>multivariate Gaussian</em> distribution, with a class-specific mean vector and a <em>common covariance matrix</em>.</p></li>
<li><p><strong>Multivariate Gaussian Density:</strong></p></li>
</ul>
<p><span class="math display">\[
f(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu)^T\Sigma^{-1}(x - \mu)\right)
\]</span></p>
</section>
<section id="multivariate-gaussian-deconstructed" class="level2">
<h2 class="anchored" data-anchor-id="multivariate-gaussian-deconstructed">Multivariate Gaussian: Deconstructed</h2>
<ul>
<li><em>x</em>: Vector of predictor values (x‚ÇÅ, x‚ÇÇ, ‚Ä¶, x<sub>p</sub>).</li>
<li>Œº: <em>Mean vector</em> (Œº‚ÇÅ, Œº‚ÇÇ, ‚Ä¶, Œº<sub>p</sub>). Mean of each predictor. In LDA, each class has its own Œº<sub>k</sub>.</li>
<li>Œ£: <em>Covariance matrix</em>. p √ó p matrix describing relationships (covariances) between all predictor pairs. In LDA, <em>common</em> Œ£ for all classes.</li>
<li>|Œ£|: <em>Determinant</em> of Œ£.</li>
<li>Generalizes the one-dimensional normal distribution to multiple dimensions.</li>
</ul>
</section>
<section id="lda-with-multiple-predictors-the-discriminant-function" class="level2">
<h2 class="anchored" data-anchor-id="lda-with-multiple-predictors-the-discriminant-function">LDA with Multiple Predictors: The Discriminant Function</h2>
<ul>
<li><p>Plug the multivariate Gaussian density into Bayes‚Äô theorem and simplify.</p></li>
<li><p>Classify to the class with the largest:</p></li>
</ul>
<p><span class="math display">\[
\delta_k(x) = x^T \Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + \log \pi_k
\]</span></p>
<ul>
<li><strong>Key:</strong> Still <em>linear</em> in <em>x</em>. LDA produces linear decision boundaries even with multiple predictors.</li>
<li>We estimate the parameters (Œºk, Œ£, œÄk) from the training data, similar to one-dimension case.</li>
</ul>
</section>
<section id="lda-example-with-three-classes-visual" class="level2">
<h2 class="anchored" data-anchor-id="lda-example-with-three-classes-visual">LDA: Example with Three Classes (Visual)</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_6.svg" class="img-fluid figure-img"></p>
<figcaption>Multivariate Gaussian distribution with three classes, and LDA decision boundaries</figcaption>
</figure>
</div>
</section>
<section id="lda-example-three-classes-left-panel---ellipses" class="level2">
<h2 class="anchored" data-anchor-id="lda-example-three-classes-left-panel---ellipses">LDA Example (Three Classes): Left Panel - Ellipses</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_6.svg" class="img-fluid figure-img"></p>
<figcaption>Multivariate Gaussian distribution with three classes, and LDA decision boundaries</figcaption>
</figure>
</div>
</div>
</div>
</div>
<ul>
<li><strong>Left Panel:</strong>
<ul>
<li>Two-dimensional example (p=2), three classes (K=3).</li>
<li>Ellipses: Contours of constant probability density for each class (95% probability, assuming multivariate Gaussian).</li>
<li>Dashed: <em>Bayes decision boundaries</em>. Optimal boundaries, given true distributions.</li>
</ul></li>
</ul>
</section>
<section id="lda-example-three-classes-right-panel---scatterplot-and-boundaries" class="level2">
<h2 class="anchored" data-anchor-id="lda-example-three-classes-right-panel---scatterplot-and-boundaries">LDA Example (Three Classes): Right Panel - Scatterplot and Boundaries</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_6.svg" class="img-fluid figure-img"></p>
<figcaption>Multivariate Gaussian distribution with three classes, and LDA decision boundaries</figcaption>
</figure>
</div>
</div>
</div>
</div>
<ul>
<li><strong>Right Panel:</strong>
<ul>
<li>20 observations from each class.</li>
<li>Solid: <em>LDA decision boundaries</em>. Found by LDA using estimated parameters.</li>
</ul></li>
<li><strong>Key:</strong> LDA approximates Bayes boundaries. LDA boundaries are linear; Bayes boundaries are slightly curved.</li>
</ul>
</section>
<section id="lda-on-the-default-data-assessing-performance" class="level2">
<h2 class="anchored" data-anchor-id="lda-on-the-default-data-assessing-performance">LDA on the Default Data: Assessing Performance</h2>
<ul>
<li><p>Applying LDA to Default data (predicting default from <code>balance</code> and <code>student</code>).</p></li>
<li><p><strong>Training Error Rate:</strong> 2.75%. Seems low, but be careful!</p></li>
<li><p><strong>Two Problems with Training Error:</strong></p>
<ol type="1">
<li><strong>Overfitting:</strong> Training error is usually <em>lower</em> than <em>test error</em>. The model might fit training data too closely, not generalizing well to new data.</li>
<li><strong>Imbalanced Data:</strong> Only 3.33% of individuals defaulted in training data. <em>Imbalanced</em>. A classifier always predicting ‚Äúno default‚Äù (the <em>null classifier</em>) would have a 3.33% error rate. Our 2.75% isn‚Äôt much better!</li>
</ol></li>
</ul>
</section>
<section id="confusion-matrix-a-detailed-performance-view" class="level2">
<h2 class="anchored" data-anchor-id="confusion-matrix-a-detailed-performance-view">Confusion Matrix: A Detailed Performance View</h2>
<ul>
<li>A <strong>confusion matrix</strong> summarizes classifier performance. It shows counts of:
<ul>
<li>True Positives (TP)</li>
<li>True Negatives (TN)</li>
<li>False Positives (FP) - Type I error</li>
<li>False Negatives (FN) - Type II error</li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 35%">
<col style="width: 25%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">Predicted No Default</th>
<th style="text-align: left;">Predicted Default</th>
<th style="text-align: left;">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Actual No Default</td>
<td style="text-align: left;">9644</td>
<td style="text-align: left;">23</td>
<td style="text-align: left;">9667</td>
</tr>
<tr class="even">
<td style="text-align: left;">Actual Default</td>
<td style="text-align: left;">252</td>
<td style="text-align: left;">81</td>
<td style="text-align: left;">333</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Total</td>
<td style="text-align: left;">9896</td>
<td style="text-align: left;">104</td>
<td style="text-align: left;">10000</td>
</tr>
</tbody>
</table>
</section>
<section id="confusion-matrix-interpreted" class="level2">
<h2 class="anchored" data-anchor-id="confusion-matrix-interpreted">Confusion Matrix: Interpreted</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 35%">
<col style="width: 25%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">Predicted No Default</th>
<th style="text-align: left;">Predicted Default</th>
<th style="text-align: left;">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Actual No Default</td>
<td style="text-align: left;">9644</td>
<td style="text-align: left;">23</td>
<td style="text-align: left;">9667</td>
</tr>
<tr class="even">
<td style="text-align: left;">Actual Default</td>
<td style="text-align: left;">252</td>
<td style="text-align: left;">81</td>
<td style="text-align: left;">333</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Total</td>
<td style="text-align: left;">9896</td>
<td style="text-align: left;">104</td>
<td style="text-align: left;">10000</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<ul>
<li><p><strong>Rows:</strong> <em>Actual</em> class labels.</p></li>
<li><p><strong>Columns:</strong> <em>Predicted</em> class labels.</p></li>
<li><p><strong>From the table:</strong></p>
<ul>
<li><strong>TN:</strong> 9644 non-defaulters, correctly predicted ‚Äúno default‚Äù.</li>
<li><strong>FP:</strong> 23 non-defaulters, <em>incorrectly</em> predicted ‚Äúdefault‚Äù.</li>
<li><strong>FN:</strong> 252 defaulters, <em>incorrectly</em> predicted ‚Äúno default‚Äù.</li>
<li><strong>TP:</strong> 81 defaulters, correctly predicted ‚Äúdefault‚Äù.</li>
</ul></li>
</ul>
</div>
</div>
</div>
</section>
<section id="confusion-matrix-key-insights-and-trade-offs" class="level2">
<h2 class="anchored" data-anchor-id="confusion-matrix-key-insights-and-trade-offs">Confusion Matrix: Key Insights and Trade-offs</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 35%">
<col style="width: 25%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">Predicted No Default</th>
<th style="text-align: left;">Predicted Default</th>
<th style="text-align: left;">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Actual No Default</td>
<td style="text-align: left;">9644</td>
<td style="text-align: left;">23</td>
<td style="text-align: left;">9667</td>
</tr>
<tr class="even">
<td style="text-align: left;">Actual Default</td>
<td style="text-align: left;">252</td>
<td style="text-align: left;">81</td>
<td style="text-align: left;">333</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Total</td>
<td style="text-align: left;">9896</td>
<td style="text-align: left;">104</td>
<td style="text-align: left;">10000</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<ul>
<li><strong>Key Problem:</strong> LDA misclassifies 252/333 defaulters (75.7%). Low <em>sensitivity</em> (recall) ‚Äì not good at identifying positive cases (defaulters).</li>
<li><strong>However:</strong> LDA correctly classifies 99.8% of non-defaulters (1 - 23/9667). High <em>specificity</em> ‚Äì good at identifying negative cases.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="modifying-the-threshold-adjusting-sensitivity-and-specificity" class="level2">
<h2 class="anchored" data-anchor-id="modifying-the-threshold-adjusting-sensitivity-and-specificity">Modifying the Threshold: Adjusting Sensitivity and Specificity</h2>
<ul>
<li><p>LDA (and Bayes classifier) typically use a 0.5 threshold for posterior probability. If Pr(default = Yes | X) &gt; 0.5, predict ‚Äúdefault‚Äù.</p></li>
<li><p>We can <em>modify</em> this threshold to change the sensitivity/specificity trade-off.</p></li>
<li><p><strong>Lowering the Threshold (e.g., to 0.2):</strong></p>
<ul>
<li><em>Increases</em> sensitivity: Identify <em>more</em> defaulters (more TPs).</li>
<li><em>Decreases</em> specificity: Incorrectly classify <em>more</em> non-defaulters as defaulters (more FPs).</li>
</ul></li>
<li><p><strong>Raising the Threshold (e.g., to 0.8):</strong></p>
<ul>
<li><em>Decreases</em> sensitivity: Identify <em>fewer</em> defaulters (fewer TPs).</li>
<li><em>Increases</em> specificity: Incorrectly classify <em>fewer</em> non-defaulters (fewer FPs).</li>
</ul></li>
<li><p><strong>Best threshold depends on context and relative costs of errors (FP vs.&nbsp;FN).</strong></p></li>
</ul>
</section>
<section id="roc-curves-visualizing-performance-across-thresholds" class="level2">
<h2 class="anchored" data-anchor-id="roc-curves-visualizing-performance-across-thresholds">ROC Curves: Visualizing Performance Across Thresholds</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_7.svg" class="img-fluid figure-img"></p>
<figcaption>Error rates as a function of threshold</figcaption>
</figure>
</div>
</section>
<section id="roc-curves-explanation" class="level2">
<h2 class="anchored" data-anchor-id="roc-curves-explanation">ROC Curves: Explanation</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_7.svg" class="img-fluid figure-img"></p>
<figcaption>Error rates as a function of threshold</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<ul>
<li>Shows various error rates as threshold changes.</li>
<li><strong>Black solid line:</strong> Overall error rate.</li>
<li><strong>Blue dashed line:</strong> Fraction of defaulters incorrectly classified (1 - sensitivity).</li>
<li><strong>Orange dotted line:</strong> Fraction of errors among non-defaulters (false positive rate, 1 - specificity).</li>
</ul>
</div>
</div>
</div>
<ul>
<li><p>A <strong>Receiver Operating Characteristic (ROC) curve</strong> visualizes classifier performance across <em>all possible thresholds</em>.</p></li>
<li><p><strong>Axes:</strong></p>
<ul>
<li>X-axis: False Positive Rate (FPR) = 1 - Specificity</li>
<li>Y-axis: True Positive Rate (TPR) = Sensitivity</li>
</ul></li>
<li><p><strong>Each point on the ROC curve represents a different threshold.</strong></p></li>
</ul>
</section>
<section id="roc-curve-interpreting-the-shape" class="level2">
<h2 class="anchored" data-anchor-id="roc-curve-interpreting-the-shape">ROC Curve: Interpreting the Shape</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_8.svg" class="img-fluid figure-img"></p>
<figcaption>ROC curve for LDA classifier on Default data</figcaption>
</figure>
</div>
<ul>
<li><p><strong>Ideal ROC Curve:</strong> Hugs the top-left corner. High sensitivity (TPR) with low FPR.</p></li>
<li><p><strong>Worst-Case ROC Curve:</strong> Diagonal line from bottom-left to top-right (dotted line). No better than random guessing.</p></li>
<li><p><strong>Area Under the Curve (AUC):</strong> Summarizes performance.</p>
<ul>
<li>AUC = 1: Perfect classifier.</li>
<li>AUC = 0.5: No better than random.</li>
<li>AUC &gt; 0.5: Better than random. Higher AUC is better.</li>
</ul></li>
</ul>
</section>
<section id="quadratic-discriminant-analysis-qda-relaxing-assumptions" class="level2">
<h2 class="anchored" data-anchor-id="quadratic-discriminant-analysis-qda-relaxing-assumptions">Quadratic Discriminant Analysis (QDA): Relaxing Assumptions</h2>
<ul>
<li><p><strong>QDA</strong> is another generative model, like LDA.</p></li>
<li><p><strong>Key Difference:</strong> QDA <em>relaxes</em> the assumption of a <em>common</em> covariance matrix.</p></li>
<li><p><strong>QDA Assumption:</strong> Each class has its <em>own</em> covariance matrix, Œ£<sub>k</sub>. Spread and orientation can differ for each class.</p></li>
<li><p><strong>Discriminant Function:</strong> Becomes <em>quadratic</em> in <em>x</em>:</p></li>
</ul>
<p><span class="math display">\[
\delta_k(x) = -\frac{1}{2}x^T\Sigma_k^{-1}x + x^T\Sigma_k^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma_k^{-1}\mu_k - \frac{1}{2}\log|\Sigma_k| + \log \pi_k
\]</span></p>
</section>
<section id="qda-discriminant-function-explained" class="level2">
<h2 class="anchored" data-anchor-id="qda-discriminant-function-explained">QDA Discriminant Function: Explained</h2>
<ul>
<li><p>Similar terms to LDA, but with Œ£<sub>k</sub> (covariance matrix for class <em>k</em>) instead of Œ£.</p></li>
<li><p><strong>Key:</strong> <em>Quadratic</em> in <em>x</em>. Decision boundaries are curves (ellipses, parabolas, hyperbolas), not lines.</p></li>
<li><p>We estimate parameters (Œºk, Œ£k, œÄk) from training data.</p></li>
</ul>
</section>
<section id="lda-vs.-qda-the-bias-variance-trade-off" class="level2">
<h2 class="anchored" data-anchor-id="lda-vs.-qda-the-bias-variance-trade-off">LDA vs.&nbsp;QDA: The Bias-Variance Trade-Off</h2>
<ul>
<li><p><strong>QDA is more flexible than LDA.</strong> More parameters to estimate (own covariance matrix for each class).</p></li>
<li><p><strong>QDA: Higher variance, potentially lower bias.</strong> More flexible, so can fit data better (lower bias), but more prone to overfitting (higher variance).</p></li>
<li><p><strong>LDA: Lower variance, potentially higher bias.</strong> Stronger assumption (common covariance), so less flexible (higher bias), but less likely to overfit (lower variance).</p></li>
<li><p><strong>Which is better?</strong> Depends on data and bias-variance trade-off.</p>
<ul>
<li><strong>LDA better when:</strong>
<ul>
<li><em>Fewer</em> training observations. Reducing variance is crucial.</li>
<li>Common covariance assumption is reasonable.</li>
</ul></li>
<li><strong>QDA recommended when:</strong>
<ul>
<li>Training set is <em>large</em>. Variance less of a concern.</li>
<li>Common covariance assumption is clearly <em>not</em> tenable (classes have very different spreads/orientations).</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="lda-vs.-qda-a-visual-comparison" class="level2">
<h2 class="anchored" data-anchor-id="lda-vs.-qda-a-visual-comparison">LDA vs.&nbsp;QDA: A Visual Comparison</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_9.svg" class="img-fluid figure-img"></p>
<figcaption>LDA and QDA decision boundaries</figcaption>
</figure>
</div>
</section>
<section id="lda-vs.-qda-example-left-panel---common-correlation" class="level2">
<h2 class="anchored" data-anchor-id="lda-vs.-qda-example-left-panel---common-correlation">LDA vs.&nbsp;QDA Example: Left Panel - Common Correlation</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_9.svg" class="img-fluid figure-img"></p>
<figcaption>LDA and QDA decision boundaries</figcaption>
</figure>
</div>
</div>
</div>
</div>
<ul>
<li><strong>Left Panel (Common Correlation):</strong>
<ul>
<li>Classes have <em>same</em> correlation between predictors.</li>
<li>Bayes boundary (dashed): Linear.</li>
<li>LDA boundary (dotted): Linear, approximates Bayes well.</li>
<li>QDA boundary (solid): Quadratic, <em>worse</em> than LDA here. QDA overfits.</li>
</ul></li>
</ul>
</section>
<section id="lda-vs.-qda-example-right-panel---different-correlations" class="level2">
<h2 class="anchored" data-anchor-id="lda-vs.-qda-example-right-panel---different-correlations">LDA vs.&nbsp;QDA Example: Right Panel - Different Correlations</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_9.svg" class="img-fluid figure-img"></p>
<figcaption>LDA and QDA decision boundaries</figcaption>
</figure>
</div>
</div>
</div>
</div>
<ul>
<li><strong>Right Panel (Different Correlations):</strong>
<ul>
<li>Classes have <em>different</em> correlations.</li>
<li>Bayes boundary (dashed): Quadratic.</li>
<li>QDA boundary (solid): Quadratic, approximates Bayes well.</li>
<li>LDA boundary (dotted): Linear, performs <em>poorly</em>. Cannot capture the quadratic relationship.</li>
</ul></li>
</ul>
</section>
<section id="naive-bayes-the-naive-assumption" class="level2">
<h2 class="anchored" data-anchor-id="naive-bayes-the-naive-assumption">Naive Bayes: The ‚ÄúNaive‚Äù Assumption</h2>
<ul>
<li><p><strong>Naive Bayes</strong> is another generative model. It makes a <em>very strong</em> simplifying assumption.</p></li>
<li><p><strong>Assumption:</strong> <em>Within each class, the p predictors are independent</em>. Knowing the class, the value of one predictor gives no info about others.</p></li>
<li><p>Mathematically: f<sub>k</sub>(x) = f<sub>k1</sub>(x‚ÇÅ) √ó f<sub>k2</sub>(x‚ÇÇ) √ó ‚Ä¶ √ó f<sub>kp</sub>(x<sub>p</sub>)</p>
<ul>
<li>f<sub>kj</sub>(x<sub>j</sub>): Density function of <em>j</em>th predictor <em>within class k</em>.</li>
</ul></li>
</ul>
</section>
<section id="naive-bayes-explained" class="level2">
<h2 class="anchored" data-anchor-id="naive-bayes-explained">Naive Bayes: Explained</h2>
<ul>
<li><p>Independence assumption simplifies things <em>dramatically</em>. Instead of estimating a complex <em>p</em>-dimensional joint density f<sub>k</sub>(x), we estimate <em>p</em> one-dimensional densities f<sub>kj</sub>(x<sub>j</sub>).</p></li>
<li><p>Plug into Bayes‚Äô theorem:</p></li>
</ul>
<p><span class="math display">\[
\Pr(Y = k|X = x) = \frac{\pi_k \times f_{k1}(x_1) \times f_{k2}(x_2) \times \dots \times f_{kp}(x_p)}{\sum_{l=1}^K \pi_l \times f_{l1}(x_1) \times f_{l2}(x_2) \times \dots \times f_{lp}(x_p)}
\]</span></p>
</section>
<section id="naive-bayes-estimating-one-dimensional-densities" class="level2">
<h2 class="anchored" data-anchor-id="naive-bayes-estimating-one-dimensional-densities">Naive Bayes: Estimating One-Dimensional Densities</h2>
<ul>
<li><p>Key is estimating f<sub>kj</sub>(x<sub>j</sub>). Depends on whether X<sub>j</sub> is quantitative or qualitative.</p></li>
<li><p><strong>If X<sub>j</sub> is quantitative:</strong></p>
<ul>
<li><p><strong>Option 1 (Parametric):</strong> Assume X<sub>j</sub> | Y = k ~ N(Œº<sub>jk</sub>, œÉ<sub>j</sub>¬≤). Estimate mean (Œº<sub>jk</sub>) and variance (œÉ<sub>j</sub>¬≤) for each predictor <em>j</em> within each class <em>k</em>. <em>Different</em> variance for each predictor and class (unlike LDA).</p></li>
<li><p><strong>Option 2 (Non-parametric):</strong></p>
<ul>
<li><em>Histogram</em>: Divide range of X<sub>j</sub> into bins, count observations in each bin for each class.</li>
<li><em>Kernel Density Estimation (KDE)</em>: More sophisticated, produces smooth density estimate.</li>
</ul></li>
</ul></li>
<li><p><strong>If X<sub>j</sub> is qualitative:</strong></p>
<ul>
<li>Count <em>proportion</em> of training observations for each level of predictor <em>within each class</em>. Direct probability estimate.</li>
</ul></li>
</ul>
</section>
<section id="naive-bayes-example-illustrative" class="level2">
<h2 class="anchored" data-anchor-id="naive-bayes-example-illustrative">Naive Bayes: Example (Illustrative)</h2>
<ul>
<li><p>Two classes (K=2), three predictors (p=3). The first two predictors are quantitative, and the third predictor is qualitative with three levels.</p></li>
<li><p>Shows estimated one-dimensional density functions fkj.</p></li>
</ul>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_10_1.svg" class="img-fluid figure-img"></p>
<figcaption>f11</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_10_2.svg" class="img-fluid figure-img"></p>
<figcaption>f12</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_10_3.svg" class="img-fluid figure-img"></p>
<figcaption>f13</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_10_4.svg" class="img-fluid figure-img"></p>
<figcaption>f21</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_10_5.svg" class="img-fluid figure-img"></p>
<figcaption>f22</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_10_6.svg" class="img-fluid figure-img"></p>
<figcaption>f23</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="naive-bayes-surprisingly-good-performance" class="level2">
<h2 class="anchored" data-anchor-id="naive-bayes-surprisingly-good-performance">Naive Bayes: Surprisingly Good Performance!</h2>
<ul>
<li><p>Despite the strong (and often unrealistic) independence assumption, Naive Bayes often works well.</p></li>
<li><p><strong>Why?</strong> Reduces variance. Fewer parameters to estimate, so less prone to overfitting, especially with small data relative to predictors.</p></li>
<li><p><strong>Bias-Variance Trade-off:</strong> Naive Bayes has high bias (independence assumption) but low variance. Can be a good trade-off.</p></li>
</ul>
</section>
<section id="comparing-classification-methods-an-analytical-view" class="level2">
<h2 class="anchored" data-anchor-id="comparing-classification-methods-an-analytical-view">Comparing Classification Methods: An Analytical View</h2>
<ul>
<li>Logistic regression, LDA, QDA, Naive Bayes: All can be expressed in terms of maximizing Pr(Y = k | X), or log-odds relative to baseline (K):</li>
</ul>
<p><span class="math display">\[
\log\left(\frac{\Pr(Y = k|X = x)}{\Pr(Y = K|X = x)}\right)
\]</span></p>
<ul>
<li>The <em>form</em> of this log-odds expression distinguishes the methods.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 30%">
<col style="width: 70%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Log-Odds Form</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Logistic Regression</td>
<td style="text-align: left;">Œ≤<sub>k0</sub> + Œ£<sub>j=1</sub><sup>p</sup> Œ≤<sub>kj</sub>x<sub>j</sub> (linear)</td>
</tr>
<tr class="even">
<td style="text-align: left;">LDA</td>
<td style="text-align: left;">a<sub>k</sub> + Œ£<sub>j=1</sub><sup>p</sup> b<sub>kj</sub>x<sub>j</sub> (linear)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">QDA</td>
<td style="text-align: left;">a<sub>k</sub> + Œ£<sub>j=1</sub><sup>p</sup> b<sub>kj</sub>x<sub>j</sub> + Œ£<sub>j=1</sub><sup>p</sup> Œ£<sub>l=1</sub><sup>p</sup> c<sub>kj,l</sub>x<sub>j</sub>x<sub>l</sub> (quadratic)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Naive Bayes</td>
<td style="text-align: left;">a<sub>k</sub> + Œ£<sub>j=1</sub><sup>p</sup> g<sub>kj</sub>(x<sub>j</sub>) (additive)</td>
</tr>
</tbody>
</table>
</section>
<section id="comparison-explained" class="level2">
<h2 class="anchored" data-anchor-id="comparison-explained">Comparison: Explained</h2>
<ul>
<li><p><strong>Logistic Regression:</strong> Log-odds are <em>linear</em> in predictors. Coefficients (Œ≤<sub>kj</sub>) estimated by MLE.</p></li>
<li><p><strong>LDA:</strong> Log-odds also <em>linear</em>. Coefficients (a<sub>k</sub>, b<sub>kj</sub>) from multivariate normality and common covariance assumptions.</p></li>
<li><p><strong>QDA:</strong> Log-odds are <em>quadratic</em>. Allows for more complex boundaries.</p></li>
<li><p><strong>Naive Bayes:</strong> Log-odds are <em>additive</em>. Each predictor contributes independently through g<sub>kj</sub>(x<sub>j</sub>). Reflects independence.</p></li>
<li><p><strong>Relationships:</strong></p>
<ul>
<li>LDA is special case of QDA (equal covariance matrices).</li>
<li>Any classifier with <em>linear</em> boundary is special case of Naive Bayes.</li>
<li>Logistic regression has same <em>linear</em> form as LDA, but coefficients estimated differently (MLE vs.&nbsp;normal assumptions).</li>
</ul></li>
<li><p>None of these methods universally dominate the others. The best choice depends on the characteristics of data.</p></li>
</ul>
</section>
<section id="a-comparison-of-classification-methodsan-empirical-comparison" class="level2">
<h2 class="anchored" data-anchor-id="a-comparison-of-classification-methodsan-empirical-comparison">A comparison of classification methodsÔºöAn empirical comparison</h2>
<ul>
<li>Test error rates of different methods are compared through six different scenarios.</li>
</ul>
<style>
.half-width {
  width: 50%;
  float: left;
}
</style>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_11.svg" class="img-fluid figure-img"></p>
<figcaption>FIGURE 4.11</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_12.svg" class="img-fluid figure-img"></p>
<figcaption>FIGURE 4.12</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="empirical-comparison-details" class="level2">
<h2 class="anchored" data-anchor-id="empirical-comparison-details">Empirical Comparison: Details</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_11.svg" class="img-fluid figure-img"></p>
<figcaption>FIGURE 4.11</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_12.svg" class="img-fluid figure-img"></p>
<figcaption>FIGURE 4.12</figcaption>
</figure>
</div>
</div>
</div>
</div>
<ul>
<li><p>Six scenarios. Scenarios 1-3: Bayes boundaries are linear. Scenarios 4-6: Bayes boundaries are non-linear.</p></li>
<li><p>p=2 quantitative predictors in each scenario.</p></li>
<li><p>For each scenario, 100 random training sets generated.</p></li>
<li><p><strong>Scenario Details:</strong></p>
<ul>
<li><strong>Scenario 1:</strong> Two classes, predictors correlation 0, n = 100</li>
<li><strong>Scenario 2:</strong> Two classes, predictors correlation -0.5, n = 100</li>
<li><strong>Scenario 3:</strong> Two classes, predictors correlation 0.5, n = 100, modified predictors (nonlinear boundary).</li>
<li><strong>Scenario 4:</strong> Two classes, independent standard normals, response from QDA (quadratic boundary).</li>
<li><strong>Scenario 5:</strong> Same as 4, but response from more complex quadratic.</li>
<li><strong>Scenario 6:</strong> Two classes, independent predictors, complex response (highly non-linear boundary). Compare with K-nearest neighbors (KNN).</li>
</ul></li>
<li><p><strong>Results:</strong></p>
<ul>
<li><em>Linear</em> boundaries (1 and 2): LDA and logistic regression do well.</li>
<li>Moderately non-linear (3): QDA outperforms LDA, but logistic regression still good.</li>
<li>More complex (4, 5, 6): QDA can be better than LDA and logistic.</li>
<li><em>Very</em> complex (6): Non-parametric KNN can be superior, <em>but</em> smoothness (K) must be chosen carefully.</li>
</ul></li>
</ul>
</section>
<section id="generalized-linear-models-glms-beyond-normality" class="level2">
<h2 class="anchored" data-anchor-id="generalized-linear-models-glms-beyond-normality">Generalized Linear Models (GLMs): Beyond Normality</h2>
<ul>
<li><p>So far:</p>
<ul>
<li><em>Quantitative</em> responses: Least squares linear regression.</li>
<li><em>Qualitative</em> responses: Classification methods (logistic, LDA, QDA, Naive Bayes).</li>
</ul></li>
<li><p>What if <em>Y</em> is neither?</p></li>
<li><p><strong>Example:</strong> Predicting hourly users of a bike-sharing program.</p></li>
<li><p><strong>Characteristics:</strong></p>
<ul>
<li><em>Non-negative integers</em>: Can‚Äôt have negative or fractional users.</li>
<li><em>Counts</em>: Number of events (rentals) in a time period.</li>
</ul></li>
</ul>
</section>
<section id="why-not-linear-regression-for-counts" class="level2">
<h2 class="anchored" data-anchor-id="why-not-linear-regression-for-counts">Why Not Linear Regression for Counts?</h2>
<ul>
<li><p>Problems with using linear regression:</p>
<ol type="1">
<li><strong>Negative Predictions:</strong> Linear regression can predict negative values (nonsensical for counts).</li>
<li><strong>Mean-Variance Relationship:</strong> For counts, variance often <em>increases</em> with the mean. Linear regression assumes constant variance.</li>
<li><strong>Non-Continuous:</strong> Response is discrete. Linear regression assumes continuous.</li>
</ol></li>
<li><p><strong>Solution:</strong> <strong>Poisson regression</strong>. A more natural approach for count data.</p></li>
</ul>
</section>
<section id="poisson-regression-modeling-counts" class="level2">
<h2 class="anchored" data-anchor-id="poisson-regression-modeling-counts">Poisson Regression: Modeling Counts</h2>
<ul>
<li><p><strong>Poisson Distribution:</strong> Often used to model <em>counts</em>.</p></li>
<li><p><strong>Probability Mass Function:</strong> If Y is Poisson, probability of <em>k</em> events is:</p></li>
</ul>
<p><span class="math display">\[
\Pr(Y = k) = \frac{e^{-\lambda}\lambda^k}{k!}  \quad \text{for } k = 0, 1, 2, \dots
\]</span></p>
<ul>
<li><p><em>k</em>: Number of events (non-negative integer).</p></li>
<li><p><em>k!</em>: <em>k</em> factorial (k! = k √ó (k-1) √ó ‚Ä¶ √ó 2 √ó 1). 0! = 1.</p></li>
<li><p>Œª: Parameter (Œª &gt; 0). <em>Expected value</em> (mean) <em>and</em> <em>variance</em> of Y.</p></li>
<li><p><strong>Key:</strong> Mean and variance are <em>equal</em>. As mean (Œª) increases, variance increases. Suitable for count data.</p></li>
</ul>
</section>
<section id="poisson-regression-model-connecting-mean-to-predictors" class="level2">
<h2 class="anchored" data-anchor-id="poisson-regression-model-connecting-mean-to-predictors">Poisson Regression Model: Connecting Mean to Predictors</h2>
<ul>
<li>Model the <em>mean</em> (Œª) as a function of predictors.</li>
</ul>
<p><span class="math display">\[
\log(\lambda(X_1, \dots, X_p)) = \beta_0 + \beta_1X_1 + \dots + \beta_pX_p
\]</span></p>
<p>or equivalently</p>
<p><span class="math display">\[
\lambda(X_1, \dots, X_p) = e^{\beta_0 + \beta_1X_1 + \dots + \beta_pX_p}
\]</span></p>
</section>
<section id="poisson-regression-model-explained" class="level2">
<h2 class="anchored" data-anchor-id="poisson-regression-model-explained">Poisson Regression Model: Explained</h2>
<ul>
<li><p>Œª(X‚ÇÅ, X‚ÇÇ, ‚Ä¶, X<sub>p</sub>): Mean (and variance) of Poisson, depends on predictors.</p></li>
<li><p>Œ≤‚ÇÄ, Œ≤‚ÇÅ, ‚Ä¶, Œ≤<sub>p</sub>: Coefficients estimated from data.</p></li>
<li><p><strong>Link Function:</strong> <em>Log</em> link. <em>Logarithm</em> of mean is linear in predictors.</p></li>
<li><p><strong>Why log link?</strong></p>
<ul>
<li><strong>Non-negativity:</strong> Ensures predicted mean (Œª) is <em>non-negative</em>. Exponential function (e<sup>x</sup>) is always positive.</li>
<li><strong>Linearity:</strong> Models relationship between predictors and <em>log</em> of mean as linear.</li>
</ul></li>
<li><p><strong>Estimation:</strong> Use MLE to estimate coefficients.</p></li>
</ul>
</section>
<section id="comparing-poisson-regression-model-to-the-linear-regression-model" class="level2">
<h2 class="anchored" data-anchor-id="comparing-poisson-regression-model-to-the-linear-regression-model">Comparing Poisson Regression Model to the linear regression model</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<ul>
<li>Interpretation of the coefficients
<ul>
<li>In linear regression, coefficients represent linear increase.</li>
<li>In Poisson regression, coefficients represents the change in the log-mean for a one-unit increase in X, or increase the expected count by a factor of e<sup>Œ≤</sup>.</li>
</ul></li>
<li>Mean-variance relationship
<ul>
<li>In linear regression, variance doesn‚Äôt change when mean changes.</li>
<li>In Poisson regression, variance is equal to mean. The larger the mean, the larger the variance.</li>
</ul></li>
<li>Nonnegative fitted values
<ul>
<li>In linear regression, fitted values could be negative.</li>
<li>In Poisson regression, fitted values will always be positive.</li>
</ul></li>
<li>Please read text material for details.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="generalized-linear-models-glms-the-big-picture" class="level2">
<h2 class="anchored" data-anchor-id="generalized-linear-models-glms-the-big-picture">Generalized Linear Models (GLMs): The Big Picture</h2>
<ul>
<li><p>Three regression models:</p>
<ol type="1">
<li><strong>Linear Regression:</strong> Quantitative responses (normal distribution, constant variance).</li>
<li><strong>Logistic Regression:</strong> Binary responses (Bernoulli/binomial distribution).</li>
<li><strong>Poisson Regression:</strong> Count responses (Poisson distribution).</li>
</ol></li>
<li><p>Common characteristics:</p>
<ol type="1">
<li><strong>Predictors and Response:</strong> Predictors (X‚ÇÅ, X‚ÇÇ, ‚Ä¶, X<sub>p</sub>) predict response (Y).</li>
<li><strong>Conditional Distribution:</strong> Given predictors, <em>Y</em> belongs to a <em>family of distributions</em>.</li>
<li><strong>Modeling the Mean:</strong> Model <em>mean</em> of <em>Y</em>, conditional on predictors.</li>
<li><strong>Link Function:</strong> <em>Link function</em> (Œ∑) relates mean of <em>Y</em> to linear combination of predictors.</li>
</ol></li>
<li><p><strong>General Form (GLM):</strong></p></li>
</ul>
<p><span class="math display">\[
\eta(E(Y|X_1, \dots, X_p)) = \beta_0 + \beta_1X_1 + \dots + \beta_pX_p
\]</span></p>
<ul>
<li><strong>Components:</strong>
<ul>
<li><em>Response (Y)</em> and its distribution.</li>
<li><em>Predictors (X1, X2, ‚Ä¶, Xp)</em>.</li>
<li><em>Link Function (Œ∑)</em>.</li>
<li><em>Linear Predictor</em>: Œ≤‚ÇÄ + Œ≤‚ÇÅX‚ÇÅ + ‚Ä¶ + Œ≤<sub>p</sub>X<sub>p</sub></li>
</ul></li>
</ul>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<ul>
<li><strong>Classification:</strong> Predicting <em>qualitative</em> responses.</li>
<li>Key methods:
<ul>
<li><strong>Logistic Regression:</strong> Models <em>probability</em> using logistic function. Directly models Pr(Y = k | X).</li>
<li><strong>LDA:</strong> Generative. Assumes multivariate Gaussian predictors, <em>common</em> covariance.</li>
<li><strong>QDA:</strong> Like LDA, but <em>own</em> covariance matrix per class.</li>
<li><strong>Naive Bayes:</strong> Generative. Assumes <em>independence</em> of predictors within each class.</li>
</ul></li>
<li><strong>Choosing:</strong> Depends on data and bias-variance trade-off.
<ul>
<li>LDA/logistic: Linear boundaries.</li>
<li>QDA: More flexible, non-linear, but higher variance.</li>
<li>Naive Bayes: Simple, surprisingly good, despite independence.</li>
</ul></li>
<li><strong>ROC Curves:</strong> Evaluate performance across thresholds. AUC summarizes.</li>
<li><strong>Generalized linear models</strong> handle responses from non-normal distributions, and Poisson regression model is an example for count data.</li>
</ul>
</section>
<section id="thoughts-and-discussion" class="level2">
<h2 class="anchored" data-anchor-id="thoughts-and-discussion">Thoughts and Discussion ü§î</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<ul>
<li>Real-world scenarios for each method?
<ul>
<li><em>Logistic:</em> Credit scoring, medical diagnosis, marketing (churn).</li>
<li><em>LDA:</em> Predictors approximately normal, similar covariance. Some image recognition/signal processing.</li>
<li><em>QDA:</em> Normal predictors, <em>different</em> covariances. More complex problems.</li>
<li><em>Naive Bayes:</em> Text classification (spam filtering), document categorization, sentiment analysis.</li>
</ul></li>
<li>Choosing the ‚Äúbest‚Äù method?
<ul>
<li><em>Understand Data:</em> Explore. Linear/nonlinear? Normal predictors?</li>
<li><em>Assumptions:</em> Violations?</li>
<li><em>Bias-Variance:</em> Flexible (high variance, low bias) or constrained (low variance, high bias)? Depends on dataset size/complexity.</li>
<li><em>Metrics:</em> Cross-validation for test error.
<ul>
<li>Accuracy</li>
<li>Sensitivity (Recall)</li>
<li>Specificity</li>
<li>Precision</li>
<li>F1-score</li>
<li>AUC</li>
</ul></li>
<li><em>Experiment:</em> Try different methods, compare with cross-validation.</li>
</ul></li>
<li>Limitations? When might they fail?
<ul>
<li><em>Logistic:</em> Assumes linear log-odds. Unstable with severe separation.</li>
<li><em>LDA:</em> Multivariate normality, common covariance. Outliers.</li>
<li><em>QDA:</em> Multivariate normality. Overfitting with small data.</li>
<li><em>Naive Bayes:</em> Independence often violated.</li>
</ul></li>
<li>How does the bias-variance trade-off play a role in choosing a classification method?
<ul>
<li><em>High Bias, Low Variance:</em> Simpler models (e.g., LDA, Naive Bayes) tend to have higher bias but lower variance. They are less likely to overfit, but they might miss complex relationships in the data.</li>
<li><em>Low Bias, High Variance:</em> More complex models (e.g., QDA) tend to have lower bias but higher variance. They can fit the data more closely, but they are more prone to overfitting, especially with small datasets.</li>
<li>The optimal balance depends on the specific dataset and the goals of the analysis.</li>
</ul></li>
<li>Can we apply the knowledge of mean-variance relationship and fitted values to choose suitable regression model?
<ul>
<li>Yes.</li>
<li>If variance increases as the mean increase and the response is count data, Poisson regression could be considered.</li>
<li>If the fitted values are probabilities, which must be between 0 and 1, then models like logistic regression could be considered.</li>
<li>Understanding the characteristics of data will guide us to select suitable models.</li>
</ul></li>
</ul>
</div>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/qiufei\.github\.io\/web-slide-r");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>üîã<a href="https://posit.co"><img src="https://posit.co/wp-content/themes/Posit/assets/images/posit-logo-2024.png" class="img-fluid" alt="Posit" width="65"></a></p>
</div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 ÈÇ±È£û ¬© 2025
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://beian.miit.gov.cn">
<p>ÊµôICPÂ§á 2024072710Âè∑-1</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33021202002511">
<p>ÊµôÂÖ¨ÁΩëÂÆâÂ§á 33021202002511Âè∑</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:hfutqiufei@163.com">
      <i class="bi bi-envelope-at-fill" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>