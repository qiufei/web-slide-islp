##  Welcome 

::: {color="#FFFFFF"}
**Welcome to Statistical Learning!**
:::

::: {color="#FFFFFF"}
This chapter provides a gentle introduction to the exciting world of statistical learning, a powerful toolkit for understanding data. We will explore the core concepts, see real-world examples, and start our journey toward building predictive models.
:::


## What is Statistical Learning?

Statistical learning refers to a vast set of tools for *understanding data*. These tools can be broadly classified into two categories:

::: columns
::: {.column width="50%"}
**1. Supervised Learning:**

-   Building a model to *predict* or *estimate* an output based on one or more inputs (also known as predictors, features, or independent variables).
-   Think of it like teaching a computer to learn from examples where you provide both the questions (inputs) and the answers (outputs).
- **Example:** Predicting a house price based on its size, location, and number of bedrooms.

:::

::: {.column width="50%"}
**2. Unsupervised Learning:**

-   Discovering relationships and structure in data *without* a predefined output variable.
-   Here, you're letting the computer explore the data and find patterns on its own.
-   **Example:** Grouping customers into different segments based on their purchasing behavior, without knowing in advance what those segments should be.
:::
:::

## Supervised vs. Unsupervised: A Visual Analogy

::: {layout-valign="center"}
Imagine you have a basket of fruits.

-   **Supervised Learning:** You tell a child, "This is an apple, this is a banana, this is an orange." Then you show them a new fruit and ask, "What is this?"
-   **Unsupervised Learning:** You give the child the basket and say, "Sort these fruits into groups however you think is best." The child might group them by color, shape, or size, discovering inherent patterns without being told what to look for.
:::

## Data Mining, Machine Learning, and Statistical Learning

These terms are often used interchangeably, but there are subtle differences:

```{mermaid}
graph LR
    A[Data Mining] --> C(Common Ground)
    B[Machine Learning] --> C
    D[Statistical Learning] --> C
    C --> E[Insights & Predictions]
```

-   **Data Mining:** Focuses on discovering patterns and extracting knowledge from *large* datasets, often using techniques from database management and computer science.
-   **Machine Learning:** Primarily concerned with building algorithms that can *learn from and make predictions on data*. Emphasizes predictive accuracy and computational efficiency.
-   **Statistical Learning:** A subfield of statistics that emphasizes *model interpretability and understanding the uncertainty* associated with predictions. Provides a rigorous statistical framework for machine learning.

## Real-World Applications

Let's explore some real-world data sets:

1.  **Wage Data:** Analyzing factors that influence a person's wage.
2.  **Stock Market Data:** Predicting stock market movements.
3.  **Gene Expression Data:** Identifying groups of genes with similar expression patterns.

We will use these data sets throughout the course to illustrate various statistical learning techniques.

## Wage Data: Understanding Income

We want to understand how a person's *wage* is related to their:

-   **Age** 
-   **Education**
-   **Year** (calendar year)

::: callout-note
The goal is to build a model that can predict a person's wage based on these factors.
:::

## Wage Data: Wage vs. Age

![Wage as a function of age](figures/1_1.png)

::: {style="font-size: 0.8em;"}

-  **Left Panel:** This scatterplot shows individual wages plotted against age.  The blue line represents the *average* wage for a given age.
-  **Trend:**  Wage generally *increases* with age until around 60, then *decreases*. This suggests a *non-linear* relationship.
-  **Variability:**  There's significant spread around the average, meaning age alone isn't a perfect predictor.

:::

## Wage Data: Wage vs. Year & Education

![Wage as a function of year and education](figures/1_1.png)

::: {style="font-size: 0.8em;"}

-  **Center Panel:** Shows wage versus year.  There's a gradual *increase* in average wage over time (2003-2009).
-  **Right Panel:** Boxplots of wage for different education levels (1 = lowest, 5 = highest).  Higher education generally leads to *higher* wages.
-  **Conclusion:** Combining age, year, and education will likely provide the *most accurate* wage prediction.

:::
## Stock Market Data: Predicting Direction

-   **Goal:** Predict whether the S&P 500 stock index will *increase* or *decrease* on a given day.
-   **Input:** Percentage changes in the index over the *previous 5 days*.
-   **Output:**  Categorical (qualitative) â€“ either "Up" or "Down". This is a *classification* problem.

## Stock Market Data: Previous Day's Change

![Boxplots of previous day's change](figures/1_2.png)

::: {style="font-size: 0.8em;"}

-   **Left Panel:** Boxplots show the distribution of the *previous day's* percentage change, separated by whether the market went "Up" or "Down" *today*.
-  **Observation:** The two boxplots are very *similar*, suggesting that yesterday's performance is *not* a strong predictor of today's direction.
- **Center and Right Panel**:The boxplots show the *2 days previous* and *3 days previous* percentage change, separated by whether the market went "Up" or "Down" *today*. The two boxplots are very *similar*.
:::

## Stock Market Data: Predicting with QDA

![Quadratic Discriminant Analysis](figures/1_3.png)

::: {style="font-size: 0.8em;"}
- A statistical learning method called *quadratic discriminant analysis (QDA)* was used to predict market direction.
- **Result:** The model correctly predicted the direction about 60% of the time. This is better than random guessing (50%), but still far from perfect. It suggests weak trends *might* exist.
:::
## Gene Expression Data: Clustering

-   **Goal:** Identify *groups* (clusters) of cancer cell lines based on their gene expression measurements.
-   **Input:**  6,830 gene expression measurements for each of 64 cancer cell lines.
-   **Output:** *None*. This is an *unsupervised learning* problem (specifically, a *clustering* problem).

## Gene Expression Data: Principal Components

![Principal Components Analysis](figures/1_4.png)

::: {style="font-size: 0.75em;"}
- **Challenge:** Visualizing 6,830 dimensions is impossible!
- **Solution:** *Principal Component Analysis (PCA)* reduces the data to two dimensions (Z1 and Z2) that capture the most important information.
-   **Left Panel:** Each point represents a cell line, colored by a *suggested* cluster. At least four groups seems clear.
-   **Right Panel:** Same plot, but points are colored by the *actual* cancer type (14 types).
-   **Observation:** Cell lines with the same cancer type tend to cluster together, validating the unsupervised clustering. This means that even though we didn't *tell* the algorithm the cancer types, it was able to *discover* them (to some extent) from the gene expression data.
:::

## A Brief History of Statistical Learning

::: {style="font-size: 0.8em;"}

-   **Early Beginnings (19th Century):** Least squares (linear regression).
-   **Mid-20th Century (1930s-1970s):** Linear discriminant analysis, logistic regression, generalized linear models.
-   **Computational Revolution (1980s onwards):** Non-linear methods become feasible (e.g., trees, generalized additive models). Neural Networks and Support Vector Machine were proposed.
-   **Modern Era:** Statistical learning emerges as a distinct field, fueled by powerful software (like Python) and increasing data availability.

:::
## Notation

::: {style="font-size: 0.8em;"}
-   **n:** Number of *observations* (data points).
-   **p:** Number of *variables* (features, predictors).
-   **x<sub>ij</sub>:** Value of the *j*th variable for the *i*th observation.
-   **X:** A matrix (n x p) representing the data. Think of it as a spreadsheet.
-  **y<sub>i</sub>:** the ith observation of the variable on which we wish to make predictions
-   **Bold lowercase (e.g., a):** A vector of length *n*.
-   **Normal lowercase (e.g., a):** A scalar (single number) or a vector *not* of length *n*.
-   **Bold capitals (e.g., A):** A matrix.

:::
## Notation: Example (Wage Data)

::: {style="font-size: 0.8em;"}
-   n = 3000 (3000 people)
-   p = 11 (variables like year, age, education, etc.)
-   x<sub>23</sub> would be the value of the 3rd variable (e.g., education) for the 2nd person.
- **X** is a 3000 x 11 matrix.
-  y<sub>1</sub>: the first person's wage.
:::

## Summary

::: {style="font-size: 0.9em;"}
-   Statistical learning provides tools to understand data, both with and without a specific outcome to predict.
-   Supervised learning aims to predict an output, while unsupervised learning explores data structure.
-   Data mining, machine learning, and statistical learning are related but have different emphases.
-   Real-world applications include predicting wages, stock market movements, and clustering gene expression data.
-   Understanding notation is crucial for following the rest of the course.
:::

## Thoughts and Discussion

::: {style="font-size: 0.9em;"}
-   Can you think of other examples of supervised and unsupervised learning problems in your field of interest?
-   Why is it important to understand the *limitations* of statistical learning models, even when they achieve good predictive accuracy?
-   What are the potential ethical implications of using statistical learning models in areas like hiring, loan applications, or criminal justice?
- How do you imagine the combination of increasing data availability, more powerful hardware, and user-friendly software will further transform the field of statistical learning, making more sophisticated machine learning available to even more researchers?
:::

