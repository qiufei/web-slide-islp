<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Introduction to Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<link href="../https://assets.qiufei.site/personal/profile.jpg" rel="icon" type="image/jpeg">
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-8ea72dc5fed832574809a9c94082fbbb.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-681fbf911679f9b3dbf9743eb275ba49.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-7e49aeac8059a213a463aa1a739e8272.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://qiufei.github.io"> 
<span class="menu-text">È¶ñÈ°µ</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://qiufei.github.io/web-slide-r"> 
<span class="menu-text">RËØæ‰ª∂</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://qiufei.github.io/web-slide-marketing"> 
<span class="menu-text">Ëê•ÈîÄËØæ‰ª∂</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Introduction to Deep Learning</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="what-is-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="what-is-deep-learning">What is Deep Learning?</h2>
<ul>
<li><p><strong>Very Active Research Area:</strong> üöÄ Deep learning is at the forefront of artificial intelligence research, with new discoveries and advancements happening constantly.</p></li>
<li><p><strong>Subfield of Machine Learning:</strong> It‚Äôs a specialized branch of machine learning, focusing on algorithms inspired by the structure and function of the human brain.</p></li>
<li><p><strong>Neural Networks are Key:</strong> The core building block of deep learning is the <strong>neural network</strong>, a complex structure of interconnected nodes (called ‚Äúneurons‚Äù) that can learn intricate patterns from data.</p></li>
</ul>
</section>
<section id="what-is-machine-learning" class="level2">
<h2 class="anchored" data-anchor-id="what-is-machine-learning">What is Machine Learning?</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li><p>Machine learning is a field of study that gives computers the ability to learn without being explicitly programmed. - Arthur Samuel, 1959</p></li>
<li><p>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E. - Tom M. Mitchell, 1997</p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Machine learning focuses on the development of algorithms that can learn from and make predictions on data. These algorithms operate by building a model from example inputs in order to make data-driven predictions or decisions, rather than following strictly static program instructions.</p>
</div>
</div>
</section>
<section id="what-is-statistical-learning" class="level2">
<h2 class="anchored" data-anchor-id="what-is-statistical-learning">What is Statistical Learning?</h2>
<ul>
<li><p>Statistical learning is a framework for understanding data based on statistics. It involves building a statistical model for inference or prediction.</p></li>
<li><p>The goal is often to estimate a function <span class="math inline">\(f\)</span> such that <span class="math inline">\(Y = f(X) + \epsilon\)</span>, where <span class="math inline">\(X\)</span> represents the input variables, <span class="math inline">\(Y\)</span> represents the output variable, and <span class="math inline">\(\epsilon\)</span> is the random error term.</p></li>
<li><p><strong>Emphasis:</strong> Statistical learning emphasizes models and their interpretability and precision, offering tools to assess the model (such as confidence intervals).</p></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Statistical learning focuses on drawing inferences from data using statistical models. Unlike machine learning, which often prioritizes predictive accuracy, statistical learning also focuses on the interpretability of models and understanding the underlying data generation process.</p>
</div>
</div>
</section>
<section id="deep-learning-in-action" class="level2">
<h2 class="anchored" data-anchor-id="deep-learning-in-action">Deep Learning in Action</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_1_1.png" class="img-fluid figure-img"></p>
<figcaption>A simple neural network.</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This image visually represents a simple neural network. The circles represent <em>neurons</em>, and the lines connecting them represent the <em>connections</em> between neurons. The input layer receives data, the hidden layers process it, and the output layer produces the result.</p>
</div>
</div>
</section>
<section id="defining-the-landscape-data-mining-machine-learning-and-statistical-learning" class="level2">
<h2 class="anchored" data-anchor-id="defining-the-landscape-data-mining-machine-learning-and-statistical-learning">Defining the Landscape: Data Mining, Machine Learning, and Statistical Learning</h2>
<p>Let‚Äôs clarify these related terms, which are often used interchangeably, but have important distinctions:</p>
<ul>
<li><p><strong>Data Mining:</strong> üîç The process of discovering patterns, anomalies, and insights from <em>large</em> datasets. It‚Äôs like ‚Äúunearthing‚Äù valuable information hidden within a mountain of data.</p></li>
<li><p><strong>Machine Learning:</strong> ü§ñ Algorithms that can <em>learn</em> from data and improve their performance <em>without</em> being explicitly programmed. Focuses on making accurate <em>predictions</em> or <em>decisions</em>.</p></li>
<li><p><strong>Statistical Learning:</strong> üìä A subfield of statistics focused on statistical <em>models</em> for understanding data. Bridges statistics and machine learning, often emphasizing <em>interpretability</em>.</p></li>
</ul>
</section>
<section id="relationships-between-concepts" class="level2">
<h2 class="anchored" data-anchor-id="relationships-between-concepts">Relationships Between Concepts</h2>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
    A[Data Mining] --&gt; C(Common Ground)
    B[Machine Learning] --&gt; C
    D[Statistical Learning] --&gt; C
    C --&gt; E[Insights &amp; Predictions]
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This diagram illustrates the overlapping nature of these fields. They all share the common goal of extracting insights and making predictions from data, but they approach this goal with different tools and emphases. Data mining is often applied to very large datasets, machine learning focuses on prediction, and statistical learning emphasizes inference and model understanding.</p>
</div>
</div>
</section>
<section id="a-bit-of-history" class="level2">
<h2 class="anchored" data-anchor-id="a-bit-of-history">A Bit of History</h2>
<ul>
<li><strong>Early Days (Late 1980s):</strong> Neural networks first gained attention, sparking initial excitement.</li>
<li><strong>Stabilization:</strong> The initial hype subsided as researchers explored the practical challenges of training and applying these networks.</li>
<li><strong>Decline in Popularity:</strong> Other machine learning methods, like Support Vector Machines (SVMs), boosting, and random forests, became more popular. These were considered more ‚Äúautomatic.‚Äù</li>
<li><strong>The Resurgence (Post-2010):</strong> Neural networks made a spectacular comeback, rebranded as ‚ÄúDeep Learning.‚Äù</li>
</ul>
</section>
<section id="drivers-of-the-deep-learning-renaissance" class="level2">
<h2 class="anchored" data-anchor-id="drivers-of-the-deep-learning-renaissance">Drivers of the Deep Learning Renaissance</h2>
<p>What fueled this resurgence? Three key factors:</p>
<ol type="1">
<li><strong>New Architectures:</strong> Researchers developed more sophisticated and effective neural network architectures.</li>
<li><strong>Larger Datasets:</strong> The explosion of digital data provided the massive datasets needed to train complex models.</li>
<li><strong>More Computing Power:</strong> Advances in hardware, especially GPUs (Graphics Processing Units), made training large models feasible.</li>
</ol>
</section>
<section id="neural-networks-the-building-blocks" class="level2">
<h2 class="anchored" data-anchor-id="neural-networks-the-building-blocks">Neural Networks: The Building Blocks</h2>
<ul>
<li><p><strong>Brain Inspiration:</strong> üß† Neural networks are <em>inspired</em> by the human brain, mimicking how neurons connect and process information.</p></li>
<li><p><strong>Interconnected Nodes:</strong> Composed of interconnected nodes (‚Äúneurons‚Äù), organized in <em>layers</em>.</p></li>
<li><p><strong>Learning Complex Relationships:</strong> Can learn intricate, <em>non-linear</em> relationships between inputs (data) and outputs (predictions).</p></li>
<li><p><strong>Foundation of Deep Learning:</strong> Form the foundation for most deep learning models.</p></li>
</ul>
</section>
<section id="single-layer-neural-networks-the-basics" class="level2">
<h2 class="anchored" data-anchor-id="single-layer-neural-networks-the-basics">Single Layer Neural Networks: The Basics</h2>
<ul>
<li><p><strong>Input:</strong> It starts with an input vector <span class="math inline">\(\mathbf{X} = (X_1, X_2, \dots, X_p)\)</span>, representing your data (e.g., pixel values of an image).</p></li>
<li><p><strong>Goal:</strong> Build a non-linear function <span class="math inline">\(f(\mathbf{X})\)</span> to predict a response <span class="math inline">\(Y\)</span> (e.g., the category of an image).</p></li>
</ul>
</section>
<section id="single-layer-neural-network-structure" class="level2">
<h2 class="anchored" data-anchor-id="single-layer-neural-network-structure">Single Layer Neural Network Structure</h2>
<ul>
<li><p><strong>Input Layer:</strong> Receives the input features <span class="math inline">\(X_1, \dots, X_p\)</span>.</p></li>
<li><p><strong>Hidden Layer:</strong> Computes <em>activations</em> <span class="math inline">\(A_k = h_k(\mathbf{X})\)</span>. These are non-linear transformations of linear combinations of the inputs. Think of them as ‚Äúhidden features.‚Äù</p></li>
<li><p><strong>Output Layer:</strong> A linear model that uses the activations from the hidden layer, producing the final output <span class="math inline">\(f(\mathbf{X})\)</span>.</p></li>
</ul>
</section>
<section id="learning-the-functions" class="level2">
<h2 class="anchored" data-anchor-id="learning-the-functions">Learning the Functions</h2>
<ul>
<li>The functions <span class="math inline">\(h_k(\cdot)\)</span> in the hidden layer are not pre-defined; they are <em>learned</em> during training. This allows the network to adapt to the data.</li>
</ul>
</section>
<section id="visualizing-a-single-layer-network" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-a-single-layer-network">Visualizing a Single Layer Network</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_1.svg" class="img-fluid figure-img"></p>
<figcaption>A single-layer neural network.</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This diagram shows a single-layer neural network. The input layer (left) receives the data. The hidden layer (middle) performs the non-linear transformation. The output layer (right) produces the prediction. The arrows represent the weights that connect the neurons.</p>
</div>
</div>
</section>
<section id="single-layer-network-the-math" class="level2">
<h2 class="anchored" data-anchor-id="single-layer-network-the-math">Single Layer Network: The Math</h2>
<p>The neural network model can be expressed mathematically as:</p>
<p><span class="math display">\[
f(\mathbf{X}) = \beta_0 + \sum_{k=1}^K \beta_k h_k(\mathbf{X})
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
A_k = h_k(\mathbf{X}) = g\left(w_{k0} + \sum_{j=1}^p w_{kj}X_j\right)
\]</span></p>
</section>
<section id="breaking-down-the-equation" class="level2">
<h2 class="anchored" data-anchor-id="breaking-down-the-equation">Breaking Down the Equation</h2>
<ul>
<li><span class="math inline">\(K\)</span>: The number of <em>hidden units</em> (neurons in the hidden layer).</li>
<li><span class="math inline">\(g(\cdot)\)</span>: The <em>activation function</em> (introduces non-linearity).</li>
<li><span class="math inline">\(w_{kj}\)</span> and <span class="math inline">\(\beta_k\)</span>: The <em>weights</em> or <em>parameters</em> of the network (learned during training).</li>
<li><span class="math inline">\(A_k\)</span>: called <em>activations</em>.</li>
</ul>
</section>
<section id="activation-functions-introducing-non-linearity" class="level2">
<h2 class="anchored" data-anchor-id="activation-functions-introducing-non-linearity">Activation Functions: Introducing Non-Linearity</h2>
<ul>
<li><p><strong>Why Non-Linearity?</strong> Without it, the network would collapse into a simple linear model. Activation functions allow learning complex, non-linear relationships.</p></li>
<li><p><strong>Common Choices:</strong></p>
<ul>
<li><p><strong>Sigmoid:</strong> <span class="math inline">\(g(z) = \frac{1}{1 + e^{-z}}\)</span> (Squashes values between 0 and 1. A ‚Äúsmooth‚Äù on/off switch.)</p></li>
<li><p><strong>ReLU (Rectified Linear Unit):</strong> <span class="math inline">\(g(z) = \max(0, z)\)</span> (Computationally efficient. Zero for negative inputs, the input itself for positive inputs.)</p></li>
</ul></li>
</ul>
</section>
<section id="visualizing-activation-functions" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-activation-functions">Visualizing Activation Functions</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_2.svg" class="img-fluid figure-img"></p>
<figcaption>Sigmoid (left) and ReLU (right) activation functions.</figcaption>
</figure>
</div>
<ul>
<li><strong>Left (Sigmoid):</strong> Notice the smooth, S-shaped curve. It ‚Äúsquashes‚Äù any input value into the range between 0 and 1. This was historically popular but can lead to problems during training (vanishing gradients).</li>
</ul>
</section>
<section id="visualizing-activation-functions-1" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-activation-functions-1">Visualizing Activation Functions</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_2.svg" class="img-fluid figure-img"></p>
<figcaption>Sigmoid (left) and ReLU (right) activation functions.</figcaption>
</figure>
</div>
<ul>
<li><strong>Right (ReLU):</strong> Notice how it‚Äôs simply zero for negative inputs and a straight line for positive inputs. This simple form is computationally efficient and helps with training deeper networks. It‚Äôs a very popular choice in modern deep learning.</li>
</ul>
</section>
<section id="why-non-linearity-matters-capturing-complexity" class="level2">
<h2 class="anchored" data-anchor-id="why-non-linearity-matters-capturing-complexity">Why Non-linearity Matters: Capturing Complexity</h2>
<ul>
<li><p><strong>Real-World Complexity:</strong> Relationships in the real world are rarely simple straight lines.</p></li>
<li><p><strong>Interactions:</strong> Non-linearities enable the model to capture <em>interactions</em> between input variables. The effect of one variable might depend on another.</p></li>
</ul>
</section>
<section id="example-modeling-interactions" class="level2">
<h2 class="anchored" data-anchor-id="example-modeling-interactions">Example: Modeling Interactions</h2>
<ul>
<li><p>Consider:</p>
<ul>
<li><span class="math inline">\(p=2\)</span> inputs (<span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>).</li>
<li><span class="math inline">\(K=2\)</span> hidden units.</li>
<li><span class="math inline">\(g(z) = z^2\)</span> (a simple non-linear activation function).</li>
</ul></li>
<li><p>With appropriate weights, this can model the interaction <span class="math inline">\(X_1X_2\)</span>:</p>
<p><span class="math display">\[
f(\mathbf{X}) = X_1X_2
\]</span></p></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>By choosing the weights <span class="math inline">\(w_{10} = 0, w_{11} = 1, w_{12} = 1, w_{20} = 0, w_{21} = 1, w_{22} = -1\)</span> and activation function <span class="math inline">\(g(z)=z^2\)</span>, the activations can be formed as: <span class="math inline">\(A_1 = (X_1 + X_2)^2, A_2 = (X_1 - X_2)^2\)</span>. If we further set <span class="math inline">\(\beta_0 = 0, \beta_1=0.25, \beta_2 = -0.25\)</span>, the output becomes: <span class="math inline">\(f(\mathbf{X}) = 0.25(X_1 + X_2)^2 - 0.25(X_1 - X_2)^2 = X_1X_2\)</span>. This illustrates how non-linear activation functions, combined with appropriate weights, allow neural networks to capture interaction effects.</p>
</div>
</div>
</section>
<section id="multilayer-neural-networks-going-deeper" class="level2">
<h2 class="anchored" data-anchor-id="multilayer-neural-networks-going-deeper">Multilayer Neural Networks: Going Deeper</h2>
<ul>
<li><p><strong>Multiple Hidden Layers:</strong> Modern neural networks often have <em>multiple</em> hidden layers, stacked one after another (‚Äúdeep‚Äù learning).</p></li>
<li><p><strong>Hierarchical Representations:</strong> Each layer builds upon the previous, creating increasingly complex and abstract representations.</p></li>
</ul>
</section>
<section id="mnist-digit-classification" class="level2">
<h2 class="anchored" data-anchor-id="mnist-digit-classification">MNIST Digit Classification</h2>
<ul>
<li><p><strong>MNIST:</strong> A classic dataset: images of handwritten digits (0-9).</p></li>
<li><p><strong>Image Format:</strong> Each image is 28x28 pixels, grayscale values (0-255). 784 (28*28) input features.</p></li>
</ul>
</section>
<section id="visualizing-a-multilayer-network-mnist" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-a-multilayer-network-mnist">Visualizing a Multilayer Network (MNIST)</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_3.svg" class="img-fluid figure-img"></p>
<figcaption>A multilayer neural network for MNIST digit classification.</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This diagram shows a multilayer neural network for classifying handwritten digits from the MNIST dataset. The input layer is large (784 pixels). Subsequent layers progressively reduce the number of units. The output layer has 10 units (one for each digit).</p>
</div>
</div>
</section>
<section id="the-mnist-dataset-details" class="level2">
<h2 class="anchored" data-anchor-id="the-mnist-dataset-details">The MNIST Dataset: Details</h2>
<ul>
<li><p><strong>Goal:</strong> Classify each image into its correct digit category (0-9).</p></li>
<li><p><strong>One-Hot Encoding:</strong> Output: a vector <span class="math inline">\(\mathbf{Y} = (Y_0, Y_1, \dots, Y_9)\)</span>, where <span class="math inline">\(Y_i = 1\)</span> if the image represents digit <span class="math inline">\(i\)</span>, and 0 otherwise.</p></li>
<li><p><strong>Dataset Size:</strong> 60,000 training images and 10,000 test images.</p></li>
</ul>
</section>
<section id="mnist-examples" class="level2">
<h2 class="anchored" data-anchor-id="mnist-examples">MNIST Examples</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_4.svg" class="img-fluid figure-img"></p>
<figcaption>Examples of handwritten digits from the MNIST dataset.</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This figure shows examples of handwritten digits from the MNIST dataset. The goal is to train a neural network to correctly classify these images. Notice the variations in handwriting style, making this a non-trivial task.</p>
</div>
</div>
</section>
<section id="multilayer-networks-key-differences" class="level2">
<h2 class="anchored" data-anchor-id="multilayer-networks-key-differences">Multilayer Networks: Key Differences</h2>
<ul>
<li><p><strong>Multiple Hidden Layers:</strong> Layers like <span class="math inline">\(L_1\)</span> (e.g., 256 units), <span class="math inline">\(L_2\)</span> (e.g., 128 units). Each layer learns increasingly complex features.</p></li>
<li><p><strong>Multiple Outputs:</strong> Output layer can have multiple units (e.g., 10 for MNIST).</p></li>
<li><p><strong>Multitask Learning:</strong> A single network can predict multiple responses simultaneously.</p></li>
<li><p><strong>Loss Function:</strong> Choice depends on the task. For multi-class classification (like MNIST), use <em>cross-entropy</em> loss.</p></li>
</ul>
</section>
<section id="multilayer-networks-mathematical-formulation-13" class="level2">
<h2 class="anchored" data-anchor-id="multilayer-networks-mathematical-formulation-13">Multilayer Networks: Mathematical Formulation (1/3)</h2>
<ul>
<li><p><strong>First Hidden Layer:</strong> Similar to the single-layer, but with a superscript (1) for the layer:</p>
<p><span class="math display">\[
A_k^{(1)} = h_k^{(1)}(\mathbf{X}) = g\left(w_{k0}^{(1)} + \sum_{j=1}^p w_{kj}^{(1)} X_j\right)
\]</span></p></li>
</ul>
</section>
<section id="multilayer-networks-mathematical-formulation-23" class="level2">
<h2 class="anchored" data-anchor-id="multilayer-networks-mathematical-formulation-23">Multilayer Networks: Mathematical Formulation (2/3)</h2>
<ul>
<li><p><strong>Second Hidden Layer:</strong> Takes activations from the <em>first</em> hidden layer as input:</p>
<p><span class="math display">\[
A_l^{(2)} = h_l^{(2)}(\mathbf{X}) = g\left(w_{l0}^{(2)} + \sum_{k=1}^{K_1} w_{lk}^{(2)} A_k^{(1)}\right)
\]</span></p>
<ul>
<li>Notice how <span class="math inline">\(A_k^{(1)}\)</span> (output of the first layer) is now the input.</li>
</ul></li>
</ul>
</section>
<section id="multilayer-networks-mathematical-formulation-33" class="level2">
<h2 class="anchored" data-anchor-id="multilayer-networks-mathematical-formulation-33">Multilayer Networks: Mathematical Formulation (3/3)</h2>
<ul>
<li><p><strong>Output Layer (multi-class classification):</strong> Use <em>softmax</em> for probabilities:</p>
<p><span class="math display">\[
f_m(\mathbf{X}) = \Pr(Y = m | \mathbf{X}) = \frac{e^{Z_m}}{\sum_{m'=0}^9 e^{Z_{m'}}}
\]</span></p>
<p>where <span class="math inline">\(Z_m = \beta_{m0} + \sum_{l=1}^{K_2} \beta_{ml} A_l^{(2)}\)</span>.</p>
<ul>
<li>Softmax ensures outputs are probabilities (between 0 and 1, sum to 1).</li>
</ul></li>
</ul>
</section>
<section id="loss-function-and-optimization" class="level2">
<h2 class="anchored" data-anchor-id="loss-function-and-optimization">Loss Function and Optimization</h2>
<ul>
<li><p><strong>Cross-Entropy Loss (multi-class classification):</strong></p>
<p><span class="math display">\[
-\sum_{i=1}^n \sum_{m=0}^9 y_{im} \log(f_m(\mathbf{x}_i))
\]</span></p>
<ul>
<li>Measures the difference between predicted probabilities and true labels (one-hot encoded).</li>
</ul></li>
<li><p><strong>Goal:</strong> Find weights (all <span class="math inline">\(w\)</span> and <span class="math inline">\(\beta\)</span> values) that <em>minimize</em> this loss.</p></li>
<li><p><strong>Optimization:</strong> Use <em>gradient descent</em> (and variants).</p></li>
<li><p><strong>Weights:</strong> refers to all trainable parameters, including the coefficients and bias terms.</p></li>
</ul>
</section>
<section id="comparison-with-linear-models-mnist" class="level2">
<h2 class="anchored" data-anchor-id="comparison-with-linear-models-mnist">Comparison with Linear Models (MNIST)</h2>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Method</th>
<th>Test Error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Neural Network + Ridge</td>
<td>2.3%</td>
</tr>
<tr class="even">
<td>Neural Network + Dropout</td>
<td>1.8%</td>
</tr>
<tr class="odd">
<td>Multinomial Logistic Regression</td>
<td>7.2%</td>
</tr>
<tr class="even">
<td>Linear Discriminant Analysis</td>
<td>12.7%</td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This table shows that neural networks significantly outperform traditional linear methods (like multinomial logistic regression and linear discriminant analysis) on the MNIST digit classification task. This highlights the power of deep learning for complex pattern recognition. Ridge and Dropout are regularization techniques.</p>
</div>
</div>
</section>
<section id="convolutional-neural-networks-cnns-specialized-for-images" class="level2">
<h2 class="anchored" data-anchor-id="convolutional-neural-networks-cnns-specialized-for-images">Convolutional Neural Networks (CNNs): Specialized for Images</h2>
<ul>
<li><p><strong>Image Classification (and more):</strong> CNNs are designed for images (and data with spatial structure, like audio).</p></li>
<li><p><strong>Human Vision Inspiration:</strong> Inspired by how humans process visual information, focusing on local patterns.</p></li>
<li><p><strong>Key Idea:</strong> Learn <em>local</em> features (edges, corners) and combine them for <em>global</em> patterns (objects).</p></li>
</ul>
</section>
<section id="cnn-architecture-key-components" class="level2">
<h2 class="anchored" data-anchor-id="cnn-architecture-key-components">CNN Architecture: Key Components</h2>
<ul>
<li><p><strong>Convolutional Layers:</strong></p>
<ul>
<li>Apply <em>convolution filters</em> (small templates/kernels) to the image.</li>
<li>Detect local features (edges, textures). Filters are <em>learned</em>.</li>
<li><em>Weight sharing</em>: <em>Same</em> filter applied across the <em>entire</em> image (efficient).</li>
</ul></li>
<li><p><strong>Pooling Layers:</strong></p>
<ul>
<li><em>Downsample</em> feature maps.</li>
<li>Reduce dimensionality, provide <em>translation invariance</em> (small shifts don‚Äôt change output much).</li>
<li><em>Max pooling</em>: Takes the maximum value in a region.</li>
</ul></li>
<li><p><strong>Flatten Layer:</strong> convert multi-dimension feature maps to vector.</p></li>
<li><p><strong>Fully Connected Layers:</strong> Standard neural network layers for classification.</p></li>
</ul>
</section>
<section id="cnn-example-tiger-classification" class="level2">
<h2 class="anchored" data-anchor-id="cnn-example-tiger-classification">CNN Example: Tiger Classification üêÖ</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_6.svg" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<ul>
<li><strong>Input:</strong> An image of a tiger.</li>
<li><strong>Convolutional Layers:</strong> Early layers find edges, stripes.</li>
<li><strong>Pooling Layers:</strong> Downsample, provide invariance.</li>
<li><strong>Higher Layers:</strong> Combine features (eyes, ears).</li>
<li><strong>Output:</strong> Probability of a tiger.</li>
</ul>
</section>
<section id="convolution-operation-a-closer-look" class="level2">
<h2 class="anchored" data-anchor-id="convolution-operation-a-closer-look">Convolution Operation: A Closer Look</h2>
<ul>
<li><p><strong>Convolution Filter:</strong> A small matrix (e.g., 3x3) of numbers (a <em>kernel</em>).</p></li>
<li><p><strong>Sliding the Filter:</strong> ‚ÄúSlid‚Äù across the image, one pixel at a time (or larger <em>stride</em>).</p></li>
<li><p><strong>Element-wise Multiplication and Summation:</strong> At each position, element-wise multiplication between filter and image, then sum.</p></li>
<li><p><strong>Convolved Image:</strong> Produces a single value in the <em>convolved image</em> (<em>feature map</em>).</p></li>
<li><p><strong>Different Filters, Different Features:</strong> Different filters detect different features.</p></li>
</ul>
</section>
<section id="visualizing-convolution" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-convolution">Visualizing Convolution</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_7.svg" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This figure illustrates the convolution operation. The filter (small square) is moved across the input image (large square). At each position, the element-wise product of the filter and the corresponding part of the image is computed, and the results are summed to produce a single value in the convolved image.</p>
</div>
</div>
</section>
<section id="convolution-operation-example" class="level2">
<h2 class="anchored" data-anchor-id="convolution-operation-example">Convolution Operation: Example</h2>
<ul>
<li><p>Input Image: <span class="math display">\[
Original Image = \begin{bmatrix}
a &amp; b &amp; c\\
d &amp; e &amp; f\\
g &amp; h &amp; i\\
j &amp; k &amp; l
\end{bmatrix}
\]</span></p></li>
<li><p>Filter: <span class="math display">\[
ConvolutionFilter = \begin{bmatrix}
\alpha &amp; \beta\\
\gamma &amp; \delta
\end{bmatrix}
\]</span></p></li>
<li><p>Convolved Image: <span class="math display">\[
Convolved Image = \begin{bmatrix}
aa + b\beta + d\gamma + e\delta &amp; ba + c\beta + e\gamma + f\delta\\
da + e\beta + g\gamma + h\delta &amp; ea + f\beta + h\gamma + i\delta\\
ga + h\beta + j\gamma + k\delta &amp; ha + i\beta + k\gamma + l\delta
\end{bmatrix}
\]</span></p></li>
</ul>
</section>
<section id="convolutional-layer-details" class="level2">
<h2 class="anchored" data-anchor-id="convolutional-layer-details">Convolutional Layer: Details</h2>
<ul>
<li><p><strong>Multiple Channels:</strong> Color images: 3 channels (Red, Green, Blue). Filters match input channels.</p></li>
<li><p><strong>Multiple Filters:</strong> A layer uses <em>many</em> filters (e.g., 32, 64) for various features.</p></li>
<li><p><strong>ReLU Activation:</strong> Usually, ReLU <em>after</em> convolution.</p></li>
<li><p><strong>Detector Layer:</strong> Convolution + ReLU is a <em>detector layer</em>.</p></li>
</ul>
</section>
<section id="pooling-layer-downsampling" class="level2">
<h2 class="anchored" data-anchor-id="pooling-layer-downsampling">Pooling Layer: Downsampling</h2>
<ul>
<li><p><strong>Purpose:</strong> Reduce spatial dimensions (width, height).</p></li>
<li><p><strong>Max Pooling:</strong> Takes the <em>maximum</em> in a non-overlapping region (e.g., 2x2).</p></li>
<li><p><strong>Benefits:</strong></p>
<ul>
<li><strong>Reduces Computation:</strong> Smaller feature maps, fewer computations.</li>
<li><strong>Translation Invariance:</strong> Robust to small shifts.</li>
</ul></li>
</ul>
</section>
<section id="max-pooling-example" class="level2">
<h2 class="anchored" data-anchor-id="max-pooling-example">Max Pooling: Example</h2>
<p>Input:</p>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 2 &amp; 5 &amp; 3 \\
3 &amp; 0 &amp; 1 &amp; 2 \\
2 &amp; 1 &amp; 3 &amp; 4 \\
1 &amp; 1 &amp; 2 &amp; 0 \\
\end{bmatrix}
\]</span></p>
<p>Output (2x2 max pooling):</p>
<p><span class="math display">\[
\begin{bmatrix}
3 &amp; 5 \\
2 &amp; 4
\end{bmatrix}
\]</span></p>
</section>
<section id="putting-it-all-together-a-complete-cnn-architecture" class="level2">
<h2 class="anchored" data-anchor-id="putting-it-all-together-a-complete-cnn-architecture">Putting it All Together: A Complete CNN Architecture</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_8.svg" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<ul>
<li><strong>Sequence of Layers:</strong> Convolutional and pooling layers.</li>
<li><strong>Feature Extraction:</strong> Convolutional layers extract.</li>
<li><strong>Downsampling:</strong> Pooling layers downsample.</li>
</ul>
</section>
<section id="putting-it-all-together-a-complete-cnn-architecture-1" class="level2">
<h2 class="anchored" data-anchor-id="putting-it-all-together-a-complete-cnn-architecture-1">Putting it All Together: A Complete CNN Architecture</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_8.svg" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<ul>
<li><strong>Flatten Layer:</strong> Converts 3D feature maps to 1D vector.</li>
<li><strong>Fully Connected Layers:</strong> Classification.</li>
<li><strong>Softmax Output Layer:</strong> Probabilities for each class.</li>
</ul>
</section>
<section id="data-augmentation-expanding-the-training-set" class="level2">
<h2 class="anchored" data-anchor-id="data-augmentation-expanding-the-training-set">Data Augmentation: Expanding the Training Set</h2>
<ul>
<li><p><strong>Purpose:</strong> Artificially increase training set size.</p></li>
<li><p><strong>Method:</strong> Random transformations:</p>
<ul>
<li>Rotation üìê</li>
<li>Zoom üîç</li>
<li>Shift ‚ÜîÔ∏éÔ∏è</li>
<li>Flip üîÑ</li>
</ul></li>
<li><p><strong>Benefits:</strong></p>
<ul>
<li><strong>Reduces Overfitting:</strong> Model sees more variations.</li>
<li><strong>Improves Generalization:</strong> More robust.</li>
<li><strong>Regularization:</strong> Acts as regularization.</li>
</ul></li>
</ul>
</section>
<section id="data-augmentation-examples" class="level2">
<h2 class="anchored" data-anchor-id="data-augmentation-examples">Data Augmentation: Examples</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_9.svg" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This figure shows examples of data augmentation. A single image of a digit is transformed in various ways (rotated, shifted, zoomed) to create multiple training examples. This helps the model generalize better to unseen data and reduces overfitting.</p>
</div>
</div>
</section>
<section id="pretrained-classifiers-leveraging-existing-knowledge" class="level2">
<h2 class="anchored" data-anchor-id="pretrained-classifiers-leveraging-existing-knowledge">Pretrained Classifiers: Leveraging Existing Knowledge</h2>
<ul>
<li><p><strong>Leverage Pre-trained Models:</strong> Use models trained on <em>massive</em> datasets (e.g., ImageNet).</p></li>
<li><p><strong>Example: ResNet50:</strong> Popular pre-trained CNN.</p></li>
<li><p><strong>Weight Freezing:</strong> Use convolutional layers as <em>feature extractors</em>. Freeze weights, train only final layers. Effective for small datasets.</p></li>
</ul>
</section>
<section id="pretrained-classifier-illustration" class="level2">
<h2 class="anchored" data-anchor-id="pretrained-classifier-illustration">Pretrained Classifier: Illustration</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_10.svg" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This figure shows how a pretrained classifier (ResNet50) can be used. The convolutional layers (pretrained on ImageNet) are frozen, and only the final layers are trained on a new dataset. This leverages the knowledge learned from a large dataset, even if your own dataset is small.</p>
</div>
</div>
</section>
<section id="pretrained-classifiers-example-predictions-table-10.10" class="level2">
<h2 class="anchored" data-anchor-id="pretrained-classifiers-example-predictions-table-10.10">Pretrained Classifiers: Example Predictions (Table 10.10)</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 15%">
<col style="width: 14%">
<col style="width: 7%">
<col style="width: 17%">
<col style="width: 7%">
<col style="width: 15%">
<col style="width: 7%">
</colgroup>
<thead>
<tr class="header">
<th>Image</th>
<th>True Label</th>
<th>Prediction 1</th>
<th>Prob 1</th>
<th>Prediction 2</th>
<th>Prob 2</th>
<th>Prediction 3</th>
<th>Prob 3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Flamingo)</td>
<td>Flamingo</td>
<td>Flamingo</td>
<td>0.83</td>
<td>Spoonbill</td>
<td>0.17</td>
<td>White stork</td>
<td>0.00</td>
</tr>
<tr class="even">
<td>(Cooper‚Äôs Hawk)</td>
<td>Cooper‚Äôs Hawk</td>
<td>Kite</td>
<td>0.60</td>
<td>Great grey owl</td>
<td>0.09</td>
<td>Nail</td>
<td>0.12</td>
</tr>
<tr class="odd">
<td>(Cooper‚Äôs Hawk)</td>
<td>Cooper‚Äôs Hawk</td>
<td>Fountain</td>
<td>0.35</td>
<td>nail</td>
<td>0.12</td>
<td>hook</td>
<td>0.07</td>
</tr>
<tr class="even">
<td>(Lhasa Apso)</td>
<td>Lhasa Apso</td>
<td>Tibetan terrier</td>
<td>0.56</td>
<td>Lhasa</td>
<td>0.32</td>
<td>cocker spaniel</td>
<td>0.03</td>
</tr>
<tr class="odd">
<td>(Cat)</td>
<td>Cat</td>
<td>Old English sheepdog</td>
<td>0.82</td>
<td>Shih-Tzu</td>
<td>0.04</td>
<td>Persian cat</td>
<td>0.04</td>
</tr>
<tr class="even">
<td>(Cape weaver)</td>
<td>Cape weaver</td>
<td>jacamar</td>
<td>0.28</td>
<td>macaw</td>
<td>0.12</td>
<td>robin</td>
<td>0.12</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>Predictions and Probabilities:</strong> The table displays the top 3 predictions and associated probabilities from ResNet50 for different images.</li>
</ul>
</section>
<section id="pretrained-classifiers-example-predictions-table-10.10-1" class="level2">
<h2 class="anchored" data-anchor-id="pretrained-classifiers-example-predictions-table-10.10-1">Pretrained Classifiers: Example Predictions (Table 10.10)</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 15%">
<col style="width: 14%">
<col style="width: 7%">
<col style="width: 17%">
<col style="width: 7%">
<col style="width: 15%">
<col style="width: 7%">
</colgroup>
<thead>
<tr class="header">
<th>Image</th>
<th>True Label</th>
<th>Prediction 1</th>
<th>Prob 1</th>
<th>Prediction 2</th>
<th>Prob 2</th>
<th>Prediction 3</th>
<th>Prob 3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Flamingo)</td>
<td>Flamingo</td>
<td>Flamingo</td>
<td>0.83</td>
<td>Spoonbill</td>
<td>0.17</td>
<td>White stork</td>
<td>0.00</td>
</tr>
<tr class="even">
<td>(Cooper‚Äôs Hawk)</td>
<td>Cooper‚Äôs Hawk</td>
<td>Kite</td>
<td>0.60</td>
<td>Great grey owl</td>
<td>0.09</td>
<td>Nail</td>
<td>0.12</td>
</tr>
<tr class="odd">
<td>(Cooper‚Äôs Hawk)</td>
<td>Cooper‚Äôs Hawk</td>
<td>Fountain</td>
<td>0.35</td>
<td>nail</td>
<td>0.12</td>
<td>hook</td>
<td>0.07</td>
</tr>
<tr class="even">
<td>(Lhasa Apso)</td>
<td>Lhasa Apso</td>
<td>Tibetan terrier</td>
<td>0.56</td>
<td>Lhasa</td>
<td>0.32</td>
<td>cocker spaniel</td>
<td>0.03</td>
</tr>
<tr class="odd">
<td>(Cat)</td>
<td>Cat</td>
<td>Old English sheepdog</td>
<td>0.82</td>
<td>Shih-Tzu</td>
<td>0.04</td>
<td>Persian cat</td>
<td>0.04</td>
</tr>
<tr class="even">
<td>(Cape weaver)</td>
<td>Cape weaver</td>
<td>jacamar</td>
<td>0.28</td>
<td>macaw</td>
<td>0.12</td>
<td>robin</td>
<td>0.12</td>
</tr>
</tbody>
</table>
<ul>
<li><p><strong>Correct Predictions:</strong> In several cases (Flamingo, Lhasa Apso), the model correctly predicts the true label with high probability.</p></li>
<li><p><strong>Reasonable Alternatives:</strong> Even when incorrect, the model often provides reasonable alternative predictions (e.g., Spoonbill and White stork for Flamingo).</p></li>
<li><p><strong>Uncertainty:</strong> The probabilities show the model‚Äôs uncertainty. Lower probabilities indicate less confidence.</p></li>
<li><p><strong>Limitation:</strong> Pretrained models are limited by the data they were trained on. They might misclassify objects not well-represented in the original training data.</p></li>
</ul>
</section>
<section id="document-classification-beyond-images" class="level2">
<h2 class="anchored" data-anchor-id="document-classification-beyond-images">Document Classification: Beyond Images</h2>
<ul>
<li><p><strong>Another Key Application:</strong> Deep learning is effective for document classification.</p></li>
<li><p><strong>Goal:</strong> Predict document attributes:</p>
<ul>
<li>Sentiment (positive, negative)</li>
<li>Topic (sports, politics)</li>
<li>Author</li>
<li>Spam detection</li>
</ul></li>
<li><p><strong>Featurization:</strong> Converting text to numerical representation.</p></li>
<li><p><strong>Example:</strong> Sentiment analysis of IMDb movie reviews.</p></li>
</ul>
</section>
<section id="bag-of-words-model-a-simple-featurization" class="level2">
<h2 class="anchored" data-anchor-id="bag-of-words-model-a-simple-featurization">Bag-of-Words Model: A Simple Featurization</h2>
<ul>
<li><p><strong>Simplest Approach:</strong> Surprisingly effective.</p></li>
<li><p><strong>How it Works:</strong></p>
<ol type="1">
<li>Create a <em>dictionary</em> of all unique words.</li>
<li>Represent each document as a vector: presence (1) or absence (0) of each word. (Or use word counts).</li>
</ol></li>
<li><p><strong>Ignores Word Order:</strong> ‚ÄúBag‚Äù of words ‚Äì order lost.</p></li>
<li><p><strong>Sparse Representation:</strong> Vectors are mostly zeros.</p></li>
<li><p><strong>Bag-of-n-grams:</strong> Consider sequences of <em>n</em> words (captures some context).</p></li>
</ul>
</section>
<section id="recurrent-neural-networks-rnns-designed-for-sequences" class="level2">
<h2 class="anchored" data-anchor-id="recurrent-neural-networks-rnns-designed-for-sequences">Recurrent Neural Networks (RNNs): Designed for Sequences</h2>
<ul>
<li><p><strong>Sequential Data:</strong> RNNs for:</p>
<ul>
<li>Text (sequences of words)</li>
<li>Time series (measurements over time)</li>
<li>Audio</li>
<li>DNA</li>
</ul></li>
<li><p><strong>Key Idea:</strong> Process input <em>one element at a time</em>, maintaining a ‚Äúhidden state‚Äù (memory) of previous elements.</p></li>
<li><p><strong>Unrolling the RNN:</strong> Visualizing it ‚Äúunrolled‚Äù in time.</p></li>
</ul>
</section>
<section id="rnn-architecture-visualized" class="level2">
<h2 class="anchored" data-anchor-id="rnn-architecture-visualized">RNN Architecture: Visualized</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_12.svg" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This figure shows the architecture of a recurrent neural network (RNN). The input sequence (X) is processed one element at a time. The hidden state (A) is updated at each step, carrying information from previous steps. The output (O) can be produced at each step or only at the final step. Crucially, the <em>same</em> weights (W, U, B) are used at each step (weight sharing). This allows the RNN to handle sequences of varying lengths.</p>
</div>
</div>
</section>
<section id="rnn-mathematical-formulation" class="level2">
<h2 class="anchored" data-anchor-id="rnn-mathematical-formulation">RNN: Mathematical Formulation</h2>
<ul>
<li><p><strong>Hidden State Update:</strong></p>
<p><span class="math display">\[
\mathbf{A}_{lk} = g\left(w_{k0} + \sum_{j=1}^p w_{kj} \mathbf{X}_{lj} + \sum_{s=1}^K u_{ks} \mathbf{A}_{l-1,s}\right)
\]</span></p></li>
<li><p><strong>Output:</strong></p>
<p><span class="math display">\[
\mathbf{O}_l = \beta_0 + \sum_{k=1}^K \beta_k \mathbf{A}_{lk}
\]</span></p></li>
<li><p><span class="math inline">\(g(\cdot)\)</span>: activation function (e.g., ReLU).</p></li>
<li><p><span class="math inline">\(\mathbf{W, U, B}\)</span>: shared weight matrices.</p></li>
</ul>
</section>
<section id="explaining-the-rnn-equations" class="level2">
<h2 class="anchored" data-anchor-id="explaining-the-rnn-equations">Explaining the RNN Equations</h2>
<ul>
<li><p><strong>Hidden State Update:</strong> New state (<span class="math inline">\(\mathbf{A}_{lk}\)</span>) depends on:</p>
<ul>
<li>Current input (<span class="math inline">\(\mathbf{X}_{lj}\)</span>).</li>
<li><em>Previous</em> hidden state (<span class="math inline">\(\mathbf{A}_{l-1,s}\)</span>) ‚Äì the memory.</li>
<li>Weights (<span class="math inline">\(\mathbf{W, U}\)</span>).</li>
</ul></li>
<li><p><strong>Output:</strong> Output (<span class="math inline">\(\mathbf{O}_l\)</span>) is a linear combination of the hidden state.</p></li>
<li><p><strong>Weight Sharing:</strong> <em>Same</em> weight matrices (<span class="math inline">\(\mathbf{W, U, B}\)</span>) at <em>every</em> time step.</p></li>
</ul>
</section>
<section id="rnns-for-document-classification" class="level2">
<h2 class="anchored" data-anchor-id="rnns-for-document-classification">RNNs for Document Classification</h2>
<ul>
<li><p><strong>Beyond Bag-of-Words:</strong> Use the <em>sequence</em> of words.</p></li>
<li><p><strong>Word Embeddings:</strong> Represent each word as a dense vector (word embedding). Captures semantic relationships (e.g., ‚Äúking,‚Äù ‚Äúqueen‚Äù). word2vec, GloVe.</p></li>
<li><p><strong>Embedding Layer:</strong> Maps each word (one-hot encoded) to its embedding.</p></li>
<li><p><strong>Processing the Sequence:</strong> RNN processes embeddings, updates state, produces output (e.g., sentiment).</p></li>
</ul>
</section>
<section id="word-embeddings-example" class="level2">
<h2 class="anchored" data-anchor-id="word-embeddings-example">Word Embeddings: Example</h2>
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_13.svg" class="img-fluid"></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>One hot encoding vector length is the vocabulary size.</li>
<li>Word embeddings provide dense representation, where semantically similar words have similar vectors. For example, ‚Äúking‚Äù and ‚Äúqueen‚Äù would be closer to each other than ‚Äúking‚Äù and ‚Äúapple‚Äù.</li>
</ul>
</div>
</div>
</section>
<section id="rnns-for-time-series-forecasting" class="level2">
<h2 class="anchored" data-anchor-id="rnns-for-time-series-forecasting">RNNs for Time Series Forecasting</h2>
<ul>
<li><p><strong>Time Series Prediction:</strong> RNNs are useful.</p></li>
<li><p><strong>Example:</strong> Stock market trading volume.</p></li>
<li><p><strong>Input:</strong> Past values (volume, return, volatility).</p></li>
<li><p><strong>Output:</strong> Predicted volume for next day.</p></li>
<li><p><strong>Autocorrelation:</strong> Values correlated with past (today‚Äôs volume similar to yesterday‚Äôs). RNNs capture this.</p></li>
</ul>
</section>
<section id="time-series-data-example" class="level2">
<h2 class="anchored" data-anchor-id="time-series-data-example">Time Series Data: Example</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_14.svg" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This figure shows an example of time series data: daily trading statistics from the New York Stock Exchange (NYSE). The data includes log trading volume (top), the Dow Jones Industrial Average (DJIA) return (middle), and log volatility (bottom). Notice the patterns and fluctuations over time. Predicting future values of such series is a common task in finance.</p>
</div>
</div>
</section>
<section id="autocorrelation-function-acf" class="level2">
<h2 class="anchored" data-anchor-id="autocorrelation-function-acf">Autocorrelation Function (ACF)</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_15.svg" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<ul>
<li><p><strong>Measures Correlation with Past:</strong> The ACF measures correlation with <em>lagged</em> values (past).</p></li>
<li><p><strong>Interpreting the ACF:</strong> Shows how strongly related values are at different lags. High ACF at lag 1: today‚Äôs value correlated with yesterday‚Äôs.</p></li>
</ul>
</section>
<section id="rnn-forecaster-setting-up-the-data" class="level2">
<h2 class="anchored" data-anchor-id="rnn-forecaster-setting-up-the-data">RNN Forecaster: Setting up the Data</h2>
<ul>
<li><p><strong>Input Sequence:</strong> <span class="math inline">\(L\)</span> past observations (e.g., <span class="math inline">\(L=5\)</span> days): <span class="math display">\[
\mathbf{X}_1 = \begin{pmatrix} v_{t-L} \\ r_{t-L} \\ z_{t-L} \end{pmatrix}, \mathbf{X}_2 = \begin{pmatrix} v_{t-L+1} \\ r_{t-L+1} \\ z_{t-L+1} \end{pmatrix}, \dots, \mathbf{X}_L = \begin{pmatrix} v_{t-1} \\ r_{t-1} \\ z_{t-1} \end{pmatrix}
\]</span></p></li>
<li><p><span class="math inline">\(v_t\)</span>: trading volume on day <span class="math inline">\(t\)</span>.</p>
<ul>
<li><span class="math inline">\(r_t\)</span>: return on day <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(z_t\)</span>: volatility on day <span class="math inline">\(t\)</span>.</li>
</ul></li>
<li><p><strong>Output:</strong> <span class="math inline">\(Y = v_t\)</span> (trading volume on day <span class="math inline">\(t\)</span>, to predict).</p></li>
<li><p><strong>Creating Training Examples:</strong> Many <span class="math inline">\((\mathbf{X}, Y)\)</span> pairs from historical data. Sequence of past, corresponding future value.</p></li>
</ul>
</section>
<section id="rnn-forecasting-results" class="level2">
<h2 class="anchored" data-anchor-id="rnn-forecasting-results">RNN Forecasting Results</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_16.svg" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<ul>
<li><p><strong>Performance:</strong> RNN achieves <span class="math inline">\(R^2\)</span> of 0.42 on test data. Explains 42% of variance in volume.</p></li>
<li><p><strong>Comparison to Baseline:</strong> Outperforms a baseline using yesterday‚Äôs volume.</p></li>
</ul>
</section>
<section id="autoregression-ar-model-a-traditional-approach" class="level2">
<h2 class="anchored" data-anchor-id="autoregression-ar-model-a-traditional-approach">Autoregression (AR) Model: A Traditional Approach</h2>
<ul>
<li><p><strong>Traditional Time Series Model:</strong> Autoregression (AR) is classic.</p></li>
<li><p><strong>Linear Prediction:</strong> Predicts based on <em>linear</em> combination of past.</p></li>
<li><p><strong>AR(L) Model:</strong> Uses <span class="math inline">\(L\)</span> previous values:</p>
<p><span class="math display">\[
\hat{v}_t = \beta_0 + \beta_1 v_{t-1} + \beta_2 v_{t-2} + \dots + \beta_L v_{t-L}
\]</span></p></li>
<li><p><strong>Including Other Variables:</strong> Can include lagged values of other variables (return, volatility).</p></li>
<li><p><strong>RNN as a Non-Linear Extension:</strong> RNN is a <em>non-linear</em> extension of AR. RNN: complex, non-linear relationships. AR: linear.</p></li>
</ul>
</section>
<section id="long-short-term-memory-lstm-a-more-powerful-rnn" class="level2">
<h2 class="anchored" data-anchor-id="long-short-term-memory-lstm-a-more-powerful-rnn">Long Short-Term Memory (LSTM): A More Powerful RNN</h2>
<ul>
<li><p><strong>Addressing Vanishing Gradients:</strong> LSTMs address ‚Äúvanishing gradient‚Äù problem (in standard RNNs with long sequences).</p></li>
<li><p><strong>Two Hidden States:</strong></p>
<ul>
<li><strong>Short-term memory:</strong> Like standard RNN.</li>
<li><strong>Long-term memory:</strong> Retains information longer.</li>
</ul></li>
<li><p><strong>Improved Performance:</strong> LSTMs often better, especially for long sequences.</p></li>
</ul>
</section>
<section id="fitting-neural-networks-the-optimization-challenge" class="level2">
<h2 class="anchored" data-anchor-id="fitting-neural-networks-the-optimization-challenge">Fitting Neural Networks: The Optimization Challenge</h2>
<ul>
<li><p><strong>Complex Optimization:</strong> Fitting (finding optimal weights) is non-convex.</p></li>
<li><p><strong>Local Minima:</strong> Loss function has many local minima.</p></li>
<li><p><strong>Key Techniques:</strong></p>
<ul>
<li><strong>Gradient Descent:</strong> Iteratively adjust weights ‚Äúdownhill.‚Äù</li>
<li><strong>Backpropagation:</strong> Efficient gradient computation.</li>
<li><strong>Regularization:</strong> Prevent overfitting (ridge, lasso, dropout).</li>
<li><strong>Stochastic Gradient Descent (SGD):</strong> Small batches for faster, robust optimization.</li>
<li><strong>Early Stopping:</strong> Stop training when validation error starts to increase.</li>
</ul></li>
</ul>
</section>
<section id="gradient-descent-illustration" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent-illustration">Gradient Descent: Illustration</h2>
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_17.svg" class="img-fluid"></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Gradient descent is like rolling a ball down a hill. The ball will eventually settle at the lowest point (the minimum). The learning rate controls how big of a step the ball takes at each iteration. A large learning rate might cause the ball to overshoot, while a small learning rate might make the descent slow.</p>
</div>
</div>
</section>
<section id="gradient-descent-the-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent-the-algorithm">Gradient Descent: The Algorithm</h2>
<ul>
<li><p><strong>Iterative Algorithm:</strong> Gradient descent is iterative.</p></li>
<li><p><strong>Goal:</strong> Find parameters (<span class="math inline">\(\theta\)</span>) that <em>minimize</em> loss <span class="math inline">\(R(\theta)\)</span>.</p></li>
<li><p><strong>Steps:</strong></p>
<ol type="1">
<li><p><strong>Initialize</strong> <span class="math inline">\(\theta\)</span> (often randomly).</p></li>
<li><p><strong>Repeatedly update</strong> <span class="math inline">\(\theta\)</span> opposite the gradient:</p>
<p><span class="math display">\[
\theta^{(m+1)} \leftarrow \theta^{(m)} - \rho \nabla R(\theta^{(m)})
\]</span></p></li>
<li><p><span class="math inline">\(\rho\)</span>: <em>learning rate</em> (step size). Smaller: smaller steps (slower). Larger: larger steps (might overshoot).</p></li>
</ol></li>
</ul>
</section>
<section id="backpropagation-efficient-gradient-computation" class="level2">
<h2 class="anchored" data-anchor-id="backpropagation-efficient-gradient-computation">Backpropagation: Efficient Gradient Computation</h2>
<ul>
<li><p><strong>Efficient Gradient Calculation:</strong> Backpropagation is efficient.</p></li>
<li><p><strong>Chain Rule:</strong> Uses chain rule to ‚Äúpropagate‚Äù error backward.</p></li>
<li><p><strong>Essential for Training:</strong> Essential; without it, training is slow.</p></li>
</ul>
</section>
<section id="regularization-preventing-overfitting" class="level2">
<h2 class="anchored" data-anchor-id="regularization-preventing-overfitting">Regularization: Preventing Overfitting</h2>
<ul>
<li><p><strong>Purpose:</strong> Prevent <em>overfitting</em> (model learns training data <em>too</em> well, performs poorly on unseen data).</p></li>
<li><p><strong>Methods:</strong></p>
<ul>
<li><strong>Ridge/Lasso:</strong> Penalty for large weights.</li>
<li><strong>Dropout:</strong> Randomly ‚Äúdrop out‚Äù units during training (more robust features).</li>
<li><strong>Early Stopping:</strong> Monitor <em>validation set</em> performance. Stop when validation error increases (even if training error improves).</li>
</ul></li>
</ul>
</section>
<section id="stochastic-gradient-descent-sgd-speeding-up-training" class="level2">
<h2 class="anchored" data-anchor-id="stochastic-gradient-descent-sgd-speeding-up-training">Stochastic Gradient Descent (SGD): Speeding Up Training</h2>
<ul>
<li><p><strong>Minibatches:</strong> Use small <em>batches</em> of data (not entire dataset) for gradient.</p></li>
<li><p><strong>Faster Updates:</strong> Much faster weight updates.</p></li>
<li><p><strong>Randomness:</strong> Randomness helps escape local minima.</p></li>
<li><p><strong>Standard Approach:</strong> SGD (and variants) is standard.</p></li>
</ul>
</section>
<section id="training-and-validation-errors" class="level2">
<h2 class="anchored" data-anchor-id="training-and-validation-errors">Training and Validation Errors</h2>
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_18.svg" class="img-fluid"></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>It‚Äôs essential to monitor both training and validation error during training. The training error typically decreases as the model learns. The validation error initially decreases, but if it starts to <em>increase</em>, it indicates <em>overfitting</em>. Early stopping is a form of regularization that stops the training process before overfitting becomes significant.</p>
</div>
</div>
</section>
<section id="dropout-learning-illustration" class="level2">
<h2 class="anchored" data-anchor-id="dropout-learning-illustration">Dropout Learning: Illustration</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_19.svg" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Dropout randomly removes units (neurons) during training. This forces the network to learn more robust features and prevents it from relying too heavily on any single neuron. It‚Äôs like training multiple different networks and averaging their predictions.</p>
</div>
</div>
</section>
<section id="network-tuning-finding-the-right-architecture" class="level2">
<h2 class="anchored" data-anchor-id="network-tuning-finding-the-right-architecture">Network Tuning: Finding the Right Architecture</h2>
<ul>
<li><p><strong>Architecture and Hyperparameters:</strong> Choosing architecture (layers, units, activations) and hyperparameters (learning rate, batch size, regularization) is crucial.</p></li>
<li><p><strong>Key Considerations:</strong></p>
<ul>
<li>Number of hidden layers.</li>
<li>Units per layer.</li>
<li>Regularization (dropout, ridge/lasso).</li>
<li>Learning rate.</li>
<li>Batch size.</li>
<li>Epochs (passes through training data).</li>
</ul></li>
<li><p><strong>Trial and Error:</strong> Finding optimal settings: trial and error (time-consuming). Systematic: grid search, random search. Advanced: Bayesian optimization.</p></li>
</ul>
</section>
<section id="interpolation-and-double-descent-a-surprising-phenomenon" class="level2">
<h2 class="anchored" data-anchor-id="interpolation-and-double-descent-a-surprising-phenomenon">Interpolation and Double Descent: A Surprising Phenomenon</h2>
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F10_20.svg" class="img-fluid"></p>
<ul>
<li><p><strong>Interpolation:</strong> Model <em>perfectly</em> fits training data (zero training error).</p></li>
<li><p><strong>Double Descent:</strong> Test error <em>decreases</em> again after increasing (as complexity increases beyond interpolation).</p></li>
<li><p><strong>Not a Contradiction:</strong> <em>Does not</em> contradict bias-variance tradeoff.</p></li>
</ul>
</section>
<section id="double-descent-explanation" class="level2">
<h2 class="anchored" data-anchor-id="double-descent-explanation">Double Descent: Explanation</h2>
<ul>
<li>Test error shows the U-shape at first, then decreases as model complexity further increases.</li>
<li>Regularization is helpful to reduce test error.</li>
<li>Does <em>not</em> contradict the bias-variance tradeoff. The classical bias-variance tradeoff applies within a fixed model class. Double descent occurs when we consider a <em>sequence</em> of increasingly complex models.</li>
</ul>
</section>
<section id="when-to-use-deep-learning-choosing-the-right-tool" class="level2">
<h2 class="anchored" data-anchor-id="when-to-use-deep-learning-choosing-the-right-tool">When to Use Deep Learning: Choosing the Right Tool</h2>
<ul>
<li><p><strong>Large Datasets:</strong> Deep learning shines with <em>lots</em> of data. Small datasets: simpler models might be better (interpretable).</p></li>
<li><p><strong>Complex Relationships:</strong> Highly non-linear, complex: deep learning effective.</p></li>
<li><p><strong>Difficult Feature Engineering:</strong> Deep learning automates feature learning.</p></li>
<li><p><strong>Interpretability is Less Important:</strong> Deep learning: ‚Äúblack boxes.‚Äù Interpretability crucial: simpler models.</p></li>
<li><p><strong>Computational Resources:</strong> Training: computationally expensive (GPUs).</p></li>
</ul>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<ul>
<li><strong>Deep Learning Power:</strong> Powerful techniques (neural networks), learning complex patterns.</li>
<li><strong>CNNs and RNNs:</strong> CNNs: images. RNNs: sequential data.</li>
<li><strong>Optimization Challenges:</strong> Complex, but software simplifies.</li>
<li><strong>Regularization is Key:</strong> Prevent overfitting, improve generalization.</li>
<li><strong>Data and Complexity:</strong> Excels with large data, complex relationships.</li>
<li><strong>Consider Simpler Models:</strong> Simpler model, good performance, interpretable: might be better.</li>
</ul>
</section>
<section id="thoughts-and-discussion" class="level2">
<h2 class="anchored" data-anchor-id="thoughts-and-discussion">Thoughts and Discussion</h2>
<ul>
<li><p><strong>Ethical Implications:</strong> Ethics of deep learning (facial recognition, loans, hiring)? Fairness, avoid bias?</p></li>
<li><p><strong>Interpretability:</strong> How to make models more interpretable? Why is it important?</p></li>
<li><p><strong>Limitations:</strong> Limitations of deep learning? When are other methods (decision trees, SVMs) better?</p></li>
<li><p><strong>Future of Deep Learning:</strong> Future evolution? New architectures, applications?</p></li>
<li><p><strong>Model Complexity</strong>: How to decide model complexity? What will happen if we use a too simple or too complex model?</p></li>
<li><p><strong>Data Quality</strong>: What are the actions to ensure data quality, and remove bias in data sets?</p></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/qiufei\.github\.io\/web-slide-r");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>üîã<a href="https://posit.co"><img src="https://posit.co/wp-content/themes/Posit/assets/images/posit-logo-2024.png" class="img-fluid" alt="Posit" width="65"></a></p>
</div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 ÈÇ±È£û ¬© 2025
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://beian.miit.gov.cn">
<p>ÊµôICPÂ§á 2024072710Âè∑-1</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33021202002511">
<p>ÊµôÂÖ¨ÁΩëÂÆâÂ§á 33021202002511Âè∑</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:hfutqiufei@163.com">
      <i class="bi bi-envelope-at-fill" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>