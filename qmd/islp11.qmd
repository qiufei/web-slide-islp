## Introduction

::: {style="font-size: 0.8em;"}
Welcome to the fascinating world of **survival analysis**! 🕰️ In this chapter, we delve into analyzing a unique type of data: the *time until an event occurs*. This is different from typical regression problems because of a crucial concept: **censoring**.

Think of a medical study tracking patient survival after cancer treatment. Some patients might still be alive at the study's end. We know they survived *at least* that long, but not their exact survival time. This is censored data.

**Key Concepts:**
- **Survival Analysis:** Statistical methods for analyzing time-to-event data.
- **Censored Data:** Observations where the event of interest has not occurred for all subjects by the end of the observation period.
- **Event:** The outcome of interest (e.g., death, recovery, machine failure, customer churn).
- **Survival Time:** The time until the event occurs.

We will explore how to deal with censoring and effectively extract information using tools like survival analysis.
:::

## Why Survival Analysis?

::: {style="font-size: 0.8em;"}
Survival analysis isn't limited to medical studies. It's relevant in many fields:

-   **Medicine:** Predicting patient survival times, time to disease recurrence.
-   **Business:** Modeling customer churn (time until a customer cancels a subscription).
-   **Engineering:** Assessing the reliability of components (time until failure).
-   **Finance:** Evaluating credit risk (time until default).
- **Even beyond time:** Modeling weight when scales have upper limits. Any weight above that limit is censored.

::: incremental
- **Key Insight:** Survival analysis techniques allows us to work with *incomplete* information. We don't need to observe the event for every individual to gain valuable insights.
:::
:::

## Survival and Censoring Times

::: {style="font-size: 0.8em;"}
Let's define some core concepts:

-   **Survival Time (T):** The true time until the event of interest occurs. Also called *failure time* or *event time*.
-   **Censoring Time (C):** The time at which observation ends, either because the study ends, the patient drops out, or the event occurs.
-   **Observed Time (Y):** What we actually see: either the survival time or the censoring time. Mathematically,  Y = min(T, C).
-   **Status Indicator (δ):** Tells us whether we observed the event (δ = 1) or the observation was censored (δ = 0).

$$
\delta =
\begin{cases}
1 & \text{if } T \leq C \text{ (event observed)} \\
0 & \text{if } T > C \text{ (censored)}
\end{cases}
$$

These variables form the basis of survival analysis.
:::

## Visualizing Censored Data

::: {style="font-size: 0.8em;"}
Let's consider a simple example:

![alt text](figures/11_1.png)

::: fragment
-   **Patients 1 & 3:** The event (e.g., death) was *observed*. We know their exact survival times.
-   **Patient 2:** Was *alive* at the end of the study. Their survival time is censored.
-   **Patient 4:** *Dropped out* of the study. Also censored.
:::

::: fragment
**Important:** Censored observations still provide valuable information! They tell us the event *didn't* happen before a certain time.
:::
:::

## A Closer Look at Censoring

::: {style="font-size: 0.8em;"}
Censoring isn't always straightforward. The *reason* for censoring matters.

::: nonincremental
-   **Independent Censoring:** The censoring mechanism is unrelated to the survival time (conditional on features). This is a crucial assumption for many survival analysis methods.
-   **Example of violation:** Patients dropping out because they are very sick. This biases the results, making survival times seem longer than they are.
:::

::: {layout-ncol=2}
**Types of Censoring**

-   **Right Censoring:** Most common.  We know the event time is *greater than* the observed time (T ≥ Y).
-   **Left Censoring:** We know the event time is *less than or equal* to the observed time.
-   **Interval Censoring:** We know the event time falls within a specific interval.

![alt text](figures/11_1.png)
:::

::: callout-note
We will focus mainly on **right censoring**, the most prevalent type in practice.
:::
:::

## The Kaplan-Meier Survival Curve

::: {style="font-size: 0.8em;"}
The **survival curve**, denoted by S(t), is a fundamental concept. It gives the probability of surviving *past* time t:

$$S(t) = Pr(T > t)$$

::: fragment
The larger the value of S(t), the more likely the event will occur at time greater than t.
:::

::: fragment
How do we estimate S(t) from data with censoring? The **Kaplan-Meier estimator** is a powerful tool.
:::
:::

## Estimating the Survival Curve: Challenges

::: {style="font-size: 0.8em;"}
Let's consider the `BrainCancer` dataset. We want to estimate S(20): the probability of surviving at least 20 months.  Naive approaches fail:

1.  **Simply using Y > 20:**  This ignores that Y is not always the true survival time (due to censoring).  It underestimates survival.
2.  **Ignoring censored observations:** This throws away valuable information. A patient censored at 19.9 months almost certainly would have survived past 20.

::: callout-tip
The Kaplan-Meier estimator elegantly handles censoring to provide a more accurate estimate.
:::
:::

## The Kaplan-Meier Estimator: Intuition

::: {style="font-size: 0.8em;"}
The Kaplan-Meier estimator works *sequentially*, considering events as they unfold in time.

::: {layout-ncol=2}
-   **Key Idea:** At each death time, we calculate the *conditional* probability of surviving that time point, given survival up to that point.
-   We then multiply these conditional probabilities together to get the overall survival probability.

- Let $d_1 < d_2 < ... < d_K$ be the distinct death times.
- $q_k$: Number of deaths at time $d_k$.
- $r_k$: Number of individuals *at risk* (alive and in the study) just before $d_k$. This is the *risk set*.
:::

::: fragment
The Kaplan-Meier estimator formula is:
:::

::: fragment
$$
\hat{S}(d_k) = \prod_{j=1}^{k} \left( \frac{r_j - q_j}{r_j} \right)
$$
:::

::: fragment
For times between death times, $\hat{S}(t)$ remains constant, creating a step-like curve.
:::
:::

## The Kaplan-Meier Estimator: Explanation

::: {style="font-size: 0.8em;"}

The formula is derived from the law of total probability:

$$
Pr(T > d_k) = Pr(T > d_k | T > d_{k-1})Pr(T > d_{k-1}) + Pr(T>d_k|T \leq d_{k-1})Pr(T\leq d_{k-1})
$$

::: fragment
Since $d_{k-1} < d_k$, $Pr(T>d_k|T \leq d_{k-1}) = 0$, then the above formula is:
$$
S(d_k) = Pr(T > d_k) = Pr(T > d_k | T > d_{k-1})Pr(T > d_{k-1})
$$
:::

::: fragment
Plug in $S(t)$ and rearrange the above formula:
$$
S(d_k) = Pr(T>d_k|T>d_{k-1}) \times ... \times Pr(T>d_2|T>d_1)Pr(T>d_1)
$$
:::

::: fragment
We estimate each term on the right-hand side using the fraction of the risk set at time $d_j$ who survived past time $d_j$:

$$\widehat{Pr}(T > d_j | T > d_{j-1}) = (r_j - q_j) / r_j$$
:::

::: fragment
Finally, we arrive at the Kaplan-Meier estimator:
:::

::: fragment
$$
\hat{S}(d_k) = \prod_{j=1}^{k} \left( \frac{r_j - q_j}{r_j} \right)
$$
:::
:::

## Kaplan-Meier Curve: Example

::: {style="font-size: 0.8em;"}
Here's the Kaplan-Meier curve for the `BrainCancer` data:

![alt text](figures/11_2.png)

-   The curve steps down at each observed death time.
-   The height of the curve at any time point represents the estimated survival probability.
-   The estimated probability of survival past 20 months is 71%.
:::

## Comparing Survival Curves: The Log-Rank Test

::: {style="font-size: 0.8em;"}
Often, we want to compare survival curves between groups (e.g., males vs. females).

![alt text](figures/11_3.png)

::: fragment
The **log-rank test** is a statistical test for comparing survival curves. It accounts for censoring.
:::
:::

## Log-Rank Test: Details

::: {style="font-size: 0.7em;"}
The log-rank test examines events sequentially, like the Kaplan-Meier estimator.

::: {layout-ncol=2}
- At each death time $d_k$, we construct a 2x2 table:

|             | Group 1   | Group 2   | Total    |
|-------------|-----------|-----------|----------|
| Died        | $q_{1k}$  | $q_{2k}$  | $q_k$    |
| Survived    | $r_{1k}-q_{1k}$ | $r_{2k}-q_{2k}$ | $r_k-q_k$ |
| Total       | $r_{1k}$  | $r_{2k}$  | $r_k$    |
|             |           |           |           |

- $r_{1k}$, $r_{2k}$: Number at risk in each group at time $d_k$.
- $q_{1k}$, $q_{2k}$: Number of deaths in each group at time $d_k$.

-   **Key Idea:** If there's no difference in survival, we'd expect the *proportion* of deaths in each group to be proportional to the number at risk.
:::

::: fragment
The log-rank test statistic (W) is calculated based on the *observed* and *expected* number of deaths in group 1:

$$
W = \frac{\sum_{k=1}^{K}(q_{1k} - \mu_k)}{\sqrt{\sum_{k=1}^{K}Var(q_{1k})}}
$$
where
$ \mu_k = \frac{r_{1k}}{r_k}q_k $
and
$Var(q_{1k}) = \frac{q_k(r_{1k}/r_k)(1-r_{1k}/r_k)(r_k - q_k)}{r_k - 1}$
:::

::: fragment
Under the null hypothesis (no difference in survival), W approximately follows a standard normal distribution.
:::
:::

## Log-Rank Test: Brain Cancer Example

::: {style="font-size: 0.8em;"}
Comparing survival times of males and females in the `BrainCancer` data:

-   Log-rank test statistic W = 1.2.
-   Two-sided p-value = 0.2 (using the theoretical null distribution).
-   We *cannot* reject the null hypothesis of no difference in survival curves between males and females.
:::

## Regression Models with a Survival Response

::: {style="font-size: 0.8em;"}
So far, we've looked at describing survival curves and comparing them between groups. Now, we want to *predict* survival time based on covariates (features).

::: {layout-ncol=2}
-   We have observations of the form $(Y_i, \delta_i, X_i)$, where:
    -   $Y_i$ is the observed time (min(T, C)).
    -   $\delta_i$ is the status indicator.
    -   $X_i$ is a vector of features.

- A simple linear regression of log(Y) on X is problematic due to censoring.

- *Why not regress on Y directly?*: We are interested in T not Y.
:::

::: fragment
-   **Solution:** Use a sequential approach, similar to Kaplan-Meier and the log-rank test.  We introduce the **hazard function**.
:::
:::

## The Hazard Function

::: {style="font-size: 0.8em;"}
The **hazard function**, h(t), is also known as the *hazard rate* or *force of mortality*. It represents the *instantaneous* risk of the event occurring at time t, given survival up to time t:

$$h(t) = \lim_{\Delta t \to 0} \frac{Pr(t < T \leq t + \Delta t | T > t)}{\Delta t}$$

::: fragment
-   Think of it as the "death rate" in a tiny interval after time t, given survival up to t.
-   It's closely related to the survival curve, S(t).
-   It's crucial for modeling survival data as a function of covariates.
:::
:::

## Hazard Function: More Details
::: {style="font-size: 0.8em;"}

$$
\begin{aligned}
h(t) &= \lim_{\Delta t \to 0} Pr((t<T\le t+\Delta t)\cap (T>t))/\Delta t \over Pr(T>t) \\
&= \lim_{\Delta t \to 0} {Pr(t<T\le t+\Delta t) / \Delta t \over Pr(T>t)} \\
&= {f(t) \over S(t)}
\end{aligned}
$$

where
$$
f(t) = \lim_{\Delta t\to 0} {Pr(t<T\le t+\Delta t)\over \Delta t}
$$

::: fragment
$f(t)$ is probability density function.
:::

::: fragment
The likelihood associated with the i-th observation is:

$$
L_i = \begin{cases}
f(y_i) \quad \text{if the i-th observation is not censored} \\
S(y_i) \quad \text{if the i-th observation is censored}
\end{cases} \\
= f(y_i)^{\delta_i}S(y_i)^{1-\delta_i}
$$
:::

::: fragment
If $Y=y_i$ and the i-th observation is not censored, then the likelihood is the probability of dying in a tiny interval around time $y_i$. If the i-th observation is censored, then the likelihood is the probability of surviving at least until time $y_i$
:::
:::

## Cox Proportional Hazards Model

::: {style="font-size: 0.8em;"}
The **Cox proportional hazards model** is a powerful and flexible approach to model the relationship between covariates and the hazard function.

::: fragment
**The Proportional Hazards Assumption:**
:::

$$h(t|x_i) = h_0(t) \exp(\sum_{j=1}^{p} x_{ij}\beta_j)$$

::: {layout-ncol=2}
-   $h(t|x_i)$: Hazard function for an individual with features $x_i$.
-   $h_0(t)$:  **Baseline hazard function**.  This is the hazard for an individual with all features equal to zero. It's left *unspecified*.
-   $\exp(\sum_{j=1}^{p} x_{ij}\beta_j)$:  **Relative risk**. It's a multiplicative factor that scales the baseline hazard based on the features.

- The key is that we don't assume a specific form for $h_0(t)$.  This makes the model very flexible.
- A one-unit increase in $x_{ij}$ multiplies the hazard by a factor of $\exp(\beta_j)$.
:::
:::

## Proportional Hazards: Illustration

::: {style="font-size: 0.8em;"}
![alt text](figures/11_4.png)

- **Top Row:** Proportional hazards *holds*. Log hazard functions are parallel; survival curves don't cross.
- **Bottom Row:** Proportional hazards *doesn't hold*.  Log hazard and survival curves cross.
:::

## Cox Proportional Hazards Model: Estimation

::: {style="font-size: 0.7em;"}
How do we estimate the coefficients, $\beta$, in the Cox model *without* knowing $h_0(t)$?  We use the **partial likelihood**.

::: {layout-ncol=2}
- Assume no ties in failure times.
- Consider the *i*th observation, which *fails* at time $y_i$ ($\delta_i = 1$).
- What's the probability that *this* observation fails at $y_i$, given the set of individuals at risk at that time?

- The probability that i-th observation is the one to fail at time $y_i$ is:

$$
\frac{h_0(y_i) \exp(\sum_{j=1}^p x_{ij}\beta_j)}{\sum_{i':y_{i'}\ge y_i}h_0(y_i)\exp(\sum_{j=1}^{p}x_{i'j}\beta_j)} = \frac{\exp(\sum_{j=1}^p x_{ij}\beta_j)}{\sum_{i':y_{i'}\ge y_i}\exp(\sum_{j=1}^{p}x_{i'j}\beta_j)}
$$
:::

::: fragment
- Crucially, $h_0(y_i)$ cancels out!
- The **partial likelihood** is the product of these probabilities over all *uncensored* observations:
:::

::: fragment
$$PL(\beta) = \prod_{i:\delta_i = 1} \frac{\exp(\sum_{j=1}^p x_{ij}\beta_j)}{\sum_{i':y_{i'}\ge y_i}\exp(\sum_{j=1}^{p}x_{i'j}\beta_j)}$$
:::

::: fragment
- We estimate $\beta$ by maximizing the partial likelihood.
- This is done numerically (no closed-form solution).
:::
:::

## Cox Model: Example (Brain Cancer Data)

::: {style="font-size: 0.8em;"}
Let's apply the Cox model to the `BrainCancer` data:

| Variable                | Coefficient | Std. error | z-statistic | p-value |
|-------------------------|-------------|------------|-------------|---------|
| sex[Male]               | 0.18        | 0.36       | 0.51        | 0.61    |
| diagnosis[LG Glioma]    | 0.92        | 0.64       | 1.43        | 0.15    |
| diagnosis[HG Glioma]    | 2.15        | 0.45       | 4.78        | 0.00    |
| diagnosis[Other]        | 0.89        | 0.66       | 1.35        | 0.18    |
| loc[Supratentorial]     | 0.44        | 0.70       | 0.63        | 0.53    |
| ki                      | -0.05       | 0.02       | -3.00       | <0.01   |
| gtv                     | 0.03        | 0.02       | 1.54        | 0.12    |
| stereo[SRT]             | 0.18        | 0.60       | 0.30        | 0.77    |

::: {layout-ncol=2}
-   **Interpretation:**
    -   Males have an estimated hazard 1.2 times greater than females (e<sup>0.18</sup>), but this is not statistically significant.
    -   Higher Karnofsky index (ki) is associated with a *lower* hazard (e<sup>-0.05</sup> = 0.95), and this effect is significant.

- The p-value associated with a coefficient tests the null hypothesis that the coefficient is zero.
:::
:::

## Cox Model: Example (Publication Data)
::: {style="font-size: 0.8em;"}
Next, we will introduce the dataset `Publication`, involving the time to publication of journal papers reporting the results of clinical trials funded by the National Heart, Lung, and Blood Institue. For 244 trials, the time in months until publication is recorded.
:::
::: {style="font-size: 0.8em;"}

::: {layout-ncol=2}
![alt text](figures/11_5.png)

- Using the log-rank test, we can test whether the studies with positive results have significant difference in publication time.

- The figure shows slight evidence that time until publication is lower for studies with a positive result.
- However, the log-rank test yields a very unimpressive p-value of 0.36.

:::
::: {style="font-size: 0.8em;"}
Now, let's fit Cox's proportional hazards model using all available features:

| Variable          | Coefficient | Std. error | z-statistic | p-value |
|-------------------|-------------|------------|-------------|---------|
| posres[Yes]       | 0.55        | 0.18       | 3.02        | 0.00    |
| multi[Yes]        | 0.15        | 0.31       | 0.47        | 0.64    |
| clinend[Yes]      | 0.51        | 0.27       | 1.89        | 0.06    |
| mech[K01]         | 1.05        | 1.06       | 1.00        | 0.32    |
| mech[K23]         | -0.48       | 1.05       | -0.45       | 0.65    |
| mech[P01]         | -0.31       | 0.78       | -0.40       | 0.69    |
| mech[P50]         | 0.60        | 1.06       | 0.57        | 0.57    |
| mech[R01]         | 0.10        | 0.32       | 0.30        | 0.76    |
| mech[R18]         | 1.05        | 1.05       | 0.99        | 0.32    |
| mech[R21]         | -0.05       | 1.06       | -0.04       | 0.97    |
| mech[R24, K24]    | 0.81        | 1.05       | 0.77        | 0.44    |
| mech[R42]         | -14.78      | 3414.38    | -0.00       | 1.00    |
| mech[R44]         | -0.57       | 0.77       | -0.73       | 0.46    |
| mech[RC2]         | -14.92      | 2243.60    | -0.01       | 0.99    |
| mech[U01]         | -0.22       | 0.32       | -0.70       | 0.48    |
| mech[U54]         | 0.47        | 1.07       | 0.44        | 0.66    |
| sampsize          | 0.00        | 0.00       | 0.19        | 0.85    |
| budget            | 0.00        | 0.00       | 1.67        | 0.09    |
| impact            | 0.06        | 0.01       | 8.23        | 0.00    |

- We find that the chance of publication of a study with a positive result is $e^{0.55} = 1.74$ time higher than the chance of publication of a study with a negative result at any point in time, holding all other covariates fixed.

:::
:::

## Cox Model: Example (Publication Data) - Adjusted Curves

::: {style="font-size: 0.8em;"}
![alt text](figures/11_6.png)

- After *adjusting* for other covariates (using representative values), we see a much clearer difference in survival curves between positive and negative results.
- This highlights the importance of considering multiple predictors.
:::

## Shrinkage for the Cox Model

::: {style="font-size: 0.8em;"}
We can apply shrinkage methods (like ridge and lasso) to the Cox model.

::: {layout-ncol=2}
-   **Idea:**  Minimize a penalized version of the negative log partial likelihood:

    $$-\log\left(\prod_{i:\delta_i=1} \frac{\exp(\sum_{j=1}^p x_{ij}\beta_j)}{\sum_{i':y_{i'}\ge y_i}\exp(\sum_{j=1}^{p}x_{i'j}\beta_j)}\right) + \lambda P(\beta)$$

    -   $\lambda$: Tuning parameter.
    -   $P(\beta)$: Penalty term (e.g., lasso: $\sum_{j=1}^p |\beta_j|$).

- We now apply lasso-penalized Cox model to the `Publication` data.

- The figure on the right hand displays the cross-validation results.
- Note the "U-shape" of the partial likelihood deviance.
- Specifically, the cross-validation error is minimized when just two predictors, `budget` and `impact`, have non-zero estimated coefficients.

![Partial likelihood deviance](figures/11_7.png)
:::
:::

## Assessing Model Fit on Test Data
::: {style="font-size: 0.8em;"}
We can use risk score to categorize the observations based on their "risk". For example, we use the risk score:
$$
budget_i \cdot \hat{\beta}_{budget} + impact_i \cdot \hat{\beta}_{impact}
$$
where $\hat{\beta}_{budget}$ and $\hat{\beta}_{impact}$ are the coefficients estimates for these two features from the training set.

![alt text](figures/11_8.png)

The figure shows that there is clear seperation between the three strata, and that the strata are correctly ordered in terms of low, medium, and high risk of publication.
:::

## Additional Topics

::: {style="font-size: 0.8em;"}
-   **Area Under the Curve (AUC) for Survival Analysis:**  Harrell's concordance index (C-index) generalizes AUC to survival data, accounting for censoring.
-   **Choice of Time Scale:** The definition of "time zero" can be crucial and depends on the context.
-   **Time-Dependent Covariates:** The Cox model can handle predictors that change over time.
-   **Checking the Proportional Hazards Assumption:**  Visual checks (log hazard plots) and stratification can help assess the assumption.
-   **Survival Trees:** Tree-based methods can be adapted for survival analysis.
:::

## Summary

::: {style="font-size: 0.8em;"}
-   Survival analysis deals with *time-to-event* data, where **censoring** is a key challenge.
-   The **Kaplan-Meier estimator** provides a non-parametric estimate of the survival curve.
-   The **log-rank test** compares survival curves between groups.
-   The **Cox proportional hazards model** allows us to model the relationship between covariates and the *hazard function*, making it a powerful tool for prediction.
-   Shrinkage methods can be applied to the Cox model.
-   Various extensions and considerations (AUC, time scale, time-dependent covariates, proportional hazards assumption, survival trees) broaden the applicability of survival analysis.
:::

## Thoughts and Discussion

::: {style="font-size: 0.8em;"}
-   How might survival analysis be applied in *your* field of interest?
-   What are the ethical considerations when dealing with survival data, especially in medical contexts?
-   How could you explain the concept of censoring to someone without a statistical background?
-   Can you think of situations where the proportional hazards assumption might be violated? How could you address this?
-   What are the limitations of survival analysis? What types of questions can it *not* answer?
:::

