<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Unsupervised Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<link href="../https://assets.qiufei.site/personal/profile.jpg" rel="icon" type="image/jpeg">
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-8ea72dc5fed832574809a9c94082fbbb.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-681fbf911679f9b3dbf9743eb275ba49.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-7e49aeac8059a213a463aa1a739e8272.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://qiufei.github.io"> 
<span class="menu-text">È¶ñÈ°µ</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Unsupervised Learning</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction-what-is-statistical-learning" class="level2">
<h2 class="anchored" data-anchor-id="introduction-what-is-statistical-learning">Introduction: What is Statistical Learning?</h2>
<p>Statistical learning refers to a set of tools and techniques used to understand and extract insights from data. It‚Äôs a broad field encompassing both supervised and unsupervised learning approaches. The goal is to build models that can either predict an outcome (supervised) or discover patterns (unsupervised) in data. üìä</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Key Idea:</strong> Statistical learning provides a framework for using data to gain knowledge and make predictions.</p>
</div>
</div>
</div>
</section>
<section id="supervised-vs.-unsupervised-learning-overview" class="level2">
<h2 class="anchored" data-anchor-id="supervised-vs.-unsupervised-learning-overview">Supervised vs.&nbsp;Unsupervised Learning: Overview</h2>
<p>Let‚Äôs start by understanding the fundamental difference between supervised and unsupervised learning. Think of it like learning with a teacher versus learning on your own.</p>
<ul>
<li><strong>Supervised Learning:</strong> We have a ‚Äúteacher‚Äù (the response variable, Y) guiding the learning process. üë®‚Äçüè´ The goal is to learn a function that maps inputs to outputs.</li>
<li><strong>Unsupervised Learning:</strong> We‚Äôre exploring the data ‚Äúwithout a teacher‚Äù to discover hidden patterns. üïµÔ∏è‚Äç‚ôÄÔ∏è There‚Äôs no explicit output variable to predict.</li>
</ul>
</section>
<section id="supervised-learning" class="level2">
<h2 class="anchored" data-anchor-id="supervised-learning">Supervised Learning</h2>
<ul>
<li><strong>Supervised Learning:</strong> We have a set of features (X1, X2, ‚Ä¶, Xp) and a response variable (Y). The goal is to <em>predict</em> Y using the Xs. We ‚Äúteach‚Äù the algorithm by providing examples of inputs and their corresponding outputs.</li>
<li><strong>Examples:</strong> Linear regression (predicting house prices), logistic regression (predicting customer churn), Support Vector Machine (SVM) (image classification).</li>
</ul>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
    A[Features (X)] --&gt; B(Model);
    C[Response (Y)] --&gt; B;
    B --&gt; D[Predictions];
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>The model learns the relationship between features (X) and the response (Y).</p>
</div>
</div>
</div>
</section>
<section id="unsupervised-learning" class="level2">
<h2 class="anchored" data-anchor-id="unsupervised-learning">Unsupervised Learning</h2>
<ul>
<li><strong>Unsupervised Learning:</strong> We only have features (X1, X2, ‚Ä¶, Xp), <em>without</em> any response variable Y. No ‚Äúteaching‚Äù or ‚Äúsupervision‚Äù ‚Äì the algorithm explores the data on its own.</li>
<li><strong>Goal:</strong> Discover <em>interesting patterns</em> and <em>structure</em> in the data; find relationships among the features and/or observations.</li>
<li><strong>Examples:</strong> Principal Component Analysis (PCA) (dimensionality reduction), Clustering (finding groups of similar customers).</li>
</ul>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
    A[Features (X)] --&gt; B(Model);
    B --&gt; C[Patterns &amp; Structure];
style A fill:#f9f,stroke:#333,stroke-width:2px
style B fill:#ccf,stroke:#333,stroke-width:2px
style C fill:#ccf,stroke:#333,stroke-width:2px
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>The model identifies inherent structure within the features (X).</p>
</div>
</div>
</div>
</section>
<section id="what-is-unsupervised-learning-detailed" class="level2">
<h2 class="anchored" data-anchor-id="what-is-unsupervised-learning-detailed">What is Unsupervised Learning? (Detailed)</h2>
<p>Unsupervised learning is a collection of statistical methods used when we only have input data (features) and no corresponding output (response variable). Because there‚Äôs no ‚Äúcorrect answer‚Äù to guide the analysis, we call it ‚Äúunsupervised.‚Äù We‚Äôre essentially detectives, exploring the data to uncover hidden relationships and structures. üîç It‚Äôs about learning the underlying structure of the data itself.</p>
</section>
<section id="key-goals-of-unsupervised-learning" class="level2">
<h2 class="anchored" data-anchor-id="key-goals-of-unsupervised-learning">Key Goals of Unsupervised Learning</h2>
<p>Unsupervised learning serves several important purposes:</p>
<ul>
<li><strong>Data Visualization:</strong> Finding ways to represent complex, high-dimensional data in a visually intuitive manner, often by reducing its dimensionality. Think of it like creating a map of your data, making it easier to navigate. üó∫Ô∏è</li>
<li><strong>Discover Subgroups:</strong> Identifying clusters or groups within the data. This could be groups of customers with similar buying habits, or groups of genes with related functions. üßë‚Äçü§ù‚Äçüßë This helps us understand the heterogeneity within the data.</li>
<li><strong>Data Pre-processing:</strong> Preparing data for supervised learning techniques. For example, we might use unsupervised learning to reduce the number of features or to create new, more informative features before applying a supervised learning algorithm. ‚öôÔ∏è This can improve the performance and efficiency of supervised models.</li>
</ul>
</section>
<section id="types-of-unsupervised-learning" class="level2">
<h2 class="anchored" data-anchor-id="types-of-unsupervised-learning">Types of Unsupervised Learning</h2>
<p>We‚Äôll focus on two primary types of unsupervised learning:</p>
<ol type="1">
<li><strong>Principal Components Analysis (PCA):</strong> Primarily used for data visualization and dimensionality reduction, making complex data easier to understand by finding the most important ‚Äúdirections‚Äù in the data.</li>
<li><strong>Clustering:</strong> Used to discover unknown subgroups or clusters within a dataset, grouping similar observations together.</li>
</ol>
</section>
<section id="supervised-vs.-unsupervised-learning-comparison-table" class="level2">
<h2 class="anchored" data-anchor-id="supervised-vs.-unsupervised-learning-comparison-table">Supervised vs.&nbsp;Unsupervised Learning: Comparison Table</h2>
<p>Let‚Äôs solidify our understanding with a side-by-side comparison:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 43%">
<col style="width: 46%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Supervised Learning</th>
<th style="text-align: left;">Unsupervised Learning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Goal</strong></td>
<td style="text-align: left;">Predict a response variable (Y) based on input features (X).</td>
<td style="text-align: left;">Discover patterns, structure, and relationships within the data (X).</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Data</strong></td>
<td style="text-align: left;">Features (X) and a corresponding response variable (Y).</td>
<td style="text-align: left;">Features (X) only; no response variable.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Evaluation</strong></td>
<td style="text-align: left;">Clear metrics (e.g., accuracy, R-squared, precision, recall) to assess performance.</td>
<td style="text-align: left;">More subjective, harder to evaluate; often relies on visual inspection or domain knowledge.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Examples</strong></td>
<td style="text-align: left;">Regression, classification, support vector machines.</td>
<td style="text-align: left;">PCA, clustering, association rule mining.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>‚ÄúCorrect Answer‚Äù</strong></td>
<td style="text-align: left;">Yes (the response variable provides the ‚Äúground truth‚Äù).</td>
<td style="text-align: left;">No (no response variable, so no ‚Äúground truth‚Äù).</td>
</tr>
</tbody>
</table>
</section>
<section id="supervised-vs.-unsupervised-a-note-of-caution" class="level2">
<h2 class="anchored" data-anchor-id="supervised-vs.-unsupervised-a-note-of-caution">Supervised vs.&nbsp;Unsupervised: A Note of Caution</h2>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>Unsupervised learning is often more challenging than supervised learning. Why? Because there‚Äôs no easy way to check our results! We don‚Äôt have a ‚Äúground truth‚Äù (like a response variable) to compare against. This means the process is more exploratory and subjective, requiring careful interpretation and domain expertise. ü§î</p>
</div>
</div>
</div>
</section>
<section id="applications-of-unsupervised-learning" class="level2">
<h2 class="anchored" data-anchor-id="applications-of-unsupervised-learning">Applications of Unsupervised Learning</h2>
<p>Unsupervised learning has become incredibly valuable across numerous fields:</p>
<ul>
<li><strong>Genomics:</strong> Researchers studying cancer might use unsupervised learning to analyze gene expression data. This can help identify different subtypes of cancer, leading to more targeted treatments. üß¨</li>
<li><strong>E-commerce:</strong> Online retailers use unsupervised learning to group customers with similar browsing and purchasing patterns. This allows for personalized product recommendations, increasing sales and customer satisfaction. üõçÔ∏è</li>
<li><strong>Search Engines:</strong> Unsupervised learning can be used to cluster users based on their click histories, leading to more relevant search results. üîé</li>
<li><strong>Marketing:</strong> Identifying <em>market segments</em> (groups of customers with shared characteristics) for targeted advertising campaigns. üéØ</li>
</ul>
</section>
<section id="applications-of-unsupervised-learning-cont." class="level2">
<h2 class="anchored" data-anchor-id="applications-of-unsupervised-learning-cont.">Applications of Unsupervised Learning (Cont.)</h2>
<p>These are just a few examples. The power of unsupervised learning lies in its ability to extract insights from data <em>without</em> needing a predefined outcome, making it a versatile tool for discovery in various domains. The ability to uncover hidden patterns is what makes it so powerful.</p>
</section>
<section id="diving-into-principal-component-analysis-pca" class="level2">
<h2 class="anchored" data-anchor-id="diving-into-principal-component-analysis-pca">Diving into Principal Component Analysis (PCA)</h2>
<p>Now, let‚Äôs explore our first unsupervised learning technique: Principal Component Analysis (PCA). PCA is like taking a high-dimensional dataset (many features) and finding the best way to ‚Äúflatten‚Äù it while preserving as much of its original information as possible. It‚Äôs a dimensionality reduction technique that finds the most important ‚Äúdirections‚Äù in your data.</p>
</section>
<section id="what-does-pca-do" class="level2">
<h2 class="anchored" data-anchor-id="what-does-pca-do">What does PCA do?</h2>
<ul>
<li><strong>Dimensionality Reduction:</strong> PCA simplifies data by finding a smaller set of <em>representative variables</em>, called <strong>principal components</strong>. These components capture most of the variability in the original data.</li>
<li><strong>Data Visualization:</strong> It allows us to visualize high-dimensional data in lower dimensions (e.g., 2D or 3D plots), making it easier to spot patterns and relationships that would be hidden in higher dimensions.</li>
<li><strong>Unsupervised:</strong> PCA only uses the features (X) and doesn‚Äôt rely on any response variable (Y). It focuses solely on the relationships <em>between</em> the features.</li>
<li><strong>Feature Space Directions:</strong> Identifies the directions in the feature space along which the original data varies most. These are the ‚Äúprincipal components‚Äù.</li>
<li><strong>Data Pre-processing:</strong> PCA can create new, uncorrelated features that can be used in subsequent supervised learning models. This can improve model performance and reduce overfitting.</li>
</ul>
</section>
<section id="what-are-principal-components-explained" class="level2">
<h2 class="anchored" data-anchor-id="what-are-principal-components-explained">What are Principal Components? (Explained)</h2>
<p>Imagine you have a dataset with many variables (features). PCA helps you find new variables, called <strong>principal components</strong>, which are <em>linear combinations</em> of the original features. It‚Äôs like creating new ‚Äúsummary‚Äù variables from the original ones, but in a way that maximizes the captured variance.</p>
</section>
<section id="understanding-the-first-principal-component-z1" class="level2">
<h2 class="anchored" data-anchor-id="understanding-the-first-principal-component-z1">Understanding the First Principal Component (Z1)</h2>
<ul>
<li><p><strong>First Principal Component (Z1):</strong> This is the most important principal component. It‚Äôs the normalized linear combination of the original features that captures the <em>largest variance</em> in the data. It represents the direction of greatest variability in the data cloud.</p>
<ul>
<li><strong>Normalized:</strong> The sum of the squared coefficients (loadings) equals 1. This ensures that the variance isn‚Äôt artificially inflated by using large coefficients. It‚Äôs a mathematical constraint for uniqueness.</li>
<li><strong>Loadings:</strong> The coefficients (œÜ) in the linear combination are called <em>loadings</em>. They tell us how much each original feature contributes to the principal component. A large loading (in absolute value) means the feature has a strong influence on that component.</li>
</ul></li>
</ul>
</section>
<section id="subsequent-principal-components" class="level2">
<h2 class="anchored" data-anchor-id="subsequent-principal-components">Subsequent Principal Components</h2>
<ul>
<li><strong>Subsequent Principal Components:</strong> These are also linear combinations of the original features, but they capture the <em>most remaining variance</em>, with the constraint that they are <em>uncorrelated</em> (orthogonal) to the previous components. Each component captures a different ‚Äúdirection‚Äù of variability in the data, and they are all perpendicular to each other.</li>
</ul>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Formula for the first principal component:</strong> <span class="math display">\[Z_1 = \phi_{11}X_1 + \phi_{21}X_2 + ... + \phi_{p1}X_p\]</span> where <span class="math inline">\(\sum_{j=1}^{p} \phi_{j1}^2 = 1\)</span> (normalization constraint). The values <span class="math inline">\(\phi_{11},...,\phi_{p1}\)</span> are the <em>loadings</em> of the first principal component.</p>
</div>
</div>
</div>
</section>
<section id="geometric-interpretation-of-pca" class="level2">
<h2 class="anchored" data-anchor-id="geometric-interpretation-of-pca">Geometric Interpretation of PCA</h2>
<p>The principal component loading vectors define directions in the feature space.</p>
<ul>
<li><strong>First Principal Component:</strong> Represents the direction along which the data points <em>vary the most</em>. It‚Äôs the line of best fit through the data, minimizing the squared distances from the points to the line.</li>
<li><strong>Second Principal Component:</strong> The direction, <em>orthogonal</em> (perpendicular) to the first, that captures the <em>next most variance</em>. And so on for subsequent components.</li>
</ul>
</section>
<section id="geometric-interpretation-of-pca-visual" class="level2">
<h2 class="anchored" data-anchor-id="geometric-interpretation-of-pca-visual">Geometric Interpretation of PCA (Visual)</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F6_14.svg" class="img-fluid figure-img"></p>
<figcaption>alt text</figcaption>
</figure>
</div>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>This figure, from an advertising dataset, shows the first two principal components. The green solid line represents the first principal component direction (Z1). The blue dashed line represents the second (Z2). Because this is a two-dimensional example, we only have two components. The lines show the directions of greatest variability in the data. The origin is at the mean of each feature.</p>
</div>
</div>
</div>
</section>
<section id="example-usarrests-data" class="level2">
<h2 class="anchored" data-anchor-id="example-usarrests-data">Example: USArrests Data</h2>
<p>Let‚Äôs apply PCA to a real-world dataset: the <code>USArrests</code> dataset. This dataset contains crime statistics for each of the 50 US states.</p>
<ul>
<li><strong>Features:</strong> <code>Murder</code>, <code>Assault</code>, <code>UrbanPop</code>, <code>Rape</code> (all measured per 100,000 residents).</li>
<li><strong>Goal:</strong> Visualize the data and identify patterns using PCA. We want to see if we can reduce these four crime variables down to a smaller number of ‚Äúsummary‚Äù variables.</li>
</ul>
</section>
<section id="usarrests-data-pca-biplot---image" class="level2">
<h2 class="anchored" data-anchor-id="usarrests-data-pca-biplot---image">USArrests Data: PCA Biplot - Image</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F12_1.svg" class="img-fluid figure-img"></p>
<figcaption>First two principal components for the USArrests data.</figcaption>
</figure>
</div>
</section>
<section id="usarrests-data-pca-biplot---explanation-1" class="level2">
<h2 class="anchored" data-anchor-id="usarrests-data-pca-biplot---explanation-1">USArrests Data: PCA Biplot - Explanation 1</h2>
<p>This plot is called a <strong>biplot</strong>. It shows both the principal component <em>scores</em> (blue state names) and the principal component <em>loadings</em> (orange arrows). The scores represent the projection of each state onto the plane defined by the first two principal components.</p>
</section>
<section id="usarrests-data-pca-biplot---explanation-2" class="level2">
<h2 class="anchored" data-anchor-id="usarrests-data-pca-biplot---explanation-2">USArrests Data: PCA Biplot - Explanation 2</h2>
<p>The loadings (orange arrows) show the contribution of each original variable to the principal components. For example, the loading for <code>Rape</code> on the first component is 0.54, and on the second component is 0.17. The word <code>Rape</code> is centered at the point (0.54, 0.17). The length and direction of the arrows indicate the strength and direction of the relationship.</p>
</section>
<section id="usarrests-data-pca-biplot---explanation-3" class="level2">
<h2 class="anchored" data-anchor-id="usarrests-data-pca-biplot---explanation-3">USArrests Data: PCA Biplot - Explanation 3</h2>
<p>The axes are labeled as PC1 and PC2. Each point represents a state, and its position is determined by its scores on the two principal components. States closer together in this plot have similar crime profiles.</p>
</section>
<section id="interpreting-the-usarrests-biplot-cont." class="level2">
<h2 class="anchored" data-anchor-id="interpreting-the-usarrests-biplot-cont.">Interpreting the USArrests Biplot (Cont.)</h2>
<ul>
<li><strong>First Principal Component (PC1):</strong> This component places roughly equal weight on <code>Murder</code>, <code>Assault</code>, and <code>Rape</code>, with much less weight on <code>UrbanPop</code>. This suggests that PC1 primarily captures the overall level of violent crime. States with high scores on PC1 have high crime rates across these three categories.</li>
<li><strong>Second Principal Component (PC2):</strong> This component places most of its weight on <code>UrbanPop</code>, suggesting that it represents the degree of urbanization. States with high scores on PC2 are more urbanized.</li>
<li><strong>Correlation:</strong> The proximity of <code>Murder</code>, <code>Assault</code>, and <code>Rape</code> in the biplot indicates that these variables are positively correlated. <code>UrbanPop</code> is farther away, indicating it‚Äôs less correlated with the other three.</li>
</ul>
</section>
<section id="interpreting-the-usarrests-biplot-loading-table---image" class="level2">
<h2 class="anchored" data-anchor-id="interpreting-the-usarrests-biplot-loading-table---image">Interpreting the USArrests Biplot: Loading Table - Image</h2>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">PC1</th>
<th style="text-align: left;">PC2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Murder</td>
<td style="text-align: left;">0.536</td>
<td style="text-align: left;">-0.418</td>
</tr>
<tr class="even">
<td style="text-align: left;">Assault</td>
<td style="text-align: left;">0.583</td>
<td style="text-align: left;">-0.188</td>
</tr>
<tr class="odd">
<td style="text-align: left;">UrbanPop</td>
<td style="text-align: left;">0.278</td>
<td style="text-align: left;">0.873</td>
</tr>
<tr class="even">
<td style="text-align: left;">Rape</td>
<td style="text-align: left;">0.543</td>
<td style="text-align: left;">0.167</td>
</tr>
</tbody>
</table>
</section>
<section id="interpreting-the-usarrests-biplot-loading-table---explanation-1" class="level2">
<h2 class="anchored" data-anchor-id="interpreting-the-usarrests-biplot-loading-table---explanation-1">Interpreting the USArrests Biplot: Loading Table - Explanation 1</h2>
<p>This table shows the numerical values of the loading vectors for each principal component. These numbers correspond to the lengths and directions of the arrows in the biplot.</p>
</section>
<section id="interpreting-the-usarrests-biplot-loading-table---explanation-2" class="level2">
<h2 class="anchored" data-anchor-id="interpreting-the-usarrests-biplot-loading-table---explanation-2">Interpreting the USArrests Biplot: Loading Table - Explanation 2</h2>
<p>States with large positive scores on PC1 (e.g., California, Nevada, Florida) have high crime rates, as indicated by the biplot and the large positive loadings for Murder, Assault, and Rape on PC1.</p>
</section>
<section id="interpreting-the-usarrests-biplot-loading-table---explanation-3" class="level2">
<h2 class="anchored" data-anchor-id="interpreting-the-usarrests-biplot-loading-table---explanation-3">Interpreting the USArrests Biplot: Loading Table - Explanation 3</h2>
<p>States with large positive scores on PC2 (e.g., California) have high urbanization, as indicated by the biplot and the large positive loading for UrbanPop on PC2.</p>
</section>
<section id="another-interpretation-of-pca-closest-linear-surfaces" class="level2">
<h2 class="anchored" data-anchor-id="another-interpretation-of-pca-closest-linear-surfaces">Another Interpretation of PCA: Closest Linear Surfaces</h2>
<p>PCA can also be understood as finding <em>linear surfaces</em> that are closest to the data points.</p>
<ul>
<li><strong>First Principal Component:</strong> The <em>line</em> in p-dimensional space that is closest to the n observations (in terms of average squared Euclidean distance). It‚Äôs the best-fitting line in the sense of minimizing the sum of squared distances from the points to the line.</li>
<li><strong>First Two Principal Components:</strong> The <em>plane</em> that is closest to the observations. It‚Äôs the best-fitting plane in the same least-squares sense.</li>
<li><strong>And so on‚Ä¶</strong> For higher dimensions, PCA finds the best-fitting hyperplanes (generalizations of planes to higher dimensions).</li>
</ul>
</section>
<section id="another-interpretation-of-pca-visual---image" class="level2">
<h2 class="anchored" data-anchor-id="another-interpretation-of-pca-visual---image">Another Interpretation of PCA: Visual - Image</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F12_2.svg" class="img-fluid figure-img"></p>
<figcaption>Ninety observations simulated in three dimensions.</figcaption>
</figure>
</div>
</section>
<section id="another-interpretation-of-pca-visual---explanation-left" class="level2">
<h2 class="anchored" data-anchor-id="another-interpretation-of-pca-visual---explanation-left">Another Interpretation of PCA: Visual - Explanation (Left)</h2>
<p><em>Left</em>: The first two principal component directions (shown in green) span the plane that best fits the data. It‚Äôs like finding the ‚Äúflattest‚Äù plane that passes through the cloud of points, minimizing the distances from the points to the plane.</p>
</section>
<section id="another-interpretation-of-pca-visual---explanation-right" class="level2">
<h2 class="anchored" data-anchor-id="another-interpretation-of-pca-visual---explanation-right">Another Interpretation of PCA: Visual - Explanation (Right)</h2>
<p><em>Right</em>: The first two principal component score vectors give the coordinates of the projection of the 90 observations onto this plane. Projecting the data onto this plane gives us a lower-dimensional representation, capturing the most important aspects of the data‚Äôs variability.</p>
</section>
<section id="proportion-of-variance-explained-pve" class="level2">
<h2 class="anchored" data-anchor-id="proportion-of-variance-explained-pve">Proportion of Variance Explained (PVE)</h2>
<p>How much information do we <em>lose</em> when we project our data onto the first few principal components? The <strong>Proportion of Variance Explained (PVE)</strong> helps us answer this question. It quantifies the amount of information retained by each principal component.</p>
</section>
<section id="understanding-pve" class="level2">
<h2 class="anchored" data-anchor-id="understanding-pve">Understanding PVE</h2>
<ul>
<li><strong>Total Variance:</strong> The sum of the variances of all the original features (assuming the features have been centered). This represents the total variability in the original dataset.</li>
<li><strong>Variance Explained by the m-th PC:</strong> The variance of the m-th principal component. This is the amount of variability captured by that single component.</li>
<li><strong>PVE of the m-th PC:</strong> The <em>proportion</em> of the total variance that is explained by the m-th principal component. It tells us how much information is retained by that component, expressed as a percentage of the total.</li>
</ul>
</section>
<section id="pve-formula" class="level2">
<h2 class="anchored" data-anchor-id="pve-formula">PVE Formula</h2>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Formula for PVE of the m-th PC:</strong> <span class="math display">\[\frac{\sum_{i=1}^{n} z_{im}^2}{\sum_{j=1}^{p} \sum_{i=1}^{n} x_{ij}^2}\]</span> Where: - <em>z<sub>im</sub></em> is the score of the i-th observation on the m-th principal component. - <em>x<sub>ij</sub></em> is the value of the j-th feature for the i-th observation (after centering). The denominator represents the total variance.</p>
</div>
</div>
</div>
</section>
<section id="pve-usarrests-example-visual---image" class="level2">
<h2 class="anchored" data-anchor-id="pve-usarrests-example-visual---image">PVE: USArrests Example (Visual) - Image</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F12_3.svg" class="img-fluid figure-img"></p>
<figcaption>Scree plot and cumulative PVE plot for the USArrests data.</figcaption>
</figure>
</div>
</section>
<section id="pve-usarrests-example-visual---explanation-left" class="level2">
<h2 class="anchored" data-anchor-id="pve-usarrests-example-visual---explanation-left">PVE: USArrests Example (Visual) - Explanation (Left)</h2>
<p><em>Left</em>: A <strong>scree plot</strong>, showing the PVE of each principal component. Each bar represents a principal component, and the height of the bar indicates the proportion of variance explained by that component.</p>
</section>
<section id="pve-usarrests-example-visual---explanation-right" class="level2">
<h2 class="anchored" data-anchor-id="pve-usarrests-example-visual---explanation-right">PVE: USArrests Example (Visual) - Explanation (Right)</h2>
<p><em>Right</em>: The cumulative PVE. This plot shows the cumulative proportion of variance explained as we add more principal components. It helps us see how much variance is explained by the first few components together.</p>
</section>
<section id="pve-usarrests-example-interpretation" class="level2">
<h2 class="anchored" data-anchor-id="pve-usarrests-example-interpretation">PVE: USArrests Example (Interpretation)</h2>
<ul>
<li><strong>PC1:</strong> Explains 62.0% of the variance in the data.</li>
<li><strong>PC2:</strong> Explains 24.7% of the variance.</li>
<li><strong>Together:</strong> PC1 and PC2 explain almost 87% of the total variance.</li>
</ul>
<p>This means that the biplot (from earlier slides) provides a reasonably good two-dimensional summary of the data, capturing a large portion of its variability. The scree plot helps us decide how many components to keep. We often look for an ‚Äúelbow‚Äù in the plot ‚Äì a point where the PVE starts to drop off significantly. This suggests that adding further components adds little additional information.</p>
</section>
<section id="scaling-the-variables-a-crucial-step" class="level2">
<h2 class="anchored" data-anchor-id="scaling-the-variables-a-crucial-step">Scaling the Variables: A Crucial Step</h2>
<p>Before performing PCA, we usually <em>scale</em> the variables to have a standard deviation of one (also called standardization or z-scoring). This is a very important step in most cases!</p>
</section>
<section id="why-scale" class="level2">
<h2 class="anchored" data-anchor-id="why-scale">Why Scale?</h2>
<ul>
<li><strong>Different Units/Variances:</strong> If variables are measured in different units (e.g., meters and kilograms) or have vastly different variances (e.g., one variable ranges from 0 to 1, another from 0 to 1000), the variables with the <em>largest variances</em> will dominate the principal components, regardless of whether they are actually the most <em>important</em> or informative.</li>
<li><strong>Equal Weight:</strong> Scaling prevents this by putting all variables on a ‚Äúlevel playing field.‚Äù It ensures that each variable contributes equally to the principal components, based on its <em>relative</em> variability, not its absolute scale.</li>
</ul>
</section>
<section id="when-not-to-scale" class="level2">
<h2 class="anchored" data-anchor-id="when-not-to-scale">When <em>Not</em> to Scale</h2>
<ul>
<li><strong>Same Units:</strong> If variables are measured in the <em>same units</em> (e.g., gene expression levels measured using the same technology), and the differences in variance are scientifically meaningful, we might <em>not</em> want to scale. In this case, the differences in variance might reflect real biological differences.</li>
</ul>
</section>
<section id="scaling-usarrests-example-visual---image" class="level2">
<h2 class="anchored" data-anchor-id="scaling-usarrests-example-visual---image">Scaling: USArrests Example (Visual) - Image</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F12_4.svg" class="img-fluid figure-img"></p>
<figcaption>Effect of scaling on the USArrests biplot.</figcaption>
</figure>
</div>
</section>
<section id="scaling-usarrests-example-visual---explanation-left" class="level2">
<h2 class="anchored" data-anchor-id="scaling-usarrests-example-visual---explanation-left">Scaling: USArrests Example (Visual) - Explanation (Left)</h2>
<p><em>Left</em>: PCA with scaled variables (this is the same as the biplot we saw earlier). Notice that all variables contribute reasonably to PC1.</p>
</section>
<section id="scaling-usarrests-example-visual---explanation-right" class="level2">
<h2 class="anchored" data-anchor-id="scaling-usarrests-example-visual---explanation-right">Scaling: USArrests Example (Visual) - Explanation (Right)</h2>
<p><em>Right</em>: PCA with unscaled variables. Notice how <code>Assault</code> dominates the first principal component in the unscaled version simply because it has the highest variance among the four variables. Scaling gives a more balanced representation, reflecting the relative importance of the variables.</p>
</section>
<section id="how-many-principal-components-to-use" class="level2">
<h2 class="anchored" data-anchor-id="how-many-principal-components-to-use">How Many Principal Components to Use?</h2>
<p>This is a common question, and unfortunately, there‚Äôs no single ‚Äúmagic number‚Äù that works for all datasets. The best approach depends on the specific context, the dataset, and the goals of the analysis.</p>
</section>
<section id="guidelines-for-choosing-the-number-of-components" class="level2">
<h2 class="anchored" data-anchor-id="guidelines-for-choosing-the-number-of-components">Guidelines for Choosing the Number of Components</h2>
<ul>
<li><strong>Scree Plot:</strong> Look for an ‚Äúelbow‚Äù in the scree plot ‚Äì a point where the PVE drops off significantly. This suggests that adding more components beyond that point doesn‚Äôt provide much additional information.</li>
<li><strong>Interpretation:</strong> Keep enough components to capture the <em>interesting patterns</em> in the data. If you can interpret the first few components in a meaningful way, that‚Äôs a good sign.</li>
<li><strong>Ad Hoc:</strong> The process is inherently somewhat subjective and requires judgment.</li>
<li><strong>Supervised Learning:</strong> If PCA is used for pre-processing in a supervised learning context (e.g., Principal Components Regression), we can use cross-validation to select the optimal number of components. This involves trying different numbers of components and seeing which number leads to the best predictive performance on unseen data.</li>
</ul>
</section>
<section id="clustering-methods-finding-subgroups" class="level2">
<h2 class="anchored" data-anchor-id="clustering-methods-finding-subgroups">Clustering Methods: Finding Subgroups</h2>
<p>Now, let‚Äôs move on to the second major type of unsupervised learning: <strong>clustering</strong>. Clustering aims to find <em>subgroups</em> (clusters) within a dataset, grouping observations that are similar to each other.</p>
</section>
<section id="clustering-the-goal" class="level2">
<h2 class="anchored" data-anchor-id="clustering-the-goal">Clustering: The Goal</h2>
<ul>
<li><strong>Goal:</strong> Partition observations into groups (clusters) so that observations <em>within</em> a group are similar, and observations in <em>different</em> groups are dissimilar. It‚Äôs like sorting a collection of objects into meaningful categories based on their shared characteristics.</li>
<li><strong>Similarity:</strong> What does ‚Äúsimilar‚Äù mean? This is a crucial, and often domain-specific, consideration. We need to define how we measure the similarity or dissimilarity between observations, and this choice can significantly affect the results.</li>
<li><strong>Unsupervised:</strong> We‚Äôre looking for structure <em>without</em> a predefined outcome or ‚Äúcorrect answer.‚Äù We don‚Äôt know the ‚Äútrue‚Äù clusters beforehand.</li>
</ul>
</section>
<section id="two-main-types-of-clustering" class="level2">
<h2 class="anchored" data-anchor-id="two-main-types-of-clustering">Two Main Types of Clustering</h2>
<p>We‚Äôll cover two main types of clustering:</p>
<ol type="1">
<li><strong>K-Means Clustering:</strong> Partitions data into a <em>pre-specified</em> number (K) of clusters. We have to tell the algorithm how many clusters we <em>expect</em> to find.</li>
<li><strong>Hierarchical Clustering:</strong> Builds a <em>hierarchy</em> of clusters, represented by a dendrogram (a tree-like diagram). This approach doesn‚Äôt require us to pre-specify the number of clusters; we can decide <em>after</em> seeing the dendrogram.</li>
</ol>
</section>
<section id="k-means-clustering-a-popular-choice" class="level2">
<h2 class="anchored" data-anchor-id="k-means-clustering-a-popular-choice">K-Means Clustering: A Popular Choice</h2>
<p>K-means clustering is a simple, widely used, and efficient clustering algorithm.</p>
<ul>
<li><strong>Input:</strong> A dataset and a desired number of clusters, <em>K</em>.</li>
<li><strong>Output:</strong> Assigns each observation to <em>exactly one</em> of K clusters.</li>
<li><strong>Goal:</strong> Minimize the <em>within-cluster variation</em>. This means we want the observations within each cluster to be as close to each other as possible, forming tight, compact groups.</li>
</ul>
</section>
<section id="k-means-clustering-visual---image" class="level2">
<h2 class="anchored" data-anchor-id="k-means-clustering-visual---image">K-Means Clustering (Visual) - Image</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F12_7.svg" class="img-fluid figure-img"></p>
<figcaption>K-means clustering results on simulated data.</figcaption>
</figure>
</div>
</section>
<section id="k-means-clustering-visual---explanation-k2" class="level2">
<h2 class="anchored" data-anchor-id="k-means-clustering-visual---explanation-k2">K-Means Clustering (Visual) - Explanation (K=2)</h2>
<p>This figure shows the results of applying K-means clustering with K=2 (two clusters). The color of each observation indicates the cluster to which it was assigned. The algorithm has separated the data into two distinct groups.</p>
</section>
<section id="k-means-clustering-visual---explanation-k3" class="level2">
<h2 class="anchored" data-anchor-id="k-means-clustering-visual---explanation-k3">K-Means Clustering (Visual) - Explanation (K=3)</h2>
<p>Here, K=3. The algorithm has identified three clusters. Notice how the cluster assignments change as we change the value of K.</p>
</section>
<section id="k-means-clustering-visual---explanation-k4" class="level2">
<h2 class="anchored" data-anchor-id="k-means-clustering-visual---explanation-k4">K-Means Clustering (Visual) - Explanation (K=4)</h2>
<p>With K=4, the data is further divided. The choice of K significantly impacts the resulting clusters.</p>
</section>
<section id="the-k-means-algorithm-step-by-step" class="level2">
<h2 class="anchored" data-anchor-id="the-k-means-algorithm-step-by-step">The K-Means Algorithm: Step-by-Step</h2>
<ol type="1">
<li><strong>Initialization:</strong> Randomly assign each observation to one of the K clusters. This is our initial ‚Äúguess‚Äù at the cluster assignments. These are <em>random</em> starting points.</li>
<li><strong>Iteration:</strong> Repeat the following steps until the cluster assignments stop changing (or until a maximum number of iterations is reached):
<ol type="a">
<li><strong>Compute Centroids:</strong> For each cluster, calculate the <em>centroid</em>. The centroid is the mean vector of all the observations in that cluster. It represents the ‚Äúcenter‚Äù of the cluster in the feature space.</li>
<li><strong>Reassign Observations:</strong> Assign each observation to the cluster whose centroid is <em>closest</em> (usually using Euclidean distance). This step refines the cluster assignments based on the current centroids, moving observations between clusters to minimize within-cluster distances.</li>
</ol></li>
</ol>
</section>
<section id="the-k-means-algorithm-local-optima" class="level2">
<h2 class="anchored" data-anchor-id="the-k-means-algorithm-local-optima">The K-Means Algorithm (Local Optima)</h2>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>The K-means algorithm is guaranteed to decrease the within-cluster variation at each step. However, it finds a <em>local</em> optimum, not necessarily the <em>global</em> optimum. This means the final cluster assignments can depend on the initial random assignments. Different starting points can lead to different final clusters.</p>
</div>
</div>
</div>
</section>
<section id="k-means-an-illustrative-example" class="level2">
<h2 class="anchored" data-anchor-id="k-means-an-illustrative-example">K-Means: An Illustrative Example</h2>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
    A[Data] --&gt; B(Step 1: Randomly Assign Clusters);
    B --&gt; C(Iteration 1, Step 2a: Compute Centroids);
    C --&gt; D(Iteration 1, Step 2b: Reassign Observations);
    D --&gt; E(Iteration 2, Step 2a: Compute Centroids);
     E --&gt; F(Iteration 2, Step 2b: Reassign Observations);
    F --&gt; G(Continue Iterating Until Convergence);
    G --&gt; H(Final Cluster Assignments);
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style H fill:#ccf,stroke:#333,stroke-width:4px
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>This flowchart illustrates the iterative nature of the K-means algorithm. The process continues until the cluster assignments no longer change, indicating that a local optimum has been reached.</p>
</section>
<section id="k-means-algorithm-in-action---image" class="level2">
<h2 class="anchored" data-anchor-id="k-means-algorithm-in-action---image">K-Means Algorithm in Action - Image</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F12_8.svg" class="img-fluid figure-img"></p>
<figcaption>Progress of the K-means algorithm.</figcaption>
</figure>
</div>
</section>
<section id="k-means-algorithm-in-action---explanation-first-row" class="level2">
<h2 class="anchored" data-anchor-id="k-means-algorithm-in-action---explanation-first-row">K-Means Algorithm in Action - Explanation (First Row)</h2>
<p>This figure shows the progress of the K-means algorithm over several iterations. The crosses represent the cluster centroids. The first row shows the random initialization of cluster assignments and the initial centroids.</p>
</section>
<section id="k-means-algorithm-in-action---explanation-second-row" class="level2">
<h2 class="anchored" data-anchor-id="k-means-algorithm-in-action---explanation-second-row">K-Means Algorithm in Action - Explanation (Second Row)</h2>
<p>The second row shows the state after the first iteration. The centroids have moved, and some observations have been reassigned to different clusters.</p>
</section>
<section id="k-means-algorithm-in-action---explanation-third-and-fourth-rows" class="level2">
<h2 class="anchored" data-anchor-id="k-means-algorithm-in-action---explanation-third-and-fourth-rows">K-Means Algorithm in Action - Explanation (Third and Fourth Rows)</h2>
<p>The third and fourth rows show further iterations. The algorithm continues to update the centroids and reassign observations until convergence.</p>
</section>
<section id="k-means-the-problem-of-local-optima" class="level2">
<h2 class="anchored" data-anchor-id="k-means-the-problem-of-local-optima">K-Means: The Problem of Local Optima</h2>
<p>Because K-means finds a local optimum, the results can vary depending on the <em>initial random assignment</em> of observations to clusters. Different starting points can lead to different final cluster assignments, and some solutions may be better (lower within-cluster variation) than others.</p>
</section>
<section id="dealing-with-local-optima" class="level2">
<h2 class="anchored" data-anchor-id="dealing-with-local-optima">Dealing with Local Optima</h2>
<ul>
<li><strong>Recommendation:</strong> Run K-means multiple times with different initializations and choose the solution with the <em>lowest</em> within-cluster variation. This helps to mitigate the problem of getting stuck in a poor local optimum. It increases the chances of finding a good solution, although it doesn‚Äôt guarantee finding the global optimum.</li>
</ul>
</section>
<section id="k-means-local-optima-visual---image" class="level2">
<h2 class="anchored" data-anchor-id="k-means-local-optima-visual---image">K-Means: Local Optima (Visual) - Image</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F12_9.svg" class="img-fluid figure-img"></p>
<figcaption>K-means clustering with different initializations.</figcaption>
</figure>
</div>
</section>
<section id="k-means-local-optima-visual---explanation-1" class="level2">
<h2 class="anchored" data-anchor-id="k-means-local-optima-visual---explanation-1">K-Means: Local Optima (Visual) - Explanation 1</h2>
<p>This figure shows K-means clustering performed six times on the same data, each with a different random initialization.</p>
</section>
<section id="k-means-local-optima-visual---explanation-2" class="level2">
<h2 class="anchored" data-anchor-id="k-means-local-optima-visual---explanation-2">K-Means: Local Optima (Visual) - Explanation 2</h2>
<p>Three different local optima were obtained. This highlights the variability in the results due to the random initialization.</p>
</section>
<section id="k-means-local-optima-visual---explanation-3" class="level2">
<h2 class="anchored" data-anchor-id="k-means-local-optima-visual---explanation-3">K-Means: Local Optima (Visual) - Explanation 3</h2>
<p>One of these local optima (bottom right) resulted in a better separation between the clusters. This emphasizes the importance of running K-means multiple times and comparing the results.</p>
</section>
<section id="hierarchical-clustering-a-different-approach" class="level2">
<h2 class="anchored" data-anchor-id="hierarchical-clustering-a-different-approach">Hierarchical Clustering: A Different Approach</h2>
<p>Hierarchical clustering offers an alternative to K-means, building a <em>hierarchy</em> of clusters. It provides a more nuanced view of the relationships between observations.</p>
</section>
<section id="advantages-of-hierarchical-clustering" class="level2">
<h2 class="anchored" data-anchor-id="advantages-of-hierarchical-clustering">Advantages of Hierarchical Clustering</h2>
<ul>
<li><strong>No Need to Pre-specify K:</strong> Unlike K-means, we don‚Äôt need to pre-specify the number of clusters (K). We can choose the number of clusters <em>after</em> the algorithm has run by examining the resulting dendrogram. This provides more flexibility.</li>
<li><strong>Dendrogram:</strong> The output of hierarchical clustering is a <strong>dendrogram</strong>, a tree-like diagram that visually represents the hierarchy of clusters and the relationships between observations. It provides a visual summary of the clustering process.</li>
</ul>
</section>
<section id="agglomerative-clustering-bottom-up" class="level2">
<h2 class="anchored" data-anchor-id="agglomerative-clustering-bottom-up">Agglomerative Clustering (Bottom-Up)</h2>
<ul>
<li><strong>Agglomerative (Bottom-Up):</strong> We‚Äôll focus on agglomerative clustering, which is the most common type of hierarchical clustering. It starts with each observation as its own cluster and successively merges the most similar clusters until only one cluster remains, building the hierarchy from the bottom up.</li>
</ul>
</section>
<section id="hierarchical-clustering-dendrogram---image" class="level2">
<h2 class="anchored" data-anchor-id="hierarchical-clustering-dendrogram---image">Hierarchical Clustering: Dendrogram - Image</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F12_11.svg" class="img-fluid figure-img"></p>
<figcaption>Dendrogram of hierarchically clustering the data.</figcaption>
</figure>
</div>
</section>
<section id="hierarchical-clustering-dendrogram---explanation" class="level2">
<h2 class="anchored" data-anchor-id="hierarchical-clustering-dendrogram---explanation">Hierarchical Clustering: Dendrogram - Explanation</h2>
<p>This dendrogram visually represents the hierarchy of clusters. Each leaf represents an observation, and the branches show how clusters are merged.</p>
</section>
<section id="interpreting-a-dendrogram" class="level2">
<h2 class="anchored" data-anchor-id="interpreting-a-dendrogram">Interpreting a Dendrogram</h2>
<ul>
<li><strong>Leaves:</strong> The leaves at the bottom of the dendrogram represent individual observations.</li>
<li><strong>Fusions:</strong> As you move up the tree, leaves and branches <em>fuse</em> together. These fusions represent the merging of similar clusters. Earlier fusions indicate greater similarity.</li>
<li><strong>Height of Fusion:</strong> The height at which two clusters fuse indicates their <em>dissimilarity</em>. Lower fusions mean the merged clusters are more similar. Higher fusions mean the clusters are more dissimilar. The height provides a measure of the distance between merged clusters.</li>
<li><strong>Cutting the Dendrogram:</strong> A horizontal cut across the dendrogram gives a specific number of clusters. The height of the cut determines the number of clusters obtained. This is how we can choose the number of clusters <em>after</em> running the algorithm.</li>
</ul>
</section>
<section id="interpreting-a-dendrogram-visual---image" class="level2">
<h2 class="anchored" data-anchor-id="interpreting-a-dendrogram-visual---image">Interpreting a Dendrogram (Visual) - Image</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F12_12.svg" class="img-fluid figure-img"></p>
<figcaption>Interpreting a dendrogram.</figcaption>
</figure>
</div>
</section>
<section id="interpreting-a-dendrogram-visual---explanation-left" class="level2">
<h2 class="anchored" data-anchor-id="interpreting-a-dendrogram-visual---explanation-left">Interpreting a Dendrogram (Visual) - Explanation (Left)</h2>
<p><em>Left</em>: A dendrogram generated using Euclidean distance and complete linkage. Observations 5 and 7 are quite similar, as indicated by the low height of their fusion. Observations 1 and 6 are also quite similar.</p>
</section>
<section id="interpreting-a-dendrogram-visual---explanation-right" class="level2">
<h2 class="anchored" data-anchor-id="interpreting-a-dendrogram-visual---explanation-right">Interpreting a Dendrogram (Visual) - Explanation (Right)</h2>
<p><em>Right</em>: The raw data that was used to generate the dendrogram. This allows us to see how the dendrogram reflects the relationships in the original data. The dendrogram accurately captures the similarity between observations 5 and 7, and between observations 1 and 6.</p>
</section>
<section id="the-hierarchical-clustering-algorithm-step-by-step" class="level2">
<h2 class="anchored" data-anchor-id="the-hierarchical-clustering-algorithm-step-by-step">The Hierarchical Clustering Algorithm: Step-by-Step</h2>
<ol type="1">
<li><strong>Initialization:</strong> Begin with each observation as its own cluster (n clusters). Calculate all pairwise dissimilarities between the observations (e.g., using Euclidean distance).</li>
<li><strong>Iteration:</strong> For i = n, n-1, ‚Ä¶, 2:
<ol type="a">
<li><strong>Find Most Similar Clusters:</strong> Identify the two <em>most similar</em> clusters (the two clusters with the smallest dissimilarity).</li>
<li><strong>Merge Clusters:</strong> <em>Fuse</em> these two clusters into a single cluster. The dissimilarity between these two clusters is represented by the <em>height</em> in the dendrogram where their branches fuse.</li>
<li><strong>Update Dissimilarities:</strong> Calculate the new pairwise inter-cluster dissimilarities between the remaining i-1 clusters. This is where different <em>linkage</em> methods come into play. The choice of linkage affects how the distances between clusters are calculated.</li>
</ol></li>
</ol>
</section>
<section id="the-key-question-linkage" class="level2">
<h2 class="anchored" data-anchor-id="the-key-question-linkage">The Key Question: Linkage</h2>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Key Question:</strong> How do we define the dissimilarity between <em>clusters</em> (groups of observations), not just between individual observations? We know how to calculate the distance between two <em>points</em>, but how do we calculate the distance between two <em>sets</em> of points? This is where <strong>linkage</strong> comes in.</p>
</div>
</div>
</div>
</section>
<section id="linkage-defining-inter-cluster-dissimilarity" class="level2">
<h2 class="anchored" data-anchor-id="linkage-defining-inter-cluster-dissimilarity">Linkage: Defining Inter-Cluster Dissimilarity</h2>
<p><strong>Linkage</strong> defines how we measure the dissimilarity between two <em>groups</em> of observations (clusters). It‚Äôs a crucial choice in hierarchical clustering. There are several different linkage methods:</p>
</section>
<section id="linkage-methods-a-table" class="level2">
<h2 class="anchored" data-anchor-id="linkage-methods-a-table">Linkage Methods: A Table</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 6%">
<col style="width: 93%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Linkage</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Complete</td>
<td style="text-align: left;"><em>Maximal</em> intercluster dissimilarity. Calculates the dissimilarity between the <em>most dissimilar</em> points in the two clusters (the largest pairwise distance).</td>
</tr>
<tr class="even">
<td style="text-align: left;">Single</td>
<td style="text-align: left;"><em>Minimal</em> intercluster dissimilarity. Calculates the dissimilarity between the <em>most similar</em> points in the two clusters (the smallest pairwise distance).</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Average</td>
<td style="text-align: left;"><em>Mean</em> intercluster dissimilarity. Calculates the average dissimilarity between all pairs of points in the two clusters.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Centroid</td>
<td style="text-align: left;">Dissimilarity between the <em>centroids</em> (means) of the two clusters.</td>
</tr>
</tbody>
</table>
</section>
<section id="linkage-recommendations" class="level2">
<h2 class="anchored" data-anchor-id="linkage-recommendations">Linkage: Recommendations</h2>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>Average and complete linkage are generally preferred over single and centroid linkage. Single linkage can lead to ‚Äúchaining,‚Äù where clusters become elongated and stringy. Centroid linkage can sometimes lead to undesirable <em>inversions</em> in the dendrogram, where clusters merge at a lower height than their individual members, making interpretation difficult.</p>
</div>
</div>
</div>
</section>
<section id="choice-of-dissimilarity-measure" class="level2">
<h2 class="anchored" data-anchor-id="choice-of-dissimilarity-measure">Choice of Dissimilarity Measure</h2>
<p>In addition to choosing a linkage method, we also need to choose a <em>dissimilarity measure</em> between individual observations. This is the foundation upon which the clustering is built.</p>
</section>
<section id="common-dissimilarity-measures" class="level2">
<h2 class="anchored" data-anchor-id="common-dissimilarity-measures">Common Dissimilarity Measures</h2>
<ul>
<li><strong>Euclidean Distance:</strong> The most common choice. Measures the straight-line distance between two points in the feature space: <span class="math display">\[\sqrt{\sum_{j=1}^{p}(x_{ij} - x_{i'j})^2}\]</span>. Suitable when features are continuous and on similar scales (or have been scaled).</li>
<li><strong>Correlation-Based Distance:</strong> Considers two observations to be similar if their features are <em>highly correlated</em>, even if their absolute values are far apart in terms of Euclidean distance. Useful when we‚Äôre interested in the <em>shape</em> of the feature profiles, rather than their magnitude. It‚Äôs calculated as <code>1 - correlation</code>.</li>
</ul>
</section>
<section id="dissimilarity-measures-an-example---image" class="level2">
<h2 class="anchored" data-anchor-id="dissimilarity-measures-an-example---image">Dissimilarity Measures: An Example - Image</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F12_15.svg" class="img-fluid figure-img"></p>
<figcaption>Euclidean vs.&nbsp;correlation-based distance.</figcaption>
</figure>
</div>
</section>
<section id="dissimilarity-measures-an-example---explanation-top" class="level2">
<h2 class="anchored" data-anchor-id="dissimilarity-measures-an-example---explanation-top">Dissimilarity Measures: An Example - Explanation (Top)</h2>
<p>The top panel shows three observations with two features. In terms of Euclidean distance, observations 1 and 3 are most similar, and observations 2 and 3 are most dissimilar.</p>
</section>
<section id="dissimilarity-measures-an-example---explanation-bottom" class="level2">
<h2 class="anchored" data-anchor-id="dissimilarity-measures-an-example---explanation-bottom">Dissimilarity Measures: An Example - Explanation (Bottom)</h2>
<p>The bottom panel shows the same observations, but now we consider correlation-based distance. Observations 1 and 2 have a perfect correlation of 1, so their correlation-based distance is 0. Observation 3 is negatively correlated with observations 1 and 2.</p>
<p>The choice of dissimilarity measure depends on the <em>type of data</em> and the <em>scientific question</em> being addressed. Euclidean distance focuses on the magnitude of differences, while correlation-based distance focuses on the pattern of changes across features.</p>
</section>
<section id="practical-issues-in-clustering" class="level2">
<h2 class="anchored" data-anchor-id="practical-issues-in-clustering">Practical Issues in Clustering</h2>
<p>Clustering, while powerful, comes with some practical challenges that require careful consideration.</p>
<ul>
<li><strong>Scaling:</strong> Should we scale the variables before clustering? (Usually yes, for the same reasons as in PCA: to give equal weight to each variable and prevent variables with large variances from dominating the results.)</li>
<li><strong>Small Decisions, Big Consequences:</strong> Seemingly small choices, such as the dissimilarity measure, linkage method, and scaling, can have a <em>substantial impact</em> on the clustering results. There‚Äôs no single ‚Äúcorrect‚Äù set of choices, and different choices can lead to very different clusters.</li>
<li><strong>Validating Clusters:</strong> It‚Äôs difficult to definitively know if the clusters found are <em>real</em> and meaningful, or just an artifact of the clustering process. There‚Äôs no ‚Äúground truth‚Äù to compare against in most unsupervised learning scenarios.</li>
<li><strong>Robustness:</strong> Clustering methods are often <em>not very robust</em>. Small changes in the data (e.g., adding or removing a few observations) can lead to significantly different cluster assignments.</li>
</ul>
</section>
<section id="clustering-recommendations-and-cautions" class="level2">
<h2 class="anchored" data-anchor-id="clustering-recommendations-and-cautions">Clustering: Recommendations and Cautions</h2>
<p><strong>Recommendations:</strong></p>
<ul>
<li><strong>Experiment:</strong> Try different choices of dissimilarity measure, linkage (for hierarchical clustering), and scaling. Don‚Äôt rely on a single clustering result.</li>
<li><strong>Consistency:</strong> Look for consistent patterns across different clustering results. If different methods produce similar clusters, this increases confidence in the findings.</li>
<li><strong>Domain Knowledge:</strong> Use domain knowledge to assess the plausibility and interpretability of the clusters. Do the clusters make sense in the context of the problem?</li>
</ul>
<p><strong>Caution:</strong> Clustering should be viewed as a starting point for further investigation, not as the final answer. It‚Äôs an exploratory technique that can generate hypotheses, but these hypotheses should be validated using other methods or domain knowledge. Don‚Äôt over-interpret clustering results.</p>
</section>
<section id="data-mining-machine-learning-and-statistical-learning" class="level2">
<h2 class="anchored" data-anchor-id="data-mining-machine-learning-and-statistical-learning">Data Mining, Machine Learning and Statistical Learning</h2>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
    A[Data Mining] --&gt; C(Common Ground)
    B[Machine Learning] --&gt; C
    D[Statistical Learning] --&gt; C
    C --&gt; E[Insights &amp; Predictions]
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
     style C fill:#f9f,stroke:#333,stroke-width:4px
      style E fill:#ccf,stroke:#333,stroke-width:4px
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<ul>
<li><strong>Data Mining:</strong> Focuses on discovering patterns, anomalies, and insights from large datasets, often using techniques from both machine learning and statistical learning.</li>
<li><strong>Machine Learning:</strong> Emphasizes the development of algorithms that can learn from data and make predictions without explicit programming.</li>
<li><strong>Statistical Learning:</strong> A subfield of statistics that focuses on developing and understanding models and methods for learning from data, with a strong emphasis on statistical inference and uncertainty quantification.</li>
</ul>
<p>All three fields share the common goal of extracting knowledge and making predictions from data, but they differ in their emphasis and approaches.</p>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<ul>
<li><strong>Unsupervised learning</strong> is about finding patterns and structure in data <em>without</em> a response variable (no ‚Äúteacher‚Äù to guide the learning). It‚Äôs about discovering hidden relationships.</li>
<li><strong>PCA</strong> reduces dimensionality by finding linear combinations of features (principal components) that capture the most variance. It‚Äôs useful for visualization and pre-processing data for other analyses.</li>
<li><strong>Clustering</strong> aims to find subgroups (clusters) within the data, grouping similar observations together.
<ul>
<li><strong>K-means</strong> requires pre-specifying the number of clusters (K). It‚Äôs an iterative algorithm that minimizes within-cluster variation. It‚Äôs sensitive to initialization.</li>
<li><strong>Hierarchical clustering</strong> builds a hierarchy of clusters, represented by a dendrogram. It doesn‚Äôt require pre-specifying K, and it provides a visual representation of the relationships between observations.</li>
</ul></li>
<li>Choices of dissimilarity measure, linkage (for hierarchical clustering), and scaling can significantly affect clustering results. These choices should be made carefully and thoughtfully.</li>
<li>Clustering is a powerful, but often subjective and non-robust, technique. It‚Äôs best used for exploration and hypothesis generation, not for definitive conclusions. Always consider the limitations.</li>
</ul>
</section>
<section id="thoughts-and-discussion" class="level2">
<h2 class="anchored" data-anchor-id="thoughts-and-discussion">Thoughts and Discussion</h2>
<ul>
<li>Can you think of other real-world applications where unsupervised learning might be useful? Consider areas like image analysis (grouping similar images), anomaly detection (identifying unusual transactions), or natural language processing (clustering documents by topic).</li>
<li>What are the potential limitations of relying too heavily on clustering results without further validation? How could you try to validate the clusters you find? (e.g., using external data, domain expertise, or comparing results across different methods).</li>
<li>How might you combine supervised and unsupervised learning techniques in a single analysis? For example, could you use clustering to identify subgroups and then build separate supervised models for each subgroup (this is called ‚Äúcluster-then-predict‚Äù)? Or could you use PCA to reduce dimensionality before applying a supervised learning algorithm?</li>
<li>How do you understand the differences and connections between data mining, machine learning, and statistical learning? Can you give examples of techniques used in each field?</li>
<li>What is the biggest difference do you think supervised and unsupervised learning?</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/qiufei\.github\.io\/web-slide-r");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>üîã<a href="https://posit.co"><img src="https://posit.co/wp-content/themes/Posit/assets/images/posit-logo-2024.svg" class="img-fluid" alt="Posit" width="65"></a></p>
</div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 ÈÇ±È£û ¬© 2025
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://beian.miit.gov.cn">
<p>ÊµôICPÂ§á 2024072710Âè∑-1</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33021202002511">
<p>ÊµôÂÖ¨ÁΩëÂÆâÂ§á 33021202002511Âè∑</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:hfutqiufei@163.com">
      <i class="bi bi-envelope-at-fill" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>