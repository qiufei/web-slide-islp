---
title: "Support Vector Machines"
---

## Introduction

::: {style="color: #2E86C1;"}
Welcome to the fascinating world of Support Vector Machines (SVMs)! ü§ñ In this chapter, we embark on a journey to understand these powerful supervised learning models. SVMs are renowned for their ability to perform both classification and regression tasks, and they originated in the computer science community back in the 1990s. What makes them stand out? Their exceptional "out-of-the-box" performance ‚Äì meaning they often work remarkably well with minimal need for fine-tuning. üéØ
:::

## What is Data Mining?

::: {style="color: #2E86C1;"}
Imagine you're a treasure hunter ü™ô, but instead of searching for gold in caves, you're sifting through mountains of data! ‚õ∞Ô∏è That's essentially what data mining is all about. It's the art and science of discovering hidden patterns, trends, and valuable insights from massive datasets. üîç Data mining cleverly combines techniques from diverse fields like statistics and computer science to unearth knowledge that can empower decision-making.
:::

::: callout-note
Data mining uses techniques from various fields, including statistics and computer science.
:::

## Data Mining: A Deeper Dive

::: {style="color: #2E86C1;"}
Data mining isn't just about finding *any* information; it's about finding *useful* information. Think of it like this:
:::

## Data Mining: A Deeper Dive - Pattern Discovery

::: {style="color: #2E86C1;"}
-   **Pattern Discovery:** Identifying recurring sequences or relationships. For example, finding that customers who buy product A also tend to buy product B.
:::

## Data Mining: A Deeper Dive - Anomaly Detection

::: {style="color: #2E86C1;"}
-   **Anomaly Detection:** Spotting outliers or unusual events. This could be crucial for fraud detection or identifying system failures.
:::

## Data Mining: A Deeper Dive - Association Rule Learning

::: {style="color: #2E86C1;"}
-   **Association Rule Learning:** Uncovering rules that describe how data points relate to each other.
:::

## Data Mining: A Deeper Dive - Clustering

::: {style="color: #2E86C1;"}
-   **Clustering:** Grouping similar data points together. This is useful for customer segmentation or identifying different types of network traffic.
:::

## Data Mining: A Deeper Dive - Classification

::: {style="color: #2E86C1;"}
-   **Classification:** Assigning data points to predefined categories. This is what SVMs excel at!
:::

## What is Machine Learning?

::: {style="color: #2E86C1;"}
Machine learning (ML) is a branch of artificial intelligence (AI) that empowers computers to learn from data *without* the need for explicit programming. ü§ñ Think of it as teaching a computer to learn by example, much like we teach children! üë∂ Algorithms in machine learning are designed to improve their performance on specific tasks as they are exposed to more data.
:::

::: callout-note
Machine Learning involves algorithms that can improve their performance on a task as they are exposed to more data.
:::

## Machine Learning: Key Concepts

::: {style="color: #2E86C1;"}
Let's explore some fundamental concepts in Machine Learning.
:::

## Machine Learning: Supervised Learning

::: {style="color: #2E86C1;"}
-   **Supervised Learning:** The algorithm learns from labeled data (input-output pairs). This is the category SVMs fall into.
:::

## Machine Learning: Unsupervised Learning

::: {style="color: #2E86C1;"}
-   **Unsupervised Learning:** The algorithm learns from unlabeled data, discovering patterns and structures on its own.
:::

## Machine Learning: Reinforcement Learning

::: {style="color: #2E86C1;"}
-   **Reinforcement Learning:** The algorithm learns through trial and error, receiving rewards or penalties for its actions.
:::

## Machine Learning: Features

::: {style="color: #2E86C1;"}
-   **Features:** The input variables used for learning.
:::

## Machine Learning: Model

::: {style="color: #2E86C1;"}
-   **Model:** The mathematical representation learned from the data.
:::

## Machine Learning: Training

::: {style="color: #2E86C1;"}
-   **Training:** The process of fitting a model to the training data.
:::

## Machine Learning: Testing

::: {style="color: #2E86C1;"}
-   **Testing:** Evaluating the model's performance on unseen data.
:::

## What is Statistical Learning?

::: {style="color: #2E86C1;"}
Statistical learning is a fascinating blend of statistics and machine learning. ü§î It's all about developing and applying models and algorithms for prediction and inference. However, it places a strong emphasis on statistical properties and interpretability. Think of it as bridging the gap between theoretical statistics and practical machine learning, providing a robust framework to understand *why* certain models excel and how to enhance them.
:::

::: callout-note
Statistical learning focuses on developing and applying models and algorithms for prediction and inference.
:::

## Statistical Learning: The Core Principles

::: {style="color: #2E86C1;"}
Here are the core ideas driving Statistical Learning.
:::

## Statistical Learning: Statistical Models

::: {style="color: #2E86C1;"}
-   **Statistical Models:**  Using statistical models (like linear regression, logistic regression, etc.) to represent relationships in data.
:::

## Statistical Learning: Inference

::: {style="color: #2E86C1;"}
-   **Inference:** Drawing conclusions about the population from the sample data.
:::

## Statistical Learning: Prediction

::: {style="color: #2E86C1;"}
-   **Prediction:**  Estimating future outcomes based on the learned model.
:::

## Statistical Learning: Bias-Variance Trade-off

::: {style="color: #2E86C1;"}
-   **Bias-Variance Trade-off:** A fundamental concept in statistical learning.  Balancing the model's ability to fit the training data (low bias) with its ability to generalize to new data (low variance).
:::

## Statistical Learning: Model Selection

::: {style="color: #2E86C1;"}
-   **Model Selection:** Choosing the best model from a set of candidate models.
:::

## Statistical Learning: Regularization

::: {style="color: #2E86C1;"}
-   **Regularization:** Techniques to prevent overfitting by adding a penalty to the model's complexity.
:::

## Relationship Between Concepts

```{mermaid}
graph LR
    A[Data Mining] --> C(Common Ground)
    B[Machine Learning] --> C
    D[Statistical Learning] --> C
    C --> E[Insights & Predictions]
```

## Relationship Between Concepts (Explained)

::: {style="color: #2E86C1;"}
Let's break down the diagram:

-   **Data Mining** is the overarching field, encompassing the entire process of *knowledge discovery* from data. Think of it as the big picture. üñºÔ∏è
-   **Machine Learning** provides the *algorithms* that enable computers to learn from data. It's your trusty toolbox. üß∞
-   **Statistical Learning** offers a *theoretical framework* for understanding and improving these algorithms. It emphasizes statistical rigor and interpretability.  It's the blueprint that guides your work. üìê
-   They all converge on a **Common Ground**:  leveraging data to generate insights and make predictions. They're all working towards the same goal!
:::

## Support Vector Machines: Overview

::: {style="color: #2E86C1;"}
SVM is a generalization of a simpler classifier called the **maximal margin classifier.**  We will systematically explore the following concepts, each building upon the previous:
:::
## Support Vector Machines: Overview - Maximal Margin Classifier
::: {style="color: #2E86C1;"}
1.  **Maximal Margin Classifier:** This is the foundation, but it has a limitation ‚Äì it only works when data can be perfectly separated by a straight line (or a hyperplane in higher dimensions).
:::

## Support Vector Machines: Overview - Support Vector Classifier
::: {style="color: #2E86C1;"}
2.  **Support Vector Classifier:** This is an extension that allows for some mistakes (a "soft margin"). This makes it applicable to a wider range of datasets, even those that aren't perfectly separable.
:::
## Support Vector Machines: Overview - Support Vector Machine
::: {style="color: #2E86C1;"}
3.  **Support Vector Machine:** This is a further extension that uses a clever trick called "kernels" to handle situations where the boundary between classes is not a straight line.
:::

::: aside
**Note:**  People often use "support vector machines" as a blanket term. We'll be precise in distinguishing between the three concepts.
:::

## 9.1 Maximal Margin Classifier

### 9.1.1 What is a Hyperplane?

::: {style="color: #2E86C1;"}
A hyperplane is a flat, affine subspace with a dimension *one less* than its surrounding space. It's a generalization of familiar concepts like lines and planes.  Think of it as a "divider" in higher dimensions.  It separates the space into two halves.
:::

## Hyperplane: Dimensions

::: {style="color: #2E86C1;"}
Let's understand Hyperplanes across different dimensions.
:::

## Hyperplane: 2D

::: {style="color: #2E86C1;"}
-   **In 2D:** A hyperplane is simply a line.
:::

## Hyperplane: 3D

::: {style="color: #2E86C1;"}
-   **In 3D:** A hyperplane is a plane.
:::

## Hyperplane: p Dimensions

::: {style="color: #2E86C1;"}
-   **In p dimensions:** A hyperplane is a (p-1)-dimensional flat subspace, which divides the space into two half-spaces.
:::

## Hyperplane: Mathematical Definition

::: {style="color: #2E86C1;"}
A hyperplane in p-dimensional space is defined by the equation:
:::

$$
\beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p = 0
$$

## Hyperplane: Mathematical Definition - Explanation

::: {style="color: #2E86C1;"}
-   $X = (X_1, X_2, ..., X_p)^T$ represents a point in p-dimensional space.
-   $\beta_0, \beta_1, ..., \beta_p$ are the *parameters* (coefficients) of the hyperplane. These parameters determine the orientation and position of the hyperplane.  Changing these values changes the hyperplane.
:::

## Hyperplane: Sides

::: {style="color: #2E86C1;"}
The hyperplane equation neatly divides the space into two regions:
:::

## Hyperplane: Sides - Greater than Zero

::: {style="color: #2E86C1;"}
-   $\beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p > 0$: Points on *one side* of the hyperplane.
:::

## Hyperplane: Sides - Less than Zero

::: {style="color: #2E86C1;"}
-   $\beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p < 0$: Points on the *other side* of the hyperplane.
:::

## Hyperplane: Sides - Sign Interpretation

::: {style="color: #2E86C1;"}
-   The *sign* of the left-hand side tells you which side a point is on.
:::

## Hyperplane Example (2D)

![Hyperplane in 2D](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F6_7.svg)

::: {style="color: #2E86C1;"}
*   **FIGURE 9.1**: The blue area shows where $1 + 2X_1 + 3X_2 > 0$.
:::

## Hyperplane Example (2D)

![Hyperplane in 2D](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F6_7.svg)

::: {style="color: #2E86C1;"}
*  The purple area shows where $1 + 2X_1 + 3X_2 < 0$.
:::

## Hyperplane Example (2D)

![Hyperplane in 2D](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F6_7.svg)

::: {style="color: #2E86C1;"}
* The solid line is the hyperplane, defined by $1 + 2X_1 + 3X_2 = 0$.  Points *on* the line satisfy the equation.
:::

### 9.1.2 Classification Using a Separating Hyperplane

::: {style="color: #2E86C1;"}
Let's say we have training data with *n* observations and *p* features.  These observations fall into two classes, which we'll label as -1 and 1:
:::

$$
X = \begin{bmatrix}
x_{11} & \cdots & x_{1p} \\
\vdots & \ddots & \vdots \\
x_{n1} & \cdots & x_{np}
\end{bmatrix},
\quad
y = \begin{bmatrix}
y_1 \\
\vdots \\
y_n
\end{bmatrix},
\quad y_i \in \{-1, 1\}
$$

## Classification Using a Separating Hyperplane - Explanation

::: {style="color: #2E86C1;"}
- $X$ is the feature matrix (our input data).
- $y$ is the vector of class labels (what we want to predict).
:::

## Separating Hyperplane Condition

::: {style="color: #2E86C1;"}
If a *separating hyperplane* exists (meaning the classes are linearly separable ‚Äì we can draw a straight line/plane to divide them), it must satisfy this condition:

$$
y_i(\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_px_{ip}) > 0
$$

for all *i* = 1, ..., *n*.
:::

## Separating Hyperplane Condition - Explained

::: {style="color: #2E86C1;"}
 This is a crucial condition! It means that *all* points are correctly classified. The hyperplane perfectly separates the two classes.  The $y_i$ term ensures that the sign is always positive, regardless of the class.
:::

## Classification Rule

::: {style="color: #2E86C1;"}
We can classify a new test observation $x^* = (x_1^*, x_2^*, ..., x_p^*)^T$ by simply looking at the sign of:

$$
f(x^*) = \beta_0 + \beta_1x_1^* + \beta_2x_2^* + \dots + \beta_px_p^*
$$
:::

## Classification Rule - Explanation

::: {style="color: #2E86C1;"}
-   If  $f(x^*)$> 0, we assign the observation to class 1.
-   If  $f(x^*)$< 0, we assign the observation to class -1.
-   The *magnitude* (absolute value) of  $f(x^*)$ indicates the *confidence* of the classification. A larger magnitude means the point is farther from the hyperplane, and we're more confident in our classification. üí™
:::

## Separating Hyperplanes (Visual) - Multiple Hyperplanes

![Separating Hyperplanes](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_2.svg){width=50%}

::: {style="color: #2E86C1;"}
**FIGURE 9.2 Left:** When data is linearly separable, there are *many* possible separating hyperplanes that can perfectly divide the classes.
:::
## Separating Hyperplanes (Visual) - Question

![Separating Hyperplanes](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_2.svg){width=50%}

::: {style="color: #2E86C1;"}
This raises a critical question: Which one should we choose? ü§î We don't want just *any* separating hyperplane; we want the *best* one.
:::

## Separating Hyperplanes (Visual) - Decision Boundary

![Separating Hyperplanes](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_2.svg){width=50%}

::: {style="color: #2E86C1;"}
**FIGURE 9.2 Right:** The decision boundary created by a separating hyperplane.
:::

## Separating Hyperplanes (Visual) - Classification Grid

![Separating Hyperplanes](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_2.svg){width=50%}

::: {style="color: #2E86C1;"}
The blue and purple grids show how test observations would be classified, based on which side of the hyperplane they fall on.
:::

### 9.1.3 The Maximal Margin Classifier

::: {style="color: #2E86C1;"}
If our data *can* be perfectly separated by a hyperplane, we have an infinite number of choices.  The *maximal margin classifier* provides a principled way to choose the *best* one. It selects the hyperplane that maximizes the *margin*.
:::

## Maximal Margin Classifier - Margin Definition

::: {style="color: #2E86C1;"}
-   **Margin:** The *smallest* distance from the hyperplane to *any* training observation.  It's like creating the widest possible "street" üõ£Ô∏è separating the classes, with the hyperplane running down the middle.
:::
## Maximal Margin Classifier - Hyperplane Definition
::: {style="color: #2E86C1;"}
-   **Maximal Margin Hyperplane:** The separating hyperplane that achieves the *largest possible* margin. This is the hyperplane we want!
:::

## Maximal Margin Intuition

::: {style="color: #2E86C1;"}
The maximal margin hyperplane is the "mid-line" of the widest "slab" (or "street") that we can fit between the two classes *without* touching any data points.
:::

## Maximal Margin Intuition - Generalization
::: {style="color: #2E86C1;"}
 The intuition is that a larger margin on the training data will likely lead to a larger margin on the test data, resulting in better classification performance. We want the biggest "buffer zone" we can get.
:::

## Maximal Margin Classifier (Visual)

![Maximal Margin Classifier](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_3.svg)

::: {style="color: #2E86C1;"}
**FIGURE 9.3:** The *maximal margin hyperplane* is shown as a solid line.
:::

## Maximal Margin Classifier (Visual) - Margin

![Maximal Margin Classifier](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_3.svg)

::: {style="color: #2E86C1;"}
The dashed lines define the *margin*.
:::

## Maximal Margin Classifier (Visual) - Support Vectors

![Maximal Margin Classifier](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_3.svg)

::: {style="color: #2E86C1;"}
The points touching the dashed lines are the *support vectors* ‚Äì they are crucial!
:::

## Support Vectors

::: {style="color: #2E86C1;"}
-   **Support Vectors:** The training observations that lie *exactly* on the margin (the dashed lines in the previous figure). These are the "critical" points.
-   These points *support* the maximal margin hyperplane, meaning they determine its position and orientation. If these points move, the hyperplane *must* move! üîÑ
-   A key property of the maximal margin hyperplane is that it depends *only* on the support vectors, *not* on any other observations.  This is a very important characteristic!
:::

### 9.1.4 Construction of the Maximal Margin Classifier

::: {style="color: #2E86C1;"}
The maximal margin hyperplane is found by solving a specific *optimization problem*:
:::

$$
\begin{aligned}
&\underset{\beta_0, \beta_1, \dots, \beta_p, M}{\text{maximize}} && M \\
&\text{subject to} && \sum_{j=1}^p \beta_j^2 = 1, \\
& && y_i(\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_px_{ip}) \geq M \quad \forall i = 1, \dots, n.
\end{aligned}
$$

## Maximal Margin Classifier: Optimization Problem Explained - M

::: {style="color: #2E86C1;"}
-   **M:** The margin width, which we want to *maximize*. We're trying to find the widest possible "street."
:::

## Maximal Margin Classifier: Optimization Problem Explained - Constraint 1

::: {style="color: #2E86C1;"}
-   $\sum_{j=1}^p \beta_j^2 = 1$: This constraint ensures a *unique* solution.  It doesn't restrict the *hyperplane* itself (we can always rescale the coefficients), but it scales the coefficients so we get a specific solution.
:::

## Maximal Margin Classifier: Optimization Problem Explained - Constraint 2

::: {style="color: #2E86C1;"}
-   $y_i(\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_px_{ip}) \geq M$: This is the most important constraint. It ensures that *all* observations are on the *correct* side of the hyperplane *and* at least a distance *M* away (i.e., outside the margin). This is what guarantees correct classification and a margin of at least M.
:::

### 9.1.5 The Non-separable Case

::: {style="color: #2E86C1;"}
The maximal margin classifier has a significant limitation: it works *only* if a separating hyperplane *exists*.  If the classes overlap, even a little bit, no such hyperplane exists, and the optimization problem has *no* solution. üôÅ  This is a major drawback in real-world scenarios.
:::

## Non-separable Data (Visual)

![Non-separable Data](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_4.svg)

::: {style="color: #2E86C1;"}
**FIGURE 9.4:** An example where the classes are *not* linearly separable.
:::
## Non-separable Data (Visual) - Explanation

![Non-separable Data](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_4.svg)

::: {style="color: #2E86C1;"}
 There's no straight line we can draw to perfectly separate the blue and purple points. The maximal margin classifier cannot be used here.
:::

## 9.2 Support Vector Classifiers

### 9.2.1 Overview of the Support Vector Classifier

::: {style="color: #2E86C1;"}
To address the limitations of the maximal margin classifier (its inability to handle non-separable data and its sensitivity to individual observations), we introduce the *support vector classifier*, also known as the *soft margin classifier*.
:::

## Support Vector Classifier - The Key Idea

::: {style="color: #2E86C1;"}
 The key idea is to allow some observations to be on the *wrong side of the margin* or even the *wrong side of the hyperplane*. This makes the margin "soft" ‚òÅÔ∏è, allowing for some flexibility.
:::

## Why Soft Margins?

::: {style="color: #2E86C1;"}
Let's see why we need soft margins.
:::

## Why Soft Margins? - Non-Separable Data

::: {style="color: #2E86C1;"}
-   **Non-Separable Data:**  It's *absolutely necessary* for overlapping classes, where perfect separation is simply impossible.
:::

## Why Soft Margins? - Robustness

::: {style="color: #2E86C1;"}
-   **Robustness:**  It makes the classifier *less sensitive* to individual observations, which helps to reduce overfitting. A single outlier shouldn't drastically change the decision boundary.
:::

## Sensitivity to Outliers (Visual)

![Sensitivity to Outliers](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_5.svg)

::: {style="color: #2E86C1;"}
**FIGURE 9.5:** This figure demonstrates the *sensitivity* of the maximal margin classifier.
:::

## Sensitivity to Outliers (Visual) - Explanation

![Sensitivity to Outliers](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_5.svg)

::: {style="color: #2E86C1;"}
 Adding just *one* outlier (right panel) dramatically changes the maximal margin hyperplane. This highlights the need for a more robust approach.
:::

## Soft Margin Example (Visual) - Margin Violations

![Soft Margin Example](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_6.svg){width=50%}

::: {style="color: #2E86C1;"}
**FIGURE 9.6 Left:**  In the soft margin classifier, most points are correctly classified and *outside* the margin.
:::
## Soft Margin Example (Visual) - Margin Violations Explanation

![Soft Margin Example](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_6.svg){width=50%}

::: {style="color: #2E86C1;"}
 However, some points *violate* the margin ‚Äì they are allowed to be inside the "street."
:::

## Soft Margin Example (Visual) - Misclassifications

![Soft Margin Example](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_6.svg){width=50%}

::: {style="color: #2E86C1;"}
**FIGURE 9.6 Right:**  Some points are even *misclassified* (they are on the wrong side of the *hyperplane*).
:::

## Soft Margin Example (Visual) - Misclassifications Explanation

![Soft Margin Example](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_6.svg){width=50%}

::: {style="color: #2E86C1;"}
 The soft margin allows for this to happen, but the extent of these violations is controlled by a tuning parameter.
:::

### 9.2.2 Details of the Support Vector Classifier

::: {style="color: #2E86C1;"}
The support vector classifier solves a modified optimization problem that allows for margin violations:
:::

$$
\begin{aligned}
&\underset{\beta_0, \beta_1, \dots, \beta_p, \epsilon_1, \dots, \epsilon_n, M}{\text{maximize}} && M \\
&\text{subject to} && \sum_{j=1}^p \beta_j^2 = 1, \\
& && y_i(\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_px_{ip}) \geq M(1 - \epsilon_i), \\
& && \epsilon_i \geq 0, \quad \sum_{i=1}^n \epsilon_i \leq C.
\end{aligned}
$$

## Support Vector Classifier: Optimization Problem Explained - Slack Variables

::: {style="color: #2E86C1;"}
-   $\epsilon_1, \dots, \epsilon_n$: These are called *slack variables*.  They allow observations to be on the wrong side of the margin or hyperplane.  Each observation gets its own slack variable.
:::

## Support Vector Classifier: Optimization Problem Explained - Tuning Parameter

::: {style="color: #2E86C1;"}
-   *C*: A non-negative *tuning parameter*.  Think of it as a "budget" for violations.  It controls the trade-off between maximizing the margin width and minimizing the number and severity of violations.
:::

## Support Vector Classifier: Optimization Problem Explained - Other Parts

::: {style="color: #2E86C1;"}
-   The other parts (M and the first constraint) are the same as in the maximal margin classifier.
:::

## Slack Variables Explained - No Violation

::: {style="color: #2E86C1;"}
-   $\epsilon_i = 0$: The *i*th observation is on the *correct* side of the margin (no violation).
:::

## Slack Variables Explained - Margin Violation

::: {style="color: #2E86C1;"}
-   $\epsilon_i > 0$: The *i*th observation is on the *wrong* side of the margin (a margin violation).
:::

## Slack Variables Explained - Misclassification

::: {style="color: #2E86C1;"}
-   $\epsilon_i > 1$: The *i*th observation is on the *wrong* side of the *hyperplane* (it's misclassified).
:::

## Tuning Parameter *C* - Zero Budget

::: {style="color: #2E86C1;"}
-   **C = 0:**  No budget for violations at all.  This reduces to the maximal margin classifier (if the data is separable; otherwise, there's no solution).
:::

## Tuning Parameter *C* - Small C

::: {style="color: #2E86C1;"}
-   **Small C:** We prioritize a narrow margin and allow fewer violations. This can lead to a model that is less prone to overfitting but might have higher bias.
:::

## Tuning Parameter *C* - Large C

::: {style="color: #2E86C1;"}
-   **Large C:**  We allow a wider margin and more violations.  This can lead to a model that might have lower bias but is potentially more prone to overfitting.
:::
## Tuning Parameter *C* - Cross-Validation

::: {style="color: #2E86C1;"}
-   **C** is typically chosen using *cross-validation*, a technique for evaluating model performance on unseen data.
:::

## Impact of C (Visual)

![Impact of C](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_7.svg)

::: {style="color: #2E86C1;"}
**FIGURE 9.7:**  This figure shows the effect of different *C* values.
:::

## Impact of C (Visual) - Explanation

![Impact of C](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_7.svg)

::: {style="color: #2E86C1;"}
 A larger *C* results in a wider margin and *more* support vectors (because more points are violating the margin).
:::

## Support Vectors (Revisited)

::: {style="color: #2E86C1;"}
In the support vector classifier, the *support vectors* are the observations that:
:::

## Support Vectors (Revisited) - On Margin

::: {style="color: #2E86C1;"}
-   Lie *exactly on* the margin ($\epsilon_i = 0$ and correctly classified)
:::

## Support Vectors (Revisited) - Wrong Side of Margin

::: {style="color: #2E86C1;"}
-   Lie on the *wrong side* of the margin ($0 < \epsilon_i \leq 1$)
:::

## Support Vectors (Revisited) - Wrong Side of Hyperplane

::: {style="color: #2E86C1;"}
-   Lie on the *wrong side* of the hyperplane ($\epsilon_i > 1$)
:::

## Support Vectors (Revisited) - Influence

::: {style="color: #2E86C1;"}
*Only* the support vectors affect the hyperplane's position and orientation.  Observations that are on the correct side of the margin and sufficiently far away have *no* influence. This contributes to the robustness of the SVM.
:::

## 9.3 Support Vector Machines

### 9.3.1 Classification with Non-Linear Decision Boundaries

::: {style="color: #2E86C1;"}
The support vector classifier finds *linear* decision boundaries (hyperplanes). But what if the true boundary between classes is non-linear? ü§î
:::

## Non-Linear Decision Boundaries - Feature Expansion

::: {style="color: #2E86C1;"}
One approach is to enlarge the feature space by adding polynomial features (e.g.,  $X_1^2$,  $X_1X_2$, etc.).  This *can* make the data linearly separable in the *enlarged* feature space.  However, this can be computationally expensive.
:::

## Non-linear Data (Visual)

![Non-linear Data](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_8.svg){width=50%}

::: {style="color: #2E86C1;"}
**FIGURE 9.8 Left:** It's clear that a non-linear boundary is needed to separate these classes effectively.
:::

## Non-linear Data (Visual) - Linear Ineffectiveness

![Non-linear Data](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_8.svg){width=50%}

::: {style="color: #2E86C1;"}
A straight line simply won't do.
:::

## Linear Classifier on Non-linear Data

![Non-linear Data](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_8.svg){width=50%}

::: {style="color: #2E86C1;"}
**FIGURE 9.8 Right:** A linear classifier performs *poorly* on this data.
:::
## Linear Classifier on Non-linear Data - Explanation
![Non-linear Data](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_8.svg){width=50%}

::: {style="color: #2E86C1;"}
 It cannot capture the non-linear relationship between the features and the class labels.
:::

### Feature Expansion: A Potential Problem

::: {style="color: #2E86C1;"}
Explicitly enlarging the feature space by adding polynomial features or other transformations *can* become computationally expensive (or even impossible for very high-dimensional or infinite-dimensional feature spaces). üò©  We need a more efficient way to handle non-linearity.
:::

### 9.3.2 The Support Vector Machine

::: {style="color: #2E86C1;"}
The *support vector machine (SVM)* is a powerful extension of the support vector classifier. It uses *kernels* to implicitly enlarge the feature space *without* explicitly calculating the transformed features. This is known as the "kernel trick"! ‚ú® It's a brilliant mathematical technique.
:::
## Support Vector Machine - Key Idea
::: {style="color: #2E86C1;"}
**Key Idea:** The solution to the support vector classifier depends *only* on the *inner products* of the observations, *not* on the observations themselves. This is the foundation of the kernel trick.
:::

## Inner Product

::: {style="color: #2E86C1;"}
**Inner Product:** The inner product between two vectors,  $x_i$ and $x_{i'}$, is calculated as:
:::
$$
\langle x_i, x_{i'} \rangle = \sum_{j=1}^p x_{ij}x_{i'j}
$$

## Inner Product - Explanation

::: {style="color: #2E86C1;"}
The inner product is a measure of similarity between the two vectors.
:::

## Kernels

::: {style="color: #2E86C1;"}
A *kernel* is a function that quantifies the similarity between two observations:
:::

$$
K(x_i, x_{i'})
$$

## Kernels - Linear Kernel

::: {style="color: #2E86C1;"}
-   **Linear Kernel:** $K(x_i, x_{i'}) = \langle x_i, x_{i'} \rangle = \sum_{j=1}^p x_{ij}x_{i'j}$.  Using this kernel gives you the standard support vector *classifier*.
:::

## Kernels - Polynomial Kernel

::: {style="color: #2E86C1;"}
-   **Polynomial Kernel:** $K(x_i, x_{i'}) = (1 + \langle x_i, x_{i'} \rangle)^d$. This implicitly uses polynomial features of degree *d*.  You don't have to *explicitly* calculate the polynomial features!
:::

## Kernels - Radial Kernel

::: {style="color: #2E86C1;"}
-   **Radial Kernel:** $K(x_i, x_{i'}) = \exp(-\gamma \sum_{j=1}^p (x_{ij} - x_{i'j})^2)$.  This is a very popular choice and corresponds to an *infinite-dimensional* feature space!  It's incredibly powerful.
:::

## The SVM: Using Kernels

::: {style="color: #2E86C1;"}
By replacing the inner product $\langle x_i, x_{i'} \rangle$ with a kernel $K(x_i, x_{i'})$ in the support vector classifier algorithm, we obtain the SVM. The resulting decision function is:
:::

$$
f(x) = \beta_0 + \sum_{i \in S} \alpha_i K(x, x_i)
$$

## The SVM: Using Kernels - Explanation

::: {style="color: #2E86C1;"}
where *S* is the set of *support vector* indices, and the $\alpha_i$ are parameters learned during training. Notice that the decision function depends *only* on the support vectors and the kernel function.
:::

## SVM with Polynomial Kernel (Visual)

![SVM with Polynomial Kernel](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_9.svg){width=50%}

::: {style="color: #2E86C1;"}
**FIGURE 9.9 Left:** An SVM with a polynomial kernel (degree 3) fits the non-linear data *much better* than a linear classifier could.
:::

## SVM with Polynomial Kernel (Visual) - Decision Boundary

![SVM with Polynomial Kernel](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_9.svg){width=50%}

::: {style="color: #2E86C1;"}
 The decision boundary is curved.
:::

## SVM with Radial Kernel (Visual)

![SVM with Radial Kernel](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_9.svg){width=50%}

::: {style="color: #2E86C1;"}
**FIGURE 9.9 Right:** An SVM with a radial kernel also captures the non-linear boundary effectively.
:::

## Radial Kernel Intuition

::: {style="color: #2E86C1;"}
-   The radial kernel has *local* behavior.
-   If a test observation $x^*$ is *far* from a training observation $x_i$, then $K(x^*, x_i)$ is very *small*.  This means that $x_i$ has *very little* influence on the prediction for $x^*$.
-   Only *nearby* training observations have a significant impact on the prediction.
-   $\gamma$ is a tuning parameter that controls the "reach" or "width" of the kernel. A larger $\gamma$ makes the kernel more "local" (only very close points have influence), while a smaller $\gamma$ makes it more "global" (more distant points can have some influence).
:::

## Computational Advantage of Kernels

::: {style="color: #2E86C1;"}
The *crucial* advantage of kernels is that we only need to compute  $K(x_i, x_{i'})$ for all *pairs* of training observations.  We *never* need to explicitly compute the (potentially infinite-dimensional!) feature mapping. This makes the computation feasible, even for incredibly complex feature spaces. This is the magic of the "kernel trick"!
:::

### 9.3.3 An Application to the Heart Disease Data
::: {style="color: #2E86C1;"}
The text compares SVM to LDA on the Heart Disease Data, using ROC curves for both training and testing.
:::
## ROC Curve on Training Set

![ROC_train](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_10.svg){width=50%}

::: {style="color: #2E86C1;"}
**FIGURE 9.10 Left:** On the training data, the support vector classifier (which is equivalent to an SVM with a linear kernel) performs *slightly* better than LDA (Linear Discriminant Analysis).
:::

## ROC Curve on Training Set: Overfitting?

![ROC_train](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_10.svg){width=50%}

::: {style="color: #2E86C1;"}
**FIGURE 9.10 Right:** The SVM using a radial basis kernel with $\gamma = 10^{-1}$ shows *almost perfect* performance on the *training* set.
:::

## ROC Curve on Training Set: Overfitting? - Explanation

![ROC_train](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_10.svg){width=50%}

::: {style="color: #2E86C1;"}
 This is a strong warning sign of *overfitting*! The model is likely too complex and has memorized the training data instead of learning the underlying patterns.
:::

## ROC Curve on Test Set

![ROC_test](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_11.svg){width=50%}

::: {style="color: #2E86C1;"}
**FIGURE 9.11 Left:** On the *test* data, the support vector classifier continues to have a *small advantage over LDA.
:::

##  ROC Curve on Test Set - Explanation

![ROC_test](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_11.svg){width=50%}

::: {style="color: #2E86C1;"}
 This is a more reliable measure of performance than the training set results.
:::

## ROC Curve on Test Set: Overfitting Confirmed

![ROC_test](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_11.svg){width=50%}

::: {style="color: #2E86C1;"}
**FIGURE 9.11 Right:** On the *test* data, the SVM with the radial basis kernel and  $\gamma = 10^{-1}$ (which had near-perfect training performance) now performs the *worst*.
:::
## ROC Curve on Test Set: Overfitting Confirmed Explanation

![ROC_test](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_11.svg){width=50%}

::: {style="color: #2E86C1;"}
 This confirms our suspicion of overfitting. The model that looked best on the training data is actually the worst on new data!  The SVMs with  $\gamma = 10^{-2}$ and $\gamma = 10^{-3}$ perform similarly to the support vector classifier (linear kernel), suggesting that a less complex model is better for this dataset.
:::

## 9.4 SVMs with More than Two Classes

::: {style="color: #2E86C1;"}
SVMs are naturally designed for *binary* classification (handling only two classes). To extend them to situations with K > 2 classes, two common strategies are employed:
:::

## SVMs with More than Two Classes - One-versus-One (OvO)

::: {style="color: #2E86C1;"}
1.  **One-versus-One (OvO):**  We construct $\binom{K}{2}$ SVMs, each comparing a *pair* of classes. For example, if we have classes A, B, and C, we build SVMs for A vs. B, A vs. C, and B vs. C. To classify a test observation, we use all the SVMs and assign the observation to the class that wins the most "votes." üó≥Ô∏è
:::

## SVMs with More than Two Classes - One-versus-All (OvA)

::: {style="color: #2E86C1;"}
2.  **One-versus-All (OvA):** We fit K separate SVMs, each comparing *one* class to the *rest* of the classes. For example, with classes A, B, and C, we build SVMs for A vs. (B and C), B vs. (A and C), and C vs. (A and B). We classify a test observation to the class for which the decision function value is the highest.
:::

## One-versus-One (OvO) and One-versus-All (OvA) Comparison

| Method          | Number of Classifiers | Complexity                               | Potential Issues                      |
|-----------------|-----------------------|------------------------------------------|---------------------------------------|
| One-versus-One  |   $\binom{K}{2}$        |   Quadratic in the number of classes     |  Can be computationally expensive for large K |
| One-versus-All  |   $K$                |   Linear in the number of classes         |   Class imbalance can affect performance |

::: {style="color: #2E86C1;"}
Generally, if the number of classes *K* is large, one-versus-all may be preferred for computational reasons.
:::

## 9.5 Relationship to Logistic Regression

::: {style="color: #2E86C1;"}
It turns out that SVMs are closely related to logistic regression! The support vector classifier can be rewritten in a "Loss + Penalty" form, which reveals this connection:
:::

$$
\underset{\beta_0, \beta_1, \dots, \beta_p}{\text{minimize}} \left\{ \sum_{i=1}^n \max[0, 1 - y_i(\beta_0 + \beta_1x_{i1} + \dots + \beta_px_{ip})] + \lambda \sum_{j=1}^p \beta_j^2 \right\}
$$

## Relationship to Logistic Regression - Explanation

::: {style="color: #2E86C1;"}
- The loss function, $\sum_{i=1}^n \max[0, 1 - y_i(\beta_0 + \beta_1x_{i1} + \dots + \beta_px_{ip})]$, is called the *hinge loss*. It penalizes misclassifications and margin violations.
- The penalty term, $\lambda \sum_{j=1}^p \beta_j^2$, is a *ridge* penalty (also known as L2 regularization). It encourages smaller coefficient values, which helps to prevent overfitting.
:::

## Hinge Loss vs. Logistic Regression Loss

![Loss compare](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_12.svg)

::: {style="color: #2E86C1;"}
**FIGURE 9.12:** This figure compares the hinge loss (used in SVMs) and the loss function used in logistic regression.
:::

## Hinge Loss vs. Logistic Regression Loss - Similarity

![Loss compare](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_12.svg)

::: {style="color: #2E86C1;"}
 They are remarkably similar!
:::
## Hinge Loss vs. Logistic Regression Loss - Hinge Loss Behavior
![Loss compare](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_12.svg)

::: {style="color: #2E86C1;"}
-   **Hinge Loss:** It's *exactly* zero for observations that are correctly classified and lie beyond the margin.
:::
## Hinge Loss vs. Logistic Regression Loss - Logistic Regression Loss Behavior
![Loss compare](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_12.svg)

::: {style="color: #2E86C1;"}
-   **Logistic Regression Loss:** It's never *exactly* zero, but it can become very close to zero for confident, correct predictions.
:::

## SVM vs. Logistic Regression: Conclusion

::: {style="color: #2E86C1;"}
Because of the similarity between their loss functions, SVM and logistic regression often produce similar results. However, there are some general guidelines:
:::

## SVM vs. Logistic Regression: SVM Preference

::: {style="color: #2E86C1;"}
-   **SVM:** Tends to perform better when the classes are *well-separated*.
:::

## SVM vs. Logistic Regression: Logistic Regression Preference

::: {style="color: #2E86C1;"}
-   **Logistic Regression:** Often preferred when classes *overlap* or when *probabilistic outputs* are desired (logistic regression naturally provides probabilities, while SVMs don't).
:::

## Summary

::: {style="color: #008080;"}
-   **Support Vector Machines (SVMs)** are powerful and versatile tools for classification.
-   **Maximal Margin Classifier:** The fundamental concept, applicable only to linearly separable data.
-   **Support Vector Classifier (Soft Margin):**  Handles non-separable data and enhances robustness by allowing margin violations.
-   **Support Vector Machine (Kernel Trick):**  Efficiently handles non-linear decision boundaries by using kernels to implicitly map data to high-dimensional spaces.
-   **Kernels:**  Functions that quantify the similarity between observations.  The radial kernel is a particularly popular and powerful choice.
-   **Support Vectors:**  The crucial observations that define the decision boundary. Only these points influence the model.
-   **Tuning Parameters:** *C* (in the soft margin classifier) and kernel parameters (like  $\gamma$  for the radial kernel) control the bias-variance trade-off.
-   **Relationship to Logistic Regression:**  SVMs are closely related to logistic regression, with the hinge loss being similar to the logistic regression loss function.
:::

## Thoughts and Discussion

::: {style="color: #008080;"}
Let's discuss some thought-provoking questions related to SVMs.
:::

## Thoughts and Discussion - Question 1

::: {style="color: #008080;"}
1.  **Why are support vectors so important?** What does this tell us about the robustness of SVMs?
    *Hint: Consider which points have an influence on the decision boundary and which points don't.*
:::

## Thoughts and Discussion - Question 2

::: {style="color: #008080;"}
2.  **How does the choice of kernel and its parameters affect the SVM's decision boundary?**  Specifically, think about the  $\gamma$  parameter in the radial kernel.
    *Hint:  Consider the concepts of "local" versus "global" behavior. How does  $\gamma$  control this?*
:::

## Thoughts and Discussion - Question 3

::: {style="color: #008080;"}
3.  **When might you prefer logistic regression over an SVM, and vice-versa?** Consider the characteristics of your data and the assumptions of each method.
    *Hint: Think about the degree of class separability and whether you need probabilistic outputs.*
:::

## Thoughts and Discussion - Question 4

::: {style="color: #008080;"}
4. **How does the `one-versus-one` method compare to the `one-versus-all` approach in terms of computational complexity and performance?**
     *Hint: Consider how many classifiers are needed and the potential impact of class imbalance.*
:::

## Thoughts and Discussion - Question 5
::: {style="color: #008080;"}
5.  **Can we apply the kernel trick to other linear models besides the support vector classifier?** If so, how?
    *Hint: Think about other models that rely on inner products between observations.*
:::

