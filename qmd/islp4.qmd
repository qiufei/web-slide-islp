---
title: "Statistical Learning: Classification"
---

## Welcome to Classification! üëã

::: {layout-ncol=1}
So far, we've journeyed through the world of **regression**, where we predicted *numbers*. Now, we're switching gears to **classification**, where we predict *categories*.  Think of it like sorting objects into different boxes! üì¶
:::

## Regression vs. Classification: The Big Picture

::: {layout-ncol=1}
::: {.callout-note appearance="simple"}
**Regression:** Predicting a *quantitative* (numerical) response.
:::
-   Examples:
    -   Predicting house prices üè†
    -   Estimating sales revenue üí∞
    -   Forecasting stock prices üìà
:::

## Regression vs. Classification: The Big Picture (Continued)

::: {layout-ncol=1}
::: {.callout-note appearance="simple"}
**Classification:** Predicting a *qualitative* (categorical) response.
:::
-   Examples:
    -   Diagnosing a disease (present or absent) ‚öïÔ∏è
    -   Filtering spam emails (spam or not spam) üìß
    -   Detecting fraudulent transactions (fraud or not fraud) üí≥
:::

## What is Classification?

::: {layout-ncol=1}
-   **Classification** is like sorting objects into distinct groups based on their features.
-   We assign each observation to a specific category or class.

```{mermaid}
graph LR
    A[Observation 1] --> C(Class A)
    B[Observation 2] --> D(Class B)
    C --> E[Classification]
    D --> E
```
:::

##  Classification: Probabilities First

- Many classification methods start by predicting the *probability* of an observation belonging to each category.
-  Then, based on these probabilities (and a threshold), we make the final classification.

::: {layout-ncol=1}
-   **Example:**
    -   An email: 70% chance of being spam, 30% chance of not being spam.
    -   With a 50% threshold, we'd classify it as spam! üìß
:::

## Classification vs. Regression: A Clear Comparison

::: {layout-ncol=1}

| Feature           | Regression                             | Classification                           |
| :---------------- | :------------------------------------- | :--------------------------------------- |
| Response Variable | Quantitative (numerical)               | Qualitative (categorical)                |
| Goal              | Predict a *number*                     | Predict a *category*                     |
| Example           | Predict house price                    | Predict disease presence (yes/no)       |
| Output            | Continuous value                       | Discrete category                        |

: {tbl-colwidths="[20,40,40]"}
:::

## Why NOT Linear Regression for Categories? ü§î

- Linear regression is designed for *numbers*.  What happens if we try to use it for categories?  Let's consider predicting a medical diagnosis with three possibilities:

    1.  Stroke
    2.  Drug overdose
    3.  Epileptic seizure

## A Problematic Coding Scheme ü§ï

- We *could* try to code these numerically:

    -   1 = Stroke
    -   2 = Drug overdose
    -   3 = Epileptic seizure

- But this approach introduces serious problems!

![Medical conditions coding](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2Fcoding.png){fig-align="center"}

## The Problems with Numerical Coding ‚ùå

::: {layout-ncol=1}
-   **Problem 1: Arbitrary Order:** The coding implies an order (stroke < drug overdose < epileptic seizure) that is medically *meaningless*.

-   **Problem 2: Equal Differences:**  It suggests equal differences between conditions (stroke to drug overdose = drug overdose to seizure), which is also *not* medically valid.

-   **Problem 3: Inconsistent Results:**  Different coding schemes (e.g., 1=drug overdose, 2=seizure, 3=stroke) would lead to completely *different* models and predictions!
:::

##  Binary Responses: A Special Case?

-   **Binary Responses (Two Categories):**  If we have only *two* categories (yes/no, true/false), we *can* code them as 0 and 1. Linear regression *can* be used in this case.

-   **However:**
    -   Linear regression might predict probabilities *outside* the [0, 1] range. This is nonsensical.
    -   Better methods exist that are specifically designed for binary classification (like logistic regression!).

## Example: The Default Data üí≥

- We'll use a simulated dataset called "Default" to explore classification.

::: {layout-ncol=1}
-   **Goal:** Predict whether an individual will *default* on their credit card payment.
-   **Response:** `default` (Yes/No) ‚Äì a binary, qualitative variable.
-   **Predictors:**
    -   `balance`: Credit card balance (quantitative).
    -   `income`: Annual income (quantitative).
:::

## Visualizing the Default Data

![The Default data set](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_1.svg){fig-align="center" width="800"}

## Default Data: Left Panel - Scatterplot

::: {layout-ncol=1}
![The Default data set](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_1.svg){fig-align="center" width="800"}
:::

-   **Left Panel (Scatterplot):**
    -   `income` (x-axis) vs. `balance` (y-axis).
    -   Orange: Individuals who defaulted.
    -   Blue: Individuals who did not default.
    -   We see *some* separation: defaulters tend to have higher balances, but there's overlap.

## Default Data: Center Panel - Balance Boxplots

::: {layout-ncol=1}
![The Default data set](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_1.svg){fig-align="center" width="800"}
:::

-   **Center Panel (Boxplots of Balance):**
    -   Compares `balance` distribution for defaulters (orange) and non-defaulters (blue).
    -   The orange boxplot is shifted *higher*, indicating higher balances for defaulters on average.
    -   Boxes show the interquartile range (IQR), and whiskers show the range (excluding outliers).

## Default Data: Right Panel - Income Boxplots

::: {layout-ncol=1}
![The Default data set](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_1.svg){fig-align="center" width="800"}
:::

-   **Right Panel (Boxplots of Income):**
    -   Compares `income` distribution for defaulters (orange) and non-defaulters (blue).
    -   The relationship is *much less clear* than with `balance`. The boxplots largely overlap.

## Default Data: Key Takeaway

- The visualizations suggest that `balance` is a *stronger predictor* of `default` than `income`. Higher balances seem associated with a higher probability of default. This guides our model building.

## Logistic Regression: Modeling the *Probability*

-   Logistic regression doesn't directly model the *response* (`default`). Instead, it models the *probability* that the response belongs to a specific category.

::: {layout-ncol=1}
-   **Specifically:** We model Pr(default = Yes | balance, income) ‚Äì the probability of defaulting, *given* balance and income.
-   This probability will always be between 0 and 1, which makes perfect sense! üëç
:::

## Logistic Regression: From Probability to Classification

-   Once we have the estimated probability of default, we can *classify*.

::: {layout-ncol=1}
-   **Example:**
    -   If Pr(default = Yes | balance, income) > 0.5, predict "default = Yes".
    -   If Pr(default = Yes | balance, income) ‚â§ 0.5, predict "default = No".
-   The 0.5 threshold is common, but it's adjustable.
:::

## The Logistic Function: Bending Between 0 and 1

-   We need a function that always outputs a value between 0 and 1, no matter the input. The **logistic function** is perfect for this:

$$
p(X) = \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}}
$$

##  Logistic Function: Breaking it Down

-   *p(X)*: Probability of the event (e.g., default) given predictor(s) *X*.
-   Œ≤‚ÇÄ and Œ≤‚ÇÅ: Coefficients estimated from the data. They control the curve's shape and position.
-   *e*: The base of the natural logarithm (‚âà 2.718).

-   **Crucial Feature:**  No matter the value of Œ≤‚ÇÄ + Œ≤‚ÇÅX, the output *p(X)* is *always* between 0 and 1.

## The Logistic Function: An S-Shaped Curve

-   The logistic function creates an S-shaped curve (a sigmoid).

```{mermaid}
graph LR
    A[Low X] --> B(Low p(X) ~ 0)
    B --> C{Increasing X}
    C --> D(Increasing p(X))
    D --> E[High X]
    E --> F(High p(X) ~ 1)
    style C fill:#f9f,stroke:#333,stroke-width:2px
```

-   As X increases, p(X) smoothly transitions from near 0 to near 1.

## Linear vs. Logistic: A Visual Comparison

![Classification using the Default data](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_2.svg){fig-align="center" width="800"}

## Linear Regression: The Problem

::: {layout-ncol=1}
![Classification using the Default data](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_2.svg){fig-align="center" width="800"}
:::

-   **Left Panel (Linear Regression):**
    -   Blue line: Fit of a linear regression to the `default` data (0/1 coded).
    -   *Major Issue:*  For low/high `balance`, predicted probabilities are *outside* [0, 1]!  Probabilities can't be negative or greater than 1. üôÅ

## Logistic Regression: The Solution

::: {layout-ncol=1}
![Classification using the Default data](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_2.svg){fig-align="center" width="800"}
:::

-   **Right Panel (Logistic Regression):**
    -   Orange curve: Fit of a logistic regression.
    -   Predicted probabilities are *always* between 0 and 1, as they should be! üôÇ
    -   The characteristic S-shape.

## Linear vs. Logistic: The Verdict

- Logistic regression is *far* more appropriate for modeling probabilities and binary outcomes. It respects the fundamental constraint that probabilities must be between 0 and 1.

##  Odds: Another Way to Think About Probability

-   We can rewrite the logistic model to highlight its connection to *odds*:

$$
\frac{p(X)}{1 - p(X)} = e^{\beta_0 + \beta_1X}
$$

-   **Left-hand side:** The *odds* of the event (e.g., default).
    -   Odds range from 0 to ‚àû.
    -   Odds = 1: Event is equally likely to happen or not.
    -   Odds > 1: Event is *more* likely to happen.
    -   Odds < 1: Event is *less* likely to happen.
- **Relationship:** Odds = p / (1 - p)

- **Example:** If p(X) = 0.8, odds are 0.8 / (1 - 0.8) = 4. The event is four times more likely to happen than not.

##  Log-Odds (Logit): The Linear Connection

-   Taking the *logarithm* of both sides of the odds equation:

$$
\log\left(\frac{p(X)}{1 - p(X)}\right) = \beta_0 + \beta_1X
$$

-   **Left-hand side:** The *log-odds* or *logit*.
-   **Right-hand side:** A *linear* function of X.

-   **Key Insight:** Logistic regression models the *log-odds* as a linear function of the predictors. This explains the "logistic" in its name, even though it's for classification!

##  Interpreting Logistic Regression Coefficients

-   **Linear Regression:**  Œ≤‚ÇÅ is the *average change in Y* for a one-unit increase in X.

-   **Logistic Regression:** The interpretation is different because we're modeling the log-odds.

::: {layout-ncol=1}
-   Œ≤‚ÇÅ is the *change in the log-odds* for a one-unit increase in X.
-   Equivalently, a one-unit increase in X *multiplies the odds* by e<sup>Œ≤‚ÇÅ</sup>.

-   Because of the non-linear S-curve, the *amount* p(X) changes for a one-unit increase in X depends on the *current value* of X. The change is not probability.
:::

##  Estimating Coefficients: Maximum Likelihood (MLE)

-   We use **maximum likelihood estimation (MLE)** to find the best estimates for Œ≤‚ÇÄ, Œ≤‚ÇÅ, etc.

-   **Goal of MLE:** Find the coefficient values that make the *observed data* most likely.  We want to maximize the probability of observing the actual outcomes (defaults/non-defaults) in our training data, *given* the coefficients.

## The Likelihood Function: Making Data Likely

-   The **likelihood function** for logistic regression is:

$$
l(\beta_0, \beta_1) = \prod_{i:y_i=1} p(x_i) \prod_{i':y_{i'}=0} (1 - p(x_{i'}))
$$

##  Likelihood Function: Demystified

-   This formula might look complex, but the core idea is simple.

-   We multiply the probabilities of observing each data point, *given* coefficients Œ≤‚ÇÄ and Œ≤‚ÇÅ.

-   **First Product:**  Œ†<sub>i:y·µ¢=1</sub> p(x·µ¢)
    -   Considers individuals who *did* default (y·µ¢ = 1).
    -   For each, p(x·µ¢) is the probability of default, given their predictors (x·µ¢) and the current coefficients.

-   **Second Product:**  Œ†<sub>i':y·µ¢'=0</sub> (1 - p(x·µ¢'))
    -   Considers individuals who *did not* default (y·µ¢' = 0).
    -   For each, 1 - p(x·µ¢') is the probability of *not* defaulting.

-   **Overall:** Multiplying these probabilities gives the likelihood of the entire dataset, given the coefficients. MLE finds the coefficients that maximize this product.

##  Optimization: Letting the Computer Do the Work

-   We don't maximize the likelihood function by hand! Statistical software (like R) has built-in optimization algorithms.

-   These algorithms find the values of Œ≤‚ÇÄ, Œ≤‚ÇÅ, etc., that maximize the likelihood, giving us the *maximum likelihood estimates*.

## Making Predictions: From Coefficients to Probabilities

-   With the estimated coefficients (Œ≤ÃÇ‚ÇÄ, Œ≤ÃÇ‚ÇÅ, etc.), we can predict the probability of default for *any* given predictor values.

-   Plug the values into the logistic function:

   p(X) = e<sup>(Œ≤ÃÇ‚ÇÄ + Œ≤ÃÇ‚ÇÅX)</sup> / (1 + e<sup>(Œ≤ÃÇ‚ÇÄ + Œ≤ÃÇ‚ÇÅX)</sup>)

## Prediction Example: Putting it into Practice

-   Suppose we get these estimated coefficients (illustrative values):

    -   Œ≤ÃÇ‚ÇÄ = -2
    -   Œ≤ÃÇ‚ÇÅ = 0.005 (where X is `balance`)

-   Predict default probability for two individuals:

    1.  Individual A: `balance` = $1,000
    2.  Individual B: `balance` = $2,000

-   **Calculations:**

    -   Individual A: p(X) = e<sup>(-2 + 0.005 * 1000)</sup> / (1 + e<sup>(-2 + 0.005 * 1000)</sup>) ‚âà 0.95
    -   Individual B: p(X) = e<sup>(-2 + 0.005 * 2000)</sup> / (1 + e<sup>(-2 + 0.005 * 2000)</sup>) ‚âà 1

-   **Predictions:**

    -   Individual A: Estimated probability of default ‚âà 95%.
    -   Individual B: Estimated probability of default ‚âà 100%.

##  From Probabilities to Classes: The Threshold

-   After calculating probabilities, we *classify* using a threshold.

-   **Common Threshold:** 0.5

-   **Classification Rule:**

    -   If p(X) > 0.5, predict "default = Yes".
    -   If p(X) ‚â§ 0.5, predict "default = No".

-   **In our example:**

    -   Individual A: Predict "default = Yes" (0.95 > 0.5).
    -   Individual B: Predict "default = Yes" (1 > 0.5).

## Multiple Logistic Regression: More Predictors!

-   Like linear regression, we can include *multiple* predictors.

-   **The Model:**

$$
\log\left(\frac{p(X)}{1 - p(X)}\right) = \beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p
$$

$$
p(X) = \frac{e^{\beta_0 + \beta_1X_1 + \dots + \beta_pX_p}}{1 + e^{\beta_0 + \beta_1X_1 + \dots + \beta_pX_p}}
$$

## Multiple Logistic Regression: Explained

-   X‚ÇÅ, X‚ÇÇ, ..., X<sub>p</sub>: The *p* predictors.
-   Œ≤‚ÇÄ, Œ≤‚ÇÅ, Œ≤‚ÇÇ, ..., Œ≤<sub>p</sub>: Coefficients estimated from the data.
-   The log-odds are now a linear function of *all* predictors.
-   We still use MLE to estimate coefficients.

-   **Interpretation:**
    -   Œ≤<sub>j</sub>: Change in log-odds for a one-unit increase in X<sub>j</sub>, *holding all other predictors constant*.
    -   Equivalently, a one-unit increase in X<sub>j</sub> multiplies the odds by e<sup>Œ≤<sub>j</sub></sup>, holding others constant.

##  Example: Multiple Logistic Regression on Default

-   Model: `default` ~ `balance` + `income` + `student`
-   `student` is a *dummy variable*:
    -   `student` = 1 if student, 0 if not.

|             | Coefficient | Std. error | z-statistic | p-value  |
| :---------- | :---------- | :--------- | :---------- | :------- |
| Intercept   | -10.8690    | 0.4923     | -22.08      | < 0.0001 |
| balance     | 0.0057      | 0.0002     | 24.74       | < 0.0001 |
| income      | 0.0030      | 0.0082     | 0.37        | 0.7115   |
| student[Yes]| -0.6468     | 0.2362     | -2.74       | 0.0062   |
: {tbl-colwidths="[25,25,20,15,15]"}

## Interpreting the Coefficients: A Deep Dive

-   **Intercept (Œ≤‚ÇÄ = -10.8690):** Log-odds of default for a non-student with `balance` = 0 and `income` = 0. Not directly interpretable.

-   **balance (Œ≤‚ÇÅ = 0.0057):**  For each $1 increase in `balance`, the log-odds of default increase by 0.0057, *holding income and student status constant*.  Equivalently, odds multiply by e<sup>0.0057</sup> ‚âà 1.0057.

-   **income (Œ≤‚ÇÇ = 0.0030):** For each $1 increase in `income`, the log-odds of default *increase* by 0.003, holding other variables constant. It's not significant.

-   **student[Yes] (Œ≤‚ÇÉ = -0.6468):** Being a student *decreases* the log-odds of default by 0.6468, holding `balance` and `income` constant.  Odds multiply by e<sup>-0.6468</sup> ‚âà 0.52. *Surprising!* We might expect the opposite, but this is *controlling for balance and income*.

## Confounding: Unmasking Hidden Relationships

-   The surprising student result is due to **confounding**.

![Confounding in the Default data.](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_3.svg)

## Confounding: Left Panel - Default Rates

::: {layout-ncol=1}
![Confounding in the Default data.](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_3.svg)
:::

-   **Left Panel (Default Rates vs. Balance):**
    -   Orange: Students.
    -   Blue: Non-students.
    -   Solid: Default rate at *each* balance level.
    -   Dashed: *Overall* (average) default rate.

-   **Key:** Students have *higher overall* default rates (dashed), but *lower* rates at *each balance level* (solid).

## Confounding: Right Panel - Balance Distribution

::: {layout-ncol=1}
![Confounding in the Default data.](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_3.svg)
:::

-   **Right Panel (Boxplots of Balance):**
    -   Compares `balance` distribution for students (orange) and non-students (blue).
    -   Students tend to have *higher* credit card balances.

##  Confounding: The Full Story

1.  **Students have higher balances.** (Right panel boxplots)
2.  **Higher balances lead to higher default rates.** (Our initial finding)
3.  **Thus, students have higher *overall* default rates due to higher balances.** (Dashed lines in left panel)
4.  **But, *controlling for balance*, students have *lower* default rates.** (Solid lines in left panel, and the multiple logistic regression coefficient)

-   **Confounding:** The effect of `student` status is mixed up with `balance`. Without controlling for `balance`, we get a misleading impression.

## Multinomial Logistic Regression: Beyond Two Categories

-   What if we have *more than two* response categories?

-   **Example:** Medical diagnosis:

    1.  Stroke
    2.  Drug overdose
    3.  Epileptic seizure

-   **Multinomial logistic regression** extends logistic regression to handle this.

##  Multinomial Logistic Regression: The Core Concept

-   We choose one category as a *baseline* (e.g., "stroke").
-   We model the log-odds of *each other category relative to the baseline*.

-   **Model:**

$$
\log\left(\frac{\Pr(Y = k|X = x)}{\Pr(Y = K|X = x)}\right) = \beta_{k0} + \beta_{k1}x_1 + \dots + \beta_{kp}x_p
$$

## Multinomial Logistic Regression: Details

-   *k*: Index for categories (k = 1, 2, ..., K-1). We have K-1 equations.
-   *K*: Baseline category.
-   Œ≤<sub>k0</sub>, Œ≤<sub>k1</sub>, ..., Œ≤<sub>kp</sub>: Coefficients for category *k* (relative to baseline). K-1 sets of these.
-   X = (x‚ÇÅ, x‚ÇÇ, ..., x<sub>p</sub>): Predictors.

-   This models the log-odds of being in category *k* *compared to* the baseline *K*, as a linear function of the predictors.

-   **Key:** The baseline choice is arbitrary. *Predicted probabilities* are the same regardless.

##  Multinomial Logistic Regression: Softmax Coding (Alternative)

-   **Softmax coding** is an equivalent formulation, treating all *K* categories symmetrically.

-   **Model:**

$$
\Pr(Y = k|X = x) = \frac{e^{\beta_{k0} + \beta_{k1}x_1 + \dots + \beta_{kp}x_p}}{\sum_{l=1}^K e^{\beta_{l0} + \beta_{l1}x_1 + \dots + \beta_{lp}x_p}}
$$

## Softmax Coding: Breakdown

-   This calculates Pr(Y = k | X).
-   Numerator: Exponentiated linear combination for category *k*.
-   Denominator: *Sum* of exponentiated linear combinations for *all* categories.  Ensures probabilities sum to 1.

-   **Softmax Function:** Transforms linear combinations into probabilities. Generalizes the logistic function.

-   **Use in Machine Learning:** Common in neural networks for multi-class classification.

##  Generative Models: A Different Approach

-   So far, we've *directly* modeled Pr(Y = k | X) (e.g., logistic regression).

-   **Generative models** take an *indirect* approach:

    1.  **Model the distribution of predictors *X* *separately for each response class*.**  This is Pr(X | Y = k).
    2.  **Use Bayes' theorem to "flip" this to get Pr(Y = k | X).** This is what we want for classification.

## Bayes' Theorem: The Key to "Flipping"

-   Definitions:
    -   œÄ<sub>k</sub>: *Prior probability* of an observation coming from class *k* (overall probability).
    -   f<sub>k</sub>(X) = Pr(X | Y = k): *Density function* of predictors *X* for class *k* (how predictors are distributed *within* class *k*).

-   **Bayes' Theorem:**

$$
\Pr(Y = k|X = x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^K \pi_l f_l(x)}
$$

## Bayes' Theorem: Explained

-   **Left-hand side:** Pr(Y = k | X = x) ‚Äì *Posterior probability*. Probability of class *k*, *given* predictors *x*. What we want.

-   **Right-hand side:**

    -   œÄ<sub>k</sub>: Prior probability of class *k*.
    -   f<sub>k</sub>(x): Density of predictors *X* for class *k*.
    -   Œ£<sub>l=1</sub><sup>K</sup> œÄ<sub>l</sub>f<sub>l</sub>(x): Normalizing constant (sum over all classes). Ensures probabilities sum to 1.

-   **Intuition:** Bayes' theorem updates our prior belief (œÄ<sub>k</sub>) based on evidence from predictors (f<sub>k</sub>(x)).

- To use Bayes' theorem, we must estimate œÄk and fk(x). Estimating œÄk is easy, while estimating fk(x) is hard.

##  Why Generative Models? Advantages

-   Why this indirect approach when we have direct methods like logistic regression?

-   **Stability:** With *substantial class separation* (predictors clearly distinguish classes), logistic regression parameter estimates can be unstable. Generative models can be more stable.

-   **Small Sample Size + Normality:** If predictors are approximately *normal* within each class, *and* the sample size is small, generative models like LDA can be more accurate.

-   **More Than Two Classes:** Generative models easily handle multiple classes.

## Linear Discriminant Analysis (LDA): A Generative Model

-   **LDA** is a generative model with specific assumptions about predictor distribution.

-   **LDA Assumptions:**

    1.  **Normality:** Density functions f<sub>k</sub>(x) are *normal* (Gaussian). Predictors follow a bell-shaped distribution within each class.
    2.  **Common Variance:** *Common variance* (œÉ¬≤) across all *K* classes. The spread is the same, though means may differ.

## LDA: The Normal Density (Single Predictor)

-   For one predictor (p = 1):

$$
f_k(x) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{1}{2\sigma^2}(x - \mu_k)^2\right)
$$

-   Œº<sub>k</sub>: *Mean* of predictor *X* for class *k*.
-   œÉ¬≤: *Common variance* across all classes.
-   The familiar bell-shaped curve.

## LDA: The Discriminant Function

-   Plugging f<sub>k</sub>(x) into Bayes' theorem and simplifying gives a simple result.

-   Classify to the class with the *largest*:

$$
\delta_k(x) = x \cdot \frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + \log(\pi_k)
$$

-   This is the **discriminant function**.

##  LDA Discriminant Function: Breakdown

-   Œ¥<sub>k</sub>(x): Discriminant score for class *k*. Classify to the class with the highest score.

-   **Key:**  *Linear* in *x*. Hence, *Linear* Discriminant Analysis! Decision boundaries are straight lines (or hyperplanes).

-   Terms:
    -   x ‚ãÖ (Œº<sub>k</sub> / œÉ¬≤): Influence of predictor *x* and class mean Œº<sub>k</sub>, scaled by variance.
    -   - (Œº<sub>k</sub>¬≤ / 2œÉ¬≤): Constant for each class, related to mean.
    -   log(œÄ<sub>k</sub>): Incorporates prior probability of class *k*.

##  LDA: Estimating the Parameters (From Data)

-   We don't know true parameters (œÄ<sub>k</sub>, Œº<sub>k</sub>, œÉ¬≤). We *estimate* them from training data.

-   **Estimates:**

    -   ŒºÃÇ<sub>k</sub> = (1/n<sub>k</sub>) Œ£<sub>i:y·µ¢=k</sub> x·µ¢: *Sample mean* of *X* for class *k*. Average predictor values for observations in class *k*.

    -   œÉÃÇ¬≤ = (1/(n-K)) Œ£<sub>k=1</sub><sup>K</sup> Œ£<sub>i:y·µ¢=k</sub> (x·µ¢ - ŒºÃÇ<sub>k</sub>)¬≤: *Pooled variance estimate*. Weighted average of within-class variances (reflects common variance assumption).

    -   œÄÃÇ<sub>k</sub> = n<sub>k</sub> / n: *Sample proportion* of observations in class *k*.

-   Plug *estimates* into the discriminant function for predictions.

## LDA: A Visual Example (One Predictor)

![One-dimensional normal density functions and LDA decision boundary](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_4.svg)

## LDA Example: Left Panel - Density Functions

::: {layout-ncol=1}
![One-dimensional normal density functions and LDA decision boundary](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_4.svg)
:::

-   **Left Panel:**
    -   Two normal density functions (p=1, K=2).
    -   Orange: Class 1 (f‚ÇÅ(x)).
    -   Blue: Class 2 (f‚ÇÇ(x)).
    -   Dashed: *Bayes decision boundary*. Optimal boundary, given true distributions. Where densities intersect.

## LDA Example: Right Panel - Histograms and Boundaries

::: {layout-ncol=1}
![One-dimensional normal density functions and LDA decision boundary](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_4.svg)
:::

-   **Right Panel:**
    -   Histograms of 20 observations from each class.
    -   Orange: Class 1.
    -   Blue: Class 2.
    -   Dashed: Bayes decision boundary (same as left).
    -   Solid: *LDA decision boundary*. Found by LDA using *estimated* parameters.

-   **Key:** LDA approximates the Bayes boundary. Here, with normal data and equal variances, LDA does well.

## LDA with Multiple Predictors (p > 1): Going Multivariate

-   With multiple predictors, we extend LDA's assumptions.

-   **Assumption:** Predictors X = (X‚ÇÅ, X‚ÇÇ, ..., X<sub>p</sub>) follow a *multivariate Gaussian* distribution, with a class-specific mean vector and a *common covariance matrix*.

-   **Multivariate Gaussian Density:**

$$
f(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu)^T\Sigma^{-1}(x - \mu)\right)
$$

## Multivariate Gaussian: Deconstructed

-   *x*: Vector of predictor values (x‚ÇÅ, x‚ÇÇ, ..., x<sub>p</sub>).
-   Œº: *Mean vector* (Œº‚ÇÅ, Œº‚ÇÇ, ..., Œº<sub>p</sub>). Mean of each predictor. In LDA, each class has its own Œº<sub>k</sub>.
-   Œ£: *Covariance matrix*. p √ó p matrix describing relationships (covariances) between all predictor pairs. In LDA, *common* Œ£ for all classes.
-   |Œ£|: *Determinant* of Œ£.
-   Generalizes the one-dimensional normal distribution to multiple dimensions.

##  LDA with Multiple Predictors: The Discriminant Function

-   Plug the multivariate Gaussian density into Bayes' theorem and simplify.

-   Classify to the class with the largest:

$$
\delta_k(x) = x^T \Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + \log \pi_k
$$

-   **Key:** Still *linear* in *x*. LDA produces linear decision boundaries even with multiple predictors.
- We estimate the parameters (Œºk, Œ£, œÄk) from the training data, similar to one-dimension case.

## LDA: Example with Three Classes (Visual)

![Multivariate Gaussian distribution with three classes, and LDA decision boundaries](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_6.svg)

## LDA Example (Three Classes): Left Panel - Ellipses

::: {layout-ncol=1}
![Multivariate Gaussian distribution with three classes, and LDA decision boundaries](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_6.svg)
:::

-   **Left Panel:**
    -   Two-dimensional example (p=2), three classes (K=3).
    -   Ellipses: Contours of constant probability density for each class (95% probability, assuming multivariate Gaussian).
    -   Dashed: *Bayes decision boundaries*. Optimal boundaries, given true distributions.

## LDA Example (Three Classes): Right Panel - Scatterplot and Boundaries

::: {layout-ncol=1}
![Multivariate Gaussian distribution with three classes, and LDA decision boundaries](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_6.svg)
:::

-   **Right Panel:**
    -   20 observations from each class.
    -   Solid: *LDA decision boundaries*. Found by LDA using estimated parameters.

-   **Key:** LDA approximates Bayes boundaries. LDA boundaries are linear; Bayes boundaries are slightly curved.

## LDA on the Default Data: Assessing Performance

-   Applying LDA to Default data (predicting default from `balance` and `student`).

-   **Training Error Rate:** 2.75%.  Seems low, but be careful!

-   **Two Problems with Training Error:**

    1.  **Overfitting:** Training error is usually *lower* than *test error*. The model might fit training data too closely, not generalizing well to new data.
    2.  **Imbalanced Data:** Only 3.33% of individuals defaulted in training data. *Imbalanced*. A classifier always predicting "no default" (the *null classifier*) would have a 3.33% error rate.  Our 2.75% isn't much better!

## Confusion Matrix: A Detailed Performance View

-   A **confusion matrix** summarizes classifier performance. It shows counts of:
    -   True Positives (TP)
    -   True Negatives (TN)
    -   False Positives (FP) - Type I error
    -   False Negatives (FN) - Type II error

|                 | Predicted No Default | Predicted Default | Total |
| :-------------- | :------------------- | :---------------- | :---- |
| Actual No Default | 9644                | 23               | 9667  |
| Actual Default    | 252                 | 81               | 333   |
| Total           | 9896                | 104               | 10000   |
: {tbl-colwidths="[25,35,25,15]"}

## Confusion Matrix: Interpreted

::: {layout-ncol=1}

|                 | Predicted No Default | Predicted Default | Total |
| :-------------- | :------------------- | :---------------- | :---- |
| Actual No Default | 9644                | 23               | 9667  |
| Actual Default    | 252                 | 81               | 333   |
| Total           | 9896                | 104               | 10000   |
: {tbl-colwidths="[25,35,25,15]"}

-   **Rows:** *Actual* class labels.
-   **Columns:** *Predicted* class labels.

-   **From the table:**

    -   **TN:** 9644 non-defaulters, correctly predicted "no default".
    -   **FP:** 23 non-defaulters, *incorrectly* predicted "default".
    -   **FN:** 252 defaulters, *incorrectly* predicted "no default".
    -   **TP:** 81 defaulters, correctly predicted "default".
:::

## Confusion Matrix: Key Insights and Trade-offs

::: {layout-ncol=1}

|                 | Predicted No Default | Predicted Default | Total |
| :-------------- | :------------------- | :---------------- | :---- |
| Actual No Default | 9644                | 23               | 9667  |
| Actual Default    | 252                 | 81               | 333   |
| Total           | 9896                | 104               | 10000   |
: {tbl-colwidths="[25,35,25,15]"}

-   **Key Problem:** LDA misclassifies 252/333 defaulters (75.7%). Low *sensitivity* (recall) ‚Äì not good at identifying positive cases (defaulters).
-  **However:** LDA correctly classifies 99.8% of non-defaulters (1 - 23/9667). High *specificity* ‚Äì good at identifying negative cases.
:::

## Modifying the Threshold: Adjusting Sensitivity and Specificity

-   LDA (and Bayes classifier) typically use a 0.5 threshold for posterior probability. If Pr(default = Yes | X) > 0.5, predict "default".

-   We can *modify* this threshold to change the sensitivity/specificity trade-off.

-   **Lowering the Threshold (e.g., to 0.2):**
    -   *Increases* sensitivity: Identify *more* defaulters (more TPs).
    -   *Decreases* specificity: Incorrectly classify *more* non-defaulters as defaulters (more FPs).

-   **Raising the Threshold (e.g., to 0.8):**
    -   *Decreases* sensitivity: Identify *fewer* defaulters (fewer TPs).
    -   *Increases* specificity: Incorrectly classify *fewer* non-defaulters (fewer FPs).

-   **Best threshold depends on context and relative costs of errors (FP vs. FN).**

## ROC Curves: Visualizing Performance Across Thresholds

![Error rates as a function of threshold](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_7.svg)

## ROC Curves: Explanation

::: {layout-ncol=1}
![Error rates as a function of threshold](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_7.svg)

- Shows various error rates as threshold changes.
-   **Black solid line:** Overall error rate.
-   **Blue dashed line:** Fraction of defaulters incorrectly classified (1 - sensitivity).
-   **Orange dotted line:** Fraction of errors among non-defaulters (false positive rate, 1 - specificity).
:::

-   A **Receiver Operating Characteristic (ROC) curve** visualizes classifier performance across *all possible thresholds*.

-   **Axes:**
    -   X-axis: False Positive Rate (FPR) = 1 - Specificity
    -   Y-axis: True Positive Rate (TPR) = Sensitivity

-   **Each point on the ROC curve represents a different threshold.**

## ROC Curve: Interpreting the Shape

![ROC curve for LDA classifier on Default data](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_8.svg)

-   **Ideal ROC Curve:** Hugs the top-left corner. High sensitivity (TPR) with low FPR.

-   **Worst-Case ROC Curve:** Diagonal line from bottom-left to top-right (dotted line). No better than random guessing.

-   **Area Under the Curve (AUC):** Summarizes performance.
    -   AUC = 1: Perfect classifier.
    -   AUC = 0.5: No better than random.
    -   AUC > 0.5: Better than random. Higher AUC is better.

## Quadratic Discriminant Analysis (QDA): Relaxing Assumptions

-   **QDA** is another generative model, like LDA.

-   **Key Difference:** QDA *relaxes* the assumption of a *common* covariance matrix.

-   **QDA Assumption:** Each class has its *own* covariance matrix, Œ£<sub>k</sub>. Spread and orientation can differ for each class.

-   **Discriminant Function:** Becomes *quadratic* in *x*:

$$
\delta_k(x) = -\frac{1}{2}x^T\Sigma_k^{-1}x + x^T\Sigma_k^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma_k^{-1}\mu_k - \frac{1}{2}\log|\Sigma_k| + \log \pi_k
$$

## QDA Discriminant Function: Explained

-   Similar terms to LDA, but with Œ£<sub>k</sub> (covariance matrix for class *k*) instead of Œ£.

-   **Key:** *Quadratic* in *x*. Decision boundaries are curves (ellipses, parabolas, hyperbolas), not lines.

- We estimate parameters (Œºk, Œ£k, œÄk) from training data.

## LDA vs. QDA: The Bias-Variance Trade-Off

-   **QDA is more flexible than LDA.** More parameters to estimate (own covariance matrix for each class).

-   **QDA: Higher variance, potentially lower bias.** More flexible, so can fit data better (lower bias), but more prone to overfitting (higher variance).

-   **LDA: Lower variance, potentially higher bias.** Stronger assumption (common covariance), so less flexible (higher bias), but less likely to overfit (lower variance).

-   **Which is better?** Depends on data and bias-variance trade-off.

    -   **LDA better when:**
        -   *Fewer* training observations. Reducing variance is crucial.
        -   Common covariance assumption is reasonable.

    -   **QDA recommended when:**
        -   Training set is *large*. Variance less of a concern.
        -   Common covariance assumption is clearly *not* tenable (classes have very different spreads/orientations).

## LDA vs. QDA: A Visual Comparison

![LDA and QDA decision boundaries](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_9.svg)

## LDA vs. QDA Example: Left Panel - Common Correlation

::: {layout-ncol=1}
![LDA and QDA decision boundaries](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_9.svg)
:::

-   **Left Panel (Common Correlation):**
    -   Classes have *same* correlation between predictors.
    -   Bayes boundary (dashed): Linear.
    -   LDA boundary (dotted): Linear, approximates Bayes well.
    -   QDA boundary (solid): Quadratic, *worse* than LDA here. QDA overfits.

## LDA vs. QDA Example: Right Panel - Different Correlations

::: {layout-ncol=1}
![LDA and QDA decision boundaries](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_9.svg)
:::

-   **Right Panel (Different Correlations):**
    -   Classes have *different* correlations.
    -   Bayes boundary (dashed): Quadratic.
    -   QDA boundary (solid): Quadratic, approximates Bayes well.
    -   LDA boundary (dotted): Linear, performs *poorly*. Cannot capture the quadratic relationship.

## Naive Bayes: The "Naive" Assumption

-   **Naive Bayes** is another generative model. It makes a *very strong* simplifying assumption.

-   **Assumption:** *Within each class, the p predictors are independent*. Knowing the class, the value of one predictor gives no info about others.

- Mathematically:
  f<sub>k</sub>(x) = f<sub>k1</sub>(x‚ÇÅ) √ó f<sub>k2</sub>(x‚ÇÇ) √ó ... √ó f<sub>kp</sub>(x<sub>p</sub>)
    -   f<sub>kj</sub>(x<sub>j</sub>): Density function of *j*th predictor *within class k*.

## Naive Bayes: Explained

-   Independence assumption simplifies things *dramatically*. Instead of estimating a complex *p*-dimensional joint density f<sub>k</sub>(x), we estimate *p* one-dimensional densities f<sub>kj</sub>(x<sub>j</sub>).

-   Plug into Bayes' theorem:

$$
\Pr(Y = k|X = x) = \frac{\pi_k \times f_{k1}(x_1) \times f_{k2}(x_2) \times \dots \times f_{kp}(x_p)}{\sum_{l=1}^K \pi_l \times f_{l1}(x_1) \times f_{l2}(x_2) \times \dots \times f_{lp}(x_p)}
$$

##  Naive Bayes: Estimating One-Dimensional Densities

-   Key is estimating f<sub>kj</sub>(x<sub>j</sub>). Depends on whether X<sub>j</sub> is quantitative or qualitative.

-   **If X<sub>j</sub> is quantitative:**

    -   **Option 1 (Parametric):** Assume X<sub>j</sub> | Y = k ~ N(Œº<sub>jk</sub>, œÉ<sub>j</sub>¬≤). Estimate mean (Œº<sub>jk</sub>) and variance (œÉ<sub>j</sub>¬≤) for each predictor *j* within each class *k*. *Different* variance for each predictor and class (unlike LDA).

    -   **Option 2 (Non-parametric):**
        -   *Histogram*: Divide range of X<sub>j</sub> into bins, count observations in each bin for each class.
        -   *Kernel Density Estimation (KDE)*: More sophisticated, produces smooth density estimate.

-   **If X<sub>j</sub> is qualitative:**

    -   Count *proportion* of training observations for each level of predictor *within each class*. Direct probability estimate.

## Naive Bayes: Example (Illustrative)
-   Two classes (K=2), three predictors (p=3). The first two predictors are quantitative, and the third predictor is qualitative with three levels.

-   Shows estimated one-dimensional density functions fkj.

::: {layout-ncol=1}
![f11](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_10_1.svg)
:::

::: {layout-ncol=1}
![f12](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_10_2.svg)
:::
::: {layout-ncol=1}
![f13](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_10_3.svg)
:::

::: {layout-ncol=1}
![f21](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_10_4.svg)
:::
::: {layout-ncol=1}

![f22](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_10_5.svg)
:::

::: {layout-ncol=1}
![f23](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_10_6.svg)
:::

##  Naive Bayes: Surprisingly Good Performance!

-   Despite the strong (and often unrealistic) independence assumption, Naive Bayes often works well.

-   **Why?** Reduces variance. Fewer parameters to estimate, so less prone to overfitting, especially with small data relative to predictors.

-   **Bias-Variance Trade-off:** Naive Bayes has high bias (independence assumption) but low variance. Can be a good trade-off.

##  Comparing Classification Methods: An Analytical View

-   Logistic regression, LDA, QDA, Naive Bayes: All can be expressed in terms of maximizing Pr(Y = k | X), or log-odds relative to baseline (K):

$$
\log\left(\frac{\Pr(Y = k|X = x)}{\Pr(Y = K|X = x)}\right)
$$

-   The *form* of this log-odds expression distinguishes the methods.

| Method            | Log-Odds Form                                                                                            |
| :---------------- | :------------------------------------------------------------------------------------------------------- |
| Logistic Regression | Œ≤<sub>k0</sub> + Œ£<sub>j=1</sub><sup>p</sup> Œ≤<sub>kj</sub>x<sub>j</sub>  (linear)                                               |
| LDA               | a<sub>k</sub> + Œ£<sub>j=1</sub><sup>p</sup> b<sub>kj</sub>x<sub>j</sub> (linear)                                                |
| QDA               | a<sub>k</sub> + Œ£<sub>j=1</sub><sup>p</sup> b<sub>kj</sub>x<sub>j</sub> + Œ£<sub>j=1</sub><sup>p</sup> Œ£<sub>l=1</sub><sup>p</sup> c<sub>kj,l</sub>x<sub>j</sub>x<sub>l</sub> (quadratic) |
| Naive Bayes      | a<sub>k</sub> + Œ£<sub>j=1</sub><sup>p</sup> g<sub>kj</sub>(x<sub>j</sub>) (additive)                                                  |
: {tbl-colwidths="[30,70]"}

## Comparison: Explained

-   **Logistic Regression:** Log-odds are *linear* in predictors. Coefficients (Œ≤<sub>kj</sub>) estimated by MLE.

-   **LDA:** Log-odds also *linear*. Coefficients (a<sub>k</sub>, b<sub>kj</sub>) from multivariate normality and common covariance assumptions.

-   **QDA:** Log-odds are *quadratic*. Allows for more complex boundaries.

-   **Naive Bayes:** Log-odds are *additive*. Each predictor contributes independently through g<sub>kj</sub>(x<sub>j</sub>). Reflects independence.

-   **Relationships:**

    -   LDA is special case of QDA (equal covariance matrices).
    -   Any classifier with *linear* boundary is special case of Naive Bayes.
    -   Logistic regression has same *linear* form as LDA, but coefficients estimated differently (MLE vs. normal assumptions).
- None of these methods universally dominate the others. The best choice depends on the characteristics of data.

## A comparison of classification methodsÔºöAn empirical comparison

- Test error rates of different methods are compared through six different scenarios.

```{=html}
<style>
.half-width {
  width: 50%;
  float: left;
}
</style>
```

::: {layout-ncol=1}
![FIGURE 4.11](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_11.svg)
:::

::: {layout-ncol=1}
![FIGURE 4.12](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_12.svg)
:::

## Empirical Comparison: Details

::: {layout-ncol=1}
![FIGURE 4.11](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_11.svg)

![FIGURE 4.12](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F4_12.svg)
:::

-   Six scenarios. Scenarios 1-3: Bayes boundaries are linear. Scenarios 4-6: Bayes boundaries are non-linear.
-   p=2 quantitative predictors in each scenario.
-   For each scenario, 100 random training sets generated.

-   **Scenario Details:**

    -   **Scenario 1:** Two classes, predictors correlation 0, n = 100
    -   **Scenario 2:** Two classes, predictors correlation -0.5, n = 100
    -   **Scenario 3:** Two classes, predictors correlation 0.5, n = 100, modified predictors (nonlinear boundary).
    -   **Scenario 4:** Two classes, independent standard normals, response from QDA (quadratic boundary).
    -   **Scenario 5:** Same as 4, but response from more complex quadratic.
    -   **Scenario 6:** Two classes, independent predictors, complex response (highly non-linear boundary). Compare with K-nearest neighbors (KNN).

-   **Results:**
    -   *Linear* boundaries (1 and 2): LDA and logistic regression do well.
    -   Moderately non-linear (3): QDA outperforms LDA, but logistic regression still good.
    -   More complex (4, 5, 6): QDA can be better than LDA and logistic.
    -   *Very* complex (6): Non-parametric KNN can be superior, *but* smoothness (K) must be chosen carefully.

## Generalized Linear Models (GLMs): Beyond Normality

-   So far:
    -   *Quantitative* responses: Least squares linear regression.
    -   *Qualitative* responses: Classification methods (logistic, LDA, QDA, Naive Bayes).

- What if *Y* is neither?

-   **Example:** Predicting hourly users of a bike-sharing program.

-   **Characteristics:**
    -   *Non-negative integers*: Can't have negative or fractional users.
    -   *Counts*: Number of events (rentals) in a time period.

## Why Not Linear Regression for Counts?

-   Problems with using linear regression:

    1.  **Negative Predictions:** Linear regression can predict negative values (nonsensical for counts).
    2.  **Mean-Variance Relationship:** For counts, variance often *increases* with the mean. Linear regression assumes constant variance.
    3.  **Non-Continuous:** Response is discrete. Linear regression assumes continuous.

-   **Solution:** **Poisson regression**. A more natural approach for count data.

## Poisson Regression: Modeling Counts

-   **Poisson Distribution:** Often used to model *counts*.

-   **Probability Mass Function:** If Y is Poisson, probability of *k* events is:

$$
\Pr(Y = k) = \frac{e^{-\lambda}\lambda^k}{k!}  \quad \text{for } k = 0, 1, 2, \dots
$$

-   *k*: Number of events (non-negative integer).
-   *k!*: *k* factorial (k! = k √ó (k-1) √ó ... √ó 2 √ó 1). 0! = 1.
-   Œª: Parameter (Œª > 0). *Expected value* (mean) *and* *variance* of Y.

-   **Key:** Mean and variance are *equal*. As mean (Œª) increases, variance increases. Suitable for count data.

## Poisson Regression Model: Connecting Mean to Predictors

-   Model the *mean* (Œª) as a function of predictors.

$$
\log(\lambda(X_1, \dots, X_p)) = \beta_0 + \beta_1X_1 + \dots + \beta_pX_p
$$

or equivalently

$$
\lambda(X_1, \dots, X_p) = e^{\beta_0 + \beta_1X_1 + \dots + \beta_pX_p}
$$

## Poisson Regression Model: Explained

-   Œª(X‚ÇÅ, X‚ÇÇ, ..., X<sub>p</sub>): Mean (and variance) of Poisson, depends on predictors.
-   Œ≤‚ÇÄ, Œ≤‚ÇÅ, ..., Œ≤<sub>p</sub>: Coefficients estimated from data.
-   **Link Function:** *Log* link. *Logarithm* of mean is linear in predictors.

-   **Why log link?**

    -   **Non-negativity:** Ensures predicted mean (Œª) is *non-negative*. Exponential function (e<sup>x</sup>) is always positive.
    -   **Linearity:** Models relationship between predictors and *log* of mean as linear.

-   **Estimation:** Use MLE to estimate coefficients.

## Comparing Poisson Regression Model to the linear regression model

::: {layout-ncol=1}
- Interpretation of the coefficients
    - In linear regression, coefficients represent linear increase.
    - In Poisson regression, coefficients represents the change in the log-mean for a one-unit increase in X, or increase the expected count by a factor of e<sup>Œ≤</sup>.
- Mean-variance relationship
    -  In linear regression, variance doesn't change when mean changes.
    - In Poisson regression, variance is equal to mean. The larger the mean, the larger the variance.
- Nonnegative fitted values
    - In linear regression, fitted values could be negative.
    - In Poisson regression, fitted values will always be positive.
- Please read text material for details.
:::

##  Generalized Linear Models (GLMs): The Big Picture

-   Three regression models:

    1.  **Linear Regression:** Quantitative responses (normal distribution, constant variance).
    2.  **Logistic Regression:** Binary responses (Bernoulli/binomial distribution).
    3.  **Poisson Regression:** Count responses (Poisson distribution).

-   Common characteristics:

    1.  **Predictors and Response:** Predictors (X‚ÇÅ, X‚ÇÇ, ..., X<sub>p</sub>) predict response (Y).
    2.  **Conditional Distribution:**  Given predictors, *Y* belongs to a *family of distributions*.
    3.  **Modeling the Mean:** Model *mean* of *Y*, conditional on predictors.
    4.  **Link Function:**  *Link function* (Œ∑) relates mean of *Y* to linear combination of predictors.

-   **General Form (GLM):**

$$
\eta(E(Y|X_1, \dots, X_p)) = \beta_0 + \beta_1X_1 + \dots + \beta_pX_p
$$

- **Components:**
    - *Response (Y)* and its distribution.
    - *Predictors (X1, X2, ..., Xp)*.
    - *Link Function (Œ∑)*.
    - *Linear Predictor*: Œ≤‚ÇÄ + Œ≤‚ÇÅX‚ÇÅ + ... + Œ≤<sub>p</sub>X<sub>p</sub>

## Summary

-   **Classification:** Predicting *qualitative* responses.
-   Key methods:
    -   **Logistic Regression:** Models *probability* using logistic function. Directly models Pr(Y = k | X).
    -   **LDA:** Generative. Assumes multivariate Gaussian predictors, *common* covariance.
    -   **QDA:** Like LDA, but *own* covariance matrix per class.
    -   **Naive Bayes:** Generative. Assumes *independence* of predictors within each class.
-   **Choosing:** Depends on data and bias-variance trade-off.
    -   LDA/logistic: Linear boundaries.
    -   QDA: More flexible, non-linear, but higher variance.
    -   Naive Bayes: Simple, surprisingly good, despite independence.
-   **ROC Curves:** Evaluate performance across thresholds. AUC summarizes.
- **Generalized linear models** handle responses from non-normal distributions, and Poisson regression model is an example for count data.

## Thoughts and Discussion ü§î

::: {layout-ncol=1}
-   Real-world scenarios for each method?
    -   *Logistic:* Credit scoring, medical diagnosis, marketing (churn).
    -   *LDA:* Predictors approximately normal, similar covariance.  Some image recognition/signal processing.
    -   *QDA:* Normal predictors, *different* covariances. More complex problems.
    -   *Naive Bayes:* Text classification (spam filtering), document categorization, sentiment analysis.

-   Choosing the "best" method?
    -   *Understand Data:* Explore. Linear/nonlinear? Normal predictors?
    -   *Assumptions:* Violations?
    -   *Bias-Variance:* Flexible (high variance, low bias) or constrained (low variance, high bias)? Depends on dataset size/complexity.
    -   *Metrics:* Cross-validation for test error.
        -   Accuracy
        -   Sensitivity (Recall)
        -   Specificity
        -   Precision
        -   F1-score
        -   AUC
    -   *Experiment:* Try different methods, compare with cross-validation.

-   Limitations? When might they fail?
    -   *Logistic:* Assumes linear log-odds. Unstable with severe separation.
    -   *LDA:* Multivariate normality, common covariance. Outliers.
    -   *QDA:* Multivariate normality. Overfitting with small data.
    -   *Naive Bayes:* Independence often violated.
- How does the bias-variance trade-off play a role in choosing a classification method?
    -   *High Bias, Low Variance:* Simpler models (e.g., LDA, Naive Bayes) tend to have higher bias but lower variance. They are less likely to overfit, but they might miss complex relationships in the data.
    -   *Low Bias, High Variance:* More complex models (e.g., QDA) tend to have lower bias but higher variance. They can fit the data more closely, but they are more prone to overfitting, especially with small datasets.
    -  The optimal balance depends on the specific dataset and the goals of the analysis.

- Can we apply the knowledge of mean-variance relationship and fitted values to choose suitable regression model?
    - Yes.
    - If variance increases as the mean increase and the response is count data, Poisson regression could be considered.
    - If the fitted values are probabilities, which must be between 0 and 1, then models like logistic regression could be considered.
    - Understanding the characteristics of data will guide us to select suitable models.
:::

