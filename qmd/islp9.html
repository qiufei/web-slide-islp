<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Support Vector Machines</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<link href="../https://assets.qiufei.site/personal/profile.jpg" rel="icon" type="image/jpeg">
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-8ea72dc5fed832574809a9c94082fbbb.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-681fbf911679f9b3dbf9743eb275ba49.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-7e49aeac8059a213a463aa1a739e8272.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://qiufei.github.io"> 
<span class="menu-text">È¶ñÈ°µ</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
        
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Support Vector Machines</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<div style="color: #2E86C1;">
<p>Welcome to the fascinating world of Support Vector Machines (SVMs)! ü§ñ In this chapter, we embark on a journey to understand these powerful supervised learning models. SVMs are renowned for their ability to perform both classification and regression tasks, and they originated in the computer science community back in the 1990s. What makes them stand out? Their exceptional ‚Äúout-of-the-box‚Äù performance ‚Äì meaning they often work remarkably well with minimal need for fine-tuning. üéØ</p>
</div>
</section>
<section id="what-is-data-mining" class="level2">
<h2 class="anchored" data-anchor-id="what-is-data-mining">What is Data Mining?</h2>
<div style="color: #2E86C1;">
<p>Imagine you‚Äôre a treasure hunter ü™ô, but instead of searching for gold in caves, you‚Äôre sifting through mountains of data! ‚õ∞Ô∏è That‚Äôs essentially what data mining is all about. It‚Äôs the art and science of discovering hidden patterns, trends, and valuable insights from massive datasets. üîç Data mining cleverly combines techniques from diverse fields like statistics and computer science to unearth knowledge that can empower decision-making.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Data mining uses techniques from various fields, including statistics and computer science.</p>
</div>
</div>
</section>
<section id="data-mining-a-deeper-dive" class="level2">
<h2 class="anchored" data-anchor-id="data-mining-a-deeper-dive">Data Mining: A Deeper Dive</h2>
<div style="color: #2E86C1;">
<p>Data mining isn‚Äôt just about finding <em>any</em> information; it‚Äôs about finding <em>useful</em> information. Think of it like this:</p>
</div>
</section>
<section id="data-mining-a-deeper-dive---pattern-discovery" class="level2">
<h2 class="anchored" data-anchor-id="data-mining-a-deeper-dive---pattern-discovery">Data Mining: A Deeper Dive - Pattern Discovery</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Pattern Discovery:</strong> Identifying recurring sequences or relationships. For example, finding that customers who buy product A also tend to buy product B.</li>
</ul>
</div>
</section>
<section id="data-mining-a-deeper-dive---anomaly-detection" class="level2">
<h2 class="anchored" data-anchor-id="data-mining-a-deeper-dive---anomaly-detection">Data Mining: A Deeper Dive - Anomaly Detection</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Anomaly Detection:</strong> Spotting outliers or unusual events. This could be crucial for fraud detection or identifying system failures.</li>
</ul>
</div>
</section>
<section id="data-mining-a-deeper-dive---association-rule-learning" class="level2">
<h2 class="anchored" data-anchor-id="data-mining-a-deeper-dive---association-rule-learning">Data Mining: A Deeper Dive - Association Rule Learning</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Association Rule Learning:</strong> Uncovering rules that describe how data points relate to each other.</li>
</ul>
</div>
</section>
<section id="data-mining-a-deeper-dive---clustering" class="level2">
<h2 class="anchored" data-anchor-id="data-mining-a-deeper-dive---clustering">Data Mining: A Deeper Dive - Clustering</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Clustering:</strong> Grouping similar data points together. This is useful for customer segmentation or identifying different types of network traffic.</li>
</ul>
</div>
</section>
<section id="data-mining-a-deeper-dive---classification" class="level2">
<h2 class="anchored" data-anchor-id="data-mining-a-deeper-dive---classification">Data Mining: A Deeper Dive - Classification</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Classification:</strong> Assigning data points to predefined categories. This is what SVMs excel at!</li>
</ul>
</div>
</section>
<section id="what-is-machine-learning" class="level2">
<h2 class="anchored" data-anchor-id="what-is-machine-learning">What is Machine Learning?</h2>
<div style="color: #2E86C1;">
<p>Machine learning (ML) is a branch of artificial intelligence (AI) that empowers computers to learn from data <em>without</em> the need for explicit programming. ü§ñ Think of it as teaching a computer to learn by example, much like we teach children! üë∂ Algorithms in machine learning are designed to improve their performance on specific tasks as they are exposed to more data.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Machine Learning involves algorithms that can improve their performance on a task as they are exposed to more data.</p>
</div>
</div>
</section>
<section id="machine-learning-key-concepts" class="level2">
<h2 class="anchored" data-anchor-id="machine-learning-key-concepts">Machine Learning: Key Concepts</h2>
<div style="color: #2E86C1;">
<p>Let‚Äôs explore some fundamental concepts in Machine Learning.</p>
</div>
</section>
<section id="machine-learning-supervised-learning" class="level2">
<h2 class="anchored" data-anchor-id="machine-learning-supervised-learning">Machine Learning: Supervised Learning</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Supervised Learning:</strong> The algorithm learns from labeled data (input-output pairs). This is the category SVMs fall into.</li>
</ul>
</div>
</section>
<section id="machine-learning-unsupervised-learning" class="level2">
<h2 class="anchored" data-anchor-id="machine-learning-unsupervised-learning">Machine Learning: Unsupervised Learning</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Unsupervised Learning:</strong> The algorithm learns from unlabeled data, discovering patterns and structures on its own.</li>
</ul>
</div>
</section>
<section id="machine-learning-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="machine-learning-reinforcement-learning">Machine Learning: Reinforcement Learning</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Reinforcement Learning:</strong> The algorithm learns through trial and error, receiving rewards or penalties for its actions.</li>
</ul>
</div>
</section>
<section id="machine-learning-features" class="level2">
<h2 class="anchored" data-anchor-id="machine-learning-features">Machine Learning: Features</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Features:</strong> The input variables used for learning.</li>
</ul>
</div>
</section>
<section id="machine-learning-model" class="level2">
<h2 class="anchored" data-anchor-id="machine-learning-model">Machine Learning: Model</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Model:</strong> The mathematical representation learned from the data.</li>
</ul>
</div>
</section>
<section id="machine-learning-training" class="level2">
<h2 class="anchored" data-anchor-id="machine-learning-training">Machine Learning: Training</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Training:</strong> The process of fitting a model to the training data.</li>
</ul>
</div>
</section>
<section id="machine-learning-testing" class="level2">
<h2 class="anchored" data-anchor-id="machine-learning-testing">Machine Learning: Testing</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Testing:</strong> Evaluating the model‚Äôs performance on unseen data.</li>
</ul>
</div>
</section>
<section id="what-is-statistical-learning" class="level2">
<h2 class="anchored" data-anchor-id="what-is-statistical-learning">What is Statistical Learning?</h2>
<div style="color: #2E86C1;">
<p>Statistical learning is a fascinating blend of statistics and machine learning. ü§î It‚Äôs all about developing and applying models and algorithms for prediction and inference. However, it places a strong emphasis on statistical properties and interpretability. Think of it as bridging the gap between theoretical statistics and practical machine learning, providing a robust framework to understand <em>why</em> certain models excel and how to enhance them.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Statistical learning focuses on developing and applying models and algorithms for prediction and inference.</p>
</div>
</div>
</section>
<section id="statistical-learning-the-core-principles" class="level2">
<h2 class="anchored" data-anchor-id="statistical-learning-the-core-principles">Statistical Learning: The Core Principles</h2>
<div style="color: #2E86C1;">
<p>Here are the core ideas driving Statistical Learning.</p>
</div>
</section>
<section id="statistical-learning-statistical-models" class="level2">
<h2 class="anchored" data-anchor-id="statistical-learning-statistical-models">Statistical Learning: Statistical Models</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Statistical Models:</strong> Using statistical models (like linear regression, logistic regression, etc.) to represent relationships in data.</li>
</ul>
</div>
</section>
<section id="statistical-learning-inference" class="level2">
<h2 class="anchored" data-anchor-id="statistical-learning-inference">Statistical Learning: Inference</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Inference:</strong> Drawing conclusions about the population from the sample data.</li>
</ul>
</div>
</section>
<section id="statistical-learning-prediction" class="level2">
<h2 class="anchored" data-anchor-id="statistical-learning-prediction">Statistical Learning: Prediction</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Prediction:</strong> Estimating future outcomes based on the learned model.</li>
</ul>
</div>
</section>
<section id="statistical-learning-bias-variance-trade-off" class="level2">
<h2 class="anchored" data-anchor-id="statistical-learning-bias-variance-trade-off">Statistical Learning: Bias-Variance Trade-off</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Bias-Variance Trade-off:</strong> A fundamental concept in statistical learning. Balancing the model‚Äôs ability to fit the training data (low bias) with its ability to generalize to new data (low variance).</li>
</ul>
</div>
</section>
<section id="statistical-learning-model-selection" class="level2">
<h2 class="anchored" data-anchor-id="statistical-learning-model-selection">Statistical Learning: Model Selection</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Model Selection:</strong> Choosing the best model from a set of candidate models.</li>
</ul>
</div>
</section>
<section id="statistical-learning-regularization" class="level2">
<h2 class="anchored" data-anchor-id="statistical-learning-regularization">Statistical Learning: Regularization</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Regularization:</strong> Techniques to prevent overfitting by adding a penalty to the model‚Äôs complexity.</li>
</ul>
</div>
</section>
<section id="relationship-between-concepts" class="level2">
<h2 class="anchored" data-anchor-id="relationship-between-concepts">Relationship Between Concepts</h2>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
    A[Data Mining] --&gt; C(Common Ground)
    B[Machine Learning] --&gt; C
    D[Statistical Learning] --&gt; C
    C --&gt; E[Insights &amp; Predictions]
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="relationship-between-concepts-explained" class="level2">
<h2 class="anchored" data-anchor-id="relationship-between-concepts-explained">Relationship Between Concepts (Explained)</h2>
<div style="color: #2E86C1;">
<p>Let‚Äôs break down the diagram:</p>
<ul>
<li><strong>Data Mining</strong> is the overarching field, encompassing the entire process of <em>knowledge discovery</em> from data. Think of it as the big picture. üñºÔ∏è</li>
<li><strong>Machine Learning</strong> provides the <em>algorithms</em> that enable computers to learn from data. It‚Äôs your trusty toolbox. üß∞</li>
<li><strong>Statistical Learning</strong> offers a <em>theoretical framework</em> for understanding and improving these algorithms. It emphasizes statistical rigor and interpretability. It‚Äôs the blueprint that guides your work. üìê</li>
<li>They all converge on a <strong>Common Ground</strong>: leveraging data to generate insights and make predictions. They‚Äôre all working towards the same goal!</li>
</ul>
</div>
</section>
<section id="support-vector-machines-overview" class="level2">
<h2 class="anchored" data-anchor-id="support-vector-machines-overview">Support Vector Machines: Overview</h2>
<div style="color: #2E86C1;">
<p>SVM is a generalization of a simpler classifier called the <strong>maximal margin classifier.</strong> We will systematically explore the following concepts, each building upon the previous:</p>
</div>
</section>
<section id="support-vector-machines-overview---maximal-margin-classifier" class="level2">
<h2 class="anchored" data-anchor-id="support-vector-machines-overview---maximal-margin-classifier">Support Vector Machines: Overview - Maximal Margin Classifier</h2>
<div style="color: #2E86C1;">
<ol type="1">
<li><strong>Maximal Margin Classifier:</strong> This is the foundation, but it has a limitation ‚Äì it only works when data can be perfectly separated by a straight line (or a hyperplane in higher dimensions).</li>
</ol>
</div>
</section>
<section id="support-vector-machines-overview---support-vector-classifier" class="level2">
<h2 class="anchored" data-anchor-id="support-vector-machines-overview---support-vector-classifier">Support Vector Machines: Overview - Support Vector Classifier</h2>
<div style="color: #2E86C1;">
<ol start="2" type="1">
<li><strong>Support Vector Classifier:</strong> This is an extension that allows for some mistakes (a ‚Äúsoft margin‚Äù). This makes it applicable to a wider range of datasets, even those that aren‚Äôt perfectly separable.</li>
</ol>
</div>
</section>
<section id="support-vector-machines-overview---support-vector-machine" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="support-vector-machines-overview---support-vector-machine">Support Vector Machines: Overview - Support Vector Machine</h2>
<div style="color: #2E86C1;">
<ol start="3" type="1">
<li><strong>Support Vector Machine:</strong> This is a further extension that uses a clever trick called ‚Äúkernels‚Äù to handle situations where the boundary between classes is not a straight line.</li>
</ol>
</div>

<div class="no-row-height column-margin column-container"><div class="margin-aside">
<p><strong>Note:</strong> People often use ‚Äúsupport vector machines‚Äù as a blanket term. We‚Äôll be precise in distinguishing between the three concepts.</p>
</div></div></section>
<section id="maximal-margin-classifier" class="level2">
<h2 class="anchored" data-anchor-id="maximal-margin-classifier">9.1 Maximal Margin Classifier</h2>
<section id="what-is-a-hyperplane" class="level3">
<h3 class="anchored" data-anchor-id="what-is-a-hyperplane">9.1.1 What is a Hyperplane?</h3>
<div style="color: #2E86C1;">
<p>A hyperplane is a flat, affine subspace with a dimension <em>one less</em> than its surrounding space. It‚Äôs a generalization of familiar concepts like lines and planes. Think of it as a ‚Äúdivider‚Äù in higher dimensions. It separates the space into two halves.</p>
</div>
</section>
</section>
<section id="hyperplane-dimensions" class="level2">
<h2 class="anchored" data-anchor-id="hyperplane-dimensions">Hyperplane: Dimensions</h2>
<div style="color: #2E86C1;">
<p>Let‚Äôs understand Hyperplanes across different dimensions.</p>
</div>
</section>
<section id="hyperplane-2d" class="level2">
<h2 class="anchored" data-anchor-id="hyperplane-2d">Hyperplane: 2D</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>In 2D:</strong> A hyperplane is simply a line.</li>
</ul>
</div>
</section>
<section id="hyperplane-3d" class="level2">
<h2 class="anchored" data-anchor-id="hyperplane-3d">Hyperplane: 3D</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>In 3D:</strong> A hyperplane is a plane.</li>
</ul>
</div>
</section>
<section id="hyperplane-p-dimensions" class="level2">
<h2 class="anchored" data-anchor-id="hyperplane-p-dimensions">Hyperplane: p Dimensions</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>In p dimensions:</strong> A hyperplane is a (p-1)-dimensional flat subspace, which divides the space into two half-spaces.</li>
</ul>
</div>
</section>
<section id="hyperplane-mathematical-definition" class="level2">
<h2 class="anchored" data-anchor-id="hyperplane-mathematical-definition">Hyperplane: Mathematical Definition</h2>
<div style="color: #2E86C1;">
<p>A hyperplane in p-dimensional space is defined by the equation:</p>
</div>
<p><span class="math display">\[
\beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p = 0
\]</span></p>
</section>
<section id="hyperplane-mathematical-definition---explanation" class="level2">
<h2 class="anchored" data-anchor-id="hyperplane-mathematical-definition---explanation">Hyperplane: Mathematical Definition - Explanation</h2>
<div style="color: #2E86C1;">
<ul>
<li><span class="math inline">\(X = (X_1, X_2, ..., X_p)^T\)</span> represents a point in p-dimensional space.</li>
<li><span class="math inline">\(\beta_0, \beta_1, ..., \beta_p\)</span> are the <em>parameters</em> (coefficients) of the hyperplane. These parameters determine the orientation and position of the hyperplane. Changing these values changes the hyperplane.</li>
</ul>
</div>
</section>
<section id="hyperplane-sides" class="level2">
<h2 class="anchored" data-anchor-id="hyperplane-sides">Hyperplane: Sides</h2>
<div style="color: #2E86C1;">
<p>The hyperplane equation neatly divides the space into two regions:</p>
</div>
</section>
<section id="hyperplane-sides---greater-than-zero" class="level2">
<h2 class="anchored" data-anchor-id="hyperplane-sides---greater-than-zero">Hyperplane: Sides - Greater than Zero</h2>
<div style="color: #2E86C1;">
<ul>
<li><span class="math inline">\(\beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p &gt; 0\)</span>: Points on <em>one side</em> of the hyperplane.</li>
</ul>
</div>
</section>
<section id="hyperplane-sides---less-than-zero" class="level2">
<h2 class="anchored" data-anchor-id="hyperplane-sides---less-than-zero">Hyperplane: Sides - Less than Zero</h2>
<div style="color: #2E86C1;">
<ul>
<li><span class="math inline">\(\beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p &lt; 0\)</span>: Points on the <em>other side</em> of the hyperplane.</li>
</ul>
</div>
</section>
<section id="hyperplane-sides---sign-interpretation" class="level2">
<h2 class="anchored" data-anchor-id="hyperplane-sides---sign-interpretation">Hyperplane: Sides - Sign Interpretation</h2>
<div style="color: #2E86C1;">
<ul>
<li>The <em>sign</em> of the left-hand side tells you which side a point is on.</li>
</ul>
</div>
</section>
<section id="hyperplane-example-2d" class="level2">
<h2 class="anchored" data-anchor-id="hyperplane-example-2d">Hyperplane Example (2D)</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F6_7.svg" class="img-fluid figure-img"></p>
<figcaption>Hyperplane in 2D</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<ul>
<li><strong>FIGURE 9.1</strong>: The blue area shows where <span class="math inline">\(1 + 2X_1 + 3X_2 &gt; 0\)</span>.</li>
</ul>
</div>
</section>
<section id="hyperplane-example-2d-1" class="level2">
<h2 class="anchored" data-anchor-id="hyperplane-example-2d-1">Hyperplane Example (2D)</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F6_7.svg" class="img-fluid figure-img"></p>
<figcaption>Hyperplane in 2D</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<ul>
<li>The purple area shows where <span class="math inline">\(1 + 2X_1 + 3X_2 &lt; 0\)</span>.</li>
</ul>
</div>
</section>
<section id="hyperplane-example-2d-2" class="level2">
<h2 class="anchored" data-anchor-id="hyperplane-example-2d-2">Hyperplane Example (2D)</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F6_7.svg" class="img-fluid figure-img"></p>
<figcaption>Hyperplane in 2D</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<ul>
<li>The solid line is the hyperplane, defined by <span class="math inline">\(1 + 2X_1 + 3X_2 = 0\)</span>. Points <em>on</em> the line satisfy the equation.</li>
</ul>
</div>
<section id="classification-using-a-separating-hyperplane" class="level3">
<h3 class="anchored" data-anchor-id="classification-using-a-separating-hyperplane">9.1.2 Classification Using a Separating Hyperplane</h3>
<div style="color: #2E86C1;">
<p>Let‚Äôs say we have training data with <em>n</em> observations and <em>p</em> features. These observations fall into two classes, which we‚Äôll label as -1 and 1:</p>
</div>
<p><span class="math display">\[
X = \begin{bmatrix}
x_{11} &amp; \cdots &amp; x_{1p} \\
\vdots &amp; \ddots &amp; \vdots \\
x_{n1} &amp; \cdots &amp; x_{np}
\end{bmatrix},
\quad
y = \begin{bmatrix}
y_1 \\
\vdots \\
y_n
\end{bmatrix},
\quad y_i \in \{-1, 1\}
\]</span></p>
</section>
</section>
<section id="classification-using-a-separating-hyperplane---explanation" class="level2">
<h2 class="anchored" data-anchor-id="classification-using-a-separating-hyperplane---explanation">Classification Using a Separating Hyperplane - Explanation</h2>
<div style="color: #2E86C1;">
<ul>
<li><span class="math inline">\(X\)</span> is the feature matrix (our input data).</li>
<li><span class="math inline">\(y\)</span> is the vector of class labels (what we want to predict).</li>
</ul>
</div>
</section>
<section id="separating-hyperplane-condition" class="level2">
<h2 class="anchored" data-anchor-id="separating-hyperplane-condition">Separating Hyperplane Condition</h2>
<div style="color: #2E86C1;">
<p>If a <em>separating hyperplane</em> exists (meaning the classes are linearly separable ‚Äì we can draw a straight line/plane to divide them), it must satisfy this condition:</p>
<p><span class="math display">\[
y_i(\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_px_{ip}) &gt; 0
\]</span></p>
<p>for all <em>i</em> = 1, ‚Ä¶, <em>n</em>.</p>
</div>
</section>
<section id="separating-hyperplane-condition---explained" class="level2">
<h2 class="anchored" data-anchor-id="separating-hyperplane-condition---explained">Separating Hyperplane Condition - Explained</h2>
<div style="color: #2E86C1;">
<p>This is a crucial condition! It means that <em>all</em> points are correctly classified. The hyperplane perfectly separates the two classes. The <span class="math inline">\(y_i\)</span> term ensures that the sign is always positive, regardless of the class.</p>
</div>
</section>
<section id="classification-rule" class="level2">
<h2 class="anchored" data-anchor-id="classification-rule">Classification Rule</h2>
<div style="color: #2E86C1;">
<p>We can classify a new test observation <span class="math inline">\(x^* = (x_1^*, x_2^*, ..., x_p^*)^T\)</span> by simply looking at the sign of:</p>
<p><span class="math display">\[
f(x^*) = \beta_0 + \beta_1x_1^* + \beta_2x_2^* + \dots + \beta_px_p^*
\]</span></p>
</div>
</section>
<section id="classification-rule---explanation" class="level2">
<h2 class="anchored" data-anchor-id="classification-rule---explanation">Classification Rule - Explanation</h2>
<div style="color: #2E86C1;">
<ul>
<li>If <span class="math inline">\(f(x^*)\)</span>&gt; 0, we assign the observation to class 1.</li>
<li>If <span class="math inline">\(f(x^*)\)</span>&lt; 0, we assign the observation to class -1.</li>
<li>The <em>magnitude</em> (absolute value) of <span class="math inline">\(f(x^*)\)</span> indicates the <em>confidence</em> of the classification. A larger magnitude means the point is farther from the hyperplane, and we‚Äôre more confident in our classification. üí™</li>
</ul>
</div>
</section>
<section id="separating-hyperplanes-visual---multiple-hyperplanes" class="level2">
<h2 class="anchored" data-anchor-id="separating-hyperplanes-visual---multiple-hyperplanes">Separating Hyperplanes (Visual) - Multiple Hyperplanes</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_2.svg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>Separating Hyperplanes</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p><strong>FIGURE 9.2 Left:</strong> When data is linearly separable, there are <em>many</em> possible separating hyperplanes that can perfectly divide the classes.</p>
</div>
</section>
<section id="separating-hyperplanes-visual---question" class="level2">
<h2 class="anchored" data-anchor-id="separating-hyperplanes-visual---question">Separating Hyperplanes (Visual) - Question</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_2.svg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>Separating Hyperplanes</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p>This raises a critical question: Which one should we choose? ü§î We don‚Äôt want just <em>any</em> separating hyperplane; we want the <em>best</em> one.</p>
</div>
</section>
<section id="separating-hyperplanes-visual---decision-boundary" class="level2">
<h2 class="anchored" data-anchor-id="separating-hyperplanes-visual---decision-boundary">Separating Hyperplanes (Visual) - Decision Boundary</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_2.svg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>Separating Hyperplanes</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p><strong>FIGURE 9.2 Right:</strong> The decision boundary created by a separating hyperplane.</p>
</div>
</section>
<section id="separating-hyperplanes-visual---classification-grid" class="level2">
<h2 class="anchored" data-anchor-id="separating-hyperplanes-visual---classification-grid">Separating Hyperplanes (Visual) - Classification Grid</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_2.svg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>Separating Hyperplanes</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p>The blue and purple grids show how test observations would be classified, based on which side of the hyperplane they fall on.</p>
</div>
<section id="the-maximal-margin-classifier" class="level3">
<h3 class="anchored" data-anchor-id="the-maximal-margin-classifier">9.1.3 The Maximal Margin Classifier</h3>
<div style="color: #2E86C1;">
<p>If our data <em>can</em> be perfectly separated by a hyperplane, we have an infinite number of choices. The <em>maximal margin classifier</em> provides a principled way to choose the <em>best</em> one. It selects the hyperplane that maximizes the <em>margin</em>.</p>
</div>
</section>
</section>
<section id="maximal-margin-classifier---margin-definition" class="level2">
<h2 class="anchored" data-anchor-id="maximal-margin-classifier---margin-definition">Maximal Margin Classifier - Margin Definition</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Margin:</strong> The <em>smallest</em> distance from the hyperplane to <em>any</em> training observation. It‚Äôs like creating the widest possible ‚Äústreet‚Äù üõ£Ô∏è separating the classes, with the hyperplane running down the middle.</li>
</ul>
</div>
</section>
<section id="maximal-margin-classifier---hyperplane-definition" class="level2">
<h2 class="anchored" data-anchor-id="maximal-margin-classifier---hyperplane-definition">Maximal Margin Classifier - Hyperplane Definition</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Maximal Margin Hyperplane:</strong> The separating hyperplane that achieves the <em>largest possible</em> margin. This is the hyperplane we want!</li>
</ul>
</div>
</section>
<section id="maximal-margin-intuition" class="level2">
<h2 class="anchored" data-anchor-id="maximal-margin-intuition">Maximal Margin Intuition</h2>
<div style="color: #2E86C1;">
<p>The maximal margin hyperplane is the ‚Äúmid-line‚Äù of the widest ‚Äúslab‚Äù (or ‚Äústreet‚Äù) that we can fit between the two classes <em>without</em> touching any data points.</p>
</div>
</section>
<section id="maximal-margin-intuition---generalization" class="level2">
<h2 class="anchored" data-anchor-id="maximal-margin-intuition---generalization">Maximal Margin Intuition - Generalization</h2>
<div style="color: #2E86C1;">
<p>The intuition is that a larger margin on the training data will likely lead to a larger margin on the test data, resulting in better classification performance. We want the biggest ‚Äúbuffer zone‚Äù we can get.</p>
</div>
</section>
<section id="maximal-margin-classifier-visual" class="level2">
<h2 class="anchored" data-anchor-id="maximal-margin-classifier-visual">Maximal Margin Classifier (Visual)</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_3.svg" class="img-fluid figure-img"></p>
<figcaption>Maximal Margin Classifier</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p><strong>FIGURE 9.3:</strong> The <em>maximal margin hyperplane</em> is shown as a solid line.</p>
</div>
</section>
<section id="maximal-margin-classifier-visual---margin" class="level2">
<h2 class="anchored" data-anchor-id="maximal-margin-classifier-visual---margin">Maximal Margin Classifier (Visual) - Margin</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_3.svg" class="img-fluid figure-img"></p>
<figcaption>Maximal Margin Classifier</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p>The dashed lines define the <em>margin</em>.</p>
</div>
</section>
<section id="maximal-margin-classifier-visual---support-vectors" class="level2">
<h2 class="anchored" data-anchor-id="maximal-margin-classifier-visual---support-vectors">Maximal Margin Classifier (Visual) - Support Vectors</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_3.svg" class="img-fluid figure-img"></p>
<figcaption>Maximal Margin Classifier</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p>The points touching the dashed lines are the <em>support vectors</em> ‚Äì they are crucial!</p>
</div>
</section>
<section id="support-vectors" class="level2">
<h2 class="anchored" data-anchor-id="support-vectors">Support Vectors</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Support Vectors:</strong> The training observations that lie <em>exactly</em> on the margin (the dashed lines in the previous figure). These are the ‚Äúcritical‚Äù points.</li>
<li>These points <em>support</em> the maximal margin hyperplane, meaning they determine its position and orientation. If these points move, the hyperplane <em>must</em> move! üîÑ</li>
<li>A key property of the maximal margin hyperplane is that it depends <em>only</em> on the support vectors, <em>not</em> on any other observations. This is a very important characteristic!</li>
</ul>
</div>
<section id="construction-of-the-maximal-margin-classifier" class="level3">
<h3 class="anchored" data-anchor-id="construction-of-the-maximal-margin-classifier">9.1.4 Construction of the Maximal Margin Classifier</h3>
<div style="color: #2E86C1;">
<p>The maximal margin hyperplane is found by solving a specific <em>optimization problem</em>:</p>
</div>
<p><span class="math display">\[
\begin{aligned}
&amp;\underset{\beta_0, \beta_1, \dots, \beta_p, M}{\text{maximize}} &amp;&amp; M \\
&amp;\text{subject to} &amp;&amp; \sum_{j=1}^p \beta_j^2 = 1, \\
&amp; &amp;&amp; y_i(\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_px_{ip}) \geq M \quad \forall i = 1, \dots, n.
\end{aligned}
\]</span></p>
</section>
</section>
<section id="maximal-margin-classifier-optimization-problem-explained---m" class="level2">
<h2 class="anchored" data-anchor-id="maximal-margin-classifier-optimization-problem-explained---m">Maximal Margin Classifier: Optimization Problem Explained - M</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>M:</strong> The margin width, which we want to <em>maximize</em>. We‚Äôre trying to find the widest possible ‚Äústreet.‚Äù</li>
</ul>
</div>
</section>
<section id="maximal-margin-classifier-optimization-problem-explained---constraint-1" class="level2">
<h2 class="anchored" data-anchor-id="maximal-margin-classifier-optimization-problem-explained---constraint-1">Maximal Margin Classifier: Optimization Problem Explained - Constraint 1</h2>
<div style="color: #2E86C1;">
<ul>
<li><span class="math inline">\(\sum_{j=1}^p \beta_j^2 = 1\)</span>: This constraint ensures a <em>unique</em> solution. It doesn‚Äôt restrict the <em>hyperplane</em> itself (we can always rescale the coefficients), but it scales the coefficients so we get a specific solution.</li>
</ul>
</div>
</section>
<section id="maximal-margin-classifier-optimization-problem-explained---constraint-2" class="level2">
<h2 class="anchored" data-anchor-id="maximal-margin-classifier-optimization-problem-explained---constraint-2">Maximal Margin Classifier: Optimization Problem Explained - Constraint 2</h2>
<div style="color: #2E86C1;">
<ul>
<li><span class="math inline">\(y_i(\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_px_{ip}) \geq M\)</span>: This is the most important constraint. It ensures that <em>all</em> observations are on the <em>correct</em> side of the hyperplane <em>and</em> at least a distance <em>M</em> away (i.e., outside the margin). This is what guarantees correct classification and a margin of at least M.</li>
</ul>
</div>
<section id="the-non-separable-case" class="level3">
<h3 class="anchored" data-anchor-id="the-non-separable-case">9.1.5 The Non-separable Case</h3>
<div style="color: #2E86C1;">
<p>The maximal margin classifier has a significant limitation: it works <em>only</em> if a separating hyperplane <em>exists</em>. If the classes overlap, even a little bit, no such hyperplane exists, and the optimization problem has <em>no</em> solution. üôÅ This is a major drawback in real-world scenarios.</p>
</div>
</section>
</section>
<section id="non-separable-data-visual" class="level2">
<h2 class="anchored" data-anchor-id="non-separable-data-visual">Non-separable Data (Visual)</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_4.svg" class="img-fluid figure-img"></p>
<figcaption>Non-separable Data</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p><strong>FIGURE 9.4:</strong> An example where the classes are <em>not</em> linearly separable.</p>
</div>
</section>
<section id="non-separable-data-visual---explanation" class="level2">
<h2 class="anchored" data-anchor-id="non-separable-data-visual---explanation">Non-separable Data (Visual) - Explanation</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_4.svg" class="img-fluid figure-img"></p>
<figcaption>Non-separable Data</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p>There‚Äôs no straight line we can draw to perfectly separate the blue and purple points. The maximal margin classifier cannot be used here.</p>
</div>
</section>
<section id="support-vector-classifiers" class="level2">
<h2 class="anchored" data-anchor-id="support-vector-classifiers">9.2 Support Vector Classifiers</h2>
<section id="overview-of-the-support-vector-classifier" class="level3">
<h3 class="anchored" data-anchor-id="overview-of-the-support-vector-classifier">9.2.1 Overview of the Support Vector Classifier</h3>
<div style="color: #2E86C1;">
<p>To address the limitations of the maximal margin classifier (its inability to handle non-separable data and its sensitivity to individual observations), we introduce the <em>support vector classifier</em>, also known as the <em>soft margin classifier</em>.</p>
</div>
</section>
</section>
<section id="support-vector-classifier---the-key-idea" class="level2">
<h2 class="anchored" data-anchor-id="support-vector-classifier---the-key-idea">Support Vector Classifier - The Key Idea</h2>
<div style="color: #2E86C1;">
<p>The key idea is to allow some observations to be on the <em>wrong side of the margin</em> or even the <em>wrong side of the hyperplane</em>. This makes the margin ‚Äúsoft‚Äù ‚òÅÔ∏è, allowing for some flexibility.</p>
</div>
</section>
<section id="why-soft-margins" class="level2">
<h2 class="anchored" data-anchor-id="why-soft-margins">Why Soft Margins?</h2>
<div style="color: #2E86C1;">
<p>Let‚Äôs see why we need soft margins.</p>
</div>
</section>
<section id="why-soft-margins---non-separable-data" class="level2">
<h2 class="anchored" data-anchor-id="why-soft-margins---non-separable-data">Why Soft Margins? - Non-Separable Data</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Non-Separable Data:</strong> It‚Äôs <em>absolutely necessary</em> for overlapping classes, where perfect separation is simply impossible.</li>
</ul>
</div>
</section>
<section id="why-soft-margins---robustness" class="level2">
<h2 class="anchored" data-anchor-id="why-soft-margins---robustness">Why Soft Margins? - Robustness</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Robustness:</strong> It makes the classifier <em>less sensitive</em> to individual observations, which helps to reduce overfitting. A single outlier shouldn‚Äôt drastically change the decision boundary.</li>
</ul>
</div>
</section>
<section id="sensitivity-to-outliers-visual" class="level2">
<h2 class="anchored" data-anchor-id="sensitivity-to-outliers-visual">Sensitivity to Outliers (Visual)</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_5.svg" class="img-fluid figure-img"></p>
<figcaption>Sensitivity to Outliers</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p><strong>FIGURE 9.5:</strong> This figure demonstrates the <em>sensitivity</em> of the maximal margin classifier.</p>
</div>
</section>
<section id="sensitivity-to-outliers-visual---explanation" class="level2">
<h2 class="anchored" data-anchor-id="sensitivity-to-outliers-visual---explanation">Sensitivity to Outliers (Visual) - Explanation</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_5.svg" class="img-fluid figure-img"></p>
<figcaption>Sensitivity to Outliers</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p>Adding just <em>one</em> outlier (right panel) dramatically changes the maximal margin hyperplane. This highlights the need for a more robust approach.</p>
</div>
</section>
<section id="soft-margin-example-visual---margin-violations" class="level2">
<h2 class="anchored" data-anchor-id="soft-margin-example-visual---margin-violations">Soft Margin Example (Visual) - Margin Violations</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_6.svg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>Soft Margin Example</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p><strong>FIGURE 9.6 Left:</strong> In the soft margin classifier, most points are correctly classified and <em>outside</em> the margin.</p>
</div>
</section>
<section id="soft-margin-example-visual---margin-violations-explanation" class="level2">
<h2 class="anchored" data-anchor-id="soft-margin-example-visual---margin-violations-explanation">Soft Margin Example (Visual) - Margin Violations Explanation</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_6.svg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>Soft Margin Example</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p>However, some points <em>violate</em> the margin ‚Äì they are allowed to be inside the ‚Äústreet.‚Äù</p>
</div>
</section>
<section id="soft-margin-example-visual---misclassifications" class="level2">
<h2 class="anchored" data-anchor-id="soft-margin-example-visual---misclassifications">Soft Margin Example (Visual) - Misclassifications</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_6.svg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>Soft Margin Example</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p><strong>FIGURE 9.6 Right:</strong> Some points are even <em>misclassified</em> (they are on the wrong side of the <em>hyperplane</em>).</p>
</div>
</section>
<section id="soft-margin-example-visual---misclassifications-explanation" class="level2">
<h2 class="anchored" data-anchor-id="soft-margin-example-visual---misclassifications-explanation">Soft Margin Example (Visual) - Misclassifications Explanation</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_6.svg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>Soft Margin Example</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p>The soft margin allows for this to happen, but the extent of these violations is controlled by a tuning parameter.</p>
</div>
<section id="details-of-the-support-vector-classifier" class="level3">
<h3 class="anchored" data-anchor-id="details-of-the-support-vector-classifier">9.2.2 Details of the Support Vector Classifier</h3>
<div style="color: #2E86C1;">
<p>The support vector classifier solves a modified optimization problem that allows for margin violations:</p>
</div>
<p><span class="math display">\[
\begin{aligned}
&amp;\underset{\beta_0, \beta_1, \dots, \beta_p, \epsilon_1, \dots, \epsilon_n, M}{\text{maximize}} &amp;&amp; M \\
&amp;\text{subject to} &amp;&amp; \sum_{j=1}^p \beta_j^2 = 1, \\
&amp; &amp;&amp; y_i(\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_px_{ip}) \geq M(1 - \epsilon_i), \\
&amp; &amp;&amp; \epsilon_i \geq 0, \quad \sum_{i=1}^n \epsilon_i \leq C.
\end{aligned}
\]</span></p>
</section>
</section>
<section id="support-vector-classifier-optimization-problem-explained---slack-variables" class="level2">
<h2 class="anchored" data-anchor-id="support-vector-classifier-optimization-problem-explained---slack-variables">Support Vector Classifier: Optimization Problem Explained - Slack Variables</h2>
<div style="color: #2E86C1;">
<ul>
<li><span class="math inline">\(\epsilon_1, \dots, \epsilon_n\)</span>: These are called <em>slack variables</em>. They allow observations to be on the wrong side of the margin or hyperplane. Each observation gets its own slack variable.</li>
</ul>
</div>
</section>
<section id="support-vector-classifier-optimization-problem-explained---tuning-parameter" class="level2">
<h2 class="anchored" data-anchor-id="support-vector-classifier-optimization-problem-explained---tuning-parameter">Support Vector Classifier: Optimization Problem Explained - Tuning Parameter</h2>
<div style="color: #2E86C1;">
<ul>
<li><em>C</em>: A non-negative <em>tuning parameter</em>. Think of it as a ‚Äúbudget‚Äù for violations. It controls the trade-off between maximizing the margin width and minimizing the number and severity of violations.</li>
</ul>
</div>
</section>
<section id="support-vector-classifier-optimization-problem-explained---other-parts" class="level2">
<h2 class="anchored" data-anchor-id="support-vector-classifier-optimization-problem-explained---other-parts">Support Vector Classifier: Optimization Problem Explained - Other Parts</h2>
<div style="color: #2E86C1;">
<ul>
<li>The other parts (M and the first constraint) are the same as in the maximal margin classifier.</li>
</ul>
</div>
</section>
<section id="slack-variables-explained---no-violation" class="level2">
<h2 class="anchored" data-anchor-id="slack-variables-explained---no-violation">Slack Variables Explained - No Violation</h2>
<div style="color: #2E86C1;">
<ul>
<li><span class="math inline">\(\epsilon_i = 0\)</span>: The <em>i</em>th observation is on the <em>correct</em> side of the margin (no violation).</li>
</ul>
</div>
</section>
<section id="slack-variables-explained---margin-violation" class="level2">
<h2 class="anchored" data-anchor-id="slack-variables-explained---margin-violation">Slack Variables Explained - Margin Violation</h2>
<div style="color: #2E86C1;">
<ul>
<li><span class="math inline">\(\epsilon_i &gt; 0\)</span>: The <em>i</em>th observation is on the <em>wrong</em> side of the margin (a margin violation).</li>
</ul>
</div>
</section>
<section id="slack-variables-explained---misclassification" class="level2">
<h2 class="anchored" data-anchor-id="slack-variables-explained---misclassification">Slack Variables Explained - Misclassification</h2>
<div style="color: #2E86C1;">
<ul>
<li><span class="math inline">\(\epsilon_i &gt; 1\)</span>: The <em>i</em>th observation is on the <em>wrong</em> side of the <em>hyperplane</em> (it‚Äôs misclassified).</li>
</ul>
</div>
</section>
<section id="tuning-parameter-c---zero-budget" class="level2">
<h2 class="anchored" data-anchor-id="tuning-parameter-c---zero-budget">Tuning Parameter <em>C</em> - Zero Budget</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>C = 0:</strong> No budget for violations at all. This reduces to the maximal margin classifier (if the data is separable; otherwise, there‚Äôs no solution).</li>
</ul>
</div>
</section>
<section id="tuning-parameter-c---small-c" class="level2">
<h2 class="anchored" data-anchor-id="tuning-parameter-c---small-c">Tuning Parameter <em>C</em> - Small C</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Small C:</strong> We prioritize a narrow margin and allow fewer violations. This can lead to a model that is less prone to overfitting but might have higher bias.</li>
</ul>
</div>
</section>
<section id="tuning-parameter-c---large-c" class="level2">
<h2 class="anchored" data-anchor-id="tuning-parameter-c---large-c">Tuning Parameter <em>C</em> - Large C</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Large C:</strong> We allow a wider margin and more violations. This can lead to a model that might have lower bias but is potentially more prone to overfitting.</li>
</ul>
</div>
</section>
<section id="tuning-parameter-c---cross-validation" class="level2">
<h2 class="anchored" data-anchor-id="tuning-parameter-c---cross-validation">Tuning Parameter <em>C</em> - Cross-Validation</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>C</strong> is typically chosen using <em>cross-validation</em>, a technique for evaluating model performance on unseen data.</li>
</ul>
</div>
</section>
<section id="impact-of-c-visual" class="level2">
<h2 class="anchored" data-anchor-id="impact-of-c-visual">Impact of C (Visual)</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_7.svg" class="img-fluid figure-img"></p>
<figcaption>Impact of C</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p><strong>FIGURE 9.7:</strong> This figure shows the effect of different <em>C</em> values.</p>
</div>
</section>
<section id="impact-of-c-visual---explanation" class="level2">
<h2 class="anchored" data-anchor-id="impact-of-c-visual---explanation">Impact of C (Visual) - Explanation</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_7.svg" class="img-fluid figure-img"></p>
<figcaption>Impact of C</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p>A larger <em>C</em> results in a wider margin and <em>more</em> support vectors (because more points are violating the margin).</p>
</div>
</section>
<section id="support-vectors-revisited" class="level2">
<h2 class="anchored" data-anchor-id="support-vectors-revisited">Support Vectors (Revisited)</h2>
<div style="color: #2E86C1;">
<p>In the support vector classifier, the <em>support vectors</em> are the observations that:</p>
</div>
</section>
<section id="support-vectors-revisited---on-margin" class="level2">
<h2 class="anchored" data-anchor-id="support-vectors-revisited---on-margin">Support Vectors (Revisited) - On Margin</h2>
<div style="color: #2E86C1;">
<ul>
<li>Lie <em>exactly on</em> the margin (<span class="math inline">\(\epsilon_i = 0\)</span> and correctly classified)</li>
</ul>
</div>
</section>
<section id="support-vectors-revisited---wrong-side-of-margin" class="level2">
<h2 class="anchored" data-anchor-id="support-vectors-revisited---wrong-side-of-margin">Support Vectors (Revisited) - Wrong Side of Margin</h2>
<div style="color: #2E86C1;">
<ul>
<li>Lie on the <em>wrong side</em> of the margin (<span class="math inline">\(0 &lt; \epsilon_i \leq 1\)</span>)</li>
</ul>
</div>
</section>
<section id="support-vectors-revisited---wrong-side-of-hyperplane" class="level2">
<h2 class="anchored" data-anchor-id="support-vectors-revisited---wrong-side-of-hyperplane">Support Vectors (Revisited) - Wrong Side of Hyperplane</h2>
<div style="color: #2E86C1;">
<ul>
<li>Lie on the <em>wrong side</em> of the hyperplane (<span class="math inline">\(\epsilon_i &gt; 1\)</span>)</li>
</ul>
</div>
</section>
<section id="support-vectors-revisited---influence" class="level2">
<h2 class="anchored" data-anchor-id="support-vectors-revisited---influence">Support Vectors (Revisited) - Influence</h2>
<div style="color: #2E86C1;">
<p><em>Only</em> the support vectors affect the hyperplane‚Äôs position and orientation. Observations that are on the correct side of the margin and sufficiently far away have <em>no</em> influence. This contributes to the robustness of the SVM.</p>
</div>
</section>
<section id="support-vector-machines" class="level2">
<h2 class="anchored" data-anchor-id="support-vector-machines">9.3 Support Vector Machines</h2>
<section id="classification-with-non-linear-decision-boundaries" class="level3">
<h3 class="anchored" data-anchor-id="classification-with-non-linear-decision-boundaries">9.3.1 Classification with Non-Linear Decision Boundaries</h3>
<div style="color: #2E86C1;">
<p>The support vector classifier finds <em>linear</em> decision boundaries (hyperplanes). But what if the true boundary between classes is non-linear? ü§î</p>
</div>
</section>
</section>
<section id="non-linear-decision-boundaries---feature-expansion" class="level2">
<h2 class="anchored" data-anchor-id="non-linear-decision-boundaries---feature-expansion">Non-Linear Decision Boundaries - Feature Expansion</h2>
<div style="color: #2E86C1;">
<p>One approach is to enlarge the feature space by adding polynomial features (e.g., <span class="math inline">\(X_1^2\)</span>, <span class="math inline">\(X_1X_2\)</span>, etc.). This <em>can</em> make the data linearly separable in the <em>enlarged</em> feature space. However, this can be computationally expensive.</p>
</div>
</section>
<section id="non-linear-data-visual" class="level2">
<h2 class="anchored" data-anchor-id="non-linear-data-visual">Non-linear Data (Visual)</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_8.svg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>Non-linear Data</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p><strong>FIGURE 9.8 Left:</strong> It‚Äôs clear that a non-linear boundary is needed to separate these classes effectively.</p>
</div>
</section>
<section id="non-linear-data-visual---linear-ineffectiveness" class="level2">
<h2 class="anchored" data-anchor-id="non-linear-data-visual---linear-ineffectiveness">Non-linear Data (Visual) - Linear Ineffectiveness</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_8.svg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>Non-linear Data</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p>A straight line simply won‚Äôt do.</p>
</div>
</section>
<section id="linear-classifier-on-non-linear-data" class="level2">
<h2 class="anchored" data-anchor-id="linear-classifier-on-non-linear-data">Linear Classifier on Non-linear Data</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_8.svg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>Non-linear Data</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p><strong>FIGURE 9.8 Right:</strong> A linear classifier performs <em>poorly</em> on this data.</p>
</div>
</section>
<section id="linear-classifier-on-non-linear-data---explanation" class="level2">
<h2 class="anchored" data-anchor-id="linear-classifier-on-non-linear-data---explanation">Linear Classifier on Non-linear Data - Explanation</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_8.svg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>Non-linear Data</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p>It cannot capture the non-linear relationship between the features and the class labels.</p>
</div>
<section id="feature-expansion-a-potential-problem" class="level3">
<h3 class="anchored" data-anchor-id="feature-expansion-a-potential-problem">Feature Expansion: A Potential Problem</h3>
<div style="color: #2E86C1;">
<p>Explicitly enlarging the feature space by adding polynomial features or other transformations <em>can</em> become computationally expensive (or even impossible for very high-dimensional or infinite-dimensional feature spaces). üò© We need a more efficient way to handle non-linearity.</p>
</div>
</section>
<section id="the-support-vector-machine" class="level3">
<h3 class="anchored" data-anchor-id="the-support-vector-machine">9.3.2 The Support Vector Machine</h3>
<div style="color: #2E86C1;">
<p>The <em>support vector machine (SVM)</em> is a powerful extension of the support vector classifier. It uses <em>kernels</em> to implicitly enlarge the feature space <em>without</em> explicitly calculating the transformed features. This is known as the ‚Äúkernel trick‚Äù! ‚ú® It‚Äôs a brilliant mathematical technique.</p>
</div>
</section>
</section>
<section id="support-vector-machine---key-idea" class="level2">
<h2 class="anchored" data-anchor-id="support-vector-machine---key-idea">Support Vector Machine - Key Idea</h2>
<div style="color: #2E86C1;">
<p><strong>Key Idea:</strong> The solution to the support vector classifier depends <em>only</em> on the <em>inner products</em> of the observations, <em>not</em> on the observations themselves. This is the foundation of the kernel trick.</p>
</div>
</section>
<section id="inner-product" class="level2">
<h2 class="anchored" data-anchor-id="inner-product">Inner Product</h2>
<div style="color: #2E86C1;">
<p><strong>Inner Product:</strong> The inner product between two vectors, <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_{i'}\)</span>, is calculated as:</p>
</div>
<p><span class="math display">\[
\langle x_i, x_{i'} \rangle = \sum_{j=1}^p x_{ij}x_{i'j}
\]</span></p>
</section>
<section id="inner-product---explanation" class="level2">
<h2 class="anchored" data-anchor-id="inner-product---explanation">Inner Product - Explanation</h2>
<div style="color: #2E86C1;">
<p>The inner product is a measure of similarity between the two vectors.</p>
</div>
</section>
<section id="kernels" class="level2">
<h2 class="anchored" data-anchor-id="kernels">Kernels</h2>
<div style="color: #2E86C1;">
<p>A <em>kernel</em> is a function that quantifies the similarity between two observations:</p>
</div>
<p><span class="math display">\[
K(x_i, x_{i'})
\]</span></p>
</section>
<section id="kernels---linear-kernel" class="level2">
<h2 class="anchored" data-anchor-id="kernels---linear-kernel">Kernels - Linear Kernel</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Linear Kernel:</strong> <span class="math inline">\(K(x_i, x_{i'}) = \langle x_i, x_{i'} \rangle = \sum_{j=1}^p x_{ij}x_{i'j}\)</span>. Using this kernel gives you the standard support vector <em>classifier</em>.</li>
</ul>
</div>
</section>
<section id="kernels---polynomial-kernel" class="level2">
<h2 class="anchored" data-anchor-id="kernels---polynomial-kernel">Kernels - Polynomial Kernel</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Polynomial Kernel:</strong> <span class="math inline">\(K(x_i, x_{i'}) = (1 + \langle x_i, x_{i'} \rangle)^d\)</span>. This implicitly uses polynomial features of degree <em>d</em>. You don‚Äôt have to <em>explicitly</em> calculate the polynomial features!</li>
</ul>
</div>
</section>
<section id="kernels---radial-kernel" class="level2">
<h2 class="anchored" data-anchor-id="kernels---radial-kernel">Kernels - Radial Kernel</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Radial Kernel:</strong> <span class="math inline">\(K(x_i, x_{i'}) = \exp(-\gamma \sum_{j=1}^p (x_{ij} - x_{i'j})^2)\)</span>. This is a very popular choice and corresponds to an <em>infinite-dimensional</em> feature space! It‚Äôs incredibly powerful.</li>
</ul>
</div>
</section>
<section id="the-svm-using-kernels" class="level2">
<h2 class="anchored" data-anchor-id="the-svm-using-kernels">The SVM: Using Kernels</h2>
<div style="color: #2E86C1;">
<p>By replacing the inner product <span class="math inline">\(\langle x_i, x_{i'} \rangle\)</span> with a kernel <span class="math inline">\(K(x_i, x_{i'})\)</span> in the support vector classifier algorithm, we obtain the SVM. The resulting decision function is:</p>
</div>
<p><span class="math display">\[
f(x) = \beta_0 + \sum_{i \in S} \alpha_i K(x, x_i)
\]</span></p>
</section>
<section id="the-svm-using-kernels---explanation" class="level2">
<h2 class="anchored" data-anchor-id="the-svm-using-kernels---explanation">The SVM: Using Kernels - Explanation</h2>
<div style="color: #2E86C1;">
<p>where <em>S</em> is the set of <em>support vector</em> indices, and the <span class="math inline">\(\alpha_i\)</span> are parameters learned during training. Notice that the decision function depends <em>only</em> on the support vectors and the kernel function.</p>
</div>
</section>
<section id="svm-with-polynomial-kernel-visual" class="level2">
<h2 class="anchored" data-anchor-id="svm-with-polynomial-kernel-visual">SVM with Polynomial Kernel (Visual)</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_9.svg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>SVM with Polynomial Kernel</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p><strong>FIGURE 9.9 Left:</strong> An SVM with a polynomial kernel (degree 3) fits the non-linear data <em>much better</em> than a linear classifier could.</p>
</div>
</section>
<section id="svm-with-polynomial-kernel-visual---decision-boundary" class="level2">
<h2 class="anchored" data-anchor-id="svm-with-polynomial-kernel-visual---decision-boundary">SVM with Polynomial Kernel (Visual) - Decision Boundary</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_9.svg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>SVM with Polynomial Kernel</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p>The decision boundary is curved.</p>
</div>
</section>
<section id="svm-with-radial-kernel-visual" class="level2">
<h2 class="anchored" data-anchor-id="svm-with-radial-kernel-visual">SVM with Radial Kernel (Visual)</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_9.svg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>SVM with Radial Kernel</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p><strong>FIGURE 9.9 Right:</strong> An SVM with a radial kernel also captures the non-linear boundary effectively.</p>
</div>
</section>
<section id="radial-kernel-intuition" class="level2">
<h2 class="anchored" data-anchor-id="radial-kernel-intuition">Radial Kernel Intuition</h2>
<div style="color: #2E86C1;">
<ul>
<li>The radial kernel has <em>local</em> behavior.</li>
<li>If a test observation <span class="math inline">\(x^*\)</span> is <em>far</em> from a training observation <span class="math inline">\(x_i\)</span>, then <span class="math inline">\(K(x^*, x_i)\)</span> is very <em>small</em>. This means that <span class="math inline">\(x_i\)</span> has <em>very little</em> influence on the prediction for <span class="math inline">\(x^*\)</span>.</li>
<li>Only <em>nearby</em> training observations have a significant impact on the prediction.</li>
<li><span class="math inline">\(\gamma\)</span> is a tuning parameter that controls the ‚Äúreach‚Äù or ‚Äúwidth‚Äù of the kernel. A larger <span class="math inline">\(\gamma\)</span> makes the kernel more ‚Äúlocal‚Äù (only very close points have influence), while a smaller <span class="math inline">\(\gamma\)</span> makes it more ‚Äúglobal‚Äù (more distant points can have some influence).</li>
</ul>
</div>
</section>
<section id="computational-advantage-of-kernels" class="level2">
<h2 class="anchored" data-anchor-id="computational-advantage-of-kernels">Computational Advantage of Kernels</h2>
<div style="color: #2E86C1;">
<p>The <em>crucial</em> advantage of kernels is that we only need to compute <span class="math inline">\(K(x_i, x_{i'})\)</span> for all <em>pairs</em> of training observations. We <em>never</em> need to explicitly compute the (potentially infinite-dimensional!) feature mapping. This makes the computation feasible, even for incredibly complex feature spaces. This is the magic of the ‚Äúkernel trick‚Äù!</p>
</div>
<section id="an-application-to-the-heart-disease-data" class="level3">
<h3 class="anchored" data-anchor-id="an-application-to-the-heart-disease-data">9.3.3 An Application to the Heart Disease Data</h3>
<div style="color: #2E86C1;">
<p>The text compares SVM to LDA on the Heart Disease Data, using ROC curves for both training and testing.</p>
</div>
</section>
</section>
<section id="roc-curve-on-training-set" class="level2">
<h2 class="anchored" data-anchor-id="roc-curve-on-training-set">ROC Curve on Training Set</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_10.svg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>ROC_train</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p><strong>FIGURE 9.10 Left:</strong> On the training data, the support vector classifier (which is equivalent to an SVM with a linear kernel) performs <em>slightly</em> better than LDA (Linear Discriminant Analysis).</p>
</div>
</section>
<section id="roc-curve-on-training-set-overfitting" class="level2">
<h2 class="anchored" data-anchor-id="roc-curve-on-training-set-overfitting">ROC Curve on Training Set: Overfitting?</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_10.svg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>ROC_train</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p><strong>FIGURE 9.10 Right:</strong> The SVM using a radial basis kernel with <span class="math inline">\(\gamma = 10^{-1}\)</span> shows <em>almost perfect</em> performance on the <em>training</em> set.</p>
</div>
</section>
<section id="roc-curve-on-training-set-overfitting---explanation" class="level2">
<h2 class="anchored" data-anchor-id="roc-curve-on-training-set-overfitting---explanation">ROC Curve on Training Set: Overfitting? - Explanation</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_10.svg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>ROC_train</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p>This is a strong warning sign of <em>overfitting</em>! The model is likely too complex and has memorized the training data instead of learning the underlying patterns.</p>
</div>
</section>
<section id="roc-curve-on-test-set" class="level2">
<h2 class="anchored" data-anchor-id="roc-curve-on-test-set">ROC Curve on Test Set</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_11.svg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>ROC_test</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p><strong>FIGURE 9.11 Left:</strong> On the <em>test</em> data, the support vector classifier continues to have a *small advantage over LDA.</p>
</div>
</section>
<section id="roc-curve-on-test-set---explanation" class="level2">
<h2 class="anchored" data-anchor-id="roc-curve-on-test-set---explanation">ROC Curve on Test Set - Explanation</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_11.svg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>ROC_test</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p>This is a more reliable measure of performance than the training set results.</p>
</div>
</section>
<section id="roc-curve-on-test-set-overfitting-confirmed" class="level2">
<h2 class="anchored" data-anchor-id="roc-curve-on-test-set-overfitting-confirmed">ROC Curve on Test Set: Overfitting Confirmed</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_11.svg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>ROC_test</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p><strong>FIGURE 9.11 Right:</strong> On the <em>test</em> data, the SVM with the radial basis kernel and <span class="math inline">\(\gamma = 10^{-1}\)</span> (which had near-perfect training performance) now performs the <em>worst</em>.</p>
</div>
</section>
<section id="roc-curve-on-test-set-overfitting-confirmed-explanation" class="level2">
<h2 class="anchored" data-anchor-id="roc-curve-on-test-set-overfitting-confirmed-explanation">ROC Curve on Test Set: Overfitting Confirmed Explanation</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_11.svg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>ROC_test</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p>This confirms our suspicion of overfitting. The model that looked best on the training data is actually the worst on new data! The SVMs with <span class="math inline">\(\gamma = 10^{-2}\)</span> and <span class="math inline">\(\gamma = 10^{-3}\)</span> perform similarly to the support vector classifier (linear kernel), suggesting that a less complex model is better for this dataset.</p>
</div>
</section>
<section id="svms-with-more-than-two-classes" class="level2">
<h2 class="anchored" data-anchor-id="svms-with-more-than-two-classes">9.4 SVMs with More than Two Classes</h2>
<div style="color: #2E86C1;">
<p>SVMs are naturally designed for <em>binary</em> classification (handling only two classes). To extend them to situations with K &gt; 2 classes, two common strategies are employed:</p>
</div>
</section>
<section id="svms-with-more-than-two-classes---one-versus-one-ovo" class="level2">
<h2 class="anchored" data-anchor-id="svms-with-more-than-two-classes---one-versus-one-ovo">SVMs with More than Two Classes - One-versus-One (OvO)</h2>
<div style="color: #2E86C1;">
<ol type="1">
<li><strong>One-versus-One (OvO):</strong> We construct <span class="math inline">\(\binom{K}{2}\)</span> SVMs, each comparing a <em>pair</em> of classes. For example, if we have classes A, B, and C, we build SVMs for A vs.&nbsp;B, A vs.&nbsp;C, and B vs.&nbsp;C. To classify a test observation, we use all the SVMs and assign the observation to the class that wins the most ‚Äúvotes.‚Äù üó≥Ô∏è</li>
</ol>
</div>
</section>
<section id="svms-with-more-than-two-classes---one-versus-all-ova" class="level2">
<h2 class="anchored" data-anchor-id="svms-with-more-than-two-classes---one-versus-all-ova">SVMs with More than Two Classes - One-versus-All (OvA)</h2>
<div style="color: #2E86C1;">
<ol start="2" type="1">
<li><strong>One-versus-All (OvA):</strong> We fit K separate SVMs, each comparing <em>one</em> class to the <em>rest</em> of the classes. For example, with classes A, B, and C, we build SVMs for A vs.&nbsp;(B and C), B vs.&nbsp;(A and C), and C vs.&nbsp;(A and B). We classify a test observation to the class for which the decision function value is the highest.</li>
</ol>
</div>
</section>
<section id="one-versus-one-ovo-and-one-versus-all-ova-comparison" class="level2">
<h2 class="anchored" data-anchor-id="one-versus-one-ovo-and-one-versus-all-ova-comparison">One-versus-One (OvO) and One-versus-All (OvA) Comparison</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 19%">
<col style="width: 34%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Number of Classifiers</th>
<th>Complexity</th>
<th>Potential Issues</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>One-versus-One</td>
<td><span class="math inline">\(\binom{K}{2}\)</span></td>
<td>Quadratic in the number of classes</td>
<td>Can be computationally expensive for large K</td>
</tr>
<tr class="even">
<td>One-versus-All</td>
<td><span class="math inline">\(K\)</span></td>
<td>Linear in the number of classes</td>
<td>Class imbalance can affect performance</td>
</tr>
</tbody>
</table>
<div style="color: #2E86C1;">
<p>Generally, if the number of classes <em>K</em> is large, one-versus-all may be preferred for computational reasons.</p>
</div>
</section>
<section id="relationship-to-logistic-regression" class="level2">
<h2 class="anchored" data-anchor-id="relationship-to-logistic-regression">9.5 Relationship to Logistic Regression</h2>
<div style="color: #2E86C1;">
<p>It turns out that SVMs are closely related to logistic regression! The support vector classifier can be rewritten in a ‚ÄúLoss + Penalty‚Äù form, which reveals this connection:</p>
</div>
<p><span class="math display">\[
\underset{\beta_0, \beta_1, \dots, \beta_p}{\text{minimize}} \left\{ \sum_{i=1}^n \max[0, 1 - y_i(\beta_0 + \beta_1x_{i1} + \dots + \beta_px_{ip})] + \lambda \sum_{j=1}^p \beta_j^2 \right\}
\]</span></p>
</section>
<section id="relationship-to-logistic-regression---explanation" class="level2">
<h2 class="anchored" data-anchor-id="relationship-to-logistic-regression---explanation">Relationship to Logistic Regression - Explanation</h2>
<div style="color: #2E86C1;">
<ul>
<li>The loss function, <span class="math inline">\(\sum_{i=1}^n \max[0, 1 - y_i(\beta_0 + \beta_1x_{i1} + \dots + \beta_px_{ip})]\)</span>, is called the <em>hinge loss</em>. It penalizes misclassifications and margin violations.</li>
<li>The penalty term, <span class="math inline">\(\lambda \sum_{j=1}^p \beta_j^2\)</span>, is a <em>ridge</em> penalty (also known as L2 regularization). It encourages smaller coefficient values, which helps to prevent overfitting.</li>
</ul>
</div>
</section>
<section id="hinge-loss-vs.-logistic-regression-loss" class="level2">
<h2 class="anchored" data-anchor-id="hinge-loss-vs.-logistic-regression-loss">Hinge Loss vs.&nbsp;Logistic Regression Loss</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_12.svg" class="img-fluid figure-img"></p>
<figcaption>Loss compare</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p><strong>FIGURE 9.12:</strong> This figure compares the hinge loss (used in SVMs) and the loss function used in logistic regression.</p>
</div>
</section>
<section id="hinge-loss-vs.-logistic-regression-loss---similarity" class="level2">
<h2 class="anchored" data-anchor-id="hinge-loss-vs.-logistic-regression-loss---similarity">Hinge Loss vs.&nbsp;Logistic Regression Loss - Similarity</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_12.svg" class="img-fluid figure-img"></p>
<figcaption>Loss compare</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<p>They are remarkably similar!</p>
</div>
</section>
<section id="hinge-loss-vs.-logistic-regression-loss---hinge-loss-behavior" class="level2">
<h2 class="anchored" data-anchor-id="hinge-loss-vs.-logistic-regression-loss---hinge-loss-behavior">Hinge Loss vs.&nbsp;Logistic Regression Loss - Hinge Loss Behavior</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_12.svg" class="img-fluid figure-img"></p>
<figcaption>Loss compare</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<ul>
<li><strong>Hinge Loss:</strong> It‚Äôs <em>exactly</em> zero for observations that are correctly classified and lie beyond the margin.</li>
</ul>
</div>
</section>
<section id="hinge-loss-vs.-logistic-regression-loss---logistic-regression-loss-behavior" class="level2">
<h2 class="anchored" data-anchor-id="hinge-loss-vs.-logistic-regression-loss---logistic-regression-loss-behavior">Hinge Loss vs.&nbsp;Logistic Regression Loss - Logistic Regression Loss Behavior</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F9_12.svg" class="img-fluid figure-img"></p>
<figcaption>Loss compare</figcaption>
</figure>
</div>
<div style="color: #2E86C1;">
<ul>
<li><strong>Logistic Regression Loss:</strong> It‚Äôs never <em>exactly</em> zero, but it can become very close to zero for confident, correct predictions.</li>
</ul>
</div>
</section>
<section id="svm-vs.-logistic-regression-conclusion" class="level2">
<h2 class="anchored" data-anchor-id="svm-vs.-logistic-regression-conclusion">SVM vs.&nbsp;Logistic Regression: Conclusion</h2>
<div style="color: #2E86C1;">
<p>Because of the similarity between their loss functions, SVM and logistic regression often produce similar results. However, there are some general guidelines:</p>
</div>
</section>
<section id="svm-vs.-logistic-regression-svm-preference" class="level2">
<h2 class="anchored" data-anchor-id="svm-vs.-logistic-regression-svm-preference">SVM vs.&nbsp;Logistic Regression: SVM Preference</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>SVM:</strong> Tends to perform better when the classes are <em>well-separated</em>.</li>
</ul>
</div>
</section>
<section id="svm-vs.-logistic-regression-logistic-regression-preference" class="level2">
<h2 class="anchored" data-anchor-id="svm-vs.-logistic-regression-logistic-regression-preference">SVM vs.&nbsp;Logistic Regression: Logistic Regression Preference</h2>
<div style="color: #2E86C1;">
<ul>
<li><strong>Logistic Regression:</strong> Often preferred when classes <em>overlap</em> or when <em>probabilistic outputs</em> are desired (logistic regression naturally provides probabilities, while SVMs don‚Äôt).</li>
</ul>
</div>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<div style="color: #008080;">
<ul>
<li><strong>Support Vector Machines (SVMs)</strong> are powerful and versatile tools for classification.</li>
<li><strong>Maximal Margin Classifier:</strong> The fundamental concept, applicable only to linearly separable data.</li>
<li><strong>Support Vector Classifier (Soft Margin):</strong> Handles non-separable data and enhances robustness by allowing margin violations.</li>
<li><strong>Support Vector Machine (Kernel Trick):</strong> Efficiently handles non-linear decision boundaries by using kernels to implicitly map data to high-dimensional spaces.</li>
<li><strong>Kernels:</strong> Functions that quantify the similarity between observations. The radial kernel is a particularly popular and powerful choice.</li>
<li><strong>Support Vectors:</strong> The crucial observations that define the decision boundary. Only these points influence the model.</li>
<li><strong>Tuning Parameters:</strong> <em>C</em> (in the soft margin classifier) and kernel parameters (like <span class="math inline">\(\gamma\)</span> for the radial kernel) control the bias-variance trade-off.</li>
<li><strong>Relationship to Logistic Regression:</strong> SVMs are closely related to logistic regression, with the hinge loss being similar to the logistic regression loss function.</li>
</ul>
</div>
</section>
<section id="thoughts-and-discussion" class="level2">
<h2 class="anchored" data-anchor-id="thoughts-and-discussion">Thoughts and Discussion</h2>
<div style="color: #008080;">
<p>Let‚Äôs discuss some thought-provoking questions related to SVMs.</p>
</div>
</section>
<section id="thoughts-and-discussion---question-1" class="level2">
<h2 class="anchored" data-anchor-id="thoughts-and-discussion---question-1">Thoughts and Discussion - Question 1</h2>
<div style="color: #008080;">
<ol type="1">
<li><strong>Why are support vectors so important?</strong> What does this tell us about the robustness of SVMs? <em>Hint: Consider which points have an influence on the decision boundary and which points don‚Äôt.</em></li>
</ol>
</div>
</section>
<section id="thoughts-and-discussion---question-2" class="level2">
<h2 class="anchored" data-anchor-id="thoughts-and-discussion---question-2">Thoughts and Discussion - Question 2</h2>
<div style="color: #008080;">
<ol start="2" type="1">
<li><strong>How does the choice of kernel and its parameters affect the SVM‚Äôs decision boundary?</strong> Specifically, think about the <span class="math inline">\(\gamma\)</span> parameter in the radial kernel. <em>Hint: Consider the concepts of ‚Äúlocal‚Äù versus ‚Äúglobal‚Äù behavior. How does <span class="math inline">\(\gamma\)</span> control this?</em></li>
</ol>
</div>
</section>
<section id="thoughts-and-discussion---question-3" class="level2">
<h2 class="anchored" data-anchor-id="thoughts-and-discussion---question-3">Thoughts and Discussion - Question 3</h2>
<div style="color: #008080;">
<ol start="3" type="1">
<li><strong>When might you prefer logistic regression over an SVM, and vice-versa?</strong> Consider the characteristics of your data and the assumptions of each method. <em>Hint: Think about the degree of class separability and whether you need probabilistic outputs.</em></li>
</ol>
</div>
</section>
<section id="thoughts-and-discussion---question-4" class="level2">
<h2 class="anchored" data-anchor-id="thoughts-and-discussion---question-4">Thoughts and Discussion - Question 4</h2>
<div style="color: #008080;">
<ol start="4" type="1">
<li><strong>How does the <code>one-versus-one</code> method compare to the <code>one-versus-all</code> approach in terms of computational complexity and performance?</strong> <em>Hint: Consider how many classifiers are needed and the potential impact of class imbalance.</em></li>
</ol>
</div>
</section>
<section id="thoughts-and-discussion---question-5" class="level2">
<h2 class="anchored" data-anchor-id="thoughts-and-discussion---question-5">Thoughts and Discussion - Question 5</h2>
<div style="color: #008080;">
<ol start="5" type="1">
<li><strong>Can we apply the kernel trick to other linear models besides the support vector classifier?</strong> If so, how? <em>Hint: Think about other models that rely on inner products between observations.</em></li>
</ol>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/qiufei\.github\.io\/web-slide-r");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>üîã<a href="https://posit.co"><img src="https://posit.co/wp-content/themes/Posit/assets/images/posit-logo-2024.svg" class="img-fluid" alt="Posit" width="65"></a></p>
</div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 ÈÇ±È£û ¬© 2025
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://beian.miit.gov.cn">
<p>ÊµôICPÂ§á 2024072710Âè∑-1</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33021202002511">
<p>ÊµôÂÖ¨ÁΩëÂÆâÂ§á 33021202002511Âè∑</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:hfutqiufei@163.com">
      <i class="bi bi-envelope-at-fill" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>