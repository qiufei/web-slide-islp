---
title: "Introduction to Linear Regression"
---

## What is Data Mining? 🤔

> Data mining is the process of discovering patterns, insights, and knowledge from large datasets. It involves using various techniques from statistics, computer science, and database management to extract valuable information.  It's like being a detective, but instead of solving crimes, you're uncovering hidden clues within data.

::: {.callout-note appearance="simple"}
Think of it like sifting through a mountain of data to find hidden gems of information! 💎 It's like panning for gold, but instead of gold, you're finding valuable insights!
:::

## What is Machine Learning? 🤖

> Machine learning is a field of artificial intelligence (AI) that focuses on enabling computers to learn from data without being explicitly programmed (i.e., without being given specific rules for every situation). It involves developing algorithms that can identify patterns, make predictions, and improve their performance over time as they are exposed to more data.

::: {.callout-note appearance="simple"}
Essentially, machine learning allows computers to learn and adapt, much like humans do!  The computer learns *from* the data, rather than having a programmer tell it *exactly* what to do.
:::

## What is Statistical Learning? 📈

> Statistical learning is a framework for understanding data using statistical methods. It encompasses a vast set of tools for modeling and understanding complex datasets.  It's a recently developed area in statistics and blends with parallel developments in computer science, such as machine learning.  It provides the theoretical underpinnings for many machine learning techniques.

::: {.callout-note appearance="simple"}
 It is the theoretical foundation that underpins many machine learning techniques. Think of it as the "science" behind the "magic" of machine learning. It provides the mathematical and statistical rigor.
:::

## Relationship Between Data Mining, Machine Learning, and Statistical Learning

```{mermaid}
graph LR
    A[Data Mining] --> C(Common Ground)
    B[Machine Learning] --> C
    D[Statistical Learning] --> C
    C --> E[Insights & Predictions]
```

::: {.callout-note appearance="simple"}
Data mining, machine learning, and statistical learning overlap significantly. They all involve analyzing data to extract insights and make predictions. Statistical learning provides a theoretical foundation, while machine learning focuses on algorithms and prediction. Data mining applies these techniques to large, often unstructured datasets, to uncover hidden patterns.
:::

## Introduction to Linear Regression

> Linear regression is a fundamental approach in *supervised learning*, a type of machine learning where we have labeled data (input features and corresponding output values).  It's used primarily for predicting a *quantitative response* – a numerical value (e.g., sales, price, height).

::: {.callout-note appearance="simple"}
It's like trying to find the best-fitting straight line through a set of data points. 📏 Think of plotting points on a graph and then drawing the line that best captures the overall trend.
:::

## Introduction to Linear Regression (Continued)

-   Linear regression is a *cornerstone* in statistical learning. It's widely used, extensively studied, and forms the basis for many other, more advanced techniques.
-   Many advanced statistical learning methods can be seen as *extensions* or *generalizations* of linear regression.  It's often the first technique you learn, and it's a building block for more complex methods.

## Visualizing Linear Regression

![Relationship between advertising spending and sales](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F2_1.svg)

::: {.callout-note appearance="simple"}
This plot shows the relationship between advertising spending (on TV, radio, and newspaper) and sales.  The x-axis represents the advertising budget (in thousands of dollars) for each medium, and the y-axis represents sales (in thousands of units).  Linear regression helps us understand and quantify these relationships – how much does sales increase for each dollar spent on advertising?
:::

## Key Questions in Linear Regression (1/3)

We'll explore the `Advertising` data to address several key questions:

1.  **Relationship Existence:** Is there a statistically significant connection between advertising budget and sales? 📊  In other words, is there any evidence that advertising spending affects sales at all?

## Key Questions in Linear Regression (2/3)

2.  **Relationship Strength:**  If there is a relationship, how strong is the link between budget and sales? 💪  Is it a weak, moderate, or strong connection?
3.  **Media Contribution:** Which advertising media (TV, radio, newspaper) contribute to sales? 📺📻📰  Are all media equally effective, or do some have a greater impact?
4. **Association Size:** How much does sales increase for each dollar spent on each medium? 💵 Quantifying the impact

## Key Questions in Linear Regression (3/3)

5.  **Prediction Accuracy:** Can we accurately predict future sales based on advertising spending? 🔮  How reliable are our predictions?
6.  **Linearity Check:** Is the relationship between advertising and sales linear? 📏  Does a straight line adequately capture the relationship, or is a curve a better fit?
7.  **Media Synergy:** Do the advertising media work together synergistically (interaction effect)? 🤝 Does spending on one medium *enhance* the effectiveness of another?

::: {.callout-note appearance="simple"}
Addressing these questions helps determine if a relationship exists, its strength, individual media contributions, prediction accuracy, the linearity of the connection, and potential synergy effects. By answering these, we can make informed decisions about advertising strategies.
:::

## Simple Linear Regression: The Basics

Simple linear regression predicts a quantitative response, $Y$, using a single predictor variable, $X$, assuming a *linear relationship*:

$$
Y \approx \beta_0 + \beta_1X
$$

-   $\beta_0$: Intercept (value of $Y$ when $X = 0$).  This is where the line crosses the Y-axis.
-   $\beta_1$: Slope (change in $Y$ for a one-unit increase in $X$). This determines how steep the line is.
-   $\beta_0$ and $\beta_1$ are the model *coefficients* or *parameters*. These are the values we need to estimate from the data.

## Simple Linear Regression: Equation Visualization

The equation  $Y \approx \beta_0 + \beta_1X$ represents a straight line:

```{mermaid}
graph LR
    subgraph Linear Equation
    A[Y] --> B(β₀ + β₁X)
    end
    B --> C[β₀: Intercept]
    B --> D[β₁: Slope]
    C --> E[Value of Y when X = 0]
    D --> F[Change in Y for a one-unit increase in X]

```

::: {.callout-note appearance="simple"}
This equation defines a straight line where the intercept ($\beta_0$) is the point where the line crosses the Y-axis (when X is zero), and the slope ($\beta_1$) determines the steepness and direction of the line. If β₁ is positive, Y increases as X increases. If β₁ is negative, Y decreases as X increases.
:::

## Simple Linear Regression: Example

For instance, let's regress `sales` onto `TV` advertising:

$$
\text{sales} \approx \beta_0 + \beta_1 \times \text{TV}
$$

-   $Y$: Sales (in thousands of units).  This is the *response* variable – what we're trying to predict.
-   $X$: TV advertising budget (in thousands of dollars). This is the *predictor* variable – what we're using to make the prediction.

## Simple Linear Regression: Prediction

Once we estimate the coefficients $\beta_0$ and $\beta_1$ (denoted as $\hat{\beta_0}$ and $\hat{\beta_1}$), we can *predict* sales for a given TV advertising budget (*x*):

$$
\hat{y} = \hat{\beta_0} + \hat{\beta_1}x
$$

::: {.callout-note appearance="simple"}
The "hat" symbol (^) indicates an *estimated* value. $\hat{y}$ is the *predicted* value of sales, based on our estimated linear model. It's our best guess for the sales, given the TV advertising budget.
:::

## Estimating the Coefficients: Least Squares

Our goal is to find $\hat{\beta_0}$ and $\hat{\beta_1}$ that *best fit* the data. "Best fit" means the line should be as close as possible to the data points $(x_i, y_i)$. We use the *least squares* method. Intuitively, we want to find the line that minimizes the overall "error" between the observed data and the predicted values.

## Estimating the Coefficients: Residual Sum of Squares (RSS)

The *least squares* method minimizes the *residual sum of squares* (RSS):

$$
\text{RSS} = \sum_{i=1}^{n} (y_i - \hat{y_i})^2 = \sum_{i=1}^{n} (y_i - \hat{\beta_0} - \hat{\beta_1}x_i)^2
$$

-   $y_i$: Actual sales for the $i$-th observation. The *observed* value.
-   $\hat{y_i}$: Predicted sales for the $i$-th observation.  The value predicted by our linear model.
-   $e_i = y_i - \hat{y_i}$: The *residual* for the $i$-th observation (the difference between the actual and predicted values). This is the "error" for a single data point.

## Visualizing Residuals

```{mermaid}
graph LR
    subgraph Residual Calculation
    A[Observed Data (yᵢ)] --> C{Residual (eᵢ)}
    B[Predicted Data (ŷᵢ)] --> C
    C --> D[eᵢ = yᵢ - ŷᵢ]
    end

```

::: {.callout-note appearance="simple"}
The residual ($e_i$) represents the error in our prediction for each data point. It's the vertical distance between the actual data point and the point on the regression line. Least squares aims to minimize the *sum of the squared* residuals.  We square the residuals to ensure they are all positive and to penalize larger errors more heavily.
:::

## Estimating the Coefficients: Least Squares Explained

::: {.callout-note appearance="simple"}
Least squares finds the line that minimizes the sum of the squared vertical distances between the data points and the line. We're essentially finding the line that makes the overall prediction error as small as possible. By squaring the distances, we ensure that positive and negative errors don't cancel each other out, and we give more weight to larger errors.
:::

## Estimating the Coefficients: Formulas

Using calculus (specifically, taking partial derivatives and setting them to zero), we find the values of $\hat{\beta_0}$ and $\hat{\beta_1}$ that minimize RSS:

$$
\hat{\beta_1} = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2}
$$

$$
\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}
$$

-   $\bar{x}$: Sample mean of $X$. The average value of the predictor.
-   $\bar{y}$: Sample mean of $Y$. The average value of the response.

## Interpreting the Coefficient Formulas (1/2)

The formula for the slope, $\hat{\beta_1}$, can be interpreted as:

$$
\hat{\beta_1} = \frac{\text{Covariance}(X, Y)}{\text{Variance}(X)}
$$

::: {.callout-note appearance="simple"}
The numerator measures the *covariance* between X and Y, which indicates how much X and Y vary together. The denominator measures the *variance* of X, which indicates how much X varies on its own. The slope is essentially the covariance scaled by the variance of X.
:::

## Interpreting the Coefficient Formulas (2/2)

The formula for the intercept, $\hat{\beta_0}$, ensures that the regression line passes through the point ($\bar{x}$, $\bar{y}$).

$$
\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}
$$

::: {.callout-note appearance="simple"}
This means that when X is equal to its average value ($\bar{x}$), the predicted value of Y ($\hat{y}$) will be equal to the average value of Y ($\bar{y}$).
:::

## Visualizing the Least Squares Fit

![Least Squares Fit](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F3_1.svg)

## Visualizing the Least Squares Fit: Explanation

::: {.callout-note appearance="simple"}
This figure shows the least squares regression line for `sales` versus `TV` advertising. Each grey line segment represents a *residual* – the difference between the actual sales value and the sales value predicted by the line. The least squares method finds the line that minimizes the sum of the squares of these residuals, resulting in the "best-fitting" line.
:::

## Assessing Coefficient Accuracy: Population vs. Sample

-   **Population Regression Line:** The "true" (but usually unknown) relationship: $Y = \beta_0 + \beta_1X + \epsilon$. This represents the ideal, underlying relationship in the *entire population*.  We almost never know this.
-   **Least Squares Line:**  The estimated relationship based on our *sample*: $\hat{y} = \hat{\beta_0} + \hat{\beta_1}x$. This is our best guess at the true relationship, based on the data we have. It's an *estimate* of the population regression line.
-   $\epsilon$: random error term, which captures all the factors that influence Y but are not included in the model. It represents the inherent randomness in the relationship.

## Population vs. Sample Regression Lines: Visualization

![Population vs. Sample Regression Lines](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F3_3.svg)

## Population vs Sample Regression Lines: Left Panel Explanation

::: {.callout-note appearance="simple"}
The left panel shows the true population regression line (red) and the estimated least squares line (blue) from a *single* sample. The blue line is our best estimate of the red line, based on one particular set of data.
:::

## Population vs Sample Regression Lines: Right Panel Explanation
![Population vs. Sample Regression Lines](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F3_3.svg)

::: {.callout-note appearance="simple"}

The right panel shows ten different least squares lines, each estimated from a *different* sample drawn from the same population.  The least squares lines vary, but they cluster around the true population line (red). This illustrates the concept of *sampling variability* – our estimates will vary depending on the specific sample we draw.
:::

## Assessing Coefficient Accuracy: Unbiasedness

-   $\hat{\beta_0}$ and $\hat{\beta_1}$ are *estimates* of the true (unknown) parameters $\beta_0$ and $\beta_1$.
-   These estimates are *unbiased*: on average, they will equal the true values.

## Unbiasedness Explained

::: {.callout-note appearance="simple"}
Unbiasedness means that our estimation method doesn't systematically over- or underestimate the true values. If we took many samples and calculated the estimates each time, the average of the estimates would converge to the true values.  This is a desirable property of an estimator – it means that, in the long run, our estimation method will give us the right answer.
:::

## Assessing Coefficient Accuracy: Standard Error

-   **Standard Error:** Measures the average amount that an estimate ($\hat{\beta_0}$ or $\hat{\beta_1}$) differs from the true value ($\beta_0$ or $\beta_1$). It quantifies the typical uncertainty in our estimates.  It's like a measure of the "spread" of the estimates we would get if we took many samples.

## Standard Error: Formulas

Formulas for standard errors:

$$
\text{SE}(\hat{\beta_0})^2 = \sigma^2 \left[ \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2} \right]
$$

$$
\text{SE}(\hat{\beta_1})^2 = \frac{\sigma^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}
$$
- $\sigma^2$: Variance of the error term $\epsilon$ (usually unknown, estimated by the *residual standard error*, RSE). This represents the variability of the data around the true regression line.

## Standard Error: Interpretation (1/2)

$$
\text{SE}(\hat{\beta_0})^2 = \sigma^2 \left[ \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2} \right]
$$

::: {.callout-note appearance="simple"}
The standard error of the intercept depends on the sample size (n), the variance of the error term (σ²), the average value of X (x̄), and the spread of the X values. A larger sample size (n) leads to a smaller standard error.
:::

## Standard Error: Interpretation (2/2)

$$
\text{SE}(\hat{\beta_1})^2 = \frac{\sigma^2}{\sum_{i=1}^{n}(x_i - \bar{x})^2}
$$

::: {.callout-note appearance="simple"}
Smaller standard errors indicate more precise estimates. Notice that SE($\hat{\beta_1}$) is smaller when the $x_i$ values are more spread out – having a wider range of X values (larger denominator) gives us more information about the slope, leading to a more precise estimate.  Also, a smaller error variance (σ²) leads to a smaller standard error.
:::

## Assessing Coefficient Accuracy: Confidence Intervals

-   **Confidence Interval:** A range of values that is likely to contain the true unknown value of a parameter, with a certain level of confidence (e.g., 95%).  It gives us a sense of how much our estimate might vary if we took different samples.

## Confidence Interval: Formula and Interpretation

-   Approximate 95% confidence interval for $\beta_1$:

$$
\hat{\beta_1} \pm 2 \cdot \text{SE}(\hat{\beta_1})
$$

::: {.callout-note appearance="simple"}
This means that if we were to repeatedly sample from the population and construct 95% confidence intervals, approximately 95% of those intervals would contain the true value of $\beta_1$. It gives us a range within which we are reasonably confident the true parameter lies.  The interval is centered around our estimate ($\hat{\beta_1}$) and its width depends on the standard error.
:::

## Hypothesis Testing

-   **Null Hypothesis (H₀):**  There is *no* relationship between $X$ and $Y$ ($\beta_1 = 0$). This is the "skeptical" viewpoint – we assume there's no relationship unless the data provide strong evidence otherwise.
-   **Alternative Hypothesis (Hₐ):** There is *some* relationship between $X$ and $Y$ ($\beta_1 \neq 0$). This is what we're trying to find evidence for.

## Hypothesis Testing: t-statistic and p-value

-   **t-statistic:** Measures how many standard deviations $\hat{\beta_1}$ is away from 0:

$$
t = \frac{\hat{\beta_1} - 0}{\text{SE}(\hat{\beta_1})}
$$

-   **p-value:** The probability of observing a t-statistic as extreme as, or more extreme than, the one calculated, *assuming* $H_0$ is true. It tells us how likely it is to see data like ours if there really is no relationship between X and Y.

## Hypothesis Testing: Interpretation

::: {.callout-note appearance="simple"}
A small p-value (typically < 0.05) provides evidence *against* the null hypothesis, suggesting a statistically significant relationship between X and Y. A large t-statistic (in absolute value) corresponds to a small p-value. If the p-value is small, it means it's unlikely to observe such a large t-statistic if there were truly no relationship between X and Y.
:::

## Hypothesis Testing: Example (Advertising Data)

| Predictor   | Coefficient | Std. error | t-statistic | p-value  |
| :---------- | :---------- | :--------- | :---------- | :------- |
| Intercept   | 7.0325      | 0.4578     | 15.36       | < 0.0001 |
| TV          | 0.0475      | 0.0027     | 17.67       | < 0.0001 |

## Hypothesis Testing: Example Interpretation

::: {.callout-note appearance="simple"}
The table shows the results of regressing `sales` on `TV` advertising. The very small p-value for `TV` provides strong evidence that $\beta_1 \neq 0$, meaning there *is* a statistically significant relationship between TV advertising and sales. The t-statistic for TV (17.67) is very large, indicating that the estimated coefficient for TV is many standard errors away from zero.
:::

## Assessing Model Accuracy: RSE

-   **Residual Standard Error (RSE):** An estimate of the standard deviation of the error term $\epsilon$. It represents the average amount that the response will deviate from the true regression line. It's a measure of the model's lack of fit – how much the data points tend to scatter around the regression line.

## RSE: Formula

$$
\text{RSE} = \sqrt{\frac{1}{n-2}\text{RSS}} = \sqrt{\frac{1}{n-2}\sum_{i=1}^{n}(y_i - \hat{y_i})^2}
$$

::: {.callout-note appearance="simple"}
Lower RSE values indicate a better fit, meaning the model's predictions are closer to the actual values. The RSE is measured in the units of Y. The (n-2) in the denominator represents the degrees of freedom, accounting for the fact that we've estimated two parameters (β₀ and β₁).
:::

## Assessing Model Accuracy: R²

-   **R² Statistic:** Measures the *proportion of variance explained* by the model. It's a measure of how well the model captures the variability in the response. It always falls between 0 and 1.

## R²: Formula

$$
R^2 = \frac{\text{TSS} - \text{RSS}}{\text{TSS}} = 1 - \frac{\text{RSS}}{\text{TSS}}
$$

-   **Total Sum of Squares (TSS):** $\sum(y_i - \bar{y})^2$ - Measures the total variance in the response $Y$ *before* considering the predictor. It represents the total variability in the response.

## R²: Interpretation

::: {.callout-note appearance="simple"}
R² closer to 1 indicates that a large proportion of the variability in the response is explained by the regression. An R² of 0 means the model explains none of the variability (the model is no better than just predicting the average value of Y). In simple linear regression, R² is the square of the correlation between X and Y. It can be interpreted as the percentage of the variation in Y that can be attributed to X.
:::

## Assessing Model Accuracy: Example (Advertising Data)

| Quantity                | Value  |
| :---------------------- | :----- |
| Residual standard error | 3.26   |
| R²                      | 0.612  |
| F-statistic             | 312.1  |

## Assessing Model Accuracy: Example Interpretation

::: {.callout-note appearance="simple"}
For the regression of `sales` on `TV`, the RSE is 3.26 (thousands of units). This means that, on average, the actual sales values deviate from the true regression line by about 3,260 units. The R² is 0.612, meaning that 61.2% of the variability in sales is explained by TV advertising. The F-statistic is a measure of overall model significance (relevant for multiple regression, discussed later).
:::

## Multiple Linear Regression

Extends simple linear regression to handle *multiple* predictors:

$$
Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p + \epsilon
$$

-   $\beta_j$: The average effect on $Y$ of a one-unit increase in $X_j$, *holding all other predictors fixed*. This is a crucial point – the interpretation of each coefficient is *conditional* on the other predictors being in the model.

::: {.callout-note appearance="simple"}
Each predictor now has its own slope coefficient, representing its unique contribution to the response, *while controlling for the other predictors*. This allows us to isolate the effect of each predictor.
:::

## Multiple Linear Regression: Example (Advertising Data)

$$
\text{sales} = \beta_0 + \beta_1 \times \text{TV} + \beta_2 \times \text{radio} + \beta_3 \times \text{newspaper} + \epsilon
$$

Here, we're trying to predict sales using TV, radio, *and* newspaper advertising budgets.

## Multiple Linear Regression: Example Results

| Predictor   | Coefficient | Std. error | t-statistic | p-value  |
| :---------- | :---------- | :--------- | :---------- | :------- |
| Intercept   | 2.939       | 0.3119     | 9.42        | < 0.0001 |
| TV          | 0.046       | 0.0014     | 32.81       | < 0.0001 |
| radio       | 0.189       | 0.0086     | 21.89       | < 0.0001 |
| newspaper   | -0.001      | 0.0059     | -0.18       | 0.8599   |

## Multiple Linear Regression: Example Interpretation

::: {.callout-note appearance="simple"}
*Holding TV and newspaper advertising fixed*, spending an additional $1,000 on radio advertising is associated with an increase in sales of approximately 189 units. The newspaper coefficient is not statistically significant (p-value > 0.05), suggesting that *after accounting for TV and radio advertising*, newspaper advertising does not have a significant impact on sales.
:::

## Correlation Between Predictors

|           | TV      | radio   | newspaper | sales   |
| :-------- | :------ | :------ | :-------- | :------ |
| TV        | 1.0000  | 0.0548  | 0.0567    | 0.7822  |
| radio     | 0.0548  | 1.0000  | 0.3541    | 0.5762  |
| newspaper | 0.0567  | 0.3541  | 1.0000    | 0.2283  |
| sales     | 0.7822  | 0.5762  | 0.2283    | 1.0000  |

## Correlation Matrix: Explanation

::: {.callout-note appearance="simple"}
This correlation matrix shows the pairwise correlations between the variables in the `Advertising` data. Notice the moderate correlation (0.35) between `radio` and `newspaper`. This correlation can affect the coefficients in the multiple regression model, explaining why the `newspaper` coefficient is not significant in the multiple regression, even though it might be significant in a simple linear regression with only `newspaper`. The correlation between radio and newspaper "confounds" the effect of newspaper on sales.
:::

## Important Questions in Multiple Linear Regression

1.  **Any Useful Predictors?** Is at least *one* predictor useful in predicting the response? (F-test, discussed next)
2.  **All or Subset?** Do *all* predictors help explain $Y$, or only a subset? (Variable selection)
3.  **Model Fit:** How well does the model fit the data? (RSE, R²)
4.  **Prediction:** Given predictor values, what should we predict for the response, and how accurate is our prediction? (Prediction intervals, confidence intervals)

## One: Is There a Relationship? (F-test)

-   **Null Hypothesis (H₀):**  *All* coefficients are zero ($\beta_1 = \beta_2 = \dots = \beta_p = 0$). This means *none* of the predictors are related to the response.
-   **Alternative Hypothesis (Hₐ):** At least *one* coefficient is non-zero. This means at least one predictor is related to the response.

## F-statistic: Formula

-   **F-statistic:**

$$
F = \frac{(\text{TSS} - \text{RSS})/p}{\text{RSS}/(n - p - 1)}
$$

::: {.callout-note appearance="simple"}
If there's *no* relationship between the response and predictors, the F-statistic will be close to 1. If Hₐ is true, F will be *greater* than 1. The larger the F-statistic, the stronger the evidence against the null hypothesis. The numerator represents the variance explained by the model, and the denominator represents the unexplained variance. The values *p* and *(n - p - 1)* are the degrees of freedom for the numerator and denominator, respectively.
:::

## F-test: Example (Advertising Data)

| Quantity                | Value  |
| :---------------------- | :----- |
| Residual standard error | 1.69   |
| R²                      | 0.897  |
| F-statistic             | 570    |

## F-test: Example Interpretation

::: {.callout-note appearance="simple"}
The F-statistic for the multiple regression of `sales` on `TV`, `radio`, and `newspaper` is 570. This is much larger than 1, providing strong evidence *against* the null hypothesis. The associated p-value is essentially zero, indicating that at least one advertising medium is significantly related to sales.  The high R² (0.897) also indicates a good model fit – the predictors explain a large proportion of the variance in sales.
:::

## Two: Deciding on Important Variables (Variable Selection)

-   **Goal:** Identify the *subset* of predictors that are most strongly related to the response. We want to find the most important predictors and exclude those that don't contribute meaningfully to the model. We aim for a *parsimonious* model – one that is as simple as possible while still explaining the data well.

-   **Methods:**
    -   *Forward Selection:* Start with the null model (intercept only) and add predictors one by one, based on which improves the model fit the most (e.g., largest decrease in RSS or increase in R²).
    -   *Backward Selection:* Start with all predictors and remove them one by one, based on which has the least impact on the model fit (e.g., smallest decrease in RSS or decrease in R²).
    -   *Mixed Selection:* Combination of forward and backward selection, allowing for both adding and removing predictors at each step.

## Variable Selection: Explanation

::: {.callout-note appearance="simple"}
We typically can't try all possible subsets of predictors (there are 2<sup>p</sup> of them!), so we use these more efficient, step-wise methods to find a good model. These methods provide a computationally feasible way to search for a good subset of predictors.
:::

## Three: Model Fit (RSE and R²)

-   **RSE and R²:** Same interpretations as in simple linear regression. RSE measures the average prediction error, and R² measures the proportion of variance explained.
-   **Important Note:** R² will *always* increase when more variables are added to the model, even if those variables are only weakly associated with the response. This is because adding variables always reduces the RSS on the *training* data.

## Model Fit: Caveat

::: {.callout-note appearance="simple"}
Adding more variables may not improve predictions on *new* data (test data). We need to be careful not to *overfit* the training data by including too many predictors.  Techniques like *cross-validation* can help assess model performance on unseen data and prevent overfitting. Adjusted R² is another metric that penalizes the addition of unnecessary variables.
:::

## Four: Predictions

Three sources of uncertainty in predictions:

1.  **Coefficient Uncertainty:** The least squares plane is only an *estimate* of the true population regression plane. (Reducible error – we can reduce this by getting more data).
2.  **Model Bias:** The linear model is likely an approximation of the true relationship. (Reducible error – we can reduce this by using a more flexible model).
3.  **Irreducible Error:** Even if we knew the true relationship, we couldn't predict $Y$ perfectly because of the random error $\epsilon$. (We can't reduce this).

## Addressing Prediction Uncertainty

- **Coefficient Uncertainty:** Addressed with *confidence intervals*.
- **Model Bias:** Addressed by considering more complex models (e.g., non-linear models, interaction terms).
- **Irreducible Error:** Addressed with *prediction intervals*.

## Confidence vs. Prediction Intervals

-   **Confidence Interval:** Quantifies uncertainty around the *average* response value for a given set of predictor values. It tells us where the *average* response is likely to fall, given the predictor values.
-   **Prediction Interval:** Quantifies uncertainty around a *single* response value for a given set of predictor values. It tells us where an *individual* response is likely to fall, given the predictor values.

## Confidence vs. Prediction Intervals: Comparison

::: {.callout-note appearance="simple"}
Prediction intervals are *always* wider than confidence intervals because they account for both the uncertainty in estimating the population regression plane *and* the inherent variability of individual data points around that plane (the irreducible error). Confidence intervals only account for the uncertainty in estimating the *average* response.
:::

## Qualitative Predictors

-   **Qualitative Predictor (Factor):** A variable with categorical values (levels). Examples: gender (male/female), region (North/South/East/West), type of product (A/B/C).
-   **Dummy Variable:** A numerical variable used to represent a qualitative predictor in a regression model. We convert categorical values into numerical codes.

## Dummy Variables: How They Work

    -   For a predictor with two levels: Create *one* dummy variable.
    -   For a predictor with more than two levels: Create *one fewer dummy variable than the number of levels.
    -   One level will serve as a *baseline* (reference) level.

::: {.callout-note appearance="simple"}
Each dummy variable is coded as 0 or 1, indicating the *absence* or *presence* of a particular level. The baseline level is implicitly represented when all dummy variables are 0.
:::

## Qualitative Predictors: Example (Credit Data)

We want to predict `balance` using the `own` variable (whether someone owns a house).

- Create a dummy variable:

$$
x_i = \begin{cases}
1 & \text{if person } i \text{ owns a house} \\
0 & \text{if person } i \text{ does not own a house}
\end{cases}
$$

## Qualitative Predictors: Regression Model

- Regression model:

$$
y_i = \beta_0 + \beta_1x_i + \epsilon_i = \begin{cases}
\beta_0 + \beta_1 + \epsilon_i & \text{if person } i \text{ owns a house} \\
\beta_0 + \epsilon_i & \text{if person } i \text{ does not own a house}
\end{cases}
$$

## Qualitative Predictors: Interpretation

::: {.callout-note appearance="simple"}
$\beta_0$ represents the average credit card balance for non-owners (the baseline group). $\beta_0 + \beta_1$ represents the average balance for owners. $\beta_1$ is the average *difference* in balance between owners and non-owners. The coefficient of the dummy variable represents the difference in the mean response between the level represented by the dummy variable and the baseline level.
:::

## Qualitative Predictors: More than Two Levels

Suppose we have a qualitative predictor, `region`, with three levels: North, South, and West.  We create *two* dummy variables:

$$
x_{i1} = \begin{cases}
1 & \text{if person } i \text{ is from the North} \\
0 & \text{otherwise}
\end{cases}
$$

$$
x_{i2} = \begin{cases}
1 & \text{if person } i \text{ is from the South} \\
0 & \text{otherwise}
\end{cases}
$$

## Qualitative Predictors: Multi-Level Regression

The regression model would be:

$$
y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \epsilon_i = \begin{cases}
\beta_0 + \beta_1 + \epsilon_i & \text{if North} \\
\beta_0 + \beta_2 + \epsilon_i & \text{if South} \\
\beta_0 + \epsilon_i & \text{if West (baseline)}
\end{cases}
$$

## Qualitative Predictors: Multi-Level Interpretation

::: {.callout-note appearance="simple"}
$\beta_0$ represents the average balance for people from the West (the baseline). $\beta_1$ represents the average difference in balance between people from the North and the West. $\beta_2$ represents the average difference in balance between people from the South and the West.
:::

## Interactions

-   **Additive Assumption:** The effect of one predictor on the response does *not* depend on the values of other predictors. The effect of each predictor is independent of the others.  This is a simplifying assumption, but it may not always be true.
-   **Interaction Effect (Synergy):** The effect of one predictor on the response *does* depend on the values of other predictors. The combined effect of two predictors is different from the sum of their individual effects.
-   **Interaction Term:** Include the *product* of two predictors in the model to capture the interaction effect.

## Interactions: Explanation

::: {.callout-note appearance="simple"}
Interactions allow the relationship between a predictor and the response to vary depending on the values of other predictors. They allow for a more flexible and realistic model, capturing situations where the effect of one predictor is amplified or diminished by another.
:::

## Interactions: Example (Advertising Data)

$$
\text{sales} = \beta_0 + \beta_1 \times \text{TV} + \beta_2 \times \text{radio} + \beta_3 \times (\text{TV} \times \text{radio}) + \epsilon
$$

This model includes an interaction term between TV and radio advertising.

## Interactions: Rewritten Equation

The model with the interaction term can be rewritten as:

$$
\text{sales} = \beta_0 + (\beta_1 + \beta_3 \times \text{radio}) \times \text{TV} + \beta_2 \times \text{radio} + \epsilon
$$

::: {.callout-note appearance="simple"}
Now, the slope for `TV` ($\beta_1 + \beta_3 \times \text{radio}$) depends on the value of `radio`. The interaction term ($\beta_3$) allows for synergy between the advertising media. If $\beta_3$ is positive, the effect of TV advertising increases as radio advertising increases. If $\beta_3$ is negative, the effect of TV advertising decreases as radio advertising increases.
:::

## Interactions: Qualitative and Quantitative Predictors

We can also include interactions between qualitative and quantitative predictors. For example, we could model the relationship between `balance`, `income`, and `student` (a qualitative variable indicating whether someone is a student) as:

$$
\text{balance} = \beta_0 + \beta_1 \times \text{income} + \beta_2 \times \text{student} + \beta_3 \times (\text{income} \times \text{student}) + \epsilon
$$

where `student` is a dummy variable (1 if student, 0 otherwise).

## Interaction: Qualitative and Quantitative Interpretation

::: {.callout-note appearance="simple"}
This model allows for different slopes for students and non-students. The coefficient β₃ represents the *difference* in the slope of the income-balance relationship between students and non-students. This allows the effect of income on balance to be different for students compared to non-students.
:::

## Non-linear Relationships: Polynomial Regression

-   **Linearity Assumption:** The relationship between the predictors and the response is linear. This is another simplifying assumption that may not always hold.
-   **Polynomial Regression:** Include *polynomial terms* (e.g., $X^2$, $X^3$) of the predictors in the model to capture *non-linear* relationships.

## Polynomial Regression: Example

$$
\text{mpg} = \beta_0 + \beta_1 \times \text{horsepower} + \beta_2 \times \text{horsepower}^2 + \epsilon
$$

::: {.callout-note appearance="simple"}
This is still a *linear* regression model (linear in the coefficients), but it models a *non-linear* (quadratic) relationship between `mpg` and `horsepower`. We're fitting a curve rather than a straight line to the data.
:::

## Visualizing Polynomial Regression

![Polynomial Regression](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F3_8.svg)
## Polynomial Regression: degree = 1

![Polynomial Regression](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F3_8.svg)

::: {.callout-note appearance="simple"}
Here is the linear fit (dashed), and It does not fit the data well
:::

## Polynomial Regression: degree = 2

![Polynomial Regression](https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F3_8.svg)

::: {.callout-note appearance="simple"}
The plot shows a linear fit (dashed) and a quadratic fit (degree = 2, solid) to the relationship between `horsepower` and `mpg`. The quadratic fit captures the non-linear relationship better. We can see that as horsepower increases, mpg initially decreases, but then the rate of decrease slows down, and eventually, mpg might even start to increase slightly at very high horsepower levels.
:::

## Potential Problems in Linear Regression

1.  **Non-linearity:** The relationship between response and predictors is not linear.
2.  **Correlation of Error Terms:** Errors are not independent (e.g., time series data).
3.  **Non-constant Variance of Error Terms (Heteroscedasticity):** Variance of errors changes with the response or predictors.
4.  **Outliers:** Observations with unusual response values.
5.  **High Leverage Points:** Observations with unusual predictor values.
6.  **Collinearity:** Predictors are highly correlated.

## Potential Problems: Impact

::: {.callout-note appearance="simple"}
These problems can affect the accuracy and interpretability of the regression model. They can lead to biased coefficient estimates, incorrect standard errors, and misleading conclusions. It's important to diagnose and address these issues to ensure the model is reliable and valid.
:::

## Problem 1: Non-linearity

- **Detection:** Residual plots (plot of residuals vs. predicted values or predictors). A non-linear pattern in the residual plot suggests non-linearity.
- **Solution:** Non-linear transformations of the predictors (log, square root, polynomial terms), or use non-linear regression models.

## Problem 2: Correlation of Error Terms

- **Detection:** Examine the context of the data (e.g., time series data, clustered data). Autocorrelation plots for time series data.
- **Solution:** Use time series methods (e.g., autoregressive models) or generalized least squares.

## Problem 3: Heteroscedasticity

- **Detection:** Residual plots. A "funnel" shape (increasing or decreasing spread of residuals) suggests heteroscedasticity.
- **Solution:** Transformations of the response variable (e.g., log(Y)), weighted least squares.

## Problem 4: Outliers

- **Detection:** Residual plots, studentized residuals. Observations with very large residuals (e.g., |studentized residual| > 3) are potential outliers.
- **Solution:** Investigate the outliers. If they are due to data entry errors, correct them. If not, consider robust regression methods or removing the outliers (with caution).

## Problem 5: High Leverage Points

- **Detection:** Leverage statistics. Observations with high leverage have a large influence on the regression line.
- **Solution:** Investigate the points. If they are errors, correct them. If not, consider their impact on the model.

## Problem 6: Collinearity

- **Detection:** Correlation matrix of predictors. Variance inflation factor (VIF). High correlation (> 0.7 or 0.8) or VIF values (> 5 or 10) suggest collinearity.
- **Solution:** Remove one or more of the collinear predictors, combine collinear predictors, or use regularization techniques (ridge regression, lasso).

## Summary

-   Linear regression is a fundamental and versatile tool for predicting a quantitative response.
-   It relies on assumptions about linearity, additivity, and the error terms.  It's important to check these assumptions.
-   We can assess model fit (RSE, R²), coefficient significance (t-statistics, p-values), and overall significance (F-test).
-   Multiple linear regression handles multiple predictors, allowing us to isolate the effect of each predictor while controlling for others.
-   Extensions (qualitative predictors, interactions, polynomial regression) enhance flexibility, allowing us to model more complex relationships.
-   Diagnosing and addressing potential problems is crucial for ensuring the reliability and validity of the model.

## Thoughts and Discussion

-   How do we choose the "best" model among a set of possible models? (Model selection criteria, cross-validation, adjusted R², AIC, BIC)
-   What are the limitations of linear regression, and when might other methods be more appropriate? (Non-linear relationships, complex interactions, non-normal errors, high dimensionality)
-   How can we effectively diagnose and address the potential problems in linear regression? (Residual plots, transformations, robust regression, generalized linear models)
-   How can the insights from a linear regression model be used to inform real-world decisions? (Marketing: optimizing advertising spend, finance: predicting stock prices, healthcare: identifying risk factors for diseases)
- How does the size and quality of the data affect the model outcome? (More data is generally better; garbage in, garbage out; consider potential biases in the data)
- What are some ethical considerations when using linear regression (or any statistical model) for decision-making? (Fairness, transparency, accountability)
