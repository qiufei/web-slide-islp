<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Tree-Based Methods</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<link href="../https://assets.qiufei.site/personal/profile.jpg" rel="icon" type="image/jpeg">
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-8ea72dc5fed832574809a9c94082fbbb.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-681fbf911679f9b3dbf9743eb275ba49.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-7e49aeac8059a213a463aa1a739e8272.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://qiufei.github.io"> 
<span class="menu-text">È¶ñÈ°µ</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://qiufei.github.io/web-slide-r"> 
<span class="menu-text">RËØæ‰ª∂</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://qiufei.github.io/web-slide-marketing"> 
<span class="menu-text">Ëê•ÈîÄËØæ‰ª∂</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Tree-Based Methods</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction-tree-based-methods" class="level2">
<h2 class="anchored" data-anchor-id="introduction-tree-based-methods">Introduction: Tree-Based Methods üå≥</h2>
<p>This chapter introduces tree-based methods for regression and classification. These methods involve segmenting the predictor space into simpler, more manageable regions. Think of it like creating a flowchart to make decisions!</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Tree-based methods are simple and easy to understand (interpretable). However, they often aren‚Äôt as accurate as some other machine learning techniques. We‚Äôll explore ways to make them better, like bagging, random forests, and boosting, which combine many trees to improve performance.</p>
</div>
</div>
</section>
<section id="core-concepts-data-mining" class="level2">
<h2 class="anchored" data-anchor-id="core-concepts-data-mining">Core Concepts: Data Mining</h2>
<p>Let‚Äôs start with the foundational concepts. First up: Data Mining.</p>
<p>Data mining is the process of discovering patterns, anomalies, and insights from <em>large</em> datasets. It utilizes a multidisciplinary approach, blending techniques from:</p>
<ul>
<li><strong>Statistics:</strong> For analyzing data and drawing inferences.</li>
<li><strong>Machine Learning:</strong> For building predictive models.</li>
<li><strong>Database Management:</strong> For efficiently storing and retrieving large datasets.</li>
</ul>
<p>It‚Äôs like being a detective üïµÔ∏è‚Äç‚ôÄÔ∏è, sifting through raw data to find valuable knowledge and clues. The goal is to extract <em>useful</em> information, not just any information.</p>
</section>
<section id="core-concepts-machine-learning" class="level2">
<h2 class="anchored" data-anchor-id="core-concepts-machine-learning">Core Concepts: Machine Learning</h2>
<p>Next, we have Machine Learning.</p>
<p>Machine learning is a subfield of artificial intelligence (AI). It focuses on creating algorithms that allow computers to <em>learn from data</em> without being explicitly programmed. This means we don‚Äôt give the computer a set of rules; instead, we give it data and it figures out the rules itself!</p>
<p>Key aspects:</p>
<ul>
<li><strong>Learning from Data:</strong> The core principle. The algorithm improves its performance as it is exposed to more data.</li>
<li><strong>Building Models:</strong> These models are mathematical representations of the patterns in the data. They can be used to make predictions or decisions about new, unseen data.</li>
<li><strong>No Explicit Programming:</strong> The algorithm learns the patterns and relationships within the data without being explicitly told what those patterns are.</li>
</ul>
</section>
<section id="core-concepts-statistical-learning" class="level2">
<h2 class="anchored" data-anchor-id="core-concepts-statistical-learning">Core Concepts: Statistical Learning</h2>
<p>Finally, let‚Äôs define Statistical Learning.</p>
<p>Statistical learning refers to a set of tools for modeling and understanding complex datasets. It‚Äôs a relatively new field within statistics that intersects with advances in computer science, particularly machine learning. It emphasizes both building predictive models and gaining insights into the underlying data-generating process.</p>
<p>Key aspects:</p>
<ul>
<li><strong>Modeling and Understanding:</strong> Focuses on both building accurate models and gaining insights into the relationships between variables.</li>
<li><strong>Conceptual and Practical:</strong> Provides both theoretical understanding of the methods and practical tools for applying them.</li>
<li><strong>Interdisciplinary:</strong> Combines statistical theory and computational techniques from computer science.</li>
</ul>
</section>
<section id="relationships-of-concepts" class="level2">
<h2 class="anchored" data-anchor-id="relationships-of-concepts">Relationships of Concepts</h2>
<p>The following diagram illustrates how these concepts relate to one another:</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
    A[Data Mining] --&gt; C(Common Ground)
    B[Machine Learning] --&gt; C
    D[Statistical Learning] --&gt; C
    C --&gt; E[Insights &amp; Predictions]
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Data mining, machine learning, and statistical learning all overlap. They are all about extracting insights and making predictions from data, but with different emphases and approaches. Think of them as different tools in the same toolbox üß∞, each with its own strengths.</p>
</div>
</div>
</section>
<section id="decision-trees-the-basics" class="level2">
<h2 class="anchored" data-anchor-id="decision-trees-the-basics">Decision Trees: The Basics üå≤</h2>
<p>Decision trees are a fundamental tree-based method. They work by dividing the predictor space (the space of all possible input values) using a series of splitting rules. These rules are organized in a hierarchical tree structure, making them easy to visualize and understand. They recursively partition the data based on the values of the predictor variables.</p>
<p>Key Features:</p>
<ul>
<li><strong>Simple Interpretation:</strong> Easy to understand and explain, even to non-experts. The decision-making process is transparent.</li>
<li><strong>Non-linear Relationships:</strong> Can capture complex, non-linear patterns in the data. This is a major advantage over linear models, which assume a straight-line relationship.</li>
<li><strong>Regression &amp; Classification:</strong> Can be used for both predicting continuous values (regression) and categories (classification).</li>
</ul>
</section>
<section id="regression-trees-a-simple-example" class="level2">
<h2 class="anchored" data-anchor-id="regression-trees-a-simple-example">Regression Trees: A Simple Example</h2>
<p>Let‚Äôs illustrate with a regression tree example. We‚Äôll use the <code>Hitters</code> dataset (commonly used in statistical learning examples) to predict a baseball player‚Äôs salary. This dataset contains information about Major League Baseball players.</p>
<ul>
<li><strong>Predictors:</strong> These are the input variables we‚Äôll use to make predictions.
<ul>
<li><code>Years</code>: Number of years the player has played in the major leagues.</li>
<li><code>Hits</code>: Number of hits the player made in the previous year.</li>
</ul></li>
<li><strong>Response:</strong> This is the variable we want to predict.
<ul>
<li><code>Log Salary</code>: We use the <em>logarithm</em> of salary to make the distribution more bell-shaped (normal), which often improves model performance. This transformation helps to stabilize the variance and make the data more suitable for modeling.</li>
</ul></li>
</ul>
</section>
<section id="regression-tree-for-hitters-data" class="level2">
<h2 class="anchored" data-anchor-id="regression-tree-for-hitters-data">Regression Tree for Hitters Data</h2>
<p>This image shows a regression tree built from the Hitters data:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F8_1.svg" class="img-fluid figure-img"></p>
<figcaption>Regression tree for Hitters data</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This tree predicts the <em>logarithm</em> of a player‚Äôs salary based on their <code>Years</code> of experience and <code>Hits</code> in the previous year. The numbers at the bottom (in the ‚Äúleaves‚Äù) are the <em>average</em> log(Salary) for players who fall into that specific category. These values represent the predicted log salary for players within each region.</p>
</div>
</div>
</section>
<section id="interpreting-the-regression-tree-top-split" class="level2">
<h2 class="anchored" data-anchor-id="interpreting-the-regression-tree-top-split">Interpreting the Regression Tree: Top Split</h2>
<p>Let‚Äôs break down how to interpret this tree. The <em>most important</em> split is at the very top:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F8_1.svg" class="img-fluid figure-img"></p>
<figcaption>Regression tree for Hitters data</figcaption>
</figure>
</div>
<ul>
<li><strong>Top Split:</strong> <code>Years &lt; 4.5</code>
<ul>
<li>This tells us that the number of years a player has been in the major leagues is the single most important factor in determining their salary (or rather, the log of their salary). Less experience generally means a lower salary. This variable explains the most variance in the response variable.</li>
</ul></li>
</ul>
</section>
<section id="interpreting-the-regression-tree-internal-nodes-branches" class="level2">
<h2 class="anchored" data-anchor-id="interpreting-the-regression-tree-internal-nodes-branches">Interpreting the Regression Tree: Internal Nodes &amp; Branches</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F8_1.svg" class="img-fluid figure-img"></p>
<figcaption>Regression tree for Hitters data</figcaption>
</figure>
</div>
<ul>
<li><strong>Internal Nodes:</strong> These represent further splits in the predictor space. For example, the node <code>Hits &lt; 117.5</code> divides players with less than 4.5 years of experience based on their number of hits. Each internal node represents a decision point based on a predictor variable.</li>
<li><strong>Branches:</strong> The lines connecting the nodes. They represent the ‚Äúyes‚Äù or ‚Äúno‚Äù answers to the splitting rule questions. Each branch corresponds to a specific outcome of the splitting rule.</li>
</ul>
</section>
<section id="interpreting-the-regression-tree-terminal-nodes-leaves" class="level2">
<h2 class="anchored" data-anchor-id="interpreting-the-regression-tree-terminal-nodes-leaves">Interpreting the Regression Tree: Terminal Nodes (Leaves)</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F8_1.svg" class="img-fluid figure-img"></p>
<figcaption>Regression tree for Hitters data</figcaption>
</figure>
</div>
<ul>
<li><p><strong>Terminal Nodes (Leaves):</strong> These are the final boxes at the bottom of the tree. They represent the predicted values ‚Äì in this case, the <em>mean</em> log(Salary) for players who fall into that specific region of the predictor space. These are the final prediction points of the tree.</p></li>
<li><p><strong>Example:</strong> The left branch means If a player played less than 4.5 years, his predicted log salary is 5.11.</p></li>
</ul>
</section>
<section id="regions-of-predictor-space" class="level2">
<h2 class="anchored" data-anchor-id="regions-of-predictor-space">Regions of Predictor Space</h2>
<p>The decision tree divides the predictor space (the combination of <code>Years</code> and <code>Hits</code>) into rectangular regions. This image shows how the tree‚Äôs splits correspond to these regions:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F8_2.svg" class="img-fluid figure-img"></p>
<figcaption>Three-region partition</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The tree divides the space into three rectangular regions (R1, R2, R3). Each region corresponds to a leaf (terminal node) in the decision tree. Each rectangle represents a group of players with similar predicted salaries.</p>
</div>
</div>
</section>
<section id="regions-of-predictor-space-region-details" class="level2">
<h2 class="anchored" data-anchor-id="regions-of-predictor-space-region-details">Regions of Predictor Space: Region Details</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F8_2.svg" class="img-fluid figure-img"></p>
<figcaption>Three-region partition</figcaption>
</figure>
</div>
<ul>
<li><strong>R1:</strong> Players with <code>Years &lt; 4.5</code>. This region corresponds to the leftmost leaf in the tree.</li>
<li><strong>R2:</strong> Players with <code>Years &gt;= 4.5</code> and <code>Hits &lt; 117.5</code>. This region corresponds to the middle leaf.</li>
<li><strong>R3:</strong> Players with <code>Years &gt;= 4.5</code> and <code>Hits &gt;= 117.5</code>. This region corresponds to the rightmost leaf.</li>
</ul>
</section>
<section id="building-a-regression-tree-key-steps" class="level2">
<h2 class="anchored" data-anchor-id="building-a-regression-tree-key-steps">Building a Regression Tree: Key Steps</h2>
<p>Here‚Äôs a summary of how to build a regression tree:</p>
<ol type="1">
<li><p><strong>Divide Predictor Space:</strong> Split the predictor space into <em>J</em> distinct, non-overlapping regions (R1, R2, ‚Ä¶, RJ). These regions should not overlap, and each observation should belong to exactly one region. The goal is to find regions that minimize the prediction error.</p></li>
<li><p><strong>Prediction:</strong> For any observation that falls into region Rj, we predict the response value to be the <em>mean</em> of the response values of all the <em>training observations</em> that also fall into Rj. This is the average of the response values for all observations within that region.</p></li>
</ol>
</section>
<section id="constructing-the-regions-minimizing-rss" class="level2">
<h2 class="anchored" data-anchor-id="constructing-the-regions-minimizing-rss">Constructing the Regions: Minimizing RSS</h2>
<p>The key question is: <em>how</em> do we decide where to make the splits and create these regions? The goal is to find the regions that minimize the Residual Sum of Squares (RSS). RSS is a measure of the overall prediction error of the tree.</p>
<p>The RSS formula is:</p>
<p><span class="math display">\[
\text{RSS} = \sum_{j=1}^{J} \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2
\]</span></p>
<p>Where:</p>
<ul>
<li><strong><span class="math inline">\(\hat{y}_{R_j}\)</span>:</strong> The mean response for the training observations within region <em>Rj</em>. This is the predicted value for all observations in region Rj.</li>
<li><strong><span class="math inline">\(y_i\)</span>:</strong> The actual response value for the <em>i</em>th observation.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Finding the <em>absolute best</em> set of regions to minimize RSS is computationally very expensive (infeasible in most cases). It would require checking every possible partition of the feature space. Therefore, we use a <em>top-down, greedy</em> approach called <strong>recursive binary splitting</strong>.</p>
</div>
</div>
</section>
<section id="recursive-binary-splitting-overview" class="level2">
<h2 class="anchored" data-anchor-id="recursive-binary-splitting-overview">Recursive Binary Splitting: Overview</h2>
<p>Recursive binary splitting is a clever way to build the tree efficiently. It‚Äôs a heuristic algorithm that finds a <em>good</em> solution, though not necessarily the <em>optimal</em> solution. It has these key characteristics:</p>
<ul>
<li><strong>Top-Down:</strong> We start with all observations in a single region (the entire predictor space) and successively split the predictor space.</li>
<li><strong>Greedy:</strong> At each step, we make the <em>best</em> split <em>at that particular moment</em>. We don‚Äôt look ahead to see if a different split might be better later on. This means we choose the split that minimizes the RSS <em>at the current step</em>.</li>
<li><strong>Binary:</strong> Each split divides a region into exactly <em>two</em> sub-regions. This creates a binary tree structure.</li>
</ul>
</section>
<section id="recursive-binary-splitting-the-process" class="level2">
<h2 class="anchored" data-anchor-id="recursive-binary-splitting-the-process">Recursive Binary Splitting: The Process</h2>
<p>Here‚Äôs how recursive binary splitting works step-by-step:</p>
<ol type="1">
<li><strong>Select Predictor and Cutpoint:</strong>
<ul>
<li>We consider <em>all</em> predictors (X1, X2, ‚Ä¶, Xp) and <em>all</em> possible cutpoints (values of the predictor) for each predictor.</li>
<li>We choose the predictor (Xj) and the cutpoint (s) that lead to the <em>greatest possible reduction</em> in RSS when we split the region into two new regions:
<ul>
<li>R1(j, s) = {X | Xj &lt; s} (all observations where Xj is less than s)</li>
<li>R2(j, s) = {X | Xj ‚â• s} (all observations where Xj is greater than or equal to s)</li>
</ul></li>
</ul></li>
</ol>
</section>
<section id="recursive-binary-splitting-minimization" class="level2">
<h2 class="anchored" data-anchor-id="recursive-binary-splitting-minimization">Recursive Binary Splitting: Minimization</h2>
<ol start="2" type="1">
<li><p><strong>Minimize:</strong> We find the specific values of <em>j</em> (the predictor index) and <em>s</em> (the cutpoint) that minimize the following expression:</p>
<p><span class="math display">\[
\sum_{i: x_i \in R_1(j,s)} (y_i - \hat{y}_{R_1})^2 + \sum_{i: x_i \in R_2(j,s)} (y_i - \hat{y}_{R_2})^2
\]</span></p>
<p>This is just the sum of the RSS for the two new regions. The goal is to find the predictor and cutpoint that result in the lowest combined RSS for the two resulting regions.</p></li>
</ol>
</section>
<section id="recursive-binary-splitting-repetition" class="level2">
<h2 class="anchored" data-anchor-id="recursive-binary-splitting-repetition">Recursive Binary Splitting: Repetition</h2>
<ol start="3" type="1">
<li><p><strong>Repeat:</strong> We take one of the newly created regions and repeat steps 1 and 2. We continue this process, splitting regions into smaller and smaller sub-regions, until a stopping criterion is met.</p>
<ul>
<li>A common stopping criterion is to stop splitting when a region contains fewer than a certain number of observations (e.g., 5 observations). Other stopping criteria could include reaching a maximum tree depth or a minimum improvement in RSS.</li>
</ul></li>
</ol>
</section>
<section id="visualizing-recursive-binary-splitting" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-recursive-binary-splitting">Visualizing Recursive Binary Splitting</h2>
<p>This figure illustrates the process of recursive binary splitting:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F8_3.svg" class="img-fluid figure-img"></p>
<figcaption>Recursive binary splitting</figcaption>
</figure>
</div>
<ul>
<li><strong>Top Left:</strong> The first split is made, dividing the space horizontally based on the value of X1.</li>
</ul>
</section>
<section id="visualizing-recursive-binary-splitting-1" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-recursive-binary-splitting-1">Visualizing Recursive Binary Splitting</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F8_3.svg" class="img-fluid figure-img"></p>
<figcaption>Recursive binary splitting</figcaption>
</figure>
</div>
<ul>
<li><strong>Top Middle:</strong> The next two splits are made, one in each of the previously created regions.</li>
</ul>
</section>
<section id="visualizing-recursive-binary-splitting-2" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-recursive-binary-splitting-2">Visualizing Recursive Binary Splitting</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F8_3.svg" class="img-fluid figure-img"></p>
<figcaption>Recursive binary splitting</figcaption>
</figure>
</div>
<ul>
<li><strong>Top Right:</strong> Shows the final result of recursive binary splitting in two dimensions (like our Hitters example). The lines represent the splits. The predictor space is fully partitioned into rectangles.</li>
</ul>
</section>
<section id="visualizing-recursive-binary-splitting-3" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-recursive-binary-splitting-3">Visualizing Recursive Binary Splitting</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F8_3.svg" class="img-fluid figure-img"></p>
<figcaption>Recursive binary splitting</figcaption>
</figure>
</div>
<ul>
<li><strong>Bottom Left:</strong> Shows the corresponding decision tree. Each split in the top right panel corresponds to an internal node in the tree.</li>
</ul>
</section>
<section id="visualizing-recursive-binary-splitting-4" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-recursive-binary-splitting-4">Visualizing Recursive Binary Splitting</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F8_3.svg" class="img-fluid figure-img"></p>
<figcaption>Recursive binary splitting</figcaption>
</figure>
</div>
<ul>
<li><strong>Bottom Right:</strong> Presents a perspective plot of the prediction surface. This gives you a 3D view of how the predicted value (e.g., log salary) changes across the predictor space. The predicted value is constant within each rectangle.</li>
</ul>
</section>
<section id="tree-pruning" class="level2">
<h2 class="anchored" data-anchor-id="tree-pruning">Tree Pruning ‚úÇÔ∏è</h2>
<p>The recursive binary splitting process, if allowed to continue unchecked, will often lead to a very large and complex tree. This tree will likely <em>overfit</em> the training data. Overfitting means the tree is too closely tailored to the training data (including noise) and won‚Äôt generalize well to new, unseen data. It will have low bias but high variance.</p>
<p>A smaller tree with fewer splits can have:</p>
<ul>
<li><strong>Lower Variance:</strong> Less sensitive to the specific details of the training data. It will be more stable and generalize better.</li>
<li><strong>Better Interpretability:</strong> Easier to understand and explain. A simpler model is often preferred for its clarity.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Cost complexity pruning (also known as weakest link pruning)</strong> is a technique to find the best <em>subtree</em> of the original, large tree. It helps us find a simpler tree that balances accuracy and complexity. It balances the trade-off between model fit and model size.</p>
</div>
</div>
</section>
<section id="cost-complexity-pruning" class="level2">
<h2 class="anchored" data-anchor-id="cost-complexity-pruning">Cost Complexity Pruning</h2>
<ul>
<li><p><strong>Goal:</strong> Find a subtree <em>T</em> (which is a smaller version of the full tree, <em>T0</em>) that minimizes a <em>penalized</em> version of the RSS:</p>
<p><span class="math display">\[
\sum_{m=1}^{|T|} \sum_{x_i \in R_m} (y_i - \hat{y}_{R_m})^2 + \alpha|T|
\]</span></p></li>
<li><p><strong>|T|:</strong> The number of terminal nodes (leaves) in the subtree <em>T</em>. This is a measure of the tree‚Äôs complexity.</p></li>
<li><p><strong>Œ±:</strong> A <em>tuning parameter</em> that controls the trade-off between the subtree‚Äôs complexity (number of leaves) and how well it fits the training data (RSS). A larger Œ± leads to a larger penalty for complexity.</p></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>As we increase the value of Œ±, the penalty for having many terminal nodes increases. This leads to smaller and smaller subtrees. The tuning parameter Œ± controls the balance between a complex, well-fitting tree and a simple, less-well-fitting tree.</p>
</div>
</div>
</section>
<section id="algorithm-8.1-building-a-regression-tree" class="level2">
<h2 class="anchored" data-anchor-id="algorithm-8.1-building-a-regression-tree">Algorithm 8.1: Building a Regression Tree</h2>
<p>Here‚Äôs the complete algorithm for building a regression tree, including pruning:</p>
<ol type="1">
<li><p><strong>Grow a Large Tree:</strong> Use recursive binary splitting to grow a large initial tree (<em>T0</em>) on the training data. Stop splitting when each terminal node has fewer than some pre-specified minimum number of observations.</p></li>
<li><p><strong>Cost Complexity Pruning:</strong> Apply cost complexity pruning to the large tree (<em>T0</em>) to obtain a sequence of best subtrees, as a function of the tuning parameter <em>Œ±</em>. For each value of Œ±, there is a corresponding subtree that minimizes the penalized RSS.</p></li>
<li><p><strong>Choose Œ±:</strong> Use K-fold cross-validation to choose the optimal value of <em>Œ±</em>.</p>
<ul>
<li>Divide the training data into K folds (typically K=5 or K=10).</li>
<li>For each fold, repeat steps 1 and 2 using the other K-1 folds as the training data.</li>
<li>Evaluate the mean squared prediction error on the held-out fold (the fold that wasn‚Äôt used for training).</li>
<li>Average the results over all K folds, and pick the value of <em>Œ±</em> that minimizes the average error.</li>
</ul></li>
<li><p><strong>Return Subtree:</strong> Return the subtree from Step 2 that corresponds to the chosen <em>Œ±</em>. This is your final, pruned tree. This subtree gives the best balance between fit and complexity, as determined by cross-validation.</p></li>
</ol>
</section>
<section id="classification-trees" class="level2">
<h2 class="anchored" data-anchor-id="classification-trees">Classification Trees</h2>
<p>Classification trees are very similar to regression trees, but they are used to predict a <em>qualitative</em> response (a category or class label) rather than a continuous value.</p>
<ul>
<li><strong>Prediction:</strong> Instead of predicting the mean response value in a region, we predict the <em>most commonly occurring class</em> in that region. This is the class with the highest proportion of training observations in that region.</li>
<li><strong>Interpretation:</strong> We also consider the <em>class proportions</em> within each region (the percentage of observations belonging to each class). This gives us an idea of how ‚Äúpure‚Äù each region is.</li>
</ul>
</section>
<section id="growing-a-classification-tree" class="level2">
<h2 class="anchored" data-anchor-id="growing-a-classification-tree">Growing a Classification Tree</h2>
<ul>
<li>We still use recursive binary splitting, but we <em>cannot</em> use RSS as the splitting criterion because we‚Äôre dealing with classes, not numerical values. RSS is suitable for regression, not classification.</li>
<li><strong>Alternatives to RSS:</strong> We need splitting criteria that measure the <em>impurity</em> of a node (how mixed the classes are within the node).
<ul>
<li>Classification error rate</li>
<li>Gini index</li>
<li>Entropy</li>
</ul></li>
</ul>
</section>
<section id="splitting-criteria-classification-error-rate" class="level2">
<h2 class="anchored" data-anchor-id="splitting-criteria-classification-error-rate">Splitting Criteria: Classification Error Rate</h2>
<p>The classification error rate is the simplest option:</p>
<p><span class="math display">\[
E = 1 - \max_k (\hat{p}_{mk})
\]</span></p>
<ul>
<li><strong><span class="math inline">\(\hat{p}_{mk}\)</span>:</strong> The proportion of training observations in the <em>m</em>th region that belong to the <em>k</em>th class. This is the fraction of training observations in region <em>m</em> that are from class <em>k</em>.</li>
<li><strong>Problem:</strong> The classification error rate is not sensitive enough for tree growing. It doesn‚Äôt always lead to the best splits. It is possible for different splits to result in the same classification error rate, even if one split produces more homogenous regions.</li>
</ul>
</section>
<section id="splitting-criteria-gini-index" class="level2">
<h2 class="anchored" data-anchor-id="splitting-criteria-gini-index">Splitting Criteria: Gini Index</h2>
<p>The Gini index is a more sensitive measure of node purity:</p>
<p><span class="math display">\[
G = \sum_{k=1}^{K} \hat{p}_{mk}(1 - \hat{p}_{mk})
\]</span></p>
<ul>
<li>It measures the total variance across all <em>K</em> classes.</li>
<li>A small Gini index indicates that the node is <em>pure</em> ‚Äì most of the observations in that node belong to the same class. (If all <span class="math inline">\(\hat{p}_{mk}\)</span> are close to 0 or 1, the Gini index will be small). A Gini index of 0 indicates perfect purity.</li>
</ul>
</section>
<section id="splitting-criteria-entropy" class="level2">
<h2 class="anchored" data-anchor-id="splitting-criteria-entropy">Splitting Criteria: Entropy</h2>
<p>Entropy is another measure of node impurity:</p>
<p><span class="math display">\[
D = -\sum_{k=1}^{K} \hat{p}_{mk} \log \hat{p}_{mk}
\]</span></p>
<ul>
<li>Entropy will also take on a value near zero if the node is pure (if the <span class="math inline">\(\hat{p}_{mk}\)</span> values are close to 0 or 1). An entropy of 0 indicates perfect purity. We use the convention that 0 log 0 = 0.</li>
<li>The Gini index and entropy are numerically very similar. Both are more sensitive to changes in node purity than the classification error rate.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Gini index and entropy are preferred over the classification error rate for <em>growing</em> the tree because they are more sensitive to node purity. For <em>pruning</em> the tree, any of the three criteria can be used, but the classification error rate is often preferred because it directly relates to the final prediction accuracy.</p>
</div>
</div>
</section>
<section id="example-heart-data" class="level2">
<h2 class="anchored" data-anchor-id="example-heart-data">Example: Heart Data</h2>
<p>This figure shows an unpruned classification tree built on the Heart dataset:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F8_6.svg" class="img-fluid figure-img"></p>
<figcaption>Heart data unpruned tree</figcaption>
</figure>
</div>
<ul>
<li><strong>Goal:</strong> Predict whether a patient has heart disease (<code>Yes</code> or <code>No</code>) based on 13 predictors.</li>
</ul>
</section>
<section id="example-heart-data---predictors" class="level2">
<h2 class="anchored" data-anchor-id="example-heart-data---predictors">Example: Heart Data - Predictors</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F8_6.svg" class="img-fluid figure-img"></p>
<figcaption>Heart data unpruned tree</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Notice that qualitative predictors (like <code>ChestPain</code>) can be handled directly by decision trees, without needing to create dummy variables. The tree automatically handles the different categories of qualitative predictors. For example, the <code>ChestPain</code> variable has four levels: ‚Äútypical angina,‚Äù ‚Äúatypical angina,‚Äù ‚Äúnon-anginal pain,‚Äù and ‚Äúasymptomatic.‚Äù</p>
</div>
</div>
</section>
<section id="trees-vs.-linear-models" class="level2">
<h2 class="anchored" data-anchor-id="trees-vs.-linear-models">Trees vs.&nbsp;Linear Models</h2>
<ul>
<li><strong>Linear Regression:</strong> Assumes a linear relationship between the predictors and the response: <span class="math inline">\(f(X) = \beta_0 + \sum_{j=1}^{p} X_j \beta_j\)</span>. The model assumes a constant change in the response for a unit change in each predictor.</li>
<li><strong>Regression Trees:</strong> Assume a piecewise constant model: <span class="math inline">\(f(X) = \sum_{m=1}^{M} c_m \cdot 1(X \in R_m)\)</span>. The predicted value is constant within each region.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The best model depends on the <em>true underlying relationship</em> between the predictors and the response. If the relationship is close to linear, linear regression will likely outperform a decision tree. If the relationship is highly non-linear and complex, decision trees may be better.</p>
</div>
</div>
</section>
<section id="trees-vs.-linear-models---visual-comparison" class="level2">
<h2 class="anchored" data-anchor-id="trees-vs.-linear-models---visual-comparison">Trees vs.&nbsp;Linear Models - Visual Comparison</h2>
<p>This figure compares decision boundaries of linear models and trees:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F8_7.svg" class="img-fluid figure-img"></p>
<figcaption>Trees vs.&nbsp;Linear Models</figcaption>
</figure>
</div>
<ul>
<li><strong>Top Row:</strong> The true decision boundary is linear.</li>
</ul>
</section>
<section id="trees-vs.-linear-models---visual-comparison-linear-boundary" class="level2">
<h2 class="anchored" data-anchor-id="trees-vs.-linear-models---visual-comparison-linear-boundary">Trees vs.&nbsp;Linear Models - Visual Comparison (Linear Boundary)</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F8_7.svg" class="img-fluid figure-img"></p>
<figcaption>Trees vs.&nbsp;Linear Models</figcaption>
</figure>
</div>
<ul>
<li><strong>Top Left:</strong> The linear model (left) fits the data well, as expected. The decision boundary is a straight line.</li>
<li><strong>Top Right:</strong> The decision tree (right) attempts to approximate the linear boundary with a series of rectangular regions, resulting in a less accurate fit.</li>
</ul>
</section>
<section id="trees-vs.-linear-models---visual-comparison-non-linear-boundary" class="level2">
<h2 class="anchored" data-anchor-id="trees-vs.-linear-models---visual-comparison-non-linear-boundary">Trees vs.&nbsp;Linear Models - Visual Comparison (Non-linear Boundary)</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F8_7.svg" class="img-fluid figure-img"></p>
<figcaption>Trees vs.&nbsp;Linear Models</figcaption>
</figure>
</div>
<ul>
<li><strong>Bottom Row:</strong> The true decision boundary is non-linear.</li>
</ul>
</section>
<section id="trees-vs.-linear-models---visual-comparison-non-linear-boundary-1" class="level2">
<h2 class="anchored" data-anchor-id="trees-vs.-linear-models---visual-comparison-non-linear-boundary-1">Trees vs.&nbsp;Linear Models - Visual Comparison (Non-linear Boundary)</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://axwslyfy9krb.objectstorage.ap-singapore-1.oci.customer-oci.com/n/axwslyfy9krb/b/qiufei/o/textbook%2Fisl_figures%2F8_7.svg" class="img-fluid figure-img"></p>
<figcaption>Trees vs.&nbsp;Linear Models</figcaption>
</figure>
</div>
<ul>
<li><strong>Bottom Left:</strong> The linear model (left) is unable to capture the non-linear relationship, resulting in a poor fit.</li>
<li><strong>Bottom Right:</strong> The decision tree (right) captures the non-linearity much better than the linear model (left) by partitioning the space into appropriate regions.</li>
</ul>
</section>
<section id="advantages-of-trees" class="level2">
<h2 class="anchored" data-anchor-id="advantages-of-trees">Advantages of Trees üëç</h2>
<ul>
<li><strong>Easy to Explain:</strong> Decision trees are much simpler to explain than even linear regression. They are intuitive and can be easily understood by non-experts.</li>
<li><strong>Human Decision-Making:</strong> Some people believe that decision trees more closely mirror human decision-making processes. They resemble a series of ‚Äúif-then‚Äù rules.</li>
<li><strong>Graphical Representation:</strong> They can be easily displayed graphically, making them easy to interpret. Visualizations aid in understanding the model‚Äôs logic.</li>
<li><strong>Qualitative Predictors:</strong> They can handle qualitative predictors directly, without the need to create dummy variables (one-hot encoding). This simplifies the data preparation process.</li>
</ul>
</section>
<section id="disadvantages-of-trees" class="level2">
<h2 class="anchored" data-anchor-id="disadvantages-of-trees">Disadvantages of Trees üëé</h2>
<ul>
<li><strong>Lower Predictive Accuracy:</strong> Single decision trees generally don‚Äôt have the same level of predictive accuracy as some other machine learning methods, like support vector machines or neural networks.</li>
<li><strong>Non-Robust:</strong> Small changes in the data can cause large changes in the final estimated tree. This means they can be unstable and sensitive to noise in the data.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>Aggregating</em> multiple decision trees (using techniques like bagging, random forests, and boosting) can significantly improve predictive performance and robustness. This is the idea behind ensemble methods, which combine the predictions of multiple models to create a more accurate and stable model.</p>
</div>
</div>
</section>
<section id="ensemble-methods-combining-multiple-models" class="level2">
<h2 class="anchored" data-anchor-id="ensemble-methods-combining-multiple-models">Ensemble Methods: Combining Multiple Models</h2>
<p>An <strong>ensemble method</strong> combines multiple ‚Äúweak learner‚Äù models (like decision trees) to create a more powerful model. Think of it like ‚Äúwisdom of the crowds‚Äù Crowd wisdom ‚Äì the combined judgment of a group is often better than the judgment of any individual member.</p>
<ul>
<li><strong>Weak Learner:</strong> A simple model that makes only mediocre predictions (slightly better than random guessing). A single decision tree is often a weak learner.</li>
<li><strong>Ensemble:</strong> A combination of many weak learners, which together form a strong learner. The ensemble leverages the diversity of the individual models to improve overall performance.</li>
</ul>
</section>
<section id="bagging-bootstrap-aggregation" class="level2">
<h2 class="anchored" data-anchor-id="bagging-bootstrap-aggregation">Bagging (Bootstrap Aggregation)</h2>
<ul>
<li><strong>Goal:</strong> Reduce the variance of a statistical learning method. This is especially useful for decision trees, which tend to have high variance (they are sensitive to the specific training data).</li>
<li><strong>Idea:</strong> Take many (hundreds or thousands) of ‚Äúbootstrapped‚Äù samples from the training data, build a separate decision tree on each bootstrapped sample, and then <em>average</em> the predictions of all these trees. This averaging process reduces the variance.</li>
</ul>
</section>
<section id="bagging-the-process" class="level2">
<h2 class="anchored" data-anchor-id="bagging-the-process">Bagging: The Process</h2>
<ol type="1">
<li><p><strong>Bootstrap:</strong> Generate <em>B</em> different bootstrapped training datasets. Bootstrapping means taking repeated random samples <em>with replacement</em> from the original training data. Each bootstrapped dataset has the same size as the original dataset, but some observations will be repeated, and others will be left out.</p></li>
<li><p><strong>Train:</strong> Train a decision tree on each of the <em>B</em> bootstrapped datasets. Typically, these trees are grown <em>deep</em> (large trees) and are <em>not pruned</em>. Growing deep trees allows them to capture complex patterns, and averaging reduces the risk of overfitting.</p></li>
<li><p><strong>Average:</strong> For a given test observation:</p>
<ul>
<li><strong>Regression:</strong> Average the predictions from all <em>B</em> trees.</li>
<li><strong>Classification:</strong> Take a majority vote (the class predicted by the most trees is the final prediction).</li>
</ul></li>
</ol>
</section>
<section id="bagging-out-of-bag-oob-error" class="level2">
<h2 class="anchored" data-anchor-id="bagging-out-of-bag-oob-error">Bagging: Out-of-Bag (OOB) Error</h2>
<ul>
<li><p><strong>OOB Observations:</strong> For each tree, the observations that were <em>not</em> included in the bootstrapped sample are called the ‚Äúout-of-bag‚Äù (OOB) observations. On average, about one-third of the observations are OOB for each tree.</p></li>
<li><p><strong>OOB Prediction:</strong> We can predict the response for each observation using only the trees in which that observation was OOB. This provides a way to estimate the prediction error without using a separate validation set.</p></li>
<li><p><strong>OOB Error:</strong> The OOB error is a valid estimate of the test error of the bagged model. It‚Äôs a very convenient way to assess performance, as it doesn‚Äôt require setting aside a separate validation set.</p></li>
</ul>
</section>
<section id="bagging-variable-importance" class="level2">
<h2 class="anchored" data-anchor-id="bagging-variable-importance">Bagging: Variable Importance</h2>
<ul>
<li><strong>Interpretability Loss:</strong> Bagging improves prediction accuracy, but it sacrifices some interpretability because we no longer have a single tree to examine. The combined model is more of a ‚Äúblack box.‚Äù</li>
<li><strong>Variable Importance Measures:</strong> We can still get an overall summary of the importance of each predictor.
<ul>
<li><strong>Regression:</strong> For each predictor, record the total amount that the RSS decreases due to splits on that predictor, averaged over all <em>B</em> trees. A larger average decrease indicates a more important predictor.</li>
<li><strong>Classification:</strong> For each predictor, record the total amount that the Gini index decreases due to splits on that predictor, averaged over all <em>B</em> trees. A larger average decrease indicates a more important predictor.</li>
</ul></li>
</ul>
</section>
<section id="random-forests" class="level2">
<h2 class="anchored" data-anchor-id="random-forests">Random Forests</h2>
<ul>
<li><strong>Improvement over Bagging:</strong> Random forests provide an improvement over bagged trees by introducing a small ‚Äútweak‚Äù that <em>decorrelates</em> the trees. This further reduces variance and improves prediction accuracy.</li>
<li><strong>Random Subset of Predictors:</strong> At each split in the tree-growing process, a <em>random sample</em> of <em>m</em> predictors is chosen as split candidates from the full set of <em>p</em> predictors. Typically, <span class="math inline">\(m \approx \sqrt{p}\)</span>. This means that at each split, some predictors aren‚Äôt even considered! This introduces further randomness and diversity among the trees.</li>
</ul>
</section>
<section id="random-forests-rationale" class="level2">
<h2 class="anchored" data-anchor-id="random-forests-rationale">Random Forests: Rationale</h2>
<ul>
<li><strong>Strong Predictor Problem:</strong> In bagging, if there‚Äôs one very strong predictor, most of the trees will use that predictor in the top split. This makes the trees very similar to each other (highly correlated). The predictions from highly correlated trees will be similar, and averaging them won‚Äôt reduce the variance as much as averaging predictions from uncorrelated trees.</li>
<li><strong>Decorrelation:</strong> By limiting the predictors considered at each split, random forests give other predictors a chance to be chosen. This leads to less correlated trees, and when we average the predictions of less correlated trees, we get lower variance and better overall performance.</li>
</ul>
</section>
<section id="boosting" class="level2">
<h2 class="anchored" data-anchor-id="boosting">Boosting</h2>
<ul>
<li><strong>Sequential Tree Growth:</strong> Unlike bagging and random forests, where trees are grown independently, boosting grows trees <em>sequentially</em>. Each tree is grown using information from previously grown trees. This allows boosting to focus on observations that were poorly predicted by previous trees.</li>
<li><strong>Slow Learning:</strong> Boosting ‚Äúlearns slowly‚Äù by fitting small trees to the <em>residuals</em> (the differences between the observed values and the current prediction). It gradually improves the model by focusing on the errors made by previous trees.</li>
<li><strong>No Bootstrapping:</strong> Boosting doesn‚Äôt use bootstrapped samples. Instead, it uses a modified version of the original data at each step.</li>
</ul>
</section>
<section id="boosting-the-process" class="level2">
<h2 class="anchored" data-anchor-id="boosting-the-process">Boosting: The Process</h2>
<ol type="1">
<li><p><strong>Initialize:</strong> Set the initial prediction <span class="math inline">\(\hat{f}(x)\)</span> to 0 for all observations, and set the residuals <span class="math inline">\(r_i\)</span> equal to the observed response values <span class="math inline">\(y_i\)</span>.</p></li>
<li><p><strong>Iterate (for b = 1 to B):</strong></p>
<ul>
<li>Fit a small decision tree (with <em>d</em> splits) to the <em>residuals</em> (not the original response values!). This tree is denoted as <span class="math inline">\(\hat{f}^b(x)\)</span>.</li>
<li>Update the fitted function by adding a <em>shrunken</em> version of the new tree: <span class="math display">\[\hat{f}(x) \leftarrow \hat{f}(x) + \lambda \hat{f}^b(x)\]</span></li>
<li>Update the residuals: <span class="math display">\[r_i \leftarrow r_i - \lambda \hat{f}^b(x_i)\]</span></li>
</ul></li>
<li><p><strong>Output:</strong> The boosted model is the sum of all the trees: <span class="math display">\[\hat{f}(x) = \sum_{b=1}^{B} \lambda \hat{f}^b(x)\]</span></p></li>
</ol>
</section>
<section id="boosting-tuning-parameters" class="level2">
<h2 class="anchored" data-anchor-id="boosting-tuning-parameters">Boosting: Tuning Parameters</h2>
<ul>
<li><strong>B (Number of Trees):</strong> Boosting <em>can</em> overfit if <em>B</em> is too large (although overfitting tends to occur slowly). Use cross-validation to choose <em>B</em>.</li>
<li><strong>Œª (Shrinkage Parameter):</strong> A small positive number (e.g., 0.01 or 0.001) that controls the learning rate of the boosting process. Smaller values of Œª typically require larger values of B to achieve good performance. A smaller Œª leads to slower, more gradual learning.</li>
<li><strong>d (Number of Splits):</strong> Controls the complexity of each tree. Often <em>d</em> = 1 (decision stumps, trees with just a single split) works well, leading to an <em>additive</em> model. <em>d</em> is also referred as interaction depth.</li>
</ul>
</section>
<section id="bayesian-additive-regression-trees-bart" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-additive-regression-trees-bart">Bayesian Additive Regression Trees (BART)</h2>
<p>BART, similar to other ensemble methods, employs decision trees as its foundational components. However, BART distinguishes itself through several key characteristics:</p>
<ul>
<li><strong>Random Tree Structure:</strong> Like random forests, BART incorporates randomness into the construction of its trees. The structure of each tree is not predetermined.</li>
<li><strong>Sequential Updates:</strong> Similar to boosting, BART refines its model iteratively. The model is updated in a sequential manner, using information from previous iterations.</li>
<li><strong>Tree Perturbation:</strong> Instead of fitting entirely new trees at each step, BART <em>modifies</em> existing trees from previous iterations. This modification is key to BART‚Äôs regularization properties.</li>
</ul>
</section>
<section id="bart-core-idea" class="level2">
<h2 class="anchored" data-anchor-id="bart-core-idea">BART: Core Idea</h2>
<ul>
<li><strong>Initialization:</strong> All trees begin with a single root node, predicting the mean of the response variable.</li>
<li><strong>Iteration:</strong> For each tree, BART <em>randomly perturbs</em> the tree from the previous iteration. This perturbation involves:
<ul>
<li>Changing the tree structure (adding or pruning branches, changing splitting rules).</li>
<li>Changing the predictions in the terminal nodes.</li>
</ul></li>
<li><strong>Output:</strong> A collection of prediction models (one for each iteration). The final prediction is typically the average of the predictions from all trees after a ‚Äúburn-in‚Äù period (discarding the initial iterations). This averaging helps to stabilize the predictions.</li>
</ul>
</section>
<section id="bart-key-features" class="level2">
<h2 class="anchored" data-anchor-id="bart-key-features">BART: Key Features</h2>
<ul>
<li><strong>Guards Against Overfitting:</strong> Perturbing trees instead of fitting entirely new ones limits how aggressively the model can fit the training data. This helps to prevent overfitting. The random perturbations act as a form of regularization.</li>
<li><strong>Small Trees:</strong> Individual trees in BART are usually kept small. This further contributes to regularization.</li>
<li><strong>Bayesian Interpretation:</strong> BART can be viewed as a Bayesian approach, where the tree perturbations represent draws from a posterior distribution. This provides a natural way to quantify uncertainty in the predictions (e.g., by constructing credible intervals).</li>
</ul>
</section>
<section id="summary-of-tree-ensemble-methods" class="level2">
<h2 class="anchored" data-anchor-id="summary-of-tree-ensemble-methods">Summary of Tree Ensemble Methods</h2>
<p>This table summarizes the key characteristics of the ensemble methods we‚Äôve discussed:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 14%">
<col style="width: 61%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Tree Growth</th>
<th style="text-align: left;">Data Sampling</th>
<th style="text-align: left;">Key Idea</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Bagging</td>
<td style="text-align: left;">Independent</td>
<td style="text-align: left;">Bootstrapped</td>
<td style="text-align: left;">Average many trees to reduce variance.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Random Forests</td>
<td style="text-align: left;">Independent</td>
<td style="text-align: left;">Bootstrapped +</td>
<td style="text-align: left;">Decorrelate trees by limiting the predictors considered at each split.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Random Subset</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Boosting</td>
<td style="text-align: left;">Sequential</td>
<td style="text-align: left;">None (Modified Data)</td>
<td style="text-align: left;">Learn slowly by fitting small trees to the residuals.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">BART</td>
<td style="text-align: left;">Sequential,</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">Perturb trees (modify existing trees) to avoid local optima and provide a Bayesian interpretation.</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">Perturbed</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<ul>
<li><strong>Tree-Based Methods:</strong> Powerful and versatile tools for both regression and classification.</li>
<li><strong>Interpretability vs.&nbsp;Accuracy:</strong> Single decision trees offer high interpretability but may sacrifice predictive accuracy.</li>
<li><strong>Ensemble Methods:</strong> Enhance performance by combining multiple trees. Bagging, random forests, boosting, and BART represent different strategies for creating ensembles.</li>
<li><strong>Choosing the Right Method:</strong> The best ensemble method depends on the specific dataset and problem. Consider factors like data structure, desired interpretability, and computational resources.</li>
</ul>
</section>
<section id="thoughts-and-discussion" class="level2">
<h2 class="anchored" data-anchor-id="thoughts-and-discussion">Thoughts and Discussion ü§î</h2>
<ul>
<li><strong>Interpretability:</strong> When might a single decision tree be <em>preferred</em> over an ensemble method, even if the ensemble method has slightly higher accuracy? (Hint: Think about situations where explaining the model to stakeholders is crucial, such as in medical diagnosis or loan applications.)</li>
<li><strong>Method Selection:</strong> How might you choose between bagging, random forests, and boosting for a particular problem? What factors would you consider? (Hint: Think about the size of the dataset, the number of predictors, and the potential for strong predictors to dominate.)</li>
<li><strong>Real-World Applications:</strong> Can you think of real-world scenarios where tree-based methods would be particularly well-suited? (Hint: Consider applications like fraud detection, customer churn prediction, or image classification.)</li>
<li><strong>Limitations:</strong> What are some limitations of tree-based methods, even with ensemble techniques? When might they <em>not</em> be the best choice? (Hint: Think about situations with very high-dimensional data or where the underlying relationship is very smooth and linear.)</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/qiufei\.github\.io\/web-slide-r");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>üîã<a href="https://posit.co"><img src="https://posit.co/wp-content/themes/Posit/assets/images/posit-logo-2024.png" class="img-fluid" alt="Posit" width="65"></a></p>
</div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 ÈÇ±È£û ¬© 2025
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://beian.miit.gov.cn">
<p>ÊµôICPÂ§á 2024072710Âè∑-1</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33021202002511">
<p>ÊµôÂÖ¨ÁΩëÂÆâÂ§á 33021202002511Âè∑</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:hfutqiufei@163.com">
      <i class="bi bi-envelope-at-fill" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>