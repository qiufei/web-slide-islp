<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Your Name">

<title>Linear Model Selection and Regularization</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<link href="../https://assets.qiufei.site/personal/profile.jpg" rel="icon" type="image/jpeg">
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-8ea72dc5fed832574809a9c94082fbbb.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-681fbf911679f9b3dbf9743eb275ba49.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-7e49aeac8059a213a463aa1a739e8272.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://qiufei.github.io"> 
<span class="menu-text">È¶ñÈ°µ</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://qiufei.github.io/web-slide-r"> 
<span class="menu-text">RËØæ‰ª∂</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://qiufei.github.io/web-slide-marketing"> 
<span class="menu-text">Ëê•ÈîÄËØæ‰ª∂</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Linear Model Selection and Regularization</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Your Name </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="introduction-beyond-simple-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="introduction-beyond-simple-linear-regression">Introduction: Beyond Simple Linear Regression</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p>Last time, we explored the fundamentals of linear regression, a powerful tool for modeling relationships between variables.</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p>However, the simple linear model, while interpretable and often effective, has limitations. In this chapter, we extend the linear model. We will discuss:</p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li>Ways in which the simple linear model can be improved.</li>
<li>Alternative fitting procedures instead of least squares.</li>
</ul>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p>The Goals:</p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li>Better <strong>Prediction Accuracy</strong> üí™</li>
<li>Improved <strong>Model Interpretability</strong> üßê</li>
</ul>
</div>
</div>
</div>
</section>
<section id="why-go-beyond-least-squares" class="level2">
<h2 class="anchored" data-anchor-id="why-go-beyond-least-squares">Why Go Beyond Least Squares?</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p>Let‚Äôs recall our standard linear model:</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p><span class="math display">\[
Y = \beta_0 + \beta_1X_1 + ... + \beta_pX_p + \epsilon
\]</span> (eq: 6.1)</p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li><p>We usually use Least Squares to fit this model.</p></li>
<li><p>Linear model has distinct advantages in terms of inference and competitive in relation to non-linear methods.</p></li>
<li><p>But, plain Least Squares has some limitations.</p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>When do we need to use another fitting procedure instead of least squares?</p>
</div>
</div>
</div>
</section>
<section id="limitations-of-least-squares-prediction-accuracy" class="level2">
<h2 class="anchored" data-anchor-id="limitations-of-least-squares-prediction-accuracy">Limitations of Least Squares: Prediction Accuracy</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li><p><strong>Low Bias, Low Variance (Ideal):</strong> When the true relationship is approximately linear <em>and</em> you have many more observations (n) than predictors (p) (<span class="math inline">\(n \gg p\)</span>), least squares works great! The estimates have low bias and low variance.</p></li>
<li><p><strong>High Variance (Problem):</strong> If <em>n</em> is not much larger than <em>p</em>, the least squares fit can have high variability. This leads to <em>overfitting</em> ü§Ø - the model fits the training data too closely and performs poorly on new data.</p></li>
<li><p><strong>No Unique Solution (Big Problem):</strong> If <em>p &gt; n</em> (more predictors than observations), there‚Äôs no longer a unique least squares solution! Many possible coefficient values will fit perfectly, leading to huge variance and terrible predictions.</p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Overfitting:</strong> A model that fits the training data <em>too</em> well, capturing noise and random fluctuations rather than the true underlying relationship. It won‚Äôt generalize well to new data.</p>
</div>
</div>
</div>
</section>
<section id="limitations-of-least-squares-model-interpretability" class="level2">
<h2 class="anchored" data-anchor-id="limitations-of-least-squares-model-interpretability">Limitations of Least Squares: Model Interpretability</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li><p><strong>Irrelevant Variables:</strong> Often, some predictors in your model aren‚Äôt actually related to the response. Including them adds unnecessary complexity. We‚Äôd like to <em>remove</em> these irrelevant variables.</p></li>
<li><p><strong>Least Squares Doesn‚Äôt Zero Out:</strong> Least squares rarely sets coefficients <em>exactly</em> to zero. This makes it hard to identify the truly important variables.</p></li>
<li><p><strong>Feature/Variable Selection:</strong> We want methods that automatically perform <em>feature selection</em> (or <em>variable selection</em>) ‚Äì excluding irrelevant variables to create a simpler, more interpretable model.</p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>A model with fewer, carefully selected variables is often easier to understand and explain. It highlights the key drivers of the response.</p>
</div>
</div>
</div>
</section>
<section id="three-classes-of-methods" class="level2">
<h2 class="anchored" data-anchor-id="three-classes-of-methods">Three Classes of Methods</h2>
<p>To address these limitations, we explore three main classes of methods that offer alternatives to least squares:</p>
<ol type="1">
<li><p><strong>Subset Selection:</strong> Identify a <em>subset</em> of the <em>p</em> predictors that are most related to the response. Fit a model using least squares on this reduced set of variables.</p></li>
<li><p><strong>Shrinkage (Regularization):</strong> Fit a model with <em>all p</em> predictors, but <em>shrink</em> the estimated coefficients towards zero. This reduces variance. Some methods (like the lasso) can even set coefficients exactly to zero, performing variable selection.</p></li>
<li><p><strong>Dimension Reduction:</strong> <em>Project</em> the <em>p</em> predictors into an <em>M</em>-dimensional subspace (<em>M &lt; p</em>). This means creating <em>M</em> linear combinations (projections) of the original variables. Use these projections as predictors in a least squares model.</p></li>
</ol>
</section>
<section id="subset-selection" class="level2">
<h2 class="anchored" data-anchor-id="subset-selection">1. Subset Selection</h2>
<p>We will introduce several methods to select subsets of predictors. Here we consider <em>best subset</em> and <em>stepwise model</em> selection procedures.</p>
<section id="best-subset-selection" class="level3">
<h3 class="anchored" data-anchor-id="best-subset-selection">1.1 Best Subset Selection</h3>
<ul>
<li><p><strong>The Idea:</strong> Fit a separate least squares regression for <em>every possible combination</em> of the <em>p</em> predictors. Then, choose the ‚Äúbest‚Äù model from this set.</p></li>
<li><p><strong>Exhaustive Search:</strong> If you have <em>p</em> predictors, you have 2<sup><em>p</em></sup> possible models! (e.g., 10 predictors = 1,024 models; 20 predictors = over 1 million models!)</p></li>
</ul>
</section>
</section>
<section id="best-subset-selection-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="best-subset-selection-algorithm">Best Subset Selection Algorithm</h2>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Algorithm 6.1 Best subset selection</strong></p>
</div>
</div>
</div>
<ol type="1">
<li><p><strong>Null Model (M<sub>0</sub>):</strong> A model with no predictors. It simply predicts the sample mean of the response for all observations.</p></li>
<li><p><strong>For k = 1, 2, ‚Ä¶, p:</strong> (where <em>k</em> is the number of predictors)</p>
<ul>
<li>Fit all <span class="math inline">\(\binom{p}{k}\)</span> models that contain exactly <em>k</em> predictors.</li>
<li>Pick the ‚Äúbest‚Äù model among these <span class="math inline">\(\binom{p}{k}\)</span> models, and call it M<sub>k</sub>. ‚ÄúBest‚Äù is defined as having the smallest Residual Sum of Squares (RSS) or, equivalently, the largest R<sup>2</sup>.</li>
</ul></li>
<li><p><strong>Select the ultimate best model:</strong> From the models M<sub>0</sub>, M<sub>1</sub>, ‚Ä¶, M<sub>p</sub>, choose the single best model using:</p>
<ul>
<li>Validation set error</li>
<li>Cross-validation error</li>
<li>C<sub>p</sub> (AIC)</li>
<li>BIC</li>
<li>Adjusted R<sup>2</sup> ## Best Subset Selection: Illustration</li>
</ul></li>
</ol>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/6_1.png" class="img-fluid figure-img"></p>
<figcaption>Credit data: RSS and <span class="math inline">\(R^2\)</span> for all possible models. The red frontier tracks the best model for each number of predictors.</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li><p><strong>Figure 6.1</strong>: Shows RSS and R<sup>2</sup> for all possible models on the <em>Credit</em> dataset.</p></li>
<li><p>The data contains ten predictors, but the x-axis ranges to 11. The reason is that one of the predictors is <em>categorical</em>, taking three values. It is split up into <em>two</em> dummy variables.</p></li>
<li><p>The red line connects the <em>best</em> models for each size (lowest RSS or highest R<sup>2</sup>).</p></li>
<li><p>As expected, RSS decreases and R<sup>2</sup> increases as more variables are added. However, the improvements become very small after just a few variables.</p></li>
</ul>
</div>
</div>
</div>
</section>
<section id="best-subset-selection-choosing-the-best-model" class="level2">
<h2 class="anchored" data-anchor-id="best-subset-selection-choosing-the-best-model">Best Subset Selection: Choosing the Best Model</h2>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>The RSS of these p + 1 models <em>decreases</em> monotonically, and the R¬≤ <em>increases</em> monotonically, as the number of features included in the models increases. So we can‚Äôt use them to select the best model!</p>
</div>
</div>
</div>
<ul>
<li><p><strong>Training Error vs.&nbsp;Test Error:</strong> Low RSS and high R<sup>2</sup> indicate a good fit to the <em>training</em> data. But we want a model that performs well on <em>new, unseen</em> data (low <em>test</em> error). Training error is often much smaller than test error!</p></li>
<li><p><strong>Need a Different Criterion:</strong> We can‚Äôt use RSS or R<sup>2</sup> directly to select the best model. We need to estimate the <em>test error</em>.</p></li>
</ul>
</section>
<section id="best-subset-selection-computational-limitations" class="level2">
<h2 class="anchored" data-anchor-id="best-subset-selection-computational-limitations">Best Subset Selection: Computational Limitations</h2>
<ul>
<li><strong>Exponential Growth:</strong> The number of possible models (2<sup><em>p</em></sup>) grows <em>very</em> quickly as <em>p</em> increases.</li>
<li><strong>Infeasible for Large p:</strong> Best subset selection becomes computationally infeasible for even moderately large values of <em>p</em> (e.g., <em>p</em> &gt; 40).</li>
<li><strong>Statistical Problems (Large p):</strong> With a huge search space, there‚Äôs a higher chance of finding models that fit the training data well <em>by chance</em>, even if they have no real predictive power. This leads to overfitting and high variance in the coefficient estimates.</li>
</ul>
<section id="stepwise-selection" class="level3">
<h3 class="anchored" data-anchor-id="stepwise-selection">1.2 Stepwise Selection</h3>
<p>Best subset selection is often computationally infeasible for large <em>p</em>. Thus, <em>stepwise</em> methods are attractive alternatives.</p>
<ul>
<li><strong>Stepwise methods</strong> explore a far more restricted set of models.</li>
</ul>
<section id="forward-stepwise-selection" class="level4">
<h4 class="anchored" data-anchor-id="forward-stepwise-selection">1.2.1 Forward Stepwise Selection</h4>
<ul>
<li><strong>The Idea:</strong> Start with the null model (no predictors). Add predictors one-at-a-time, always choosing the variable that gives the <em>greatest additional improvement</em> to the fit.</li>
</ul>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Algorithm 6.2 Forward stepwise selection</strong></p>
</div>
</div>
</div>
<ol type="1">
<li><p><strong>Null Model (M<sub>0</sub>):</strong> Start with the model containing no predictors.</p></li>
<li><p><strong>For k = 0, 1, ‚Ä¶, p-1:</strong></p>
<ul>
<li>Consider all <em>p - k</em> models that add <em>one</em> additional predictor to the current model (M<sub>k</sub>).</li>
<li>Choose the ‚Äúbest‚Äù of these <em>p - k</em> models (smallest RSS or highest R<sup>2</sup>), and call it M<sub>k+1</sub>.</li>
</ul></li>
<li><p><strong>Select the ultimate best model:</strong> Choose the single best model from M<sub>0</sub>, M<sub>1</sub>, ‚Ä¶, M<sub>p</sub> using validation set error, cross-validation, C<sub>p</sub>, BIC, or adjusted R<sup>2</sup>.</p></li>
</ol>
</section>
</section>
</section>
<section id="forward-stepwise-selection-computational-advantage" class="level2">
<h2 class="anchored" data-anchor-id="forward-stepwise-selection-computational-advantage">Forward Stepwise Selection: Computational Advantage</h2>
<ul>
<li><strong>Much Fewer Models:</strong> Forward stepwise selection considers many fewer models than best subset selection.
<ul>
<li>Best subset: 2<sup><em>p</em></sup> models.</li>
<li>Forward stepwise: 1 + <em>p</em>(<em>p</em>+1)/2 models.</li>
<li>Example: If <em>p</em> = 20, best subset considers over 1 million models, while forward stepwise considers only 211.</li>
</ul></li>
<li><strong>Computational Efficiency:</strong> This makes forward stepwise selection computationally feasible for much larger values of <em>p</em>.</li>
</ul>
</section>
<section id="forward-stepwise-selection-limitations" class="level2">
<h2 class="anchored" data-anchor-id="forward-stepwise-selection-limitations">Forward Stepwise Selection: Limitations</h2>
<ul>
<li><p><strong>Not Guaranteed Optimal:</strong> Forward stepwise selection is <em>not</em> guaranteed to find the best possible model out of all 2<sup><em>p</em></sup> possibilities. It‚Äôs a <em>greedy</em> algorithm ‚Äì it makes the locally optimal choice at each step, which may not lead to the globally optimal solution.</p></li>
<li><p><strong>Example:</strong></p>
<ul>
<li>Suppose the best 1-variable model contains X<sub>1</sub>.</li>
<li>The best 2-variable model might contain X<sub>2</sub> and X<sub>3</sub>.</li>
<li>Forward stepwise <em>won‚Äôt</em> find this, because it <em>must</em> keep X<sub>1</sub> in the 2-variable model.</li>
</ul></li>
</ul>
</section>
<section id="forward-stepwise-selection-vs.-best-subset-selection-an-example" class="level2">
<h2 class="anchored" data-anchor-id="forward-stepwise-selection-vs.-best-subset-selection-an-example">Forward Stepwise Selection vs.&nbsp;Best Subset Selection: An Example</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<table class="caption-top table">
<caption>Comparison on the <em>Credit</em> dataset.</caption>
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"># Variables</th>
<th style="text-align: left;">Best Subset</th>
<th style="text-align: left;">Forward Stepwise</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">One</td>
<td style="text-align: left;">rating</td>
<td style="text-align: left;">rating</td>
</tr>
<tr class="even">
<td style="text-align: left;">Two</td>
<td style="text-align: left;">rating, income</td>
<td style="text-align: left;">rating, income</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Three</td>
<td style="text-align: left;">rating, income, student</td>
<td style="text-align: left;">rating, income, student</td>
</tr>
<tr class="even">
<td style="text-align: left;">Four</td>
<td style="text-align: left;">cards, income, student, limit</td>
<td style="text-align: left;">rating, income, student, limit</td>
</tr>
</tbody>
</table>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li>The first three models selected are identical.</li>
<li>The fourth models differ.</li>
<li>But in this example, the four-variable models perform very similarly (see Figure 6.1), so the difference isn‚Äôt crucial.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="forward-stepwise-in-high-dimensions" class="level2">
<h2 class="anchored" data-anchor-id="forward-stepwise-in-high-dimensions">Forward Stepwise in High Dimensions</h2>
<ul>
<li><p><strong>n &lt; p Case:</strong> Forward stepwise selection can be used even when <em>n &lt; p</em> (more predictors than observations).</p></li>
<li><p><strong>Limitation:</strong> In this case, you can only build models up to size M<sub>n-1</sub>, because least squares can‚Äôt fit a unique solution when <em>p</em> ‚â• <em>n</em>.</p></li>
</ul>
<section id="backward-stepwise-selection" class="level4">
<h4 class="anchored" data-anchor-id="backward-stepwise-selection">1.2.2 Backward Stepwise Selection</h4>
<ul>
<li><strong>The Idea:</strong> Start with the <em>full</em> model (all <em>p</em> predictors). Remove the <em>least useful</em> predictor one-at-a-time.</li>
</ul>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Algorithm 6.3 Backward stepwise selection</strong></p>
</div>
</div>
</div>
<ol type="1">
<li><p><strong>Full Model (M<sub>p</sub>):</strong> Begin with the model containing all <em>p</em> predictors.</p></li>
<li><p><strong>For k = p, p-1, ‚Ä¶, 1:</strong></p>
<ul>
<li>Consider all <em>k</em> models that remove <em>one</em> predictor from the current model (M<sub>k</sub>).</li>
<li>Choose the ‚Äúbest‚Äù of these <em>k</em> models (smallest RSS or highest R<sup>2</sup>), and call it M<sub>k-1</sub>.</li>
</ul></li>
<li><p><strong>Select the ultimate best model:</strong> Select the single best model from M<sub>0</sub>, ‚Ä¶, M<sub>p</sub> using validation set error, cross-validation, C<sub>p</sub>, BIC, or adjusted R<sup>2</sup>.</p></li>
</ol>
</section>
</section>
<section id="backward-stepwise-selection-properties" class="level2">
<h2 class="anchored" data-anchor-id="backward-stepwise-selection-properties">Backward Stepwise Selection: Properties</h2>
<ul>
<li><p><strong>Computational Advantage:</strong> Like forward stepwise, backward stepwise considers only 1 + <em>p</em>(<em>p</em>+1)/2 models, making it computationally efficient.</p></li>
<li><p><strong>Not Guaranteed Optimal:</strong> Like forward stepwise, it‚Äôs not guaranteed to find the best possible model.</p></li>
<li><p><strong>Requirement: n &gt; p:</strong> Backward stepwise selection <em>requires</em> that <em>n &gt; p</em> (more observations than predictors) so that the full model can be fit.</p></li>
</ul>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>Forward stepwise can be used even when n &lt; p, and so is the only viable subset method when p is very large.</p>
</div>
</div>
</div>
<section id="hybrid-approaches" class="level4">
<h4 class="anchored" data-anchor-id="hybrid-approaches">1.2.3 Hybrid Approaches</h4>
<ul>
<li><strong>Combine Forward and Backward:</strong> Hybrid methods combine aspects of forward and backward stepwise selection.</li>
<li><strong>Add and Remove:</strong> Variables are added sequentially (like forward). But, after adding each new variable, the method may also <em>remove</em> any variables that no longer contribute significantly to the model fit.</li>
<li><strong>Goal:</strong> Try to mimic best subset selection while retaining the computational advantages of stepwise methods.</li>
</ul>
</section>
<section id="choosing-the-optimal-model" class="level3">
<h3 class="anchored" data-anchor-id="choosing-the-optimal-model">1.3 Choosing the Optimal Model</h3>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>Best subset selection, forward selection, and backward selection result in the creation of a set of models, each of which contains a <em>subset</em> of the <em>p</em> predictors.</p>
</div>
</div>
</div>
<ul>
<li><p><strong>The Challenge:</strong> How do we choose the <em>best</em> model from among the set of models generated by subset selection or stepwise selection? We cannot simply use the model that has the <em>smallest</em> RSS and the <em>largest</em> R<sup>2</sup>!</p></li>
<li><p><strong>Need to Estimate Test Error:</strong> We need to estimate the <em>test error</em> of each model.</p></li>
<li><p><strong>Two Main Approaches:</strong></p>
<ol type="1">
<li><strong>Indirectly Estimate Test Error:</strong> Adjust the <em>training</em> error to account for the bias due to overfitting.</li>
<li><strong>Directly Estimate Test Error:</strong> Use a validation set or cross-validation.</li>
</ol></li>
</ul>
</section>
</section>
<section id="indirectly-estimating-test-error-cp-aic-bic-adjusted-r2" class="level2">
<h2 class="anchored" data-anchor-id="indirectly-estimating-test-error-cp-aic-bic-adjusted-r2">Indirectly Estimating Test Error: C<sub>p</sub>, AIC, BIC, Adjusted R<sup>2</sup></h2>
<ul>
<li><p><strong>Training Error is Deceptive:</strong> The training set MSE (RSS/n) generally <em>underestimates</em> the test MSE. This is because least squares specifically minimizes the <em>training</em> RSS.</p></li>
<li><p><strong>Adjusting for Model Size:</strong> We need to <em>adjust</em> the training error to account for the fact that it tends to be too optimistic. Several techniques do this:</p>
<ul>
<li>C<sub>p</sub></li>
<li>Akaike Information Criterion (AIC)</li>
<li>Bayesian Information Criterion (BIC)</li>
<li>Adjusted R<sup>2</sup></li>
</ul></li>
</ul>
</section>
<section id="cp-aic-bic-adjusted-r2-formulas" class="level2">
<h2 class="anchored" data-anchor-id="cp-aic-bic-adjusted-r2-formulas">C<sub>p</sub>, AIC, BIC, Adjusted R<sup>2</sup>: Formulas</h2>
<p>For a least squares model with <em>d</em> predictors, these statistics are computed as:</p>
<ul>
<li><strong>C<sub>p</sub>:</strong> <span class="math display">\[
C_p = \frac{1}{n}(RSS + 2d\hat{\sigma}^2)
\]</span> (eq: 6.2)
<ul>
<li><span class="math inline">\(\hat{\sigma}^2\)</span> is an estimate of the error variance.</li>
<li>Adds a penalty proportional to the number of predictors.</li>
</ul></li>
<li><strong>AIC:</strong> <span class="math display">\[
AIC = \frac{1}{n}(RSS + 2d\hat{\sigma}^2)
\]</span>
<ul>
<li>For linear model with Gaussian errors, AIC is proportional to C<sub>p</sub>.</li>
</ul></li>
<li><strong>BIC:</strong> <span class="math display">\[
BIC = \frac{1}{n}(RSS + log(n)d\hat{\sigma}^2)
\]</span> (eq: 6.3)
<ul>
<li>Similar to C<sub>p</sub>, but the penalty for the number of predictors is multiplied by log(<em>n</em>).</li>
<li>Since log(<em>n</em>) &gt; 2 for <em>n</em> &gt; 7, BIC generally penalizes models with more variables more heavily than C<sub>p</sub>, leading to the selection of smaller models.</li>
</ul></li>
<li><strong>Adjusted R<sup>2</sup>:</strong> <span class="math display">\[
Adjusted \ R^2 = 1 - \frac{RSS/(n - d - 1)}{TSS/(n-1)}
\]</span> (eq: 6.4)</li>
<li>Maximizing the adjusted R¬≤ is equivalent to minimizing <span class="math inline">\(\frac{RSS}{n-d-1}\)</span>.</li>
</ul>
</section>
<section id="cp-aic-bic-adjusted-r2-interpretation" class="level2">
<h2 class="anchored" data-anchor-id="cp-aic-bic-adjusted-r2-interpretation">C<sub>p</sub>, AIC, BIC, Adjusted R<sup>2</sup>: Interpretation</h2>
<ul>
<li><p><strong>Low Values are Good (C<sub>p</sub>, AIC, BIC):</strong> For C<sub>p</sub>, AIC, and BIC, we choose the model with the <em>lowest</em> value.</p></li>
<li><p><strong>High Values are Good (Adjusted R<sup>2</sup>):</strong> For adjusted R<sup>2</sup>, we choose the model with the <em>highest</em> value.</p></li>
<li><p><strong>Theoretical Justification:</strong> C<sub>p</sub>, AIC, and BIC have theoretical justifications (though they rely on assumptions that may not always hold). Adjusted R<sup>2</sup> is more intuitive but less theoretically grounded.</p></li>
</ul>
</section>
<section id="cp-aic-bic-adjusted-r2-example" class="level2">
<h2 class="anchored" data-anchor-id="cp-aic-bic-adjusted-r2-example">C<sub>p</sub>, AIC, BIC, Adjusted R<sup>2</sup>: Example</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/6_2.png" class="img-fluid figure-img"></p>
<figcaption>C<sub>p</sub>, BIC, and adjusted R<sup>2</sup> for the best models of each size on the <em>Credit</em> data.</figcaption>
</figure>
</div>
<ul>
<li><p><strong>Figure 6.2:</strong> Shows these statistics for the <em>Credit</em> dataset.</p></li>
<li><p>C<sub>p</sub> and BIC are estimates of test MSE.</p></li>
<li><p>BIC selects a model with 4 variables (<em>income, limit, cards, student</em>).</p></li>
<li><p>C<sub>p</sub> selects a 6-variable model.</p></li>
<li><p>Adjusted R<sup>2</sup> selects a 7-variable model.</p></li>
</ul>
</section>
<section id="directly-estimating-test-error-validation-and-cross-validation" class="level2">
<h2 class="anchored" data-anchor-id="directly-estimating-test-error-validation-and-cross-validation">Directly Estimating Test Error: Validation and Cross-Validation</h2>
<ul>
<li><strong>Direct Estimation:</strong> Instead of adjusting the training error, we can <em>directly</em> estimate the test error using:
<ul>
<li>Validation set approach</li>
<li>Cross-validation approach</li>
</ul></li>
<li><strong>Advantages:</strong>
<ul>
<li>Provide a direct estimate of the test error.</li>
<li>Make fewer assumptions about the true underlying model.</li>
<li>Can be used in a wider range of model selection tasks.</li>
</ul></li>
<li><strong>Computational Cost:</strong> Historically, cross-validation was computationally expensive. Now, with fast computers, this is less of a concern.</li>
</ul>
</section>
<section id="validation-and-cross-validation-example" class="level2">
<h2 class="anchored" data-anchor-id="validation-and-cross-validation-example">Validation and Cross-Validation: Example</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/6_3.png" class="img-fluid figure-img"></p>
<figcaption>BIC, validation set error, and cross-validation error for the best models of each size on the <em>Credit</em> data.</figcaption>
</figure>
</div>
<ul>
<li><strong>Figure 6.3:</strong> Shows BIC, validation set error, and cross-validation error for the <em>Credit</em> data.</li>
<li>Validation and cross-validation both select a 6-variable model.</li>
<li>All three approaches suggest that models with 4, 5, or 6 variables are quite similar.</li>
<li><strong>One-Standard-Error Rule:</strong> A practical rule for choosing among models with similar estimated test error.
<ul>
<li>Calculate the standard error of the estimated test MSE for each model size.</li>
<li>Select the <em>smallest</em> model for which the estimated test error is within <em>one standard error</em> of the lowest point on the curve.</li>
<li>Rationale: Choose the simplest model among those that perform comparably.</li>
</ul></li>
<li>Apply to this example, we may choose the three-variable model.</li>
</ul>
</section>
<section id="shrinkage-methods" class="level2">
<h2 class="anchored" data-anchor-id="shrinkage-methods">2. Shrinkage Methods</h2>
<ul>
<li><p><strong>Alternative to Subset Selection:</strong> Instead of selecting a subset of variables, <em>shrinkage methods</em> (also called <em>regularization methods</em>) fit a model with <em>all p</em> predictors, but <em>constrain</em> or <em>regularize</em> the coefficient estimates.</p></li>
<li><p><strong>How it Works:</strong> Shrinkage methods shrink the coefficient estimates towards zero.</p></li>
<li><p><strong>Why Shrink?:</strong> Shrinking the coefficients can significantly reduce their <em>variance</em>.</p></li>
<li><p><strong>Two Main Techniques:</strong></p>
<ul>
<li>Ridge regression</li>
<li>Lasso</li>
</ul></li>
</ul>
<section id="ridge-regression" class="level3">
<h3 class="anchored" data-anchor-id="ridge-regression">2.1 Ridge Regression</h3>
<ul>
<li><strong>Recall Least Squares:</strong> Least squares minimizes the Residual Sum of Squares (RSS):</li>
</ul>
<p><span class="math display">\[
RSS = \sum_{i=1}^{n}(y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij})^2
\]</span></p>
<ul>
<li><strong>Ridge Regression:</strong> Ridge regression minimizes a slightly different quantity:</li>
</ul>
<p><span class="math display">\[
\sum_{i=1}^{n}(y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij})^2 + \lambda\sum_{j=1}^{p}\beta_j^2 = RSS + \lambda\sum_{j=1}^{p}\beta_j^2
\]</span> (eq: 6.5)</p>
<ul>
<li><strong>Œª (Tuning Parameter):</strong> Œª ‚â• 0 is a <em>tuning parameter</em> that controls the amount of shrinkage.</li>
</ul>
</section>
</section>
<section id="ridge-regression-the-shrinkage-penalty" class="level2">
<h2 class="anchored" data-anchor-id="ridge-regression-the-shrinkage-penalty">Ridge Regression: The Shrinkage Penalty</h2>
<p><span class="math display">\[
\sum_{i=1}^{n}(y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij})^2 + \lambda\sum_{j=1}^{p}\beta_j^2 = RSS + \lambda\sum_{j=1}^{p}\beta_j^2
\]</span></p>
<ul>
<li><strong>Two Parts:</strong>
<ul>
<li><strong>RSS:</strong> Measures how well the model fits the data.</li>
<li><strong>Shrinkage Penalty (ŒªŒ£Œ≤<sub>j</sub><sup>2</sup>):</strong> Penalizes large coefficients. This term is small when Œ≤<sub>1</sub>, ‚Ä¶, Œ≤<sub>p</sub> are close to zero.</li>
</ul></li>
<li><strong>Tuning Parameter (Œª):</strong>
<ul>
<li>Œª = 0: No penalty. Ridge regression is the same as least squares.</li>
<li>Œª ‚Üí ‚àû: Coefficients are shrunk all the way to zero (null model).</li>
<li>0 &lt; Œª &lt; ‚àû: Controls the <em>trade-off</em> between fitting the data well and shrinking the coefficients.</li>
</ul></li>
</ul>
</section>
<section id="ridge-regression-the-intercept" class="level2">
<h2 class="anchored" data-anchor-id="ridge-regression-the-intercept">Ridge Regression: The Intercept</h2>
<ul>
<li><p><strong>No Shrinkage on Intercept:</strong> Notice that the shrinkage penalty is <em>not</em> applied to the intercept (Œ≤<sub>0</sub>).</p></li>
<li><p><strong>Why?:</strong> We want to shrink the coefficients of the <em>predictors</em>, but not the intercept, which represents the average value of the response when all predictors are zero.</p></li>
<li><p><strong>Centering Predictors:</strong> If the predictors are <em>centered</em> (mean of zero) before performing ridge regression, then the estimated intercept will be the sample mean of the response: <span class="math inline">\(\hat{\beta}_0 = \bar{y}\)</span>.</p></li>
</ul>
</section>
<section id="ridge-regression-example-on-credit-data" class="level2">
<h2 class="anchored" data-anchor-id="ridge-regression-example-on-credit-data">Ridge Regression: Example on Credit Data</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/6_4.png" class="img-fluid figure-img"></p>
<figcaption>Standardized ridge regression coefficients for the <em>Credit</em> data, as a function of Œª and ||Œ≤<sup>Œª</sup><sub>R</sub>||<sub>2</sub> / ||Œ≤||<sub>2</sub>.</figcaption>
</figure>
</div>
<ul>
<li><strong>Figure 6.4:</strong> Shows ridge regression coefficient estimates for the <em>Credit</em> data.</li>
<li><strong>Left Panel:</strong> Coefficients plotted against Œª.
<ul>
<li>Œª = 0: Coefficients are the same as least squares.</li>
<li>As Œª increases, coefficients shrink towards zero.</li>
</ul></li>
<li><strong>Right Panel:</strong> Coefficients plotted against ||Œ≤<sup>Œª</sup><sub>R</sub>||<sub>2</sub> / ||Œ≤||<sub>2</sub></li>
<li>The x-axis can be seen as how much the ridge regression coefficient estimates have been shrunken towards zero.</li>
</ul>
</section>
<section id="ridge-regression-standardization" class="level2">
<h2 class="anchored" data-anchor-id="ridge-regression-standardization">Ridge Regression: Standardization</h2>
<ul>
<li><p><strong>Scale Equivariance (Least Squares):</strong> Least squares coefficient estimates are <em>scale equivariant</em>. Multiplying a predictor by a constant <em>c</em> simply scales the corresponding coefficient by 1/<em>c</em>.</p></li>
<li><p><strong>Scale Dependence (Ridge Regression):</strong> Ridge regression coefficients <em>can change substantially</em> when multiplying a predictor by a constant. This is because of the Œ£Œ≤<sub>j</sub><sup>2</sup> term in the penalty.</p></li>
<li><p><strong>Standardization:</strong> It‚Äôs best to apply ridge regression <em>after standardizing</em> the predictors:</p></li>
</ul>
<p><span class="math display">\[
\tilde{x}_{ij} = \frac{x_{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}(x_{ij}-\bar{x}_j)^2}}
\]</span> (eq: 6.6)</p>
<ul>
<li>This ensures that all predictors are on the same scale.</li>
</ul>
</section>
<section id="why-does-ridge-regression-improve-over-least-squares" class="level2">
<h2 class="anchored" data-anchor-id="why-does-ridge-regression-improve-over-least-squares">Why Does Ridge Regression Improve Over Least Squares?</h2>
<ul>
<li><p><strong>Bias-Variance Trade-Off:</strong> Ridge regression‚Äôs advantage comes from the <em>bias-variance trade-off</em>.</p>
<ul>
<li>As Œª increases:
<ul>
<li>Flexibility of the model <em>decreases</em>.</li>
<li>Variance <em>decreases</em>.</li>
<li>Bias <em>increases</em>.</li>
</ul></li>
</ul></li>
<li><p><strong>Finding the Sweet Spot:</strong> The goal is to find a value of Œª that reduces variance <em>more</em> than it increases bias, leading to a lower test MSE.</p></li>
</ul>
</section>
<section id="ridge-regression-bias-variance-trade-off-illustrated" class="level2">
<h2 class="anchored" data-anchor-id="ridge-regression-bias-variance-trade-off-illustrated">Ridge Regression: Bias-Variance Trade-Off Illustrated</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/6_5.png" class="img-fluid figure-img"></p>
<figcaption>Squared bias, variance, and test MSE for ridge regression on a simulated dataset.</figcaption>
</figure>
</div>
<ul>
<li><p><strong>Figure 6.5:</strong> Shows bias, variance, and test MSE for ridge regression on a simulated dataset.</p></li>
<li><p>As Œª increases, variance decreases rapidly at first, with only a small increase in bias. This leads to a decrease in MSE.</p></li>
<li><p>Eventually, the decrease in variance slows, and the increase in bias accelerates, causing the MSE to increase.</p></li>
<li><p>The minimum MSE is achieved at a moderate value of Œª.</p></li>
</ul>
</section>
<section id="when-does-ridge-regression-work-well" class="level2">
<h2 class="anchored" data-anchor-id="when-does-ridge-regression-work-well">When Does Ridge Regression Work Well?</h2>
<ul>
<li><strong>High Variance in Least Squares:</strong> Ridge regression works best in situations where the least squares estimates have <em>high variance</em>. This often happens when:
<ul>
<li><em>n</em> is not much larger than <em>p</em>.</li>
<li><em>p</em> is close to <em>n</em>.</li>
<li><em>p &gt; n</em> (though least squares doesn‚Äôt have a unique solution in this case).</li>
</ul></li>
<li><strong>Computational Advantage:</strong> Ridge regression is also computationally efficient, even for large <em>p</em>.</li>
</ul>
<section id="the-lasso" class="level3">
<h3 class="anchored" data-anchor-id="the-lasso">2.2 The Lasso</h3>
<ul>
<li><p><strong>Disadvantage of Ridge Regression:</strong> Ridge regression includes <em>all p</em> predictors in the final model. The penalty shrinks coefficients towards zero, but it doesn‚Äôt set any of them <em>exactly</em> to zero (unless Œª = ‚àû). This can make interpretation difficult when <em>p</em> is large.</p></li>
<li><p><strong>The Lasso: An Alternative:</strong> The <em>lasso</em> is a more recent alternative to ridge regression that overcomes this disadvantage.</p></li>
<li><p><strong>Lasso Penalty:</strong> The lasso uses a different penalty term:</p></li>
</ul>
<p><span class="math display">\[
\sum_{i=1}^{n}(y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij})^2 + \lambda\sum_{j=1}^{p}|\beta_j| = RSS + \lambda\sum_{j=1}^{p}|\beta_j|
\]</span> (eq: 6.7)</p>
<ul>
<li><strong>Absolute Value Penalty:</strong> The lasso uses an <em>l<sub>1</sub> penalty</em> (absolute value of coefficients) instead of an <em>l<sub>2</sub> penalty</em> (squared coefficients).</li>
</ul>
</section>
</section>
<section id="the-lasso-variable-selection" class="level2">
<h2 class="anchored" data-anchor-id="the-lasso-variable-selection">The Lasso: Variable Selection</h2>
<ul>
<li><p><strong>Shrinkage and Selection:</strong> Like ridge regression, the lasso shrinks coefficients towards zero.</p></li>
<li><p><strong>Key Difference:</strong> The l<sub>1</sub> penalty has the effect of forcing some coefficients to be <em>exactly zero</em> when Œª is sufficiently large.</p></li>
<li><p><strong>Variable Selection:</strong> This means the lasso performs <em>variable selection</em>!</p></li>
<li><p><strong>Sparse Models:</strong> The lasso yields <em>sparse models</em> ‚Äì models that involve only a subset of the variables.</p></li>
</ul>
</section>
<section id="the-lasso-example-on-credit-data" class="level2">
<h2 class="anchored" data-anchor-id="the-lasso-example-on-credit-data">The Lasso: Example on Credit Data</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/6_6.png" class="img-fluid figure-img"></p>
<figcaption>Standardized lasso coefficients for the <em>Credit</em> data, as a function of Œª and ||Œ≤<sup>Œª</sup><sub>L</sub>||<sub>1</sub> / ||Œ≤||<sub>1</sub>.</figcaption>
</figure>
</div>
<ul>
<li><p><strong>Figure 6.6:</strong> Shows lasso coefficient estimates for the <em>Credit</em> data.</p></li>
<li><p>As Œª increases, coefficients shrink towards zero. But unlike ridge regression, some coefficients are set <em>exactly</em> to zero.</p></li>
<li><p>This leads to a simpler, more interpretable model.</p></li>
</ul>
</section>
<section id="another-formulation-for-ridge-regression-and-the-lasso" class="level2">
<h2 class="anchored" data-anchor-id="another-formulation-for-ridge-regression-and-the-lasso">Another Formulation for Ridge Regression and the Lasso</h2>
<ul>
<li>Both ridge regression and the lasso can be formulated as constrained optimization problems.
<ul>
<li>Ridge Regression: <span class="math display">\[
  \underset{\beta}{minimize} \left\{ \sum_{i=1}^{n}(y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij})^2 \right\} \quad subject \ to \ \sum_{j=1}^{p}\beta_j^2 \le s
  \]</span> (eq: 6.9)</li>
<li>Lasso: <span class="math display">\[
  \underset{\beta}{minimize} \left\{ \sum_{i=1}^{n}(y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij})^2 \right\} \quad subject \ to \ \sum_{j=1}^{p}|\beta_j| \le s
  \]</span> (eq: 6.8)</li>
</ul></li>
<li>The above formulations reveal a close connection between the lasso, ridge regression, and best subset selection.</li>
</ul>
</section>
<section id="the-variable-selection-property-of-the-lasso" class="level2">
<h2 class="anchored" data-anchor-id="the-variable-selection-property-of-the-lasso">The Variable Selection Property of the Lasso</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li><strong>Why does the lasso set coefficients to zero, while ridge regression doesn‚Äôt?</strong>
<ul>
<li>Consider the constraint regions (where the solution must lie):
<ul>
<li>Ridge regression: A circle (l<sub>2</sub> constraint).</li>
<li>Lasso: A diamond (l<sub>1</sub> constraint).</li>
</ul></li>
</ul></li>
</ul>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/6_7.png" class="img-fluid figure-img"></p>
<figcaption>Contours of the error and constraint functions for the lasso (left) and ridge regression (right).</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li>The solution is the first point where the ‚Äúellipse‚Äù (contour of constant RSS) touches the constraint region.</li>
<li>Because the lasso constraint has <em>corners</em>, the ellipse often intersects at an axis, setting one coefficient to zero.</li>
<li>Ridge regression‚Äôs circular constraint doesn‚Äôt have corners, so this rarely happens.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="comparing-the-lasso-and-ridge-regression" class="level2">
<h2 class="anchored" data-anchor-id="comparing-the-lasso-and-ridge-regression">Comparing the Lasso and Ridge Regression</h2>
<ul>
<li><p><strong>Interpretability:</strong> The lasso has a major advantage in terms of <em>interpretability</em>, producing simpler models with fewer variables.</p></li>
<li><p><strong>Prediction Accuracy:</strong> Which method is better for prediction depends on the <em>true underlying relationship</em> between the predictors and the response.</p>
<ul>
<li><strong>Few Important Predictors:</strong> If only a <em>few</em> predictors are truly important (with large coefficients), the lasso tends to perform better.</li>
<li><strong>Many Important Predictors:</strong> If <em>many</em> predictors have small or moderate-sized coefficients, ridge regression tends to perform better.</li>
</ul></li>
<li><p><strong>Unknown Truth:</strong> In practice, we don‚Äôt know which scenario is true. Cross-validation can help us choose the best approach for a particular dataset.</p></li>
</ul>
</section>
<section id="comparing-the-lasso-and-ridge-regression-simulated-examples" class="level2">
<h2 class="anchored" data-anchor-id="comparing-the-lasso-and-ridge-regression-simulated-examples">Comparing the Lasso and Ridge Regression: Simulated Examples</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/6_8.png" class="img-fluid figure-img"></p>
<figcaption>Lasso and ridge regression on a simulated dataset where <em>all</em> predictors are related to the response.</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li><strong>Figure 6.8:</strong> All 45 predictors are related to the response.</li>
<li>Ridge regression slightly outperforms the lasso.</li>
<li>The minimum MSE of ridge regression is slightly smaller than that of the lasso.</li>
</ul>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/6_9.png" class="img-fluid figure-img"></p>
<figcaption>Lasso and ridge regression on a simulated dataset where only <em>two</em> predictors are related to the response.</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li><strong>Figure 6.9:</strong> Only 2 of 45 predictors are related to the response.</li>
<li>The lasso tends to outperform ridge regression.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="a-simple-special-case-for-ridge-regression-and-the-lasso" class="level2">
<h2 class="anchored" data-anchor-id="a-simple-special-case-for-ridge-regression-and-the-lasso">A Simple Special Case for Ridge Regression and the Lasso</h2>
<p>We consider a simple situation: - <span class="math inline">\(n=p\)</span>. - <span class="math inline">\(\mathbf{X}\)</span> is a <em>diagonal matrix</em> with 1‚Äôs on the diagonal. - No intercept. Then, it can be shown: - Ridge regression <em>shrinks each least squares coefficient estimate by the same proportion</em>. <span class="math display">\[
        \hat{\beta}_j^R = y_j / (1 + \lambda)
        \]</span> (eq: 6.14) - Lasso <em>soft-threshold</em> the least squares coefficient estimates. <span class="math display">\[
        \hat{\beta}_j^L =
        \begin{cases}
        y_j - \lambda/2 &amp; \text{if } y_j &gt; \lambda/2 \\
        y_j + \lambda/2 &amp; \text{if } y_j &lt; -\lambda/2 \\
        0 &amp; \text{if } |y_j| \le \lambda/2
        \end{cases}
        \]</span> (eq: 6.15)</p>
</section>
<section id="a-simple-special-case-for-ridge-regression-and-the-lasso-1" class="level2">
<h2 class="anchored" data-anchor-id="a-simple-special-case-for-ridge-regression-and-the-lasso-1">A Simple Special Case for Ridge Regression and the Lasso</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/6_10.png" class="img-fluid figure-img"></p>
<figcaption>Ridge and lasso coefficient estimates in the simple case.</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li><p><strong>Figure 6.10</strong>: It shows that:</p></li>
<li><p>Ridge regression shrinks each coefficient by same proportion.</p></li>
<li><p>Lasso shrinks all coefficients toward zero by a <em>similar amount</em>, and sufficiently small coefficients are shrunken all the way to zero.</p></li>
</ul>
</div>
</div>
</div>
<section id="selecting-the-tuning-parameter" class="level3">
<h3 class="anchored" data-anchor-id="selecting-the-tuning-parameter">2.3 Selecting the Tuning Parameter</h3>
<ul>
<li><strong>Crucial Choice:</strong> Just like with subset selection, we need to choose the tuning parameter (Œª) for ridge regression and the lasso.</li>
<li><strong>Cross-Validation:</strong> Cross-validation is a powerful method for selecting Œª.
<ol type="1">
<li>Choose a grid of Œª values.</li>
<li>Compute the cross-validation error for each value of Œª.</li>
<li>Select the Œª that gives the <em>smallest</em> cross-validation error.</li>
<li>Re-fit the model using <em>all</em> of the data with the chosen Œª.</li>
</ol></li>
</ul>
</section>
</section>
<section id="selecting-Œª-example-for-ridge-regression" class="level2">
<h2 class="anchored" data-anchor-id="selecting-Œª-example-for-ridge-regression">Selecting Œª: Example for Ridge Regression</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/6_12.png" class="img-fluid figure-img"></p>
<figcaption>Cross-validation error and coefficient estimates for ridge regression on the <em>Credit</em> data.</figcaption>
</figure>
</div>
<ul>
<li><strong>Figure 6.12:</strong> Shows cross-validation for ridge regression on the <em>Credit</em> data.</li>
<li>The optimal Œª is relatively small, indicating a small amount of shrinkage.</li>
<li>The cross-validation error curve is quite flat, suggesting that a range of Œª values would work similarly well.</li>
</ul>
</section>
<section id="selecting-Œª-example-for-lasso" class="level2">
<h2 class="anchored" data-anchor-id="selecting-Œª-example-for-lasso">Selecting Œª: Example for Lasso</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/6_13.png" class="img-fluid figure-img"></p>
<figcaption>Cross-validation error and coefficient estimates for the lasso on the simulated data from Figure 6.9.</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li><strong>Figure 6.13:</strong> Shows cross-validation for the lasso on the simulated data from Figure 6.9 (where only two predictors are truly related to the response).</li>
<li>The lasso correctly identifies the <em>two signal variables</em> (colored lines) and sets the coefficients of the noise variables (gray lines) to near zero.</li>
<li>The minimum cross-validation error occurs when only the signal variables have non-zero coefficients.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="dimension-reduction-methods" class="level2">
<h2 class="anchored" data-anchor-id="dimension-reduction-methods">3. Dimension Reduction Methods</h2>
<ul>
<li><p><strong>Different Approach:</strong> Instead of working directly with the original predictors (X<sub>1</sub>, ‚Ä¶, X<sub>p</sub>), dimension reduction methods <em>transform</em> the predictors and then fit a least squares model using the <em>transformed</em> variables.</p></li>
<li><p><strong>Linear Combinations:</strong> Create <em>M</em> linear combinations (Z<sub>1</sub>, ‚Ä¶, Z<sub>M</sub>) of the original <em>p</em> predictors, where <em>M &lt; p</em>.</p></li>
</ul>
<p><span class="math display">\[
Z_m = \sum_{j=1}^{p}\phi_{jm}X_j
\]</span> (eq: 6.16)</p>
<pre><code>-   œÜ&lt;sub&gt;jm&lt;/sub&gt; are constants.</code></pre>
<ul>
<li><strong>Reduced Dimension:</strong> Fit a linear regression model using Z<sub>1</sub>, ‚Ä¶, Z<sub>M</sub> as predictors:</li>
</ul>
<p><span class="math display">\[
y_i = \theta_0 + \sum_{m=1}^{M}\theta_mz_{im} + \epsilon_i
\]</span> (eq: 6.17)</p>
<pre><code>-   This reduces the problem from estimating *p*+1 coefficients to estimating *M*+1 coefficients.</code></pre>
</section>
<section id="dimension-reduction-why-it-works" class="level2">
<h2 class="anchored" data-anchor-id="dimension-reduction-why-it-works">Dimension Reduction: Why it Works</h2>
<ul>
<li><strong>Constraint:</strong> The coefficients in the dimension-reduced model are constrained by the linear combinations:</li>
</ul>
<p><span class="math display">\[
\beta_j = \sum_{m=1}^{M}\theta_m\phi_{jm}
\]</span> (eq: 6.18)</p>
<ul>
<li><p><strong>Bias-Variance Trade-Off:</strong> This constraint can introduce bias, but if <em>p</em> is large relative to <em>n</em>, choosing <em>M &lt;&lt; p</em> can significantly reduce the variance of the fitted coefficients.</p></li>
<li><p><strong>Two Steps:</strong></p>
<ol type="1">
<li>Obtain the transformed predictors (Z<sub>1</sub>, ‚Ä¶, Z<sub>M</sub>).</li>
<li>Fit a least squares model using these <em>M</em> predictors.</li>
</ol></li>
<li><p><strong>Different Methods:</strong> Different dimension reduction methods differ in how they choose the Z<sub>m</sub> (or, equivalently, the œÜ<sub>jm</sub>).</p></li>
<li><p>Principal components regression (PCR)</p></li>
<li><p>Partial least squares (PLS)</p></li>
</ul>
<section id="principal-components-regression-pcr" class="level3">
<h3 class="anchored" data-anchor-id="principal-components-regression-pcr">3.1 Principal Components Regression (PCR)</h3>
<ul>
<li><p><strong>Principal Components Analysis (PCA):</strong> PCA is a technique for deriving a low-dimensional set of features from a larger set of variables. (More detail in Chapter 12.)</p></li>
<li><p><strong>Unsupervised:</strong> PCA is an <em>unsupervised</em> method ‚Äì it identifies linear combinations that best represent the <em>predictors</em> (X), <em>without</em> considering the response (Y).</p></li>
</ul>
<section id="an-overview-of-principal-components-analysis" class="level4">
<h4 class="anchored" data-anchor-id="an-overview-of-principal-components-analysis">An Overview of Principal Components Analysis</h4>
<ul>
<li><p>PCA seeks to find the directions in the data along which the observations <em>vary the most</em>.</p></li>
<li><p><strong>First Principal Component:</strong> The first principal component is the direction in the data with the <em>greatest variance</em>.</p></li>
</ul>
</section>
</section>
</section>
<section id="pca-example-on-advertising-data" class="level2">
<h2 class="anchored" data-anchor-id="pca-example-on-advertising-data">PCA: Example on Advertising Data</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/6_14.png" class="img-fluid figure-img"></p>
<figcaption>Population size and ad spending for 100 cities. The first principal component is shown in green, and the second in blue.</figcaption>
</figure>
</div>
<ul>
<li><strong>Figure 6.14:</strong> Shows population size (pop) and ad spending (ad) for 100 cities.</li>
<li>The green line is the first principal component direction.</li>
<li>Projecting the observations onto this line would maximize the variance of the projected points.</li>
</ul>
</section>
<section id="pca-finding-the-first-principal-component" class="level2">
<h2 class="anchored" data-anchor-id="pca-finding-the-first-principal-component">PCA: Finding the First Principal Component</h2>
<ul>
<li><strong>Mathematical Representation:</strong> The first principal component can be written as:</li>
</ul>
<p><span class="math display">\[
Z_1 = 0.839 \times (pop - \overline{pop}) + 0.544 \times (ad - \overline{ad})
\]</span> (eq: 6.19)</p>
<pre><code>-   0.839 and 0.544 are the *principal component loadings*.
-   $\overline{pop}$ and $\overline{ad}$ are the means of pop and ad, respectively.</code></pre>
<ul>
<li><strong>Interpretation:</strong> Z<sub>1</sub> is almost an average of the two variables (since the loadings are positive and similar in size).</li>
</ul>
</section>
<section id="pca-principal-component-scores" class="level2">
<h2 class="anchored" data-anchor-id="pca-principal-component-scores">PCA: Principal Component Scores</h2>
<p><span class="math display">\[
Z_{i1} = 0.839 \times (pop_i - \overline{pop}) + 0.544 \times (ad_i - \overline{ad})
\]</span> (eq: 6.20)</p>
<ul>
<li><strong>Scores:</strong> The values z<sub>i1</sub>, ‚Ä¶, z<sub>n1</sub> are called the <em>principal component scores</em>. They represent the ‚Äúcoordinates‚Äù of the data points along the first principal component direction.</li>
</ul>
</section>
<section id="pca-another-interpretation" class="level2">
<h2 class="anchored" data-anchor-id="pca-another-interpretation">PCA: Another Interpretation</h2>
<ul>
<li><strong>Closest Line:</strong> The first principal component vector defines the line that is <em>as close as possible</em> to the data (minimizing the sum of squared perpendicular distances).</li>
</ul>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/6_15.png" class="img-fluid figure-img"></p>
<figcaption>The first principal component direction, with distances to the observations shown as dashed lines.</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<ul>
<li><strong>Figure 6.15:</strong>
<ul>
<li><strong>Left:</strong> Shows the perpendicular distances from each point to the first principal component line.</li>
<li><strong>Right:</strong> Rotates the plot so that the first principal component is horizontal. The <em>x</em>-coordinate of each point in this rotated plot is its principal component score.</li>
</ul></li>
</ul>
</div>
</div>
</div>
</section>
<section id="pca-capturing-information" class="level2">
<h2 class="anchored" data-anchor-id="pca-capturing-information">PCA: Capturing Information</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/6_16.png" class="img-fluid figure-img"></p>
<figcaption>Plots of the first principal component scores versus pop and ad.</figcaption>
</figure>
</div>
<ul>
<li><strong>Figure 6.16:</strong> Shows the first principal component scores (z<sub>i1</sub>) plotted against pop and ad.</li>
<li><strong>Strong Relationship:</strong> There‚Äôs a strong relationship, indicating that the first principal component captures much of the information in the original two variables.</li>
</ul>
</section>
<section id="pca-multiple-principal-components" class="level2">
<h2 class="anchored" data-anchor-id="pca-multiple-principal-components">PCA: Multiple Principal Components</h2>
<ul>
<li><p><strong>More than One:</strong> You can construct up to <em>p</em> distinct principal components.</p></li>
<li><p><strong>Second Principal Component:</strong> The second principal component (Z<sub>2</sub>) is:</p>
<ul>
<li>A linear combination of the variables.</li>
<li><em>Uncorrelated</em> with Z<sub>1</sub>.</li>
<li>Has the largest variance among all linear combinations uncorrelated with Z<sub>1</sub>.</li>
<li><em>Orthogonal</em> (perpendicular) to the first principal component.</li>
</ul></li>
<li><p><strong>Successive Components:</strong> Each subsequent principal component captures the maximum remaining variance, subject to being uncorrelated with the previous components.</p></li>
</ul>
</section>
<section id="pca-second-principal-component" class="level2">
<h2 class="anchored" data-anchor-id="pca-second-principal-component">PCA: Second Principal Component</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/6_17.png" class="img-fluid figure-img"></p>
<figcaption>Plots of the <em>second</em> principal component scores versus pop and ad.</figcaption>
</figure>
</div>
<ul>
<li><strong>Figure 6.17:</strong> Shows the <em>second</em> principal component scores (z<sub>i2</sub>) plotted against pop and ad.</li>
<li><strong>Weak Relationship:</strong> There‚Äôs very little relationship, indicating that the second principal component captures much less information than the first.</li>
</ul>
<section id="the-principal-components-regression-approach" class="level4">
<h4 class="anchored" data-anchor-id="the-principal-components-regression-approach">The Principal Components Regression Approach</h4>
<ul>
<li><p><strong>The Idea:</strong> Use the first <em>M</em> principal components (Z<sub>1</sub>, ‚Ä¶, Z<sub>M</sub>) as predictors in a linear regression model.</p></li>
<li><p><strong>Assumption:</strong> We assume that the directions in which X<sub>1</sub>, ‚Ä¶, X<sub>p</sub> show the most variation are the directions that are associated with Y.</p></li>
<li><p><strong>Potential for Improvement:</strong> If this assumption holds, PCR can outperform least squares, especially when <em>M &lt;&lt; p</em>, by reducing variance.</p></li>
</ul>
</section>
</section>
<section id="pcr-example" class="level2">
<h2 class="anchored" data-anchor-id="pcr-example">PCR: Example</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/6_18.png" class="img-fluid figure-img"></p>
<figcaption>PCR applied to two simulated datasets.</figcaption>
</figure>
</div>
<ul>
<li><p><strong>Figure 6.18:</strong> Shows PCR applied to the simulated datasets from Figures 6.8 and 6.9.</p></li>
<li><p>PCR with an appropriate choice of <em>M</em> can improve substantially over least squares.</p></li>
<li><p>However, in this example, PCR does <em>not</em> perform as well as ridge regression or the lasso. This is because the data were generated in a way that required many principal components to model the response well.</p></li>
</ul>
</section>
<section id="pcr-when-it-works-well" class="level2">
<h2 class="anchored" data-anchor-id="pcr-when-it-works-well">PCR: When it Works Well</h2>
<ul>
<li><strong>First Few Components are Key:</strong> PCR tends to work well when the first few principal components capture most of the variation in the predictors <em>and</em> the relationship with the response.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/6_19.png" class="img-fluid figure-img"></p>
<figcaption>PCR, ridge regression, and the lasso on a simulated dataset where the first five principal components of X contain all the information about the response Y.</figcaption>
</figure>
</div>
<ul>
<li><strong>Figure 6.19:</strong> Shows an example where the response depends only on the first five principal components.</li>
<li>PCR performs very well, achieving a low MSE with <em>M</em> = 5.</li>
<li>PCR and ridge regression slightly outperform the lasso in this case.</li>
</ul>
</section>
<section id="pcr-not-feature-selection" class="level2">
<h2 class="anchored" data-anchor-id="pcr-not-feature-selection">PCR: Not Feature Selection</h2>
<ul>
<li><p><strong>Linear Combinations:</strong> PCR is <em>not</em> a feature selection method. Each principal component is a linear combination of <em>all p</em> original features.</p></li>
<li><p><strong>Example:</strong> In the advertising data, Z<sub>1</sub> was a combination of <em>both</em> pop and ad.</p></li>
<li><p><strong>Relationship to Ridge Regression:</strong> PCR is more closely related to ridge regression than to the lasso.</p></li>
</ul>
</section>
<section id="pcr-choosing-m-and-standardization" class="level2">
<h2 class="anchored" data-anchor-id="pcr-choosing-m-and-standardization">PCR: Choosing M and Standardization</h2>
<ul>
<li><strong>Choosing M:</strong> The number of principal components (<em>M</em>) is typically chosen by cross-validation.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/6_20.png" class="img-fluid figure-img"></p>
<figcaption>PCR standardized coefficient estimates and cross-validation MSE on the <em>Credit</em> data.</figcaption>
</figure>
</div>
<ul>
<li><p><strong>Figure 6.20:</strong> Shows cross-validation for PCR on the <em>Credit</em> data.</p></li>
<li><p>The lowest cross-validation error occurs with <em>M</em> = 10, which is almost no dimension reduction.</p></li>
<li><p><strong>Standardization:</strong> It‚Äôs generally recommended to <em>standardize</em> each predictor before performing PCA. This ensures that all variables are on the same scale.</p></li>
</ul>
<section id="partial-least-squares-pls" class="level3">
<h3 class="anchored" data-anchor-id="partial-least-squares-pls">3.2 Partial Least Squares (PLS)</h3>
<ul>
<li><p><strong>Supervised Dimension Reduction:</strong> PLS is a <em>supervised</em> dimension reduction technique. Unlike PCR (which is unsupervised), PLS uses the <em>response</em> (Y) to help identify the new features (Z<sub>1</sub>, ‚Ä¶, Z<sub>M</sub>).</p></li>
<li><p><strong>Goal:</strong> Find directions that explain <em>both</em> the response <em>and</em> the predictors.</p></li>
</ul>
</section>
</section>
<section id="pls-computing-the-first-direction" class="level2">
<h2 class="anchored" data-anchor-id="pls-computing-the-first-direction">PLS: Computing the First Direction</h2>
<ol type="1">
<li><p><strong>Standardize Predictors:</strong> Standardize the <em>p</em> predictors.</p></li>
<li><p><strong>Simple Linear Regressions:</strong> Compute the coefficient from the simple linear regression of Y onto each X<sub>j</sub>.</p></li>
<li><p><strong>First PLS Direction:</strong> Set each œÜ<sub>j1</sub> in the equation for Z<sub>1</sub> (Equation 6.16) equal to this coefficient. This means PLS places the highest weight on variables that are most strongly related to the response.</p></li>
</ol>
</section>
<section id="pls-example" class="level2">
<h2 class="anchored" data-anchor-id="pls-example">PLS: Example</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/6_21.png" class="img-fluid figure-img"></p>
<figcaption>First PLS direction (solid line) and first PCR direction (dotted line) for the advertising data.</figcaption>
</figure>
</div>
<ul>
<li><strong>Figure 6.21:</strong> Shows the first PLS and PCR directions for a synthetic dataset with Sales as the response and Population Size and Advertising Spending as predictors.</li>
<li>PLS chooses a direction that emphasizes Population Size more than Advertising Spending, because Population Size is more correlated with Sales.</li>
</ul>
</section>
<section id="pls-subsequent-directions" class="level2">
<h2 class="anchored" data-anchor-id="pls-subsequent-directions">PLS: Subsequent Directions</h2>
<ul>
<li><strong>Iterative Process:</strong>
<ol type="1">
<li><strong>Adjust for Z<sub>1</sub>:</strong> Regress each variable on Z<sub>1</sub> and take the residuals. This removes the information already explained by Z<sub>1</sub>.</li>
<li><strong>Compute Z<sub>2</sub>:</strong> Compute Z<sub>2</sub> using these orthogonalized data, in the same way as Z<sub>1</sub> was computed.</li>
<li><strong>Repeat:</strong> Repeat this process <em>M</em> times to identify multiple PLS components (Z<sub>1</sub>, ‚Ä¶, Z<sub>M</sub>).</li>
</ol></li>
<li><strong>Final Model:</strong> Fit a linear model using Z<sub>1</sub>, ‚Ä¶, Z<sub>M</sub> as predictors, just like in PCR.</li>
</ul>
</section>
<section id="pls-tuning-parameter-and-standardization" class="level2">
<h2 class="anchored" data-anchor-id="pls-tuning-parameter-and-standardization">PLS: Tuning Parameter and Standardization</h2>
<ul>
<li><p><strong>Choosing M:</strong> The number of PLS directions (<em>M</em>) is a tuning parameter, typically chosen by cross-validation.</p></li>
<li><p><strong>Standardization:</strong> It‚Äôs generally recommended to standardize both the predictors and the response before performing PLS.</p></li>
<li><p><strong>Performance:</strong> In practice, PLS often performs no better than ridge regression or PCR. The supervised dimension reduction of PLS can reduce bias, but it also has the potential to increase variance.</p></li>
</ul>
</section>
<section id="considerations-in-high-dimensions" class="level2">
<h2 class="anchored" data-anchor-id="considerations-in-high-dimensions">4. Considerations in High Dimensions</h2>
<section id="high-dimensional-data" class="level3">
<h3 class="anchored" data-anchor-id="high-dimensional-data">4.1 High-Dimensional Data</h3>
<ul>
<li><p><strong>Low-Dimensional Setting:</strong> Most traditional statistical techniques are designed for the <em>low-dimensional</em> setting, where <em>n</em> (number of observations) is much greater than <em>p</em> (number of features).</p></li>
<li><p><strong>High-Dimensional Setting:</strong> In recent years, new technologies have led to a dramatic increase in the number of features that can be measured. We often encounter datasets where <em>p</em> is large, possibly even larger than <em>n</em>.</p></li>
<li><p><strong>Examples:</strong></p>
<ul>
<li><strong>Genomics:</strong> Measuring hundreds of thousands of single nucleotide polymorphisms (SNPs) to predict a trait.</li>
<li><strong>Marketing:</strong> Using all search terms entered by users of a search engine to understand online shopping patterns.</li>
</ul></li>
</ul>
</section>
<section id="what-goes-wrong-in-high-dimensions" class="level3">
<h3 class="anchored" data-anchor-id="what-goes-wrong-in-high-dimensions">4.2 What Goes Wrong in High Dimensions?</h3>
<ul>
<li><strong>Least Squares Fails:</strong> When <em>p</em> is as large as or larger than <em>n</em>, least squares <em>cannot</em> be used (or <em>should not</em> be used).</li>
<li><strong>Perfect Fit, But Useless:</strong> Least squares will always find a set of coefficients that perfectly fit the training data (zero residuals), <em>regardless</em> of whether there‚Äôs a true relationship between the features and the response.</li>
<li><strong>Overfitting:</strong> This perfect fit is a result of <em>overfitting</em>. The model is too flexible and captures noise in the data, leading to terrible performance on new data.</li>
<li><strong>Curse of Dimensionality:</strong> Adding more features, even if unrelated to response, can easily lead to the model overfitting the training data.</li>
</ul>
</section>
</section>
<section id="regression-in-high-dimensions" class="level2">
<h2 class="anchored" data-anchor-id="regression-in-high-dimensions">4.3 Regression in High Dimensions</h2>
<ul>
<li><strong>Less Flexible Methods:</strong> Many of the methods we‚Äôve discussed in this chapter ‚Äì forward stepwise selection, ridge regression, the lasso, and PCR ‚Äì are particularly useful in the high-dimensional setting.</li>
<li><strong>Avoiding Overfitting:</strong> These methods avoid overfitting by being <em>less flexible</em> than least squares.</li>
</ul>
</section>
<section id="regression-in-high-dimensions-example-with-the-lasso" class="level2">
<h2 class="anchored" data-anchor-id="regression-in-high-dimensions-example-with-the-lasso">Regression in High Dimensions: Example with the Lasso</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/6_24.png" class="img-fluid figure-img"></p>
<figcaption>The lasso performed with varying numbers of features (p) and a fixed sample size (n).</figcaption>
</figure>
</div>
<ul>
<li><p><strong>Figure 6.24</strong>: Lasso with n=100, p can be 20, 50 and 2000.</p></li>
<li><p>As the number of features increases, the test set error increases, highlights the curse of dimensionality.</p></li>
<li><p><strong>Three Key Points:</strong></p>
<ol type="1">
<li><strong>Regularization is Crucial:</strong> Regularization (or shrinkage) is essential in high-dimensional problems.</li>
<li><strong>Tuning Parameter Selection:</strong> Choosing the right tuning parameter (e.g., Œª for the lasso) is critical for good performance.</li>
<li><strong>Curse of Dimensionality:</strong> The test error tends to increase as the dimensionality of the problem increases, <em>unless</em> the additional features are truly associated with the response.</li>
</ol></li>
</ul>
<section id="interpreting-results-in-high-dimensions" class="level3">
<h3 class="anchored" data-anchor-id="interpreting-results-in-high-dimensions">4.4 Interpreting Results in High Dimensions</h3>
<ul>
<li><p><strong>Multicollinearity is Extreme:</strong> In the high-dimensional setting, multicollinearity is <em>extreme</em>. Any variable can be written as a linear combination of all the other variables.</p></li>
<li><p><strong>Cannot Identify True Predictors:</strong> This means we can <em>never</em> know exactly which variables (if any) are truly predictive of the outcome. We can only identify variables that are <em>correlated</em> with the true predictors.</p></li>
<li><p><strong>Caution in Reporting:</strong> Be very cautious when reporting results. Don‚Äôt overstate conclusions.</p></li>
<li><p><strong>Never Use Training Data for Evaluation:</strong> <em>Never</em> use training data measures (sum of squared errors, p-values, R<sup>2</sup>) as evidence of a good model fit in the high-dimensional setting. These measures will be misleadingly optimistic.</p></li>
<li><p><strong>Use Test Data or Cross-Validation:</strong> Always report results on an <em>independent test set</em> or using <em>cross-validation</em>.</p></li>
</ul>
</section>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<ul>
<li><p><strong>Beyond Least Squares:</strong> We‚Äôve explored several alternatives to least squares for linear regression:</p>
<ul>
<li>Subset selection (best subset, forward stepwise, backward stepwise)</li>
<li>Shrinkage methods (ridge regression, the lasso)</li>
<li>Dimension reduction methods (PCR, PLS)</li>
</ul></li>
<li><p><strong>Goals:</strong> These methods aim to improve:</p>
<ul>
<li>Prediction accuracy (by reducing variance, often at the cost of a small increase in bias)</li>
<li>Model interpretability (by selecting a subset of variables or shrinking coefficients)</li>
</ul></li>
<li><p><strong>High-Dimensional Data:</strong> These methods are particularly important in the high-dimensional setting (<em>p</em> ‚â• <em>n</em>), where least squares fails.</p></li>
<li><p><strong>Cross-Validation:</strong> Cross-validation is a powerful tool for selecting tuning parameters and estimating the test error of different models.</p></li>
<li><p>The choice of modeling method, the choice of tuning parameter, and the choice of assessment metrics all become especially important in high dimensions.</p></li>
</ul>
</section>
<section id="thoughts-and-discussion" class="level2">
<h2 class="anchored" data-anchor-id="thoughts-and-discussion">Thoughts and Discussion</h2>
<ul>
<li><p>Think about situations where you might encounter high-dimensional data. What are the challenges and opportunities?</p></li>
<li><p>How would you choose between the different methods we‚Äôve discussed (subset selection, ridge regression, the lasso, PCR, PLS)? What factors would you consider?</p></li>
<li><p>What are the ethical implications of using high-dimensional data for prediction, especially in sensitive areas like healthcare or finance? How can we mitigate potential biases and ensure fairness?</p></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/qiufei\.github\.io\/web-slide-r");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>üîã<a href="https://posit.co"><img src="https://posit.co/wp-content/themes/Posit/assets/images/posit-logo-2024.png" class="img-fluid" alt="Posit" width="65"></a></p>
</div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 ÈÇ±È£û ¬© 2025
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://beian.miit.gov.cn">
<p>ÊµôICPÂ§á 2024072710Âè∑-1</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33021202002511">
<p>ÊµôÂÖ¨ÁΩëÂÆâÂ§á 33021202002511Âè∑</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:hfutqiufei@163.com">
      <i class="bi bi-envelope-at-fill" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>