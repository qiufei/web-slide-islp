[
  {
    "objectID": "qmd/islp12.html",
    "href": "qmd/islp12.html",
    "title": "",
    "section": "",
    "text": "So far, most of this course has focused on supervised learning methods like regression and classification.\n\nSupervised Learning: We have a set of features (X1, X2, …, Xp) and a response variable (Y). The Goal is to predict Y using the Xs. We “teach” the algorithm.\nExamples: Linear regression, logistic regression, Support Vector Machine (SVM).\n\nNow, we’ll explore unsupervised learning.\n\nUnsupervised Learning: We only have features (X1, X2, …, Xp), without any response variable Y. No “teaching” or “supervision”.\nGoal: Discover interesting patterns and structure in the data, find relationships.\nExamples: Principal Component Analysis (PCA), Clustering."
  },
  {
    "objectID": "qmd/islp12.html#introduction-to-unsupervised-learning",
    "href": "qmd/islp12.html#introduction-to-unsupervised-learning",
    "title": "",
    "section": "",
    "text": "So far, most of this course has focused on supervised learning methods like regression and classification.\n\nSupervised Learning: We have a set of features (X1, X2, …, Xp) and a response variable (Y). The Goal is to predict Y using the Xs. We “teach” the algorithm.\nExamples: Linear regression, logistic regression, Support Vector Machine (SVM).\n\nNow, we’ll explore unsupervised learning.\n\nUnsupervised Learning: We only have features (X1, X2, …, Xp), without any response variable Y. No “teaching” or “supervision”.\nGoal: Discover interesting patterns and structure in the data, find relationships.\nExamples: Principal Component Analysis (PCA), Clustering."
  },
  {
    "objectID": "qmd/islp12.html#what-is-unsupervised-learning",
    "href": "qmd/islp12.html#what-is-unsupervised-learning",
    "title": "",
    "section": "What is Unsupervised Learning?",
    "text": "What is Unsupervised Learning?\n\nUnsupervised learning is a set of statistical tools designed for situations where we have only input features and no output or response variable. Since there’s no “correct answer” to guide the learning process, it’s called “unsupervised.” We’re exploring the data without a specific prediction task in mind.\nKey Goals:\n\nData Visualization: Find ways to represent the data to see patterns.\nDiscover Subgroups: Identify clusters or groups within the data or among the variables.\nData Pre-processing: Prepare data for supervised learning.\n\nWe will cover two major types:\n\nPrincipal Components Analysis (PCA): Used for visualization and pre-processing.\nClustering: Discovering unknown subgroups in data."
  },
  {
    "objectID": "qmd/islp12.html#supervised-vs.-unsupervised-learning",
    "href": "qmd/islp12.html#supervised-vs.-unsupervised-learning",
    "title": "",
    "section": "Supervised vs. Unsupervised Learning",
    "text": "Supervised vs. Unsupervised Learning\n\n\n\n\n\n\n\n\n\nFeature\nSupervised Learning\nUnsupervised Learning\n\n\n\n\nGoal\nPredict a response variable (Y)\nDiscover patterns and structure\n\n\nData\nFeatures (X) and response (Y)\nFeatures (X) only\n\n\nEvaluation\nClear metrics (e.g., accuracy, R-squared)\nMore subjective, harder to evaluate\n\n\nExamples\nRegression, classification\nPCA, clustering\n\n\n“Correct Answer”\nYes (the response variable)\nNo (no response variable)\n\n\n\n\n\n\n\n\n\nUnsupervised learning is often more challenging because there’s no straightforward way to check our work. It’s more exploratory and subjective. We don’t have a “ground truth” to compare our results against."
  },
  {
    "objectID": "qmd/islp12.html#applications-of-unsupervised-learning",
    "href": "qmd/islp12.html#applications-of-unsupervised-learning",
    "title": "",
    "section": "Applications of Unsupervised Learning",
    "text": "Applications of Unsupervised Learning\n\nUnsupervised learning is increasingly important in various fields:\n\nGenomics: A cancer researcher analyzes gene expression levels in patients. Unsupervised learning can help find subgroups of patients or genes, leading to a better understanding of the disease.\nE-commerce: An online shopping site identifies groups of shoppers with similar browsing and purchase histories. This allows for targeted recommendations, showing each shopper items they’re more likely to be interested in.\nSearch Engines: Search results can be customized based on the click histories of similar users.\nMarketing: Identify market segments (groups of customers) for targeted advertising.\n\nThese are just a few examples. Unsupervised learning is a powerful tool for extracting insights from data where there’s no predefined outcome."
  },
  {
    "objectID": "qmd/islp12.html#principal-component-analysis-pca",
    "href": "qmd/islp12.html#principal-component-analysis-pca",
    "title": "",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\nPrincipal Component Analysis (PCA) is a technique to reduce the dimensionality of data while retaining as much variability (information) as possible. It’s like finding the most important “directions” in your data.\n\nDimensionality Reduction: Simplifies data by finding a smaller set of representative variables (principal components).\nData Visualization: Allows us to visualize high-dimensional data in lower dimensions (e.g., 2D or 3D plots).\nUnsupervised: PCA only uses the features (X), not any response (Y).\nFeature Space Directions: Identifies the directions in the feature space along which the original data varies most.\nData Pre-processing: Can be used to create new, uncorrelated features for use in supervised learning (e.g., Principal Components Regression)."
  },
  {
    "objectID": "qmd/islp12.html#what-are-principal-components",
    "href": "qmd/islp12.html#what-are-principal-components",
    "title": "",
    "section": "What are Principal Components?",
    "text": "What are Principal Components?\n\nImagine you have many features (variables). PCA finds new variables, called principal components, that are linear combinations of the original features.\n\nFirst Principal Component (Z1): The normalized linear combination of features with the largest variance. It captures the most variability in the data.\n\nNormalized: The sum of the squared coefficients (loadings) equals 1. This prevents arbitrarily large variances.\nLoadings: The coefficients (φ) in the linear combination. They tell us how much each original feature contributes to the principal component.\n\nSubsequent Principal Components: Linear combinations that capture the most remaining variance, subject to being uncorrelated with previous components.\n\n\n\n\n\n\n\nFormula for the first principal component: \\[Z_1 = \\phi_{11}X_1 + \\phi_{21}X_2 + ... + \\phi_{p1}X_p\\] where \\(\\sum_{j=1}^{p} \\phi_{j1}^2 = 1\\) (normalization constraint)."
  },
  {
    "objectID": "qmd/islp12.html#geometric-interpretation-of-pca",
    "href": "qmd/islp12.html#geometric-interpretation-of-pca",
    "title": "",
    "section": "Geometric Interpretation of PCA",
    "text": "Geometric Interpretation of PCA\n\nThe principal component loading vectors define directions in the feature space.\n\nFirst Principal Component: The direction along which the data points vary the most.\nSecond Principal Component: The direction, orthogonal (perpendicular) to the first, that captures the next most variance. And so on.\n\nProjecting the original data points onto these directions gives us the principal component scores.\n\n\n\nalt text\n\n\n\n\n\n\n\n\nIn the figure, which displays an advertising data, the green solid line represents the first principal component direction. The blue dashed line represents the second. Since this is a 2D example, there are only two components."
  },
  {
    "objectID": "qmd/islp12.html#example-usarrests-data",
    "href": "qmd/islp12.html#example-usarrests-data",
    "title": "",
    "section": "Example: USArrests Data",
    "text": "Example: USArrests Data\n\nLet’s look at an example using the USArrests dataset, which contains crime statistics for each of the 50 US states:\n\nFeatures: Murder, Assault, UrbanPop, Rape (per 100,000 residents)\nGoal: Visualize the data and find patterns using PCA.\n\n\n\n\nFirst two principal components for the USArrests data.\n\n\n\n\n\n\n\n\nThis is a biplot. It shows both the principal component scores (blue state names) and the principal component loadings (orange arrows). For example, the loading for Rape on the first component is 0.54, and on the second component is 0.17. The word Rape is centered at the point (0.54, 0.17)."
  },
  {
    "objectID": "qmd/islp12.html#interpreting-the-usarrests-biplot",
    "href": "qmd/islp12.html#interpreting-the-usarrests-biplot",
    "title": "",
    "section": "Interpreting the USArrests Biplot",
    "text": "Interpreting the USArrests Biplot\n\n\nFirst Principal Component (PC1): Places roughly equal weight on Murder, Assault, and Rape, with much less weight on UrbanPop. This component represents overall crime rates.\nSecond Principal Component (PC2): Places most of its weight on UrbanPop, representing the level of urbanization.\nCorrelation: Murder, Assault, and Rape are close together, indicating they are correlated. UrbanPop is farther away, indicating it’s less correlated with the other three.\n\n\n\n\n\nPC1\nPC2\n\n\n\n\nMurder\n0.536\n-0.418\n\n\nAssault\n0.583\n-0.188\n\n\nUrbanPop\n0.278\n0.873\n\n\nRape\n0.543\n0.167\n\n\n\n\n\n\n\n\n\nThe table shows loading vectors. States with large positive scores on PC1 (e.g., California, Nevada, Florida) have high crime rates. States with large positive scores on PC2 (e.g., California) have high urbanization."
  },
  {
    "objectID": "qmd/islp12.html#another-interpretation-of-pca",
    "href": "qmd/islp12.html#another-interpretation-of-pca",
    "title": "",
    "section": "Another Interpretation of PCA",
    "text": "Another Interpretation of PCA\n\nBesides finding directions of greatest variance, PCA also finds linear surfaces that are closest to the data points.\n\nFirst Principal Component: The line in p-dimensional space closest to the n observations (in terms of average squared Euclidean distance).\nFirst Two Principal Components: The plane closest to the observations.\nAnd so on…\n\n\n\n\nNinety observations simulated in three dimensions.\n\n\n\n\n\n\n\n\nLeft: The first two principal component directions span the plane that best fits the data. Right: The first two principal component score vectors give the coordinates of the projection of the 90 observations onto the plane."
  },
  {
    "objectID": "qmd/islp12.html#proportion-of-variance-explained-pve",
    "href": "qmd/islp12.html#proportion-of-variance-explained-pve",
    "title": "",
    "section": "Proportion of Variance Explained (PVE)",
    "text": "Proportion of Variance Explained (PVE)\n\nHow much information is lost when we project data onto the first few principal components? We use the Proportion of Variance Explained (PVE) to measure this.\n\nTotal Variance: The sum of the variances of all the original features (assuming they are centered).\nVariance Explained by the m-th PC: The variance of the m-th principal component.\nPVE of the m-th PC: The proportion of the total variance explained by the m-th principal component.\n\n\n\n\n\n\n\nFormula for PVE of the m-th PC: \\[\\frac{\\sum_{i=1}^{n} z_{im}^2}{\\sum_{j=1}^{p} \\sum_{i=1}^{n} x_{ij}^2}\\]"
  },
  {
    "objectID": "qmd/islp12.html#pve-usarrests-example",
    "href": "qmd/islp12.html#pve-usarrests-example",
    "title": "",
    "section": "PVE: USArrests Example",
    "text": "PVE: USArrests Example\n\n\n\n\nScree plot and cumulative PVE plot for the USArrests data.\n\n\n\n\n\n\n\n\nLeft: A scree plot, showing the PVE of each principal component. Right: The cumulative PVE.\n\n\n\n\nPC1: Explains 62.0% of the variance.\nPC2: Explains 24.7% of the variance.\nTogether: PC1 and PC2 explain almost 87% of the variance.\n\nThis means that Figure 12.1 (the biplot) provides a good two-dimensional summary of the data. The scree plot helps us decide how many components to keep (look for an “elbow”)."
  },
  {
    "objectID": "qmd/islp12.html#scaling-the-variables",
    "href": "qmd/islp12.html#scaling-the-variables",
    "title": "",
    "section": "Scaling the Variables",
    "text": "Scaling the Variables\n\nBefore performing PCA, we usually scale the variables to have a standard deviation of one.\n\nWhy scale? If variables are measured in different units or have vastly different variances, the variables with the largest variances will dominate the principal components, regardless of whether they are actually the most important.\nException: If variables are measured in the same units (e.g., gene expression levels), we might not want to scale.\n\n\n\n\nEffect of scaling on the USArrests biplot.\n\n\n\n\n\n\n\n\nLeft: PCA with scaled variables (like Figure 12.1). Right: PCA with unscaled variables. Assault dominates the first PC because it has the highest variance."
  },
  {
    "objectID": "qmd/islp12.html#how-many-principal-components-to-use",
    "href": "qmd/islp12.html#how-many-principal-components-to-use",
    "title": "",
    "section": "How Many Principal Components to Use?",
    "text": "How Many Principal Components to Use?\n\nThere’s no single, definitive answer. It depends on the context and the data.\n\nScree Plot: Look for an “elbow” in the scree plot – a point where the PVE drops off significantly.\nInterpretation: Keep enough components to capture the interesting patterns in the data.\nAd Hoc: This process is inherently subjective.\nSupervised Learning: If PCA is used for pre-processing for supervised learning (e.g., Principal Components Regression), we can use cross-validation to select the number of components."
  },
  {
    "objectID": "qmd/islp12.html#clustering-methods",
    "href": "qmd/islp12.html#clustering-methods",
    "title": "",
    "section": "Clustering Methods",
    "text": "Clustering Methods\n\nClustering aims to find subgroups (clusters) within a dataset.\n\nGoal: Partition observations into groups so that observations within a group are similar, and observations in different groups are dissimilar.\nSimilarity: What does “similar” mean? This is a crucial, often domain-specific, consideration.\nUnsupervised: We’re looking for structure without a predefined outcome.\n\nWe’ll cover two main types:\n\nK-Means Clustering: Partitions data into a pre-specified number (K) of clusters.\nHierarchical Clustering: Builds a hierarchy of clusters, represented by a dendrogram."
  },
  {
    "objectID": "qmd/islp12.html#k-means-clustering",
    "href": "qmd/islp12.html#k-means-clustering",
    "title": "",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\n\nK-means clustering is a simple and widely used clustering algorithm.\n\nInput: A dataset and a desired number of clusters, K.\nOutput: Assigns each observation to exactly one of K clusters.\nGoal: Minimize the within-cluster variation.\n\n\n\n\nK-means clustering results on simulated data.\n\n\n\n\n\n\n\n\nThe results of applying K-means clustering with different values of K (the number of clusters) are presented. The color of each observation indicates the cluster to which it was assigned."
  },
  {
    "objectID": "qmd/islp12.html#the-k-means-algorithm",
    "href": "qmd/islp12.html#the-k-means-algorithm",
    "title": "",
    "section": "The K-Means Algorithm",
    "text": "The K-Means Algorithm\n\n\nInitialization: Randomly assign each observation to one of the K clusters.\nIteration: Repeat until the cluster assignments stop changing:\n\nCompute Centroids: For each cluster, calculate the centroid (the mean vector of the observations in that cluster).\nReassign Observations: Assign each observation to the cluster whose centroid is closest (usually using Euclidean distance).\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe algorithm is guaranteed to decrease the within-cluster variation at each step. It finds a local optimum, not necessarily the global optimum."
  },
  {
    "objectID": "qmd/islp12.html#k-means-an-illustrative-example",
    "href": "qmd/islp12.html#k-means-an-illustrative-example",
    "title": "",
    "section": "K-Means: An Illustrative Example",
    "text": "K-Means: An Illustrative Example\n\n\n\n\n\ngraph LR\n    A[Data] --&gt; B(Step 1: Randomly Assign Clusters);\n    B --&gt; C(Iteration 1, Step 2a: Compute Centroids);\n    C --&gt; D(Iteration 1, Step 2b: Reassign Observations);\n    D --&gt; E(Iteration 2, Step 2a: Compute Centroids);\n     E --&gt; F(Iteration 2, Step 2b: Reassign Observations);\n    F --&gt; G(Final Results);\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style G fill:#ccf,stroke:#333,stroke-width:4px\n\n\n\n\n\n\n\n\n\nProgress of the K-means algorithm."
  },
  {
    "objectID": "qmd/islp12.html#k-means-local-optima",
    "href": "qmd/islp12.html#k-means-local-optima",
    "title": "",
    "section": "K-Means: Local Optima",
    "text": "K-Means: Local Optima\n\nBecause K-means finds a local optimum, the results depend on the initial random assignment of observations to clusters.\n\nRecommendation: Run K-means multiple times with different initializations and choose the solution with the lowest within-cluster variation.\n\n\n\n\nK-means clustering with different initializations.\n\n\n\n\n\n\n\n\nNote\n\n\n\nK-means clustering was performed six times on the data, each with a different random assignment of observations. Three different local optima were obtained, one of which resulted in better separation between the clusters."
  },
  {
    "objectID": "qmd/islp12.html#hierarchical-clustering",
    "href": "qmd/islp12.html#hierarchical-clustering",
    "title": "",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\n\nHierarchical clustering builds a hierarchy of clusters, represented by a dendrogram.\n\nAdvantage: Doesn’t require pre-specifying the number of clusters (K). We can choose the number of clusters by “cutting” the dendrogram.\nDendrogram: A tree-like diagram that shows how clusters are merged.\nAgglomerative (Bottom-Up): Start with each observation as its own cluster, and successively merge the most similar clusters.\n\n\n\n\nDendrogram of hierarchically clustering the data."
  },
  {
    "objectID": "qmd/islp12.html#interpreting-a-dendrogram",
    "href": "qmd/islp12.html#interpreting-a-dendrogram",
    "title": "",
    "section": "Interpreting a Dendrogram",
    "text": "Interpreting a Dendrogram\n\n\nLeaves: Represent individual observations.\nFusions: As you move up the tree, leaves and branches fuse, representing the merging of similar clusters.\nHeight of Fusion: Indicates the dissimilarity between the merged clusters. Lower fusions mean more similar clusters.\nCutting the Dendrogram: A horizontal cut across the dendrogram gives a specific number of clusters. The height of the cut determines the number of clusters.\n\n\n\n\nInterpreting a dendrogram.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe figure illustrates how to properly interpret a dendrogram. Left: A dendrogram generated using Euclidean distance and complete linkage. Observations 5 and 7 are quite similar. Right: The raw data used to generate the dendrogram."
  },
  {
    "objectID": "qmd/islp12.html#the-hierarchical-clustering-algorithm",
    "href": "qmd/islp12.html#the-hierarchical-clustering-algorithm",
    "title": "",
    "section": "The Hierarchical Clustering Algorithm",
    "text": "The Hierarchical Clustering Algorithm\n\n\nInitialization: Start with each observation as its own cluster (n clusters). Calculate all pairwise dissimilarities (e.g., using Euclidean distance).\nIteration: For i = n, n-1, …, 2:\n\nFind Most Similar Clusters: Identify the two most similar clusters (least dissimilar).\nMerge Clusters: Fuse these two clusters into a single cluster. The dissimilarity between these clusters is the height in the dendrogram where they fuse.\nUpdate Dissimilarities: Calculate the new pairwise inter-cluster dissimilarities between the remaining i-1 clusters.\n\n\n\n\n\n\n\n\nNote\n\n\n\nKey Question: How do we define the dissimilarity between clusters (groups of observations), not just individual observations? This is where linkage comes in."
  },
  {
    "objectID": "qmd/islp12.html#linkage",
    "href": "qmd/islp12.html#linkage",
    "title": "",
    "section": "Linkage",
    "text": "Linkage\n\nLinkage defines the dissimilarity between two groups of observations.\n\n\n\n\n\n\n\nLinkage\nDescription\n\n\n\n\nComplete\nMaximal intercluster dissimilarity (dissimilarity between the most dissimilar points in the two clusters).\n\n\nSingle\nMinimal intercluster dissimilarity (dissimilarity between the most similar points in the two clusters).\n\n\nAverage\nMean intercluster dissimilarity (average dissimilarity between all pairs of points in the two clusters).\n\n\nCentroid\nDissimilarity between the centroids of the two clusters.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAverage and complete linkage are generally preferred. Centroid linkage can lead to undesirable inversions."
  },
  {
    "objectID": "qmd/islp12.html#choice-of-dissimilarity-measure",
    "href": "qmd/islp12.html#choice-of-dissimilarity-measure",
    "title": "",
    "section": "Choice of Dissimilarity Measure",
    "text": "Choice of Dissimilarity Measure\n\nBesides linkage, we also need to choose a dissimilarity measure between individual observations.\n\nEuclidean Distance: The most common choice. Measures the straight-line distance between two points.\nCorrelation-Based Distance: Considers two observations similar if their features are highly correlated, even if their values are far apart in terms of Euclidean distance.\n\nThe choice depends on the type of data and the scientific question.\n\n\n\nEuclidean vs. correlation-based distance."
  },
  {
    "objectID": "qmd/islp12.html#practical-issues-in-clustering",
    "href": "qmd/islp12.html#practical-issues-in-clustering",
    "title": "",
    "section": "Practical Issues in Clustering",
    "text": "Practical Issues in Clustering\n\n\nScaling: Should we scale the variables before clustering? (Usually yes, to give equal weight to each variable.)\nSmall Decisions, Big Consequences: Choices of dissimilarity measure, linkage, and scaling can have a large impact on the results.\nValidating Clusters: It’s hard to know if the clusters found are real or just an artifact of the clustering process.\nRobustness: Clustering methods are often not very robust to small changes in the data.\n\nRecommendation: Try different choices, look for consistent patterns, and be cautious about over-interpreting the results. Clustering should be a starting point for further investigation, not the final answer."
  },
  {
    "objectID": "qmd/islp12.html#summary",
    "href": "qmd/islp12.html#summary",
    "title": "",
    "section": "Summary",
    "text": "Summary\n\n\nUnsupervised learning is about finding patterns and structure in data without a response variable.\nPCA reduces dimensionality by finding linear combinations of features that capture the most variance. It’s useful for visualization and pre-processing.\nClustering aims to find subgroups within the data.\n\nK-means requires pre-specifying the number of clusters (K).\nHierarchical clustering builds a hierarchy of clusters, represented by a dendrogram.\n\nChoices of dissimilarity measure, linkage (for hierarchical clustering), and scaling can significantly affect clustering results.\nClustering is a powerful but often subjective and non-robust technique. It’s best used for exploration and hypothesis generation."
  },
  {
    "objectID": "qmd/islp12.html#thoughts-and-discussion",
    "href": "qmd/islp12.html#thoughts-and-discussion",
    "title": "",
    "section": "Thoughts and Discussion",
    "text": "Thoughts and Discussion\n\n\nCan you think of other real-world applications where unsupervised learning might be useful?\nWhat are the potential limitations of relying too heavily on clustering results without further validation?\nHow might you combine supervised and unsupervised learning techniques in a single analysis?\nHow do you understand the differences and connections between data mining, machine learning, and statistical learning?"
  },
  {
    "objectID": "qmd/islp9.html",
    "href": "qmd/islp9.html",
    "title": "",
    "section": "",
    "text": "In this chapter, we delve into Support Vector Machines (SVMs), a powerful set of supervised learning models used for classification and regression. Originating in the computer science community in the 1990s, SVMs have gained widespread popularity due to their effectiveness in various applications. They’re often considered one of the best “out-of-the-box” classifiers, meaning they perform well with minimal tuning."
  },
  {
    "objectID": "qmd/islp9.html#introduction",
    "href": "qmd/islp9.html#introduction",
    "title": "",
    "section": "",
    "text": "In this chapter, we delve into Support Vector Machines (SVMs), a powerful set of supervised learning models used for classification and regression. Originating in the computer science community in the 1990s, SVMs have gained widespread popularity due to their effectiveness in various applications. They’re often considered one of the best “out-of-the-box” classifiers, meaning they perform well with minimal tuning."
  },
  {
    "objectID": "qmd/islp9.html#key-concepts",
    "href": "qmd/islp9.html#key-concepts",
    "title": "",
    "section": "Key Concepts",
    "text": "Key Concepts\nBefore diving into SVM, let’s clarify some fundamental concepts:\n\n\n\nData Mining\n\nExtracting useful patterns, trends, and information from large datasets. It uses techniques from various fields, including statistics and computer science.\n\n\n\nMachine Learning\n\nA field of artificial intelligence that focuses on enabling computers to learn from data without being explicitly programmed. It involves algorithms that can improve their performance on a task as they are exposed to more data.\n\n\n\n\nStatistical Learning\n\nA subfield of statistics and machine learning. It focuses on developing and applying models and algorithms for prediction and inference, with an emphasis on statistical properties and interpretability."
  },
  {
    "objectID": "qmd/islp9.html#relationship-between-concepts",
    "href": "qmd/islp9.html#relationship-between-concepts",
    "title": "",
    "section": "Relationship Between Concepts",
    "text": "Relationship Between Concepts\n\n\n\n\n\ngraph LR\n    A[Data Mining] --&gt; C(Common Ground)\n    B[Machine Learning] --&gt; C\n    D[Statistical Learning] --&gt; C\n    C --&gt; E[Insights & Predictions]\n\n\n\n\n\n\n\n\nData Mining is the broadest field, encompassing the overall process of knowledge discovery.\nMachine Learning provides the algorithms that can learn from data.\nStatistical Learning provides a theoretical framework for understanding and improving these algorithms, focusing on statistical rigor."
  },
  {
    "objectID": "qmd/islp9.html#support-vector-machines-overview",
    "href": "qmd/islp9.html#support-vector-machines-overview",
    "title": "",
    "section": "Support Vector Machines: Overview",
    "text": "Support Vector Machines: Overview\n\nSVM is a generalization of a simpler classifier called the maximal margin classifier. We will explore the following concepts, each building upon the previous:\n\nMaximal Margin Classifier: The foundation, but limited to linearly separable data.\nSupport Vector Classifier: An extension allowing for some misclassifications (soft margin), making it applicable to a wider range of datasets.\nSupport Vector Machine: A further extension using kernels to handle non-linear class boundaries.\n\n\n\n\nNote: People often use “support vector machines” as a blanket term. We’ll be precise in distinguishing between the three concepts."
  },
  {
    "objectID": "qmd/islp9.html#maximal-margin-classifier",
    "href": "qmd/islp9.html#maximal-margin-classifier",
    "title": "",
    "section": "9.1 Maximal Margin Classifier",
    "text": "9.1 Maximal Margin Classifier\n\n9.1.1 What is a Hyperplane?\n\nA hyperplane is a flat affine subspace with a dimension one less than its ambient space. Think of it as a generalization of lines and planes:\n\nIn 2D: A hyperplane is a line.\nIn 3D: A hyperplane is a plane.\nIn p dimensions: A hyperplane is a (p-1)-dimensional flat subspace. It divides the space into two half-spaces.\n\n\nMathematical Definition:\nA hyperplane in p-dimensional space is defined by the equation:\n\\[\n\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p = 0\n\\]\n\n\n\\(X = (X_1, X_2, ..., X_p)^T\\) represents a point in p-dimensional space.\n\\(\\beta_0, \\beta_1, ..., \\beta_p\\) are the parameters (coefficients) of the hyperplane.\n\n\n\nHyperplane: Sides\n\nThe hyperplane equation divides the space:\n\n\\(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p &gt; 0\\): Points on one side.\n\\(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p &lt; 0\\): Points on the other side.\nThe sign of the left-hand side determines which side a point lies on.\n\n\n\n\n\nHyperplane in 2D\n\n\n\n\nFIGURE 9.1: The blue area indicates where 1 + 2X1 + 3X2 &gt; 0, and the purple area indicates where 1 + 2X1 + 3X2 &lt; 0.\n\n\n\n\n\n9.1.2 Classification Using a Separating Hyperplane\n\nSuppose we have training data with n observations and p features, falling into two classes (-1 and 1):\n\\[\nX = \\begin{bmatrix}\nx_{11} & \\cdots & x_{1p} \\\\\n\\vdots & \\ddots & \\vdots \\\\\nx_{n1} & \\cdots & x_{np}\n\\end{bmatrix},\n\\quad\ny = \\begin{bmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix},\n\\quad y_i \\in \\{-1, 1\\}\n\\]\nIf a separating hyperplane exists (meaning the classes are linearly separable), it satisfies:\n\\[\ny_i(\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_px_{ip}) &gt; 0\n\\]\nfor all i = 1, …, n. This means all points are classified correctly.\n\n\nClassification Rule\nWe can classify a new test observation \\(x^* = (x_1^*, x_2^*, ..., x_p^*)^T\\) using the sign of:\n\\[\nf(x^*) = \\beta_0 + \\beta_1x_1^* + \\beta_2x_2^* + \\dots + \\beta_px_p^*\n\\]\n\n\nIf \\(f(x^*)\\)&gt; 0, assign to class 1.\nIf \\(f(x^*)\\)&lt; 0, assign to class -1.\nThe magnitude of \\(f(x^*)\\) indicates the confidence of the classification. Larger magnitude means farther from the hyperplane and higher confidence.\n\n\n\n\nSeparating Hyperplanes (Visual)\n\n\n\nSeparating Hyperplanes\n\n\n\nFIGURE 9.2 Left: Multiple separating hyperplanes exist when data is linearly separable.\nFIGURE 9.2 Right: The decision boundary created by a separating hyperplane. The blue and purple grids show how test observations would be classified.\n\n\n\n\n9.1.3 The Maximal Margin Classifier\n\nIf the data is linearly separable, infinitely many separating hyperplanes exist. The maximal margin classifier chooses the hyperplane that maximizes the margin.\n\nMargin: The minimum distance from the hyperplane to any training observation. It’s like creating the widest possible “street” separating the classes.\nMaximal Margin Hyperplane: The separating hyperplane that achieves the largest margin.\n\n\n\nMaximal Margin Intuition\n\nThe maximal margin hyperplane is the “mid-line” of the widest “slab” (or “street”) that can be placed between the two classes without touching any data points. A larger margin on the training data is hoped to lead to a larger margin on the test data, and thus better classification.\n\n\n\nMaximal Margin Classifier (Visual)\n\n\n\nMaximal Margin Classifier\n\n\n\nFIGURE 9.3: The maximal margin hyperplane is shown as a solid line. The dashed lines define the margin. The points on the dashed lines are the support vectors.\n\n\n\n\nSupport Vectors\n\n\nSupport Vectors: The training observations that lie exactly on the margin (the dashed lines in the figure).\nThese points support the maximal margin hyperplane. If they move, the hyperplane moves.\nThe maximal margin hyperplane depends only on the support vectors, not on any other observations. This is a crucial property.\n\n\n\n\n9.1.4 Construction of the Maximal Margin Classifier\nThe maximal margin hyperplane is found by solving the following optimization problem:\n\\[\n\\begin{aligned}\n&\\underset{\\beta_0, \\beta_1, \\dots, \\beta_p, M}{\\text{maximize}} && M \\\\\n&\\text{subject to} && \\sum_{j=1}^p \\beta_j^2 = 1, \\\\\n& && y_i(\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_px_{ip}) \\geq M \\quad \\forall i = 1, \\dots, n.\n\\end{aligned}\n\\]\n\nM: The margin width, which we want to maximize.\n\\(\\sum \\beta_j^2 = 1\\): A constraint to ensure a unique solution. It doesn’t restrict the hyperplane itself, but it scales the coefficients.\n\\(y_i(\\dots) \\geq M\\): Ensures all observations are on the correct side of the hyperplane and at least a distance M away.\n\n\n\n9.1.5 The Non-separable Case\nThe maximal margin classifier works only if a separating hyperplane exists. If the classes overlap, no such hyperplane exists, and the optimization problem has no solution.\n\n\n\nNon-separable Data\n\n\n\nFIGURE 9.4: An example where the classes are not linearly separable."
  },
  {
    "objectID": "qmd/islp9.html#support-vector-classifiers",
    "href": "qmd/islp9.html#support-vector-classifiers",
    "title": "",
    "section": "9.2 Support Vector Classifiers",
    "text": "9.2 Support Vector Classifiers\n\n9.2.1 Overview of the Support Vector Classifier\nTo handle non-separable cases (and improve robustness even in separable cases), we introduce the support vector classifier, also known as the soft margin classifier. It allows some observations to be on the wrong side of the margin or even the wrong side of the hyperplane. This makes the margin “soft”.\n\nWhy Soft Margins?\n\nNon-Separable Data: Essential for overlapping classes.\nRobustness: Makes the classifier less sensitive to individual observations, reducing overfitting. A single outlier shouldn’t drastically change the decision boundary.\n\n\n\nSensitivity to Outliers (Visual)\n\n\n\nSensitivity to Outliers\n\n\n\nFIGURE 9.5: Adding a single outlier (right panel) dramatically changes the maximal margin hyperplane. This demonstrates the sensitivity of the maximal margin classifier.\n\n\n\nSoft Margin Example (Visual)\n\n\n\nSoft Margin Example\n\n\n\nFIGURE 9.6 Left: Most points are correctly classified and outside the margin. Some points violate the margin.\nFIGURE 9.6 Right: Some points are even misclassified (on the wrong side of the hyperplane).\n\n\n\n\n9.2.2 Details of the Support Vector Classifier\nThe support vector classifier solves a modified optimization problem:\n\\[\n\\begin{aligned}\n&\\underset{\\beta_0, \\beta_1, \\dots, \\beta_p, \\epsilon_1, \\dots, \\epsilon_n, M}{\\text{maximize}} && M \\\\\n&\\text{subject to} && \\sum_{j=1}^p \\beta_j^2 = 1, \\\\\n& && y_i(\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_px_{ip}) \\geq M(1 - \\epsilon_i), \\\\\n& && \\epsilon_i \\geq 0, \\quad \\sum_{i=1}^n \\epsilon_i \\leq C.\n\\end{aligned}\n\\]\n\n\\(\\epsilon_1, \\dots, \\epsilon_n\\): Slack variables. They allow observations to be on the wrong side of the margin or hyperplane.\nC: A non-negative tuning parameter (a “budget” for violations). It controls the trade-off between margin width and the number/severity of violations.\n\n\nSlack Variables Explained\n\n\\(\\epsilon_i = 0\\): The ith observation is on the correct side of the margin.\n\\(\\epsilon_i &gt; 0\\): The ith observation is on the wrong side of the margin (margin violation).\n\\(\\epsilon_i &gt; 1\\): The ith observation is on the wrong side of the hyperplane (misclassified).\n\n\n\nTuning Parameter C\n\nC = 0: No budget for violations. Reduces to the maximal margin classifier (if separable).\nSmall C: Narrow margin, fewer violations, potentially lower bias but higher variance (more overfitting).\nLarge C: Wider margin, more violations, potentially higher bias but lower variance (less overfitting).\nC is typically chosen using cross-validation.\n\n\n\nImpact of C (Visual)\n\n\n\nImpact of C\n\n\n\nFIGURE 9.7: Shows the effect of different C values. Larger C leads to a wider margin and more support vectors.\n\n\n\nSupport Vectors (Revisited)\nIn the support vector classifier, support vectors are the observations that:\n\nLie exactly on the margin (\\(\\epsilon_i = 0\\) and correctly classified)\nLie on the wrong side of the margin (\\(0 &lt; \\epsilon_i \\leq 1\\))\nLie on the wrong side of the hyperplane (\\(\\epsilon_i &gt; 1\\))\n\n\nOnly support vectors affect the hyperplane. Observations on the correct side of the margin and sufficiently far away have no influence. This contributes to the robustness of the SVM."
  },
  {
    "objectID": "qmd/islp9.html#support-vector-machines",
    "href": "qmd/islp9.html#support-vector-machines",
    "title": "",
    "section": "9.3 Support Vector Machines",
    "text": "9.3 Support Vector Machines\n\n9.3.1 Classification with Non-Linear Decision Boundaries\nThe support vector classifier finds linear decision boundaries. What if the true boundary is non-linear?\nOne approach: Enlarge the feature space using polynomial features (e.g., \\(X_1^2\\), \\(X_1X_2\\), etc.). This can make the data linearly separable in the enlarged space.\n\n\n\nNon-linear Data\n\n\n\nFIGURE 9.8 Left: Clearly non-linear boundary.\nFIGURE 9.8 Right: A linear classifier performs poorly.\n\nBut, explicitly enlarging the feature space can be computationally expensive (or even impossible).\n\n\n9.3.2 The Support Vector Machine\nThe support vector machine (SVM) extends the support vector classifier by using kernels to implicitly enlarge the feature space, without explicitly calculating the transformed features.\nKey Idea: The solution to the support vector classifier depends only on inner products of the observations, not the observations themselves.\nInner Product: \\(\\langle x_i, x_{i'} \\rangle = \\sum_{j=1}^p x_{ij}x_{i'j}\\).\n\nKernels\nA kernel is a function that quantifies the similarity between two observations:\n\\[\nK(x_i, x_{i'})\n\\]\n\nLinear Kernel: \\(K(x_i, x_{i'}) = \\langle x_i, x_{i'} \\rangle\\). This gives the standard support vector classifier.\nPolynomial Kernel: \\(K(x_i, x_{i'}) = (1 + \\langle x_i, x_{i'} \\rangle)^d\\).\nRadial Kernel: \\(K(x_i, x_{i'}) = \\exp(-\\gamma \\sum_{j=1}^p (x_{ij} - x_{i'j})^2)\\).\n\nBy replacing the inner product with a kernel in the support vector classifier algorithm, we get the SVM. The resulting decision function is:\n\\[\nf(x) = \\beta_0 + \\sum_{i \\in S} \\alpha_i K(x, x_i)\n\\]\nwhere S is the set of support vector indices.\n\n\nSVM with Polynomial Kernel (Visual)\n\n\n\nSVM with Polynomial Kernel\n\n\n\nFIGURE 9.9 Left: SVM with a polynomial kernel (degree 3) fits the non-linear data much better than a linear classifier.\n\n\n\nSVM with Radial Kernel (Visual)\n\n\n\nSVM with Radial Kernel\n\n\n\nFIGURE 9.9 Right: SVM with a radial kernel also captures the non-linear boundary well.\n\n\n\nRadial Kernel Intuition\n\nThe radial kernel has local behavior.\nIf a test observation \\(x^*\\) is far from a training observation \\(x_i\\), then \\(K(x^*, x_i)\\) is very small. This means \\(x_i\\) has little influence on the prediction for \\(x^*\\).\nOnly nearby training observations significantly affect the prediction.\n\\(\\gamma\\) controls the “reach” of the kernel.\n\n\n\nComputational Advantage of Kernels\nThe key advantage of kernels is that we only need to compute \\(K(x_i, x_{i'})\\) for all pairs of training observations. We never need to explicitly compute the (potentially infinite-dimensional) feature mapping. This makes the computation feasible even for very complex feature spaces.\n\n\n\n9.3.3 An Application to the Heart Disease Data\nThe text compares SVM to LDA on the Heart Disease Data, using ROC curves for both the training and testing data set.\n\n\n\nROC_train\n\n\n\nFIGURE 9.10 Left: The support vector classifier performs slightly better.\nFIGURE 9.10 Right: The SVM using a radial basis kernel with γ = 10-1 gives almost perfect fit on training set.\n\n\n\n\n\nROC_test\n\n\n\nFIGURE 9.11 Left: The support vector classifier still have slight advantage over LDA.\nFIGURE 9.11 Right: On test data, The SVM using a radial basis kernel with γ = 10-1, which had the best training performance, actually performs the worst. SVM with γ = 10-2 and γ = 10-3 is similar with support vector classifier."
  },
  {
    "objectID": "qmd/islp9.html#svms-with-more-than-two-classes",
    "href": "qmd/islp9.html#svms-with-more-than-two-classes",
    "title": "",
    "section": "9.4 SVMs with More than Two Classes",
    "text": "9.4 SVMs with More than Two Classes\nSVMs are naturally designed for binary classification. To extend them to K &gt; 2 classes, two common approaches are used:\n\nOne-versus-One: Construct (K choose 2) SVMs, each comparing a pair of classes. Classify a test observation by majority vote.\nOne-versus-All: Fit K SVMs, each comparing one class to the rest. Classify a test observation to the class with the highest decision function value."
  },
  {
    "objectID": "qmd/islp9.html#relationship-to-logistic-regression",
    "href": "qmd/islp9.html#relationship-to-logistic-regression",
    "title": "",
    "section": "9.5 Relationship to Logistic Regression",
    "text": "9.5 Relationship to Logistic Regression\nSVMs are closely related to logistic regression! The support vector classifier can be rewritten in a “Loss + Penalty” form:\n\\[\n\\underset{\\beta_0, \\beta_1, \\dots, \\beta_p}{\\text{minimize}} \\left\\{ \\sum_{i=1}^n \\max[0, 1 - y_i(\\beta_0 + \\beta_1x_{i1} + \\dots + \\beta_px_{ip})] + \\lambda \\sum_{j=1}^p \\beta_j^2 \\right\\}\n\\]\n\nThe loss function is called hinge loss.\nThe penalty term is a ridge penalty.\n\n\nHinge Loss vs. Logistic Regression Loss\n\n\n\nLoss compare\n\n\n\nFIGURE 9.12: Comparing the hinge loss (SVM) and the logistic regression loss. They are very similar.\nHinge Loss: exactly zero for correctly classified and far away observations.\nLogistic Regression Loss: never exactly zero, but can be close to zero.\n\n\nDue to the similarity of the loss functions, SVM and logistic regression often give similar results. SVM is better with well-separated classes, and logistic regression is often preferred when classes overlap."
  },
  {
    "objectID": "qmd/islp9.html#summary",
    "href": "qmd/islp9.html#summary",
    "title": "",
    "section": "Summary",
    "text": "Summary\n\nSupport Vector Machines (SVMs) are powerful classification tools.\nMaximal Margin Classifier: The foundation, for linearly separable data.\nSupport Vector Classifier (Soft Margin): Handles non-separable data and improves robustness.\nSupport Vector Machine (Kernel Trick): Handles non-linear boundaries efficiently.\nKernels: Implicitly map data to high-dimensional spaces.\nSupport Vectors: The key observations that define the decision boundary.\nTuning Parameters: C (soft margin) and kernel parameters (e.g., \\(\\gamma\\) for radial kernel) control the bias-variance trade-off.\nRelationship to Logistic Regression: SVMs are closely related, with hinge loss being similar to logistic regression’s loss."
  },
  {
    "objectID": "qmd/islp9.html#thoughts-and-discussion",
    "href": "qmd/islp9.html#thoughts-and-discussion",
    "title": "",
    "section": "Thoughts and Discussion",
    "text": "Thoughts and Discussion\n\nWhy are support vectors so important? What does this tell us about the robustness of SVMs?\nHow does the choice of kernel and its parameters affect the SVM’s decision boundary? Consider the radial kernel’s \\(\\gamma\\) parameter.\nWhen might you prefer logistic regression over an SVM, and vice-versa? Think about the characteristics of your data and the assumptions of each method.\nHow does the one-versus-one method compare to the one-versus-all approach? Think in computational complexity aspect and performance aspect.\nCan we apply the kernel trick to other linear models besides the support vector classifier? If so, how?"
  },
  {
    "objectID": "qmd/islp13.html",
    "href": "qmd/islp13.html",
    "title": "",
    "section": "",
    "text": "This chapter shifts focus from estimation and prediction to hypothesis testing, a key aspect of statistical inference.\nWe’ll explore the challenges of multiple testing: testing many null hypotheses simultaneously.\nGoal: Understand how to interpret results and avoid erroneously rejecting too many null hypotheses in a “big data” setting."
  },
  {
    "objectID": "qmd/islp13.html#introduction-multiple-testing",
    "href": "qmd/islp13.html#introduction-multiple-testing",
    "title": "",
    "section": "",
    "text": "This chapter shifts focus from estimation and prediction to hypothesis testing, a key aspect of statistical inference.\nWe’ll explore the challenges of multiple testing: testing many null hypotheses simultaneously.\nGoal: Understand how to interpret results and avoid erroneously rejecting too many null hypotheses in a “big data” setting."
  },
  {
    "objectID": "qmd/islp13.html#review-of-hypothesis-testing",
    "href": "qmd/islp13.html#review-of-hypothesis-testing",
    "title": "",
    "section": "Review of Hypothesis Testing",
    "text": "Review of Hypothesis Testing\n\n\nHypothesis testing provides a framework to answer “yes-or-no” questions using data.\nExamples of questions:\n\nIs a coefficient in linear regression equal to zero?\nIs there a difference in the expected blood pressure between a control and treatment group?"
  },
  {
    "objectID": "qmd/islp13.html#steps-in-hypothesis-testing",
    "href": "qmd/islp13.html#steps-in-hypothesis-testing",
    "title": "",
    "section": "Steps in Hypothesis Testing",
    "text": "Steps in Hypothesis Testing\n\n\nDefine Hypotheses:\n\nNull Hypothesis (H₀): The default assumption (e.g., no difference, no effect).\nAlternative Hypothesis (Hₐ): Contradicts the null (e.g., there is a difference).\n\nConstruct Test Statistic: A value summarizing evidence against H₀.\nCompute p-value: Probability of observing a test statistic as extreme or more extreme than the observed one, assuming H₀ is true.\nDecide: Based on the p-value, decide whether to reject H₀."
  },
  {
    "objectID": "qmd/islp13.html#defining-the-hypotheses",
    "href": "qmd/islp13.html#defining-the-hypotheses",
    "title": "",
    "section": "Defining the Hypotheses",
    "text": "Defining the Hypotheses\n\n\n\n\n\n\nNull Hypothesis (H₀): The baseline assumption, often representing “no effect” or “no difference.” We aim to find evidence against the null hypothesis.\n\n\n\n\n\n\n\n\n\nAlternative Hypothesis (Hₐ): Represents what we might suspect is true if H₀ is false. Often simply the negation of H₀ (e.g., if H₀ is “A=B”, then Hₐ is “A≠B”)."
  },
  {
    "objectID": "qmd/islp13.html#constructing-the-test-statistic",
    "href": "qmd/islp13.html#constructing-the-test-statistic",
    "title": "",
    "section": "Constructing the Test Statistic",
    "text": "Constructing the Test Statistic\n\nThe test statistic measures how much the data deviates from what’s expected under H₀.\nIts form depends on the specific hypothesis being tested.\nExample:\n\nTesting equality of means (μₜ = μc) between a treatment and control group: Use a two-sample t-statistic."
  },
  {
    "objectID": "qmd/islp13.html#two-sample-t-statistic",
    "href": "qmd/islp13.html#two-sample-t-statistic",
    "title": "",
    "section": "Two-Sample t-statistic",
    "text": "Two-Sample t-statistic\n\\[\nT = \\frac{\\hat{\\mu}_t - \\hat{\\mu}_c}{s \\sqrt{\\frac{1}{n_t} + \\frac{1}{n_c}}}\n\\]\nwhere\n\n$ _t, _c$: Sample means of treatment and control groups\n$ n_t, n_c $: Sample sizes of treatment and control groups\n$ s $: Pooled standard deviation\n\n\\[ s = \\sqrt{\\frac{(n_t - 1)s_t^2 + (n_c - 1)s_c^2}{n_t + n_c - 2}} \\] - \\(s_t^2\\) and \\(s_c^2\\) are unbiased estimators of the variance of the treatment and control groups, respectively. - A large (absolute) value of T provides evidence against Ho."
  },
  {
    "objectID": "qmd/islp13.html#computing-the-p-value",
    "href": "qmd/islp13.html#computing-the-p-value",
    "title": "",
    "section": "Computing the p-value",
    "text": "Computing the p-value\n\np-value: Probability of observing a test statistic as extreme as, or more extreme than, the one calculated from the data, assuming the null hypothesis is true.\nA small p-value indicates strong evidence against H₀.\n\n\n\n\n\n\n\nThe p-value is not the probability that H₀ is true. It’s the probability of the data (or more extreme data), given that H₀ is true."
  },
  {
    "objectID": "qmd/islp13.html#example-p-value-interpretation",
    "href": "qmd/islp13.html#example-p-value-interpretation",
    "title": "",
    "section": "Example: p-value Interpretation",
    "text": "Example: p-value Interpretation\n The density function for the N(0, 1) distribution, with the ver- tical line indicating a value of 2.33. 1% of the area under the curve falls to the right of the vertical line, so there is only a 2% chance of observing a N(0, 1) value that is greater than 2.33 or less than −2.33. Therefore, if a test statistic has a N(0, 1) null distribution, then an observed test statistic of T = 2.33 leads to a p-value of 0.02."
  },
  {
    "objectID": "qmd/islp13.html#null-distribution",
    "href": "qmd/islp13.html#null-distribution",
    "title": "",
    "section": "Null Distribution",
    "text": "Null Distribution\n\nThe distribution of the test statistic under H₀ is called the null distribution.\nCommon null distributions:\n\nNormal distribution\nt-distribution\nχ²-distribution\nF-distribution\n\nKnowing the null distribution allows us to calculate p-values."
  },
  {
    "objectID": "qmd/islp13.html#decision-making",
    "href": "qmd/islp13.html#decision-making",
    "title": "",
    "section": "Decision Making",
    "text": "Decision Making\n\nWe reject H₀ if the p-value is below a pre-defined significance level (α).\nSignificance level (α): The threshold for rejecting H₀ (commonly 0.05).\nIf p-value &lt; α, we reject H₀; otherwise, we fail to reject H₀.\n\n\n\n\n\n\n\nWe never “accept” H₀. We only “reject” or “fail to reject” it."
  },
  {
    "objectID": "qmd/islp13.html#type-i-and-type-ii-errors",
    "href": "qmd/islp13.html#type-i-and-type-ii-errors",
    "title": "",
    "section": "Type I and Type II Errors",
    "text": "Type I and Type II Errors\n\n\n\n\nH₀ True\nH₀ False\n\n\n\n\nReject H₀\nType I Error\nCorrect\n\n\nFail to Reject H₀\nCorrect\nType II Error\n\n\n\n\nType I error: Rejecting H₀ when it’s actually true (false positive). Probability = α.\nType II error: Failing to reject H₀ when it’s false (false negative).\nPower: Probability of correctly rejecting H₀ when it’s false (1 - Probability of Type II error)."
  },
  {
    "objectID": "qmd/islp13.html#the-challenge-of-multiple-testing",
    "href": "qmd/islp13.html#the-challenge-of-multiple-testing",
    "title": "",
    "section": "The Challenge of Multiple Testing",
    "text": "The Challenge of Multiple Testing\n\nWhen testing a single hypothesis, controlling the Type I error rate (α) is straightforward.\nProblem: When testing many hypotheses, the chance of making at least one Type I error increases dramatically, even if each individual test has a low α.\nAnalogy: Flipping many coins – even fair coins will show “all tails” eventually."
  },
  {
    "objectID": "qmd/islp13.html#illustration-the-stockbroker",
    "href": "qmd/islp13.html#illustration-the-stockbroker",
    "title": "",
    "section": "Illustration: The Stockbroker",
    "text": "Illustration: The Stockbroker\n\nA stockbroker claims to predict Apple’s stock price for 10 days.\nShe emails 1024 potential clients, each with a different prediction sequence (2¹⁰ = 1024 possibilities).\nOne client receives a perfect prediction! But it’s just due to chance, not skill.\nThis illustrates how making many “guesses” (tests) can lead to seemingly significant results purely by chance."
  },
  {
    "objectID": "qmd/islp13.html#family-wise-error-rate-fwer",
    "href": "qmd/islp13.html#family-wise-error-rate-fwer",
    "title": "",
    "section": "Family-Wise Error Rate (FWER)",
    "text": "Family-Wise Error Rate (FWER)\n\nFWER: The probability of making at least one Type I error among m hypothesis tests.\nIf all m tests are independent and all null hypotheses are true, then:\n\n\\[ \\text{FWER}(\\alpha) = 1 - (1 - \\alpha)^m \\]\n\nThis shows that FWER increases rapidly with m."
  },
  {
    "objectID": "qmd/islp13.html#fwer-example",
    "href": "qmd/islp13.html#fwer-example",
    "title": "",
    "section": "FWER Example",
    "text": "FWER Example\n\n\n\nFWER vs. Number of Hypotheses\n\n\nThe family-wise error rate, as a function of the number of hypotheses tested (displayed on the log scale), for three values of α: α = 0.05 (orange), α = 0.01 (blue), and α = 0.001 (purple). The dashed line indicates 0.05. For example, in order to control the FWER at 0.05 when testing m = 50 null hypotheses, we must control the Type I error for each null hypothesis at level α = 0.001."
  },
  {
    "objectID": "qmd/islp13.html#controlling-the-fwer",
    "href": "qmd/islp13.html#controlling-the-fwer",
    "title": "",
    "section": "Controlling the FWER",
    "text": "Controlling the FWER\n\nTo control the FWER at level α, we need stricter criteria for rejecting each individual hypothesis.\nCommon methods:\n\nBonferroni Correction: Reject H₀j if its p-value is less than α/m. Simple, but often too conservative (low power).\nHolm’s Method: A step-down procedure that’s less conservative than Bonferroni."
  },
  {
    "objectID": "qmd/islp13.html#bonferroni-correction",
    "href": "qmd/islp13.html#bonferroni-correction",
    "title": "",
    "section": "Bonferroni Correction",
    "text": "Bonferroni Correction\nFWER(α/m) ≤ m × α/m = α. - Advantages: - Simple - Guaranteed to control the FWER. - Disadvantage: - Very Conservative, may miss the true effect."
  },
  {
    "objectID": "qmd/islp13.html#holms-method",
    "href": "qmd/islp13.html#holms-method",
    "title": "",
    "section": "Holm’s Method",
    "text": "Holm’s Method\n\nSpecify α, the level at which to control the FWER.\nCompute p-values, p1, …, pm, for the m null hypotheses H01,…,H0m.\nOrder the m p-values so that p(1) ≤ p(2) ≤ … ≤ p(m).\nDefine \\[ L = \\min\\big\\{j:p_{(j)} &gt; \\frac{\\alpha}{m+1-j} \\big\\} \\]\nReject all null hypotheses H0j for which pj &lt; p(L).\n\nIt controls the FWER, but it is less conservative than Bonferroni.\nIt will reject more null hypotheses, typically resulting in fewer Type II errors and hence greater power.\nHolm will always reject more tests than Bonferroni"
  },
  {
    "objectID": "qmd/islp13.html#holms-method-illustration",
    "href": "qmd/islp13.html#holms-method-illustration",
    "title": "",
    "section": "Holm’s method illustration",
    "text": "Holm’s method illustration\n Each panel displays, for a separate simulation, the sorted p-values for tests of m = 10 null hypotheses. The p-values corresponding to the m0 = 2 true null hypotheses are displayed in black, and the rest are in red. When controlling the FWER at level 0.05, the Bonferroni procedure rejects all null hypotheses that fall below the black line, and the Holm procedure rejects all null hypotheses that fall below the blue line. The region between the blue and black lines indicates null hypotheses that are rejected using the Holm procedure but not using the Bonferroni procedure. In the center panel, the Holm procedure rejects one more null hypothesis than the Bonferroni procedure. In the right-hand panel, it rejects five more null hypotheses."
  },
  {
    "objectID": "qmd/islp13.html#other-fwer-control-methods",
    "href": "qmd/islp13.html#other-fwer-control-methods",
    "title": "",
    "section": "Other FWER Control Methods",
    "text": "Other FWER Control Methods\n\nTukey’s Method: For comparing all pairs of group means.\nScheffé’s Method: For testing any linear combination of group means after looking at the data.\nThese methods are more powerful than Bonferroni/Holm in specific situations, but less general."
  },
  {
    "objectID": "qmd/islp13.html#trade-off-fwer-and-power",
    "href": "qmd/islp13.html#trade-off-fwer-and-power",
    "title": "",
    "section": "Trade-Off: FWER and Power",
    "text": "Trade-Off: FWER and Power\n In a simulation setting in which 90% of the m null hypotheses are true, we display the power (the fraction of false null hypotheses that we successfully reject) as a function of the family-wise error rate. The curves correspond to m = 10 (orange), m = 100 (blue), and m = 500 (purple). As the value of m increases, the power decreases. The vertical dashed line indicates a FWER of 0.05. - Controlling FWER strictly (low α) reduces power (increases Type II errors). - When m is large, controlling FWER can make it very difficult to reject any null hypotheses, even false ones."
  },
  {
    "objectID": "qmd/islp13.html#false-discovery-rate-fdr",
    "href": "qmd/islp13.html#false-discovery-rate-fdr",
    "title": "",
    "section": "False Discovery Rate (FDR)",
    "text": "False Discovery Rate (FDR)\n\nFDR: The expected proportion of false positives among all rejected null hypotheses.\n\n\\[ \\text{FDR} = E\\left(\\frac{\\text{Number of Type I Errors}}{\\text{Number of Rejected Hypotheses}}\\right) = E\\left(\\frac{V}{R}\\right) \\]\n\nControlling FDR is less stringent than controlling FWER. It allows for some false positives, but aims to keep their proportion low.\nMotivation: In exploratory analyses with large m, we might be willing to tolerate some false positives to discover more true effects."
  },
  {
    "objectID": "qmd/islp13.html#benjamini-hochberg-bh-procedure",
    "href": "qmd/islp13.html#benjamini-hochberg-bh-procedure",
    "title": "",
    "section": "Benjamini-Hochberg (BH) Procedure",
    "text": "Benjamini-Hochberg (BH) Procedure\nA simple procedure to control the FDR at a desired level, q:\n\nSpecify q, the level at which to control the FDR.\nCompute p-values, p1, …, pm, for the m null hypotheses H01,…,H0m.\nOrder the m p-values so that p(1) ≤ p(2) ≤ … ≤ p(m).\nDefine \\[L = \\max\\{j : p_{(j)} \\leq qj/m\\}.\\]\nReject all null hypotheses H0j for which pj ≤ p(L).\n\n\nGuarantees FDR ≤ q under independence (or mild dependence) of p-values."
  },
  {
    "objectID": "qmd/islp13.html#bh-procedure-key-idea",
    "href": "qmd/islp13.html#bh-procedure-key-idea",
    "title": "",
    "section": "BH Procedure: Key Idea",
    "text": "BH Procedure: Key Idea\n\nThe threshold for rejecting a hypothesis depends on all the p-values (through L), not just its own.\nThis makes it less conservative than Bonferroni/Holm for FWER control."
  },
  {
    "objectID": "qmd/islp13.html#example-bh-and-bonferroni",
    "href": "qmd/islp13.html#example-bh-and-bonferroni",
    "title": "",
    "section": "Example: BH and Bonferroni",
    "text": "Example: BH and Bonferroni\n\n\n\nBH and Bonferroni\n\n\nEach panel displays the same set of m = 2,000 ordered p-values for the Fund data. The green lines indicate the p-value thresholds corresponding to FWER control, via the Bonferroni procedure, at levels α = 0.05 (left), α = 0.1 (center), and α = 0.3 (right). The orange lines indicate the p-value thresholds corresponding to FDR control, via Benjamini-Hochberg, at levels q = 0.05 (left), q = 0.1 (center), and q = 0.3 (right). When the FDR is controlled at level q = 0.1, 146 null hypotheses are rejected (center); the corresponding p-values are shown in blue. When the FDR is controlled at level q = 0.3, 279 null hypotheses are rejected (right); the corresponding p-values are shown in blue."
  },
  {
    "objectID": "qmd/islp13.html#re-sampling-approaches",
    "href": "qmd/islp13.html#re-sampling-approaches",
    "title": "",
    "section": "Re-Sampling Approaches",
    "text": "Re-Sampling Approaches\n\nSo far, we assumed we know the theoretical null distribution of the test statistic (e.g., t-distribution).\nProblem: Sometimes, the theoretical null distribution is unknown or assumptions are violated.\nSolution: Use re-sampling (e.g., permutation tests) to approximate the null distribution."
  },
  {
    "objectID": "qmd/islp13.html#permutation-test-example",
    "href": "qmd/islp13.html#permutation-test-example",
    "title": "",
    "section": "Permutation Test: Example",
    "text": "Permutation Test: Example\n\nTesting equality of means between two groups (X and Y).\nIf H₀ is true (and distributions of X and Y are the same), then randomly swapping observations between groups shouldn’t change the distribution of the test statistic.\nWe can permute the data many times, calculate the test statistic each time, and build an empirical null distribution."
  },
  {
    "objectID": "qmd/islp13.html#permutation-test-steps",
    "href": "qmd/islp13.html#permutation-test-steps",
    "title": "",
    "section": "Permutation Test: Steps",
    "text": "Permutation Test: Steps\n\nCalculate the test statistic (e.g., t-statistic) on the original data.\nRepeatedly (many times):\n\nRandomly permute the observations between groups.\nCalculate the test statistic on the permuted data.\n\nThe p-value is the proportion of permuted test statistics that are as extreme or more extreme than the original test statistic."
  },
  {
    "objectID": "qmd/islp13.html#resampling-for-fdr-control",
    "href": "qmd/islp13.html#resampling-for-fdr-control",
    "title": "",
    "section": "Resampling for FDR Control",
    "text": "Resampling for FDR Control\n\nWe can also use re-sampling to estimate the FDR directly, without calculating p-values first.\nThe key idea is to approximate:\n\nE(V) (expected number of false positives) by permuting the data and counting rejections under the null.\nR (total number of rejections) from the original data. \\[FDR = E(\\frac{V}{R}) \\approx \\frac{E(V)}{R} \\]\n\nUsing re-sampling to estimate the null distribution can pool information from multiple hypothesis."
  },
  {
    "objectID": "qmd/islp13.html#when-is-re-sampling-useful",
    "href": "qmd/islp13.html#when-is-re-sampling-useful",
    "title": "",
    "section": "When is Re-Sampling Useful?",
    "text": "When is Re-Sampling Useful?\n\nWhen the theoretical null distribution is unknown or unreliable.\nWhen assumptions for the theoretical distribution are violated (e.g., non-normality, small sample size).\nProvides a flexible way to perform hypothesis testing in complex situations."
  },
  {
    "objectID": "qmd/islp13.html#re-sampling-example",
    "href": "qmd/islp13.html#re-sampling-example",
    "title": "",
    "section": "Re-Sampling Example",
    "text": "Re-Sampling Example\n The 11th gene in the Khan dataset has a test statistic of T = −2.09. Its theoretical and re-sampling null distributions are almost identical. The theoretical p-value equals 0.041 and the re-sampling p-value equals 0.042.\n The 877th gene in the Khan dataset has a test statistic of T = −0.57. Its theoretical and re-sampling null distributions are quite different. The theoretical p-value equals 0.571, and the re-sampling p-value equals 0.673."
  },
  {
    "objectID": "qmd/islp13.html#summary",
    "href": "qmd/islp13.html#summary",
    "title": "",
    "section": "Summary",
    "text": "Summary\n\nMultiple testing poses challenges: increased risk of false positives.\nFWER control is strict (Bonferroni, Holm), but can have low power.\nFDR control is less stringent, allowing for more discoveries.\nRe-sampling methods provide a flexible way to perform inference when theoretical null distributions are unavailable or unreliable."
  },
  {
    "objectID": "qmd/islp13.html#thoughts-for-discussion",
    "href": "qmd/islp13.html#thoughts-for-discussion",
    "title": "",
    "section": "Thoughts for Discussion 🤔",
    "text": "Thoughts for Discussion 🤔\n\nHow do you choose between controlling FWER and FDR in practice?\nWhat are the trade-offs involved?\nCan you think of real-world scenarios where multiple testing is crucial?\nWhen would you prefer using resampling approach?"
  },
  {
    "objectID": "qmd/islp8.html",
    "href": "qmd/islp8.html",
    "title": "",
    "section": "",
    "text": "This chapter introduces tree-based methods for regression and classification. These methods involve segmenting the predictor space into simpler regions.\n\n\n\n\n\n\nNote\n\n\n\nTree-based methods are simple, interpretable, but often not as accurate as other methods. We’ll explore techniques like bagging, random forests, and boosting to improve their performance."
  },
  {
    "objectID": "qmd/islp8.html#introduction-tree-based-methods",
    "href": "qmd/islp8.html#introduction-tree-based-methods",
    "title": "",
    "section": "",
    "text": "This chapter introduces tree-based methods for regression and classification. These methods involve segmenting the predictor space into simpler regions.\n\n\n\n\n\n\nNote\n\n\n\nTree-based methods are simple, interpretable, but often not as accurate as other methods. We’ll explore techniques like bagging, random forests, and boosting to improve their performance."
  },
  {
    "objectID": "qmd/islp8.html#core-concepts",
    "href": "qmd/islp8.html#core-concepts",
    "title": "",
    "section": "Core Concepts",
    "text": "Core Concepts\nBefore diving into the detail, Let’s clarify some core concepts.\n\nData MiningMachine LearningStatistical Learning\n\n\nData mining is the process of discovering patterns, anomalies, and insights from large datasets using a combination of methods from statistics, machine learning, and database management. It’s about extracting useful knowledge from raw data. 🔎\n\n\nMachine learning is a subfield of artificial intelligence (AI) that focuses on developing algorithms that allow computers to learn from data, without being explicitly programmed. It involves building models that can make predictions or decisions based on input data.\n\n\nStatistical learning refers to a set of tools for modeling and understanding complex datasets. It is a recently developed area in statistics and blends with parallel developments in computer science, and in particular machine learning. The tools of statistical learning are both conceptual and practical."
  },
  {
    "objectID": "qmd/islp8.html#relationships-of-concepts",
    "href": "qmd/islp8.html#relationships-of-concepts",
    "title": "",
    "section": "Relationships of Concepts",
    "text": "Relationships of Concepts\n\n\n\n\n\ngraph LR\n    A[Data Mining] --&gt; C(Common Ground)\n    B[Machine Learning] --&gt; C\n    D[Statistical Learning] --&gt; C\n    C --&gt; E[Insights & Predictions]\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nData mining, machine learning, and statistical learning all overlap. They’re all about extracting insights and making predictions from data, but with different emphases and approaches."
  },
  {
    "objectID": "qmd/islp8.html#decision-trees-the-basics",
    "href": "qmd/islp8.html#decision-trees-the-basics",
    "title": "",
    "section": "Decision Trees: The Basics 🌲",
    "text": "Decision Trees: The Basics 🌲\nDecision trees segment the predictor space using a series of splitting rules, summarized in a tree structure. They can be used for both regression and classification.\n\nSimple Interpretation: Easy to understand and visualize.\nNon-linear Relationships: Can capture complex non-linear patterns."
  },
  {
    "objectID": "qmd/islp8.html#regression-trees-a-simple-example",
    "href": "qmd/islp8.html#regression-trees-a-simple-example",
    "title": "",
    "section": "Regression Trees: A Simple Example",
    "text": "Regression Trees: A Simple Example\nLet’s start with a regression tree example using the Hitters dataset to predict a baseball player’s salary.\n\nPredictors: Years (in the major leagues), Hits (in the previous year).\nResponse: Log Salary (to achieve a more bell-shaped distribution)."
  },
  {
    "objectID": "qmd/islp8.html#regression-tree-for-hitters-data",
    "href": "qmd/islp8.html#regression-tree-for-hitters-data",
    "title": "",
    "section": "Regression Tree for Hitters Data",
    "text": "Regression Tree for Hitters Data\n\n\n\nRegression tree for Hitters data\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis tree predicts log(Salary) based on Years and Hits. The numbers in the leaves are the mean log(Salary) for players in that region."
  },
  {
    "objectID": "qmd/islp8.html#interpreting-the-regression-tree",
    "href": "qmd/islp8.html#interpreting-the-regression-tree",
    "title": "",
    "section": "Interpreting the Regression Tree",
    "text": "Interpreting the Regression Tree\n\nTop Split: Years &lt; 4.5 is the most important factor. Less experience generally means lower salary.\nInternal Nodes: Represent splits in the predictor space (e.g., Hits &lt; 117.5).\nBranches: Connect the nodes.\nTerminal Nodes (Leaves): Represent the final predicted values (mean log(Salary) for that region).\nExample: The left branch means If a player played less than 4.5 years, his predicted log salary is 5.11."
  },
  {
    "objectID": "qmd/islp8.html#regions-of-predictor-space",
    "href": "qmd/islp8.html#regions-of-predictor-space",
    "title": "",
    "section": "Regions of Predictor Space",
    "text": "Regions of Predictor Space\n\n\n\nThree-region partition\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe tree divides the predictor space (Years, Hits) into three rectangular regions (R1, R2, R3). Each region corresponds to a leaf in the tree."
  },
  {
    "objectID": "qmd/islp8.html#building-a-regression-tree-key-steps",
    "href": "qmd/islp8.html#building-a-regression-tree-key-steps",
    "title": "",
    "section": "Building a Regression Tree: Key Steps",
    "text": "Building a Regression Tree: Key Steps\n\nDivide Predictor Space: Split the space into J distinct, non-overlapping regions (R1, R2, …, RJ).\nPrediction: For every observation in region Rj, predict the mean of the response values of the training observations in Rj."
  },
  {
    "objectID": "qmd/islp8.html#constructing-the-regions-minimizing-rss",
    "href": "qmd/islp8.html#constructing-the-regions-minimizing-rss",
    "title": "",
    "section": "Constructing the Regions: Minimizing RSS",
    "text": "Constructing the Regions: Minimizing RSS\nThe goal is to find regions (R1, …, RJ) that minimize the Residual Sum of Squares (RSS):\n\\[\n\\text{RSS} = \\sum_{j=1}^{J} \\sum_{i \\in R_j} (y_i - \\hat{y}_{R_j})^2\n\\]\n\n\\(\\hat{y}_{R_j}\\): Mean response for training observations in region Rj.\n\n\n\n\n\n\n\nTip\n\n\n\nFinding the absolute best set of regions is computationally infeasible, So a top-down, greedy approach called recursive binary splitting is used."
  },
  {
    "objectID": "qmd/islp8.html#recursive-binary-splitting",
    "href": "qmd/islp8.html#recursive-binary-splitting",
    "title": "",
    "section": "Recursive Binary Splitting",
    "text": "Recursive Binary Splitting\n\nTop-Down: Start with all observations in one region.\nGreedy: At each step, make the best split at that moment, without looking ahead.\nBinary: Each split divides a region into two sub-regions."
  },
  {
    "objectID": "qmd/islp8.html#recursive-binary-splitting-the-process",
    "href": "qmd/islp8.html#recursive-binary-splitting-the-process",
    "title": "",
    "section": "Recursive Binary Splitting: The Process",
    "text": "Recursive Binary Splitting: The Process\n\nSelect Predictor and Cutpoint: Choose the predictor (Xj) and cutpoint (s) that lead to the greatest possible reduction in RSS when splitting into regions {X|Xj &lt; s} and {X|Xj ≥ s}. Consider all predictors and all possible cutpoints.\nMinimize: Find j and s that minimize:\n\\[\n\\sum_{i: x_i \\in R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 + \\sum_{i: x_i \\in R_2(j,s)} (y_i - \\hat{y}_{R_2})^2\n\\]\nRepeat: on one of the resulting regions, continuing until a stopping criterion is met (e.g., no region has more than 5 observations)."
  },
  {
    "objectID": "qmd/islp8.html#visualizing-recursive-binary-splitting",
    "href": "qmd/islp8.html#visualizing-recursive-binary-splitting",
    "title": "",
    "section": "Visualizing Recursive Binary Splitting",
    "text": "Visualizing Recursive Binary Splitting\n\n\n\nRecursive binary splitting\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe top right shows the result of recursive binary splitting in two dimensions. The bottom left shows the corresponding tree. The bottom right presents a perspective plot of prediction surface."
  },
  {
    "objectID": "qmd/islp8.html#tree-pruning",
    "href": "qmd/islp8.html#tree-pruning",
    "title": "",
    "section": "Tree Pruning ✂️",
    "text": "Tree Pruning ✂️\nThe initial tree-building process often leads to overfitting. A smaller tree with fewer splits can have:\n\nLower variance\nBetter interpretability\n\n\n\n\n\n\n\nTip\n\n\n\nCost complexity pruning (weakest link pruning) is a technique to find the best subtree."
  },
  {
    "objectID": "qmd/islp8.html#cost-complexity-pruning",
    "href": "qmd/islp8.html#cost-complexity-pruning",
    "title": "",
    "section": "Cost Complexity Pruning",
    "text": "Cost Complexity Pruning\n\nGoal: Find a subtree T (subset of the full tree T0) that minimizes a penalized RSS:\n\\[\n\\sum_{m=1}^{|T|} \\sum_{x_i \\in R_m} (y_i - \\hat{y}_{R_m})^2 + \\alpha|T|\n\\]\n|T|: Number of terminal nodes.\nα: Tuning parameter (controls the trade-off between subtree complexity and fit).\n\n\n\n\n\n\n\nNote\n\n\n\nAs α increases, the penalty for having many terminal nodes increases, leading to smaller subtrees."
  },
  {
    "objectID": "qmd/islp8.html#algorithm-8.1-building-a-regression-tree",
    "href": "qmd/islp8.html#algorithm-8.1-building-a-regression-tree",
    "title": "",
    "section": "Algorithm 8.1 Building a Regression Tree",
    "text": "Algorithm 8.1 Building a Regression Tree\n\nGrow a Large Tree: Using recursive binary splitting, grow a large tree on the training data. Stop when each terminal node has less than some pre-specified minimum number of observations.\nCost Complexity Pruning: Apply cost complexity pruning to the large tree to get a sequence of best subtrees, as a function of α.\nChoose α: Use K-fold cross-validation to choose the optimal α.\n\nDivide training data into K folds.\nFor each fold, repeat steps 1 & 2 on the other K-1 folds.\nEvaluate mean squared prediction error on the held-out fold.\nAverage the results and pick α that minimizes the average error.\n\nReturn Subtree: Return the subtree from Step 2 that corresponds to the chosen α."
  },
  {
    "objectID": "qmd/islp8.html#classification-trees",
    "href": "qmd/islp8.html#classification-trees",
    "title": "",
    "section": "Classification Trees",
    "text": "Classification Trees\nVery similar to regression trees, but used to predict a qualitative response.\n\nPrediction: Predict the most commonly occurring class in the region.\nInterpretation: Consider class proportions in each region."
  },
  {
    "objectID": "qmd/islp8.html#growing-a-classification-tree",
    "href": "qmd/islp8.html#growing-a-classification-tree",
    "title": "",
    "section": "Growing a Classification Tree",
    "text": "Growing a Classification Tree\n\nRecursive binary splitting is used, but RSS cannot be used as the splitting criterion.\nAlternatives to RSS:\n\nClassification error rate\nGini index\nEntropy"
  },
  {
    "objectID": "qmd/islp8.html#splitting-criteria-classification-error-rate",
    "href": "qmd/islp8.html#splitting-criteria-classification-error-rate",
    "title": "",
    "section": "Splitting Criteria: Classification Error Rate",
    "text": "Splitting Criteria: Classification Error Rate\n\\[\nE = 1 - \\max_k (\\hat{p}_{mk})\n\\]\n\n\\(\\hat{p}_{mk}\\): Proportion of training observations in the mth region that are from the kth class.\nProblem: Not sensitive enough for tree growing."
  },
  {
    "objectID": "qmd/islp8.html#splitting-criteria-gini-index",
    "href": "qmd/islp8.html#splitting-criteria-gini-index",
    "title": "",
    "section": "Splitting Criteria: Gini Index",
    "text": "Splitting Criteria: Gini Index\n\\[\nG = \\sum_{k=1}^{K} \\hat{p}_{mk}(1 - \\hat{p}_{mk})\n\\]\n\nMeasure of total variance across K classes.\nSmall value if all \\(\\hat{p}_{mk}\\) are close to 0 or 1 (node purity)."
  },
  {
    "objectID": "qmd/islp8.html#splitting-criteria-entropy",
    "href": "qmd/islp8.html#splitting-criteria-entropy",
    "title": "",
    "section": "Splitting Criteria: Entropy",
    "text": "Splitting Criteria: Entropy\n\\[\nD = -\\sum_{k=1}^{K} \\hat{p}_{mk} \\log \\hat{p}_{mk}\n\\] - Takes a value near zero if the node is pure. - Gini index and entropy are numerically similar.\n\n\n\n\n\n\nTip\n\n\n\nGini index and entropy are preferred over the classification error rate for growing the tree because they are more sensitive to node purity. For pruning, any of the three can be used, but classification error is often preferred for final prediction accuracy."
  },
  {
    "objectID": "qmd/islp8.html#example-heart-data",
    "href": "qmd/islp8.html#example-heart-data",
    "title": "",
    "section": "Example: Heart Data",
    "text": "Example: Heart Data\n\n\n\nHeart data unpruned tree\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis figure shows an unpruned tree for classifying heart disease presence (Yes/No) based on 13 predictors. Qualitative predictors can be handled directly."
  },
  {
    "objectID": "qmd/islp8.html#trees-vs.-linear-models",
    "href": "qmd/islp8.html#trees-vs.-linear-models",
    "title": "",
    "section": "Trees vs. Linear Models",
    "text": "Trees vs. Linear Models\n\nLinear Regression: Assumes a linear model: \\(f(X) = \\beta_0 + \\sum_{j=1}^{p} X_j \\beta_j\\)\nRegression Trees: Assume a model of the form: \\(f(X) = \\sum_{m=1}^{M} c_m \\cdot 1(X \\in R_m)\\)\n\n\n\n\n\n\n\nTip\n\n\n\nThe best model depends on the true relationship. If the relationship is close to linear, linear regression will likely outperform a decision tree. If highly non-linear, decision trees may be better."
  },
  {
    "objectID": "qmd/islp8.html#trees-vs.-linear-models---visual-comparison",
    "href": "qmd/islp8.html#trees-vs.-linear-models---visual-comparison",
    "title": "",
    "section": "Trees vs. Linear Models - Visual Comparison",
    "text": "Trees vs. Linear Models - Visual Comparison\n\n\n\nTrees vs. Linear Models\n\n\n\n\n\n\n\n\nNote\n\n\n\nTop: Linear decision boundary. Linear model (left) is better. Bottom: Non-linear boundary. Decision tree (right) is better."
  },
  {
    "objectID": "qmd/islp8.html#advantages-of-trees",
    "href": "qmd/islp8.html#advantages-of-trees",
    "title": "",
    "section": "Advantages of Trees 👍",
    "text": "Advantages of Trees 👍\n\nEasy to Explain: Simpler to explain than even linear regression.\nHuman Decision-Making: Some believe they mirror human decision-making more closely.\nGraphical Representation: Can be displayed visually.\nQualitative Predictors: Handle qualitative predictors without dummy variables."
  },
  {
    "objectID": "qmd/islp8.html#disadvantages-of-trees",
    "href": "qmd/islp8.html#disadvantages-of-trees",
    "title": "",
    "section": "Disadvantages of Trees 👎",
    "text": "Disadvantages of Trees 👎\n\nLower Predictive Accuracy: Generally don’t have the same level of predictive accuracy as other methods.\nNon-Robust: Small changes in the data can cause large changes in the tree.\n\n\n\n\n\n\n\nTip\n\n\n\nAggregating multiple decision trees (bagging, random forests, boosting) can significantly improve predictive performance."
  },
  {
    "objectID": "qmd/islp8.html#ensemble-methods-combining-multiple-models",
    "href": "qmd/islp8.html#ensemble-methods-combining-multiple-models",
    "title": "",
    "section": "Ensemble Methods: Combining Multiple Models",
    "text": "Ensemble Methods: Combining Multiple Models\nAn ensemble method combines multiple “weak learner” models (like decision trees) to create a more powerful model.\n\nWeak Learner: A simple model with mediocre predictions.\nEnsemble: A combination of weak learners."
  },
  {
    "objectID": "qmd/islp8.html#bagging-bootstrap-aggregation",
    "href": "qmd/islp8.html#bagging-bootstrap-aggregation",
    "title": "",
    "section": "Bagging (Bootstrap Aggregation)",
    "text": "Bagging (Bootstrap Aggregation)\n\nGoal: Reduce the variance of a statistical learning method (especially useful for decision trees, which have high variance).\nIdea: Average many trees built on bootstrapped datasets."
  },
  {
    "objectID": "qmd/islp8.html#bagging-the-process",
    "href": "qmd/islp8.html#bagging-the-process",
    "title": "",
    "section": "Bagging: The Process",
    "text": "Bagging: The Process\n\nBootstrap: Generate B different bootstrapped training datasets (random samples with replacement from the original data).\nTrain: Train a decision tree on each bootstrapped dataset. Grow the trees deep (don’t prune).\nAverage: For a given test observation, average the predictions from all B trees (for regression) or take a majority vote (for classification)."
  },
  {
    "objectID": "qmd/islp8.html#bagging-out-of-bag-oob-error",
    "href": "qmd/islp8.html#bagging-out-of-bag-oob-error",
    "title": "",
    "section": "Bagging: Out-of-Bag (OOB) Error",
    "text": "Bagging: Out-of-Bag (OOB) Error\n\nOOB Observations: For each tree, the observations not used in the bootstrapped sample.\nOOB Prediction: Predict the response for each observation using only the trees where it was OOB.\nOOB Error: A valid estimate of the test error."
  },
  {
    "objectID": "qmd/islp8.html#bagging-variable-importance",
    "href": "qmd/islp8.html#bagging-variable-importance",
    "title": "",
    "section": "Bagging: Variable Importance",
    "text": "Bagging: Variable Importance\n\nInterpretability Loss: Bagging improves accuracy but sacrifices interpretability.\nVariable Importance Measures: Can still get an overall summary of predictor importance.\n\nRegression: Record the total decrease in RSS due to splits on a given predictor, averaged over all B trees.\nClassification: Record the total decrease in the Gini index due to splits on a given predictor, averaged over all B trees."
  },
  {
    "objectID": "qmd/islp8.html#random-forests",
    "href": "qmd/islp8.html#random-forests",
    "title": "",
    "section": "Random Forests",
    "text": "Random Forests\n\nImprovement over Bagging: Introduces a “tweak” that decorrelates the trees.\nRandom Subset of Predictors: At each split, consider only a random sample of m predictors (typically, \\(m \\approx \\sqrt{p}\\))."
  },
  {
    "objectID": "qmd/islp8.html#random-forests-rationale",
    "href": "qmd/islp8.html#random-forests-rationale",
    "title": "",
    "section": "Random Forests: Rationale",
    "text": "Random Forests: Rationale\n\nStrong Predictor Problem: In bagging, if there’s one very strong predictor, most trees will use it in the top split, making the trees similar.\nDecorrelation: By limiting the predictors at each split, random forests give other predictors a chance, leading to less correlated trees and lower variance when averaged."
  },
  {
    "objectID": "qmd/islp8.html#boosting",
    "href": "qmd/islp8.html#boosting",
    "title": "",
    "section": "Boosting",
    "text": "Boosting\n\nSequential Tree Growth: Trees are grown sequentially, using information from previously grown trees.\nSlow Learning: Boosting “learns slowly” by fitting small trees to the residuals.\nNo Bootstrapping: Uses a modified version of the original data."
  },
  {
    "objectID": "qmd/islp8.html#boosting-the-process",
    "href": "qmd/islp8.html#boosting-the-process",
    "title": "",
    "section": "Boosting: The Process",
    "text": "Boosting: The Process\n\nInitialize: Set the initial prediction to 0 and residuals to the observed values.\nIterate (for b = 1 to B):\n\nFit a small tree (with d splits) to the residuals.\nUpdate the fitted function by adding a shrunken version of the new tree: \\(\\hat{f}(x) \\leftarrow \\hat{f}(x) + \\lambda \\hat{f}^b(x)\\)\nUpdate the residuals: \\(r_i \\leftarrow r_i - \\lambda \\hat{f}^b(x_i)\\)\n\nOutput: The boosted model is the sum of all trees: \\(\\hat{f}(x) = \\sum_{b=1}^{B} \\lambda \\hat{f}^b(x)\\)"
  },
  {
    "objectID": "qmd/islp8.html#boosting-tuning-parameters",
    "href": "qmd/islp8.html#boosting-tuning-parameters",
    "title": "",
    "section": "Boosting: Tuning Parameters",
    "text": "Boosting: Tuning Parameters\n\nB (Number of Trees): Boosting can overfit if B is too large (but often slowly). Use cross-validation.\nλ (Shrinkage Parameter): A small positive number (e.g., 0.01 or 0.001) that controls the learning rate.\nd (Number of Splits): Controls the complexity of each tree. Often d = 1 (stumps) works well, resulting in an additive model."
  },
  {
    "objectID": "qmd/islp8.html#bayesian-additive-regression-trees-bart",
    "href": "qmd/islp8.html#bayesian-additive-regression-trees-bart",
    "title": "",
    "section": "Bayesian Additive Regression Trees (BART)",
    "text": "Bayesian Additive Regression Trees (BART)\nBART, like other ensemble methods, utilizes decision trees as building blocks. Key differentiators include:\n\nRandom Tree Structure: Similar to random forests, BART introduces randomness in tree construction.\nSequential Updates: Like boosting, BART iteratively refines its model.\nTree Perturbation: Instead of fitting entirely new trees, BART modifies existing trees from previous iterations."
  },
  {
    "objectID": "qmd/islp8.html#bart-core-idea",
    "href": "qmd/islp8.html#bart-core-idea",
    "title": "",
    "section": "BART: Core Idea",
    "text": "BART: Core Idea\n\nInitialization: All trees start with a single root node, predicting the mean of the response.\nIteration: For each tree, BART randomly perturbs the tree from the previous iteration:\n\nChange the tree structure (add/prune branches).\nChange predictions in terminal nodes.\n\nOutput: A collection of prediction models (one for each iteration). The final prediction is typically the average after a “burn-in” period."
  },
  {
    "objectID": "qmd/islp8.html#bart-key-features",
    "href": "qmd/islp8.html#bart-key-features",
    "title": "",
    "section": "BART: Key Features",
    "text": "BART: Key Features\n\nGuards Against Overfitting: Perturbing trees rather than fitting new ones limits how aggressively the model fits the data.\nSmall Trees: Individual trees are usually small.\nBayesian Interpretation: Can be viewed as a Bayesian approach, with tree perturbations representing draws from a posterior distribution."
  },
  {
    "objectID": "qmd/islp8.html#summary-of-tree-ensemble-methods",
    "href": "qmd/islp8.html#summary-of-tree-ensemble-methods",
    "title": "",
    "section": "Summary of Tree Ensemble Methods",
    "text": "Summary of Tree Ensemble Methods\n\n\n\n\n\n\n\n\n\nMethod\nTree Growth\nData Sampling\nKey Idea\n\n\n\n\nBagging\nIndependent\nBootstrapped\nAverage many trees to reduce variance.\n\n\nRandom Forests\nIndependent\nBootstrapped +\nDecorrelate trees by limiting predictors.\n\n\n\n\nRandom Subset\n\n\n\nBoosting\nSequential\nNone (Modified)\nLearn slowly by fitting to residuals.\n\n\nBART\nSequential,\nNone\nPerturb trees to avoid local optima, Bayesian.\n\n\n\nPerturbed"
  },
  {
    "objectID": "qmd/islp8.html#summary",
    "href": "qmd/islp8.html#summary",
    "title": "",
    "section": "Summary",
    "text": "Summary\n\nTree-based methods are powerful tools for both regression and classification.\nSingle decision trees are interpretable but can overfit.\nEnsemble methods (bagging, random forests, boosting, BART) improve predictive performance by combining multiple trees.\nEach ensemble method has its own approach to building and combining trees."
  },
  {
    "objectID": "qmd/islp8.html#thoughts-and-discussion",
    "href": "qmd/islp8.html#thoughts-and-discussion",
    "title": "",
    "section": "Thoughts and Discussion 🤔",
    "text": "Thoughts and Discussion 🤔\n\nWhen might a single decision tree be preferred over an ensemble method, even if its accuracy is slightly lower?\nHow might you choose between bagging, random forests, and boosting for a particular problem? What factors would you consider?\nCan you think of real-world scenarios where tree-based methods would be particularly well-suited?\nWhat are some limitations of tree-based methods, even with ensemble techniques?"
  },
  {
    "objectID": "qmd/islp11.html",
    "href": "qmd/islp11.html",
    "title": "",
    "section": "",
    "text": "Welcome to the fascinating world of survival analysis! 🕰️ In this chapter, we delve into analyzing a unique type of data: the time until an event occurs. This is different from typical regression problems because of a crucial concept: censoring.\nThink of a medical study tracking patient survival after cancer treatment. Some patients might still be alive at the study’s end. We know they survived at least that long, but not their exact survival time. This is censored data.\nKey Concepts: - Survival Analysis: Statistical methods for analyzing time-to-event data. - Censored Data: Observations where the event of interest has not occurred for all subjects by the end of the observation period. - Event: The outcome of interest (e.g., death, recovery, machine failure, customer churn). - Survival Time: The time until the event occurs.\nWe will explore how to deal with censoring and effectively extract information using tools like survival analysis."
  },
  {
    "objectID": "qmd/islp11.html#introduction",
    "href": "qmd/islp11.html#introduction",
    "title": "",
    "section": "",
    "text": "Welcome to the fascinating world of survival analysis! 🕰️ In this chapter, we delve into analyzing a unique type of data: the time until an event occurs. This is different from typical regression problems because of a crucial concept: censoring.\nThink of a medical study tracking patient survival after cancer treatment. Some patients might still be alive at the study’s end. We know they survived at least that long, but not their exact survival time. This is censored data.\nKey Concepts: - Survival Analysis: Statistical methods for analyzing time-to-event data. - Censored Data: Observations where the event of interest has not occurred for all subjects by the end of the observation period. - Event: The outcome of interest (e.g., death, recovery, machine failure, customer churn). - Survival Time: The time until the event occurs.\nWe will explore how to deal with censoring and effectively extract information using tools like survival analysis."
  },
  {
    "objectID": "qmd/islp11.html#why-survival-analysis",
    "href": "qmd/islp11.html#why-survival-analysis",
    "title": "",
    "section": "Why Survival Analysis?",
    "text": "Why Survival Analysis?\n\nSurvival analysis isn’t limited to medical studies. It’s relevant in many fields:\n\nMedicine: Predicting patient survival times, time to disease recurrence.\nBusiness: Modeling customer churn (time until a customer cancels a subscription).\nEngineering: Assessing the reliability of components (time until failure).\nFinance: Evaluating credit risk (time until default).\nEven beyond time: Modeling weight when scales have upper limits. Any weight above that limit is censored.\n\n\n\nKey Insight: Survival analysis techniques allows us to work with incomplete information. We don’t need to observe the event for every individual to gain valuable insights."
  },
  {
    "objectID": "qmd/islp11.html#survival-and-censoring-times",
    "href": "qmd/islp11.html#survival-and-censoring-times",
    "title": "",
    "section": "Survival and Censoring Times",
    "text": "Survival and Censoring Times\n\nLet’s define some core concepts:\n\nSurvival Time (T): The true time until the event of interest occurs. Also called failure time or event time.\nCensoring Time (C): The time at which observation ends, either because the study ends, the patient drops out, or the event occurs.\nObserved Time (Y): What we actually see: either the survival time or the censoring time. Mathematically, Y = min(T, C).\nStatus Indicator (δ): Tells us whether we observed the event (δ = 1) or the observation was censored (δ = 0).\n\n\\[\n\\delta =\n\\begin{cases}\n1 & \\text{if } T \\leq C \\text{ (event observed)} \\\\\n0 & \\text{if } T &gt; C \\text{ (censored)}\n\\end{cases}\n\\]\nThese variables form the basis of survival analysis."
  },
  {
    "objectID": "qmd/islp11.html#visualizing-censored-data",
    "href": "qmd/islp11.html#visualizing-censored-data",
    "title": "",
    "section": "Visualizing Censored Data",
    "text": "Visualizing Censored Data\n\nLet’s consider a simple example:\n\n\n\nalt text\n\n\n\n\nPatients 1 & 3: The event (e.g., death) was observed. We know their exact survival times.\nPatient 2: Was alive at the end of the study. Their survival time is censored.\nPatient 4: Dropped out of the study. Also censored.\n\n\n\nImportant: Censored observations still provide valuable information! They tell us the event didn’t happen before a certain time."
  },
  {
    "objectID": "qmd/islp11.html#a-closer-look-at-censoring",
    "href": "qmd/islp11.html#a-closer-look-at-censoring",
    "title": "",
    "section": "A Closer Look at Censoring",
    "text": "A Closer Look at Censoring\n\nCensoring isn’t always straightforward. The reason for censoring matters.\n\n\nIndependent Censoring: The censoring mechanism is unrelated to the survival time (conditional on features). This is a crucial assumption for many survival analysis methods.\nExample of violation: Patients dropping out because they are very sick. This biases the results, making survival times seem longer than they are.\n\n\n\n\n\n\n\n\nTypes of Censoring\n\n\n\nRight Censoring: Most common. We know the event time is greater than the observed time (T ≥ Y).\nLeft Censoring: We know the event time is less than or equal to the observed time.\nInterval Censoring: We know the event time falls within a specific interval.\n\n\n\n\n\n\n\n\nalt text\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe will focus mainly on right censoring, the most prevalent type in practice."
  },
  {
    "objectID": "qmd/islp11.html#the-kaplan-meier-survival-curve",
    "href": "qmd/islp11.html#the-kaplan-meier-survival-curve",
    "title": "",
    "section": "The Kaplan-Meier Survival Curve",
    "text": "The Kaplan-Meier Survival Curve\n\nThe survival curve, denoted by S(t), is a fundamental concept. It gives the probability of surviving past time t:\n\\[S(t) = Pr(T &gt; t)\\]\n\nThe larger the value of S(t), the more likely the event will occur at time greater than t.\n\n\nHow do we estimate S(t) from data with censoring? The Kaplan-Meier estimator is a powerful tool."
  },
  {
    "objectID": "qmd/islp11.html#estimating-the-survival-curve-challenges",
    "href": "qmd/islp11.html#estimating-the-survival-curve-challenges",
    "title": "",
    "section": "Estimating the Survival Curve: Challenges",
    "text": "Estimating the Survival Curve: Challenges\n\nLet’s consider the BrainCancer dataset. We want to estimate S(20): the probability of surviving at least 20 months. Naive approaches fail:\n\nSimply using Y &gt; 20: This ignores that Y is not always the true survival time (due to censoring). It underestimates survival.\nIgnoring censored observations: This throws away valuable information. A patient censored at 19.9 months almost certainly would have survived past 20.\n\n\n\n\n\n\n\nTip\n\n\n\nThe Kaplan-Meier estimator elegantly handles censoring to provide a more accurate estimate."
  },
  {
    "objectID": "qmd/islp11.html#the-kaplan-meier-estimator-intuition",
    "href": "qmd/islp11.html#the-kaplan-meier-estimator-intuition",
    "title": "",
    "section": "The Kaplan-Meier Estimator: Intuition",
    "text": "The Kaplan-Meier Estimator: Intuition\n\nThe Kaplan-Meier estimator works sequentially, considering events as they unfold in time.\n\n\n\n\n\n\n\nKey Idea: At each death time, we calculate the conditional probability of surviving that time point, given survival up to that point.\nWe then multiply these conditional probabilities together to get the overall survival probability.\nLet \\(d_1 &lt; d_2 &lt; ... &lt; d_K\\) be the distinct death times.\n\\(q_k\\): Number of deaths at time \\(d_k\\).\n\\(r_k\\): Number of individuals at risk (alive and in the study) just before \\(d_k\\). This is the risk set.\n\n\n\n\n\nThe Kaplan-Meier estimator formula is:\n\n\n\\[\n\\hat{S}(d_k) = \\prod_{j=1}^{k} \\left( \\frac{r_j - q_j}{r_j} \\right)\n\\]\n\n\nFor times between death times, \\(\\hat{S}(t)\\) remains constant, creating a step-like curve."
  },
  {
    "objectID": "qmd/islp11.html#the-kaplan-meier-estimator-explanation",
    "href": "qmd/islp11.html#the-kaplan-meier-estimator-explanation",
    "title": "",
    "section": "The Kaplan-Meier Estimator: Explanation",
    "text": "The Kaplan-Meier Estimator: Explanation\n\nThe formula is derived from the law of total probability:\n\\[\nPr(T &gt; d_k) = Pr(T &gt; d_k | T &gt; d_{k-1})Pr(T &gt; d_{k-1}) + Pr(T&gt;d_k|T \\leq d_{k-1})Pr(T\\leq d_{k-1})\n\\]\n\nSince \\(d_{k-1} &lt; d_k\\), \\(Pr(T&gt;d_k|T \\leq d_{k-1}) = 0\\), then the above formula is: \\[\nS(d_k) = Pr(T &gt; d_k) = Pr(T &gt; d_k | T &gt; d_{k-1})Pr(T &gt; d_{k-1})\n\\]\n\n\nPlug in \\(S(t)\\) and rearrange the above formula: \\[\nS(d_k) = Pr(T&gt;d_k|T&gt;d_{k-1}) \\times ... \\times Pr(T&gt;d_2|T&gt;d_1)Pr(T&gt;d_1)\n\\]\n\n\nWe estimate each term on the right-hand side using the fraction of the risk set at time \\(d_j\\) who survived past time \\(d_j\\):\n\\[\\widehat{Pr}(T &gt; d_j | T &gt; d_{j-1}) = (r_j - q_j) / r_j\\]\n\n\nFinally, we arrive at the Kaplan-Meier estimator:\n\n\n\\[\n\\hat{S}(d_k) = \\prod_{j=1}^{k} \\left( \\frac{r_j - q_j}{r_j} \\right)\n\\]"
  },
  {
    "objectID": "qmd/islp11.html#kaplan-meier-curve-example",
    "href": "qmd/islp11.html#kaplan-meier-curve-example",
    "title": "",
    "section": "Kaplan-Meier Curve: Example",
    "text": "Kaplan-Meier Curve: Example\n\nHere’s the Kaplan-Meier curve for the BrainCancer data:\n\n\n\nalt text\n\n\n\nThe curve steps down at each observed death time.\nThe height of the curve at any time point represents the estimated survival probability.\nThe estimated probability of survival past 20 months is 71%."
  },
  {
    "objectID": "qmd/islp11.html#comparing-survival-curves-the-log-rank-test",
    "href": "qmd/islp11.html#comparing-survival-curves-the-log-rank-test",
    "title": "",
    "section": "Comparing Survival Curves: The Log-Rank Test",
    "text": "Comparing Survival Curves: The Log-Rank Test\n\nOften, we want to compare survival curves between groups (e.g., males vs. females).\n\n\n\nalt text\n\n\n\nThe log-rank test is a statistical test for comparing survival curves. It accounts for censoring."
  },
  {
    "objectID": "qmd/islp11.html#log-rank-test-details",
    "href": "qmd/islp11.html#log-rank-test-details",
    "title": "",
    "section": "Log-Rank Test: Details",
    "text": "Log-Rank Test: Details\n\nThe log-rank test examines events sequentially, like the Kaplan-Meier estimator.\n\n\n\n\n\n\n\nAt each death time \\(d_k\\), we construct a 2x2 table:\n\n\n\n\n\n\n\nGroup 1\nGroup 2\nTotal\n\n\n\n\nDied\n\\(q_{1k}\\)\n\\(q_{2k}\\)\n\\(q_k\\)\n\n\nSurvived\n\\(r_{1k}-q_{1k}\\)\n\\(r_{2k}-q_{2k}\\)\n\\(r_k-q_k\\)\n\n\nTotal\n\\(r_{1k}\\)\n\\(r_{2k}\\)\n\\(r_k\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(r_{1k}\\), \\(r_{2k}\\): Number at risk in each group at time \\(d_k\\).\n\\(q_{1k}\\), \\(q_{2k}\\): Number of deaths in each group at time \\(d_k\\).\nKey Idea: If there’s no difference in survival, we’d expect the proportion of deaths in each group to be proportional to the number at risk.\n\n\n\n\n\nThe log-rank test statistic (W) is calculated based on the observed and expected number of deaths in group 1:\n\\[\nW = \\frac{\\sum_{k=1}^{K}(q_{1k} - \\mu_k)}{\\sqrt{\\sum_{k=1}^{K}Var(q_{1k})}}\n\\] where $ _k = q_k $ and \\(Var(q_{1k}) = \\frac{q_k(r_{1k}/r_k)(1-r_{1k}/r_k)(r_k - q_k)}{r_k - 1}\\)\n\n\nUnder the null hypothesis (no difference in survival), W approximately follows a standard normal distribution."
  },
  {
    "objectID": "qmd/islp11.html#log-rank-test-brain-cancer-example",
    "href": "qmd/islp11.html#log-rank-test-brain-cancer-example",
    "title": "",
    "section": "Log-Rank Test: Brain Cancer Example",
    "text": "Log-Rank Test: Brain Cancer Example\n\nComparing survival times of males and females in the BrainCancer data:\n\nLog-rank test statistic W = 1.2.\nTwo-sided p-value = 0.2 (using the theoretical null distribution).\nWe cannot reject the null hypothesis of no difference in survival curves between males and females."
  },
  {
    "objectID": "qmd/islp11.html#regression-models-with-a-survival-response",
    "href": "qmd/islp11.html#regression-models-with-a-survival-response",
    "title": "",
    "section": "Regression Models with a Survival Response",
    "text": "Regression Models with a Survival Response\n\nSo far, we’ve looked at describing survival curves and comparing them between groups. Now, we want to predict survival time based on covariates (features).\n\n\n\n\n\n\n\nWe have observations of the form \\((Y_i, \\delta_i, X_i)\\), where:\n\n\\(Y_i\\) is the observed time (min(T, C)).\n\\(\\delta_i\\) is the status indicator.\n\\(X_i\\) is a vector of features.\n\nA simple linear regression of log(Y) on X is problematic due to censoring.\nWhy not regress on Y directly?: We are interested in T not Y.\n\n\n\n\n\n\nSolution: Use a sequential approach, similar to Kaplan-Meier and the log-rank test. We introduce the hazard function."
  },
  {
    "objectID": "qmd/islp11.html#the-hazard-function",
    "href": "qmd/islp11.html#the-hazard-function",
    "title": "",
    "section": "The Hazard Function",
    "text": "The Hazard Function\n\nThe hazard function, h(t), is also known as the hazard rate or force of mortality. It represents the instantaneous risk of the event occurring at time t, given survival up to time t:\n\\[h(t) = \\lim_{\\Delta t \\to 0} \\frac{Pr(t &lt; T \\leq t + \\Delta t | T &gt; t)}{\\Delta t}\\]\n\n\nThink of it as the “death rate” in a tiny interval after time t, given survival up to t.\nIt’s closely related to the survival curve, S(t).\nIt’s crucial for modeling survival data as a function of covariates."
  },
  {
    "objectID": "qmd/islp11.html#hazard-function-more-details",
    "href": "qmd/islp11.html#hazard-function-more-details",
    "title": "",
    "section": "Hazard Function: More Details",
    "text": "Hazard Function: More Details\n\n\\[\n\\begin{aligned}\nh(t) &= \\lim_{\\Delta t \\to 0} Pr((t&lt;T\\le t+\\Delta t)\\cap (T&gt;t))/\\Delta t \\over Pr(T&gt;t) \\\\\n&= \\lim_{\\Delta t \\to 0} {Pr(t&lt;T\\le t+\\Delta t) / \\Delta t \\over Pr(T&gt;t)} \\\\\n&= {f(t) \\over S(t)}\n\\end{aligned}\n\\]\nwhere \\[\nf(t) = \\lim_{\\Delta t\\to 0} {Pr(t&lt;T\\le t+\\Delta t)\\over \\Delta t}\n\\]\n\n\\(f(t)\\) is probability density function.\n\n\nThe likelihood associated with the i-th observation is:\n\\[\nL_i = \\begin{cases}\nf(y_i) \\quad \\text{if the i-th observation is not censored} \\\\\nS(y_i) \\quad \\text{if the i-th observation is censored}\n\\end{cases} \\\\\n= f(y_i)^{\\delta_i}S(y_i)^{1-\\delta_i}\n\\]\n\n\nIf \\(Y=y_i\\) and the i-th observation is not censored, then the likelihood is the probability of dying in a tiny interval around time \\(y_i\\). If the i-th observation is censored, then the likelihood is the probability of surviving at least until time \\(y_i\\)"
  },
  {
    "objectID": "qmd/islp11.html#cox-proportional-hazards-model",
    "href": "qmd/islp11.html#cox-proportional-hazards-model",
    "title": "",
    "section": "Cox Proportional Hazards Model",
    "text": "Cox Proportional Hazards Model\n\nThe Cox proportional hazards model is a powerful and flexible approach to model the relationship between covariates and the hazard function.\n\nThe Proportional Hazards Assumption:\n\n\\[h(t|x_i) = h_0(t) \\exp(\\sum_{j=1}^{p} x_{ij}\\beta_j)\\]\n\n\n\n\n\n\n\n\\(h(t|x_i)\\): Hazard function for an individual with features \\(x_i\\).\n\\(h_0(t)\\): Baseline hazard function. This is the hazard for an individual with all features equal to zero. It’s left unspecified.\n\\(\\exp(\\sum_{j=1}^{p} x_{ij}\\beta_j)\\): Relative risk. It’s a multiplicative factor that scales the baseline hazard based on the features.\nThe key is that we don’t assume a specific form for \\(h_0(t)\\). This makes the model very flexible.\nA one-unit increase in \\(x_{ij}\\) multiplies the hazard by a factor of \\(\\exp(\\beta_j)\\)."
  },
  {
    "objectID": "qmd/islp11.html#proportional-hazards-illustration",
    "href": "qmd/islp11.html#proportional-hazards-illustration",
    "title": "",
    "section": "Proportional Hazards: Illustration",
    "text": "Proportional Hazards: Illustration\n\n\n\n\nalt text\n\n\n\nTop Row: Proportional hazards holds. Log hazard functions are parallel; survival curves don’t cross.\nBottom Row: Proportional hazards doesn’t hold. Log hazard and survival curves cross."
  },
  {
    "objectID": "qmd/islp11.html#cox-proportional-hazards-model-estimation",
    "href": "qmd/islp11.html#cox-proportional-hazards-model-estimation",
    "title": "",
    "section": "Cox Proportional Hazards Model: Estimation",
    "text": "Cox Proportional Hazards Model: Estimation\n\nHow do we estimate the coefficients, \\(\\beta\\), in the Cox model without knowing \\(h_0(t)\\)? We use the partial likelihood.\n\n\n\n\n\n\n\nAssume no ties in failure times.\nConsider the ith observation, which fails at time \\(y_i\\) (\\(\\delta_i = 1\\)).\nWhat’s the probability that this observation fails at \\(y_i\\), given the set of individuals at risk at that time?\nThe probability that i-th observation is the one to fail at time \\(y_i\\) is:\n\n\n\n\\[\n\\frac{h_0(y_i) \\exp(\\sum_{j=1}^p x_{ij}\\beta_j)}{\\sum_{i':y_{i'}\\ge y_i}h_0(y_i)\\exp(\\sum_{j=1}^{p}x_{i'j}\\beta_j)} = \\frac{\\exp(\\sum_{j=1}^p x_{ij}\\beta_j)}{\\sum_{i':y_{i'}\\ge y_i}\\exp(\\sum_{j=1}^{p}x_{i'j}\\beta_j)}\n\\]\n\n\n\n\n\nCrucially, \\(h_0(y_i)\\) cancels out!\nThe partial likelihood is the product of these probabilities over all uncensored observations:\n\n\n\n\\[PL(\\beta) = \\prod_{i:\\delta_i = 1} \\frac{\\exp(\\sum_{j=1}^p x_{ij}\\beta_j)}{\\sum_{i':y_{i'}\\ge y_i}\\exp(\\sum_{j=1}^{p}x_{i'j}\\beta_j)}\\]\n\n\n\nWe estimate \\(\\beta\\) by maximizing the partial likelihood.\nThis is done numerically (no closed-form solution)."
  },
  {
    "objectID": "qmd/islp11.html#cox-model-example-brain-cancer-data",
    "href": "qmd/islp11.html#cox-model-example-brain-cancer-data",
    "title": "",
    "section": "Cox Model: Example (Brain Cancer Data)",
    "text": "Cox Model: Example (Brain Cancer Data)\n\nLet’s apply the Cox model to the BrainCancer data:\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. error\nz-statistic\np-value\n\n\n\n\nsex[Male]\n0.18\n0.36\n0.51\n0.61\n\n\ndiagnosis[LG Glioma]\n0.92\n0.64\n1.43\n0.15\n\n\ndiagnosis[HG Glioma]\n2.15\n0.45\n4.78\n0.00\n\n\ndiagnosis[Other]\n0.89\n0.66\n1.35\n0.18\n\n\nloc[Supratentorial]\n0.44\n0.70\n0.63\n0.53\n\n\nki\n-0.05\n0.02\n-3.00\n&lt;0.01\n\n\ngtv\n0.03\n0.02\n1.54\n0.12\n\n\nstereo[SRT]\n0.18\n0.60\n0.30\n0.77\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nMales have an estimated hazard 1.2 times greater than females (e0.18), but this is not statistically significant.\nHigher Karnofsky index (ki) is associated with a lower hazard (e-0.05 = 0.95), and this effect is significant.\n\nThe p-value associated with a coefficient tests the null hypothesis that the coefficient is zero."
  },
  {
    "objectID": "qmd/islp11.html#cox-model-example-publication-data",
    "href": "qmd/islp11.html#cox-model-example-publication-data",
    "title": "",
    "section": "Cox Model: Example (Publication Data)",
    "text": "Cox Model: Example (Publication Data)\n\nNext, we will introduce the dataset Publication, involving the time to publication of journal papers reporting the results of clinical trials funded by the National Heart, Lung, and Blood Institue. For 244 trials, the time in months until publication is recorded.\n\n\n\n\n\n\n\n\n\n\n\nalt text\n\n\n\n\n\nUsing the log-rank test, we can test whether the studies with positive results have significant difference in publication time.\nThe figure shows slight evidence that time until publication is lower for studies with a positive result.\nHowever, the log-rank test yields a very unimpressive p-value of 0.36.\n\n\n\n\n\nNow, let’s fit Cox’s proportional hazards model using all available features:\n\n\n\nVariable\nCoefficient\nStd. error\nz-statistic\np-value\n\n\n\n\nposres[Yes]\n0.55\n0.18\n3.02\n0.00\n\n\nmulti[Yes]\n0.15\n0.31\n0.47\n0.64\n\n\nclinend[Yes]\n0.51\n0.27\n1.89\n0.06\n\n\nmech[K01]\n1.05\n1.06\n1.00\n0.32\n\n\nmech[K23]\n-0.48\n1.05\n-0.45\n0.65\n\n\nmech[P01]\n-0.31\n0.78\n-0.40\n0.69\n\n\nmech[P50]\n0.60\n1.06\n0.57\n0.57\n\n\nmech[R01]\n0.10\n0.32\n0.30\n0.76\n\n\nmech[R18]\n1.05\n1.05\n0.99\n0.32\n\n\nmech[R21]\n-0.05\n1.06\n-0.04\n0.97\n\n\nmech[R24, K24]\n0.81\n1.05\n0.77\n0.44\n\n\nmech[R42]\n-14.78\n3414.38\n-0.00\n1.00\n\n\nmech[R44]\n-0.57\n0.77\n-0.73\n0.46\n\n\nmech[RC2]\n-14.92\n2243.60\n-0.01\n0.99\n\n\nmech[U01]\n-0.22\n0.32\n-0.70\n0.48\n\n\nmech[U54]\n0.47\n1.07\n0.44\n0.66\n\n\nsampsize\n0.00\n0.00\n0.19\n0.85\n\n\nbudget\n0.00\n0.00\n1.67\n0.09\n\n\nimpact\n0.06\n0.01\n8.23\n0.00\n\n\n\n\nWe find that the chance of publication of a study with a positive result is \\(e^{0.55} = 1.74\\) time higher than the chance of publication of a study with a negative result at any point in time, holding all other covariates fixed."
  },
  {
    "objectID": "qmd/islp11.html#cox-model-example-publication-data---adjusted-curves",
    "href": "qmd/islp11.html#cox-model-example-publication-data---adjusted-curves",
    "title": "",
    "section": "Cox Model: Example (Publication Data) - Adjusted Curves",
    "text": "Cox Model: Example (Publication Data) - Adjusted Curves\n\n\n\n\nalt text\n\n\n\nAfter adjusting for other covariates (using representative values), we see a much clearer difference in survival curves between positive and negative results.\nThis highlights the importance of considering multiple predictors."
  },
  {
    "objectID": "qmd/islp11.html#shrinkage-for-the-cox-model",
    "href": "qmd/islp11.html#shrinkage-for-the-cox-model",
    "title": "",
    "section": "Shrinkage for the Cox Model",
    "text": "Shrinkage for the Cox Model\n\nWe can apply shrinkage methods (like ridge and lasso) to the Cox model.\n\n\n\n\n\n\n\nIdea: Minimize a penalized version of the negative log partial likelihood:\n\\[-\\log\\left(\\prod_{i:\\delta_i=1} \\frac{\\exp(\\sum_{j=1}^p x_{ij}\\beta_j)}{\\sum_{i':y_{i'}\\ge y_i}\\exp(\\sum_{j=1}^{p}x_{i'j}\\beta_j)}\\right) + \\lambda P(\\beta)\\]\n\n\\(\\lambda\\): Tuning parameter.\n\\(P(\\beta)\\): Penalty term (e.g., lasso: \\(\\sum_{j=1}^p |\\beta_j|\\)).\n\nWe now apply lasso-penalized Cox model to the Publication data.\nThe figure on the right hand displays the cross-validation results.\nNote the “U-shape” of the partial likelihood deviance.\nSpecifically, the cross-validation error is minimized when just two predictors, budget and impact, have non-zero estimated coefficients.\n\n\n\n\n\n\nPartial likelihood deviance"
  },
  {
    "objectID": "qmd/islp11.html#assessing-model-fit-on-test-data",
    "href": "qmd/islp11.html#assessing-model-fit-on-test-data",
    "title": "",
    "section": "Assessing Model Fit on Test Data",
    "text": "Assessing Model Fit on Test Data\n\nWe can use risk score to categorize the observations based on their “risk”. For example, we use the risk score: \\[\nbudget_i \\cdot \\hat{\\beta}_{budget} + impact_i \\cdot \\hat{\\beta}_{impact}\n\\] where \\(\\hat{\\beta}_{budget}\\) and \\(\\hat{\\beta}_{impact}\\) are the coefficients estimates for these two features from the training set.\n\n\n\nalt text\n\n\nThe figure shows that there is clear seperation between the three strata, and that the strata are correctly ordered in terms of low, medium, and high risk of publication."
  },
  {
    "objectID": "qmd/islp11.html#additional-topics",
    "href": "qmd/islp11.html#additional-topics",
    "title": "",
    "section": "Additional Topics",
    "text": "Additional Topics\n\n\nArea Under the Curve (AUC) for Survival Analysis: Harrell’s concordance index (C-index) generalizes AUC to survival data, accounting for censoring.\nChoice of Time Scale: The definition of “time zero” can be crucial and depends on the context.\nTime-Dependent Covariates: The Cox model can handle predictors that change over time.\nChecking the Proportional Hazards Assumption: Visual checks (log hazard plots) and stratification can help assess the assumption.\nSurvival Trees: Tree-based methods can be adapted for survival analysis."
  },
  {
    "objectID": "qmd/islp11.html#summary",
    "href": "qmd/islp11.html#summary",
    "title": "",
    "section": "Summary",
    "text": "Summary\n\n\nSurvival analysis deals with time-to-event data, where censoring is a key challenge.\nThe Kaplan-Meier estimator provides a non-parametric estimate of the survival curve.\nThe log-rank test compares survival curves between groups.\nThe Cox proportional hazards model allows us to model the relationship between covariates and the hazard function, making it a powerful tool for prediction.\nShrinkage methods can be applied to the Cox model.\nVarious extensions and considerations (AUC, time scale, time-dependent covariates, proportional hazards assumption, survival trees) broaden the applicability of survival analysis."
  },
  {
    "objectID": "qmd/islp11.html#thoughts-and-discussion",
    "href": "qmd/islp11.html#thoughts-and-discussion",
    "title": "",
    "section": "Thoughts and Discussion",
    "text": "Thoughts and Discussion\n\n\nHow might survival analysis be applied in your field of interest?\nWhat are the ethical considerations when dealing with survival data, especially in medical contexts?\nHow could you explain the concept of censoring to someone without a statistical background?\nCan you think of situations where the proportional hazards assumption might be violated? How could you address this?\nWhat are the limitations of survival analysis? What types of questions can it not answer?"
  },
  {
    "objectID": "qmd/islp3.html",
    "href": "qmd/islp3.html",
    "title": "",
    "section": "",
    "text": "Linear regression is a fundamental approach in supervised learning, used primarily for predicting a quantitative response.\nIt’s a cornerstone in statistical learning, widely used and extensively studied.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear regression serves as a stepping stone to more complex statistical learning methods. Many advanced techniques can be seen as extensions or generalizations of linear regression."
  },
  {
    "objectID": "qmd/islp3.html#introduction-to-linear-regression",
    "href": "qmd/islp3.html#introduction-to-linear-regression",
    "title": "",
    "section": "",
    "text": "Linear regression is a fundamental approach in supervised learning, used primarily for predicting a quantitative response.\nIt’s a cornerstone in statistical learning, widely used and extensively studied.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear regression serves as a stepping stone to more complex statistical learning methods. Many advanced techniques can be seen as extensions or generalizations of linear regression."
  },
  {
    "objectID": "qmd/islp3.html#key-questions-in-linear-regression",
    "href": "qmd/islp3.html#key-questions-in-linear-regression",
    "title": "",
    "section": "Key Questions in Linear Regression",
    "text": "Key Questions in Linear Regression\nWe explore the Advertising data to address several key questions:\n\nRelationship Existence: Is there a connection between advertising budget and sales?\nRelationship Strength: How strong is the link between budget and sales?\nMedia Contribution: Which advertising media (TV, radio, newspaper) contribute to sales?\nAssociation Size: How much does sales increase for each dollar spent on each medium?\nPrediction Accuracy: Can we accurately predict future sales?\nLinearity Check: Is the relationship between advertising and sales linear?\nMedia Synergy: Do the advertising media work together synergistically (interaction effect)?\n\n\n\n\n\n\n\nAddressing these questions helps determine if a relationship exists, its strength, individual media contributions, prediction accuracy, the linearity of the connection, and potential synergy effects."
  },
  {
    "objectID": "qmd/islp3.html#simple-linear-regression-the-basics",
    "href": "qmd/islp3.html#simple-linear-regression-the-basics",
    "title": "",
    "section": "Simple Linear Regression: The Basics",
    "text": "Simple Linear Regression: The Basics\nSimple linear regression predicts a quantitative response, \\(Y\\), using a single predictor variable, \\(X\\), assuming a linear relationship:\n\\[\nY \\approx \\beta_0 + \\beta_1X\n\\]\n\n\\(\\beta_0\\): Intercept (value of \\(Y\\) when \\(X = 0\\)).\n\\(\\beta_1\\): Slope (change in \\(Y\\) for a one-unit increase in \\(X\\)).\n\\(\\beta_0\\) and \\(\\beta_1\\) are the model coefficients or parameters.\n\n\n\n\n\n\n\nThis equation represents a straight line, where \\(\\beta_0\\) is the y-intercept, and \\(\\beta_1\\) is the slope."
  },
  {
    "objectID": "qmd/islp3.html#simple-linear-regression-example",
    "href": "qmd/islp3.html#simple-linear-regression-example",
    "title": "",
    "section": "Simple Linear Regression: Example",
    "text": "Simple Linear Regression: Example\nFor instance, let’s regress sales onto TV advertising:\n\\[\n\\text{sales} \\approx \\beta_0 + \\beta_1 \\times \\text{TV}\n\\]\n\n\\(Y\\): Sales (in thousands of units).\n\\(X\\): TV advertising budget (in thousands of dollars).\n\nOnce we estimate the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) (denoted as \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\)), we can predict sales:\n\\[\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x\n\\]\n\n\n\n\n\n\nThe “hat” symbol (^) indicates an estimated value. \\(\\hat{y}\\) is the predicted value of sales."
  },
  {
    "objectID": "qmd/islp3.html#estimating-the-coefficients-least-squares",
    "href": "qmd/islp3.html#estimating-the-coefficients-least-squares",
    "title": "",
    "section": "Estimating the Coefficients: Least Squares",
    "text": "Estimating the Coefficients: Least Squares\nOur goal is to find \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) that best fit the data, meaning the line should be as close as possible to the data points \\((x_i, y_i)\\). We use the least squares method, which minimizes the residual sum of squares (RSS):\n\\[\n\\text{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 = \\sum_{i=1}^{n} (y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i)^2\n\\]\n\n\\(y_i\\): Actual sales for the \\(i\\)-th observation.\n\\(\\hat{y_i}\\): Predicted sales for the \\(i\\)-th observation.\n\\(e_i = y_i - \\hat{y_i}\\): The residual for the \\(i\\)-th observation (the difference between the actual and predicted values).\n\n\n\n\n\n\n\nLeast squares finds the line that minimizes the sum of the squared vertical distances between the data points and the line."
  },
  {
    "objectID": "qmd/islp3.html#estimating-the-coefficients-formulas",
    "href": "qmd/islp3.html#estimating-the-coefficients-formulas",
    "title": "",
    "section": "Estimating the Coefficients: Formulas",
    "text": "Estimating the Coefficients: Formulas\nUsing calculus, we find the values of \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) that minimize RSS:\n\\[\n\\hat{\\beta_1} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n\\]\n\\[\n\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\n\\]\n\n\\(\\bar{x}\\): Sample mean of \\(X\\).\n\\(\\bar{y}\\): Sample mean of \\(Y\\).\n\n\n\n\n\n\n\nThese formulas provide the best estimates for the slope and intercept based on the available data. The slope (\\(\\hat{\\beta_1}\\)) represents the average change in Y for a one-unit increase in X, and the intercept (\\(\\hat{\\beta_0}\\)) is the predicted value of Y when X is zero."
  },
  {
    "objectID": "qmd/islp3.html#visualizing-the-least-squares-fit",
    "href": "qmd/islp3.html#visualizing-the-least-squares-fit",
    "title": "",
    "section": "Visualizing the Least Squares Fit",
    "text": "Visualizing the Least Squares Fit\n\n\n\nLeast Squares Fit\n\n\n\n\n\n\n\n\nThis figure shows the least squares regression line for sales versus TV advertising. Each grey line segment represents a residual, the difference between the observed sales value and the value predicted by the line. The least squares method minimizes the sum of the squares of these residuals."
  },
  {
    "objectID": "qmd/islp3.html#assessing-coefficient-accuracy-population-vs.-sample",
    "href": "qmd/islp3.html#assessing-coefficient-accuracy-population-vs.-sample",
    "title": "",
    "section": "Assessing Coefficient Accuracy: Population vs. Sample",
    "text": "Assessing Coefficient Accuracy: Population vs. Sample\n\nPopulation Regression Line: The “true” (but usually unknown) relationship: \\(Y = \\beta_0 + \\beta_1X + \\epsilon\\).\nLeast Squares Line: The estimated relationship based on our sample: \\(\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x\\).\n\\(\\epsilon\\): random error term\n\n\n\n\nPopulation vs. Sample Regression Lines\n\n\n\n\n\n\n\n\nThe left panel shows the true population regression line (red) and the estimated least squares line (blue) from a single sample. The right panel shows ten different least squares lines, each estimated from a different sample drawn from the same population. The least squares lines vary, but they cluster around the true population line."
  },
  {
    "objectID": "qmd/islp3.html#assessing-coefficient-accuracy-unbiasedness",
    "href": "qmd/islp3.html#assessing-coefficient-accuracy-unbiasedness",
    "title": "",
    "section": "Assessing Coefficient Accuracy: Unbiasedness",
    "text": "Assessing Coefficient Accuracy: Unbiasedness\n\n\\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) are estimates of the true (unknown) parameters \\(\\beta_0\\) and \\(\\beta_1\\).\nThese estimates are unbiased: on average, they will equal the true values. This means if we took many samples and calculated the estimates each time, the average of the estimates would converge to the true values.\n\n\n\n\n\n\n\nUnbiasedness means that our estimation method doesn’t systematically over- or underestimate the true values."
  },
  {
    "objectID": "qmd/islp3.html#assessing-coefficient-accuracy-standard-error",
    "href": "qmd/islp3.html#assessing-coefficient-accuracy-standard-error",
    "title": "",
    "section": "Assessing Coefficient Accuracy: Standard Error",
    "text": "Assessing Coefficient Accuracy: Standard Error\n\nStandard Error: Measures the average amount that an estimate (\\(\\hat{\\beta_0}\\) or \\(\\hat{\\beta_1}\\)) differs from the true value (\\(\\beta_0\\) or \\(\\beta_1\\)).\nFormulas for standard errors:\n\n\\[\n\\text{SE}(\\hat{\\beta_0})^2 = \\sigma^2 \\left[ \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} \\right]\n\\]\n\\[\n\\text{SE}(\\hat{\\beta_1})^2 = \\frac{\\sigma^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n\\] - \\(\\sigma^2\\): Variance of the error term \\(\\epsilon\\) (usually unknown, estimated by the residual standard error, RSE).\n\n\n\n\n\n\nSmaller standard errors indicate more precise estimates. Notice that SE(\\(\\hat{\\beta_1}\\)) is smaller when the \\(x_i\\) values are more spread out."
  },
  {
    "objectID": "qmd/islp3.html#assessing-coefficient-accuracy-confidence-intervals",
    "href": "qmd/islp3.html#assessing-coefficient-accuracy-confidence-intervals",
    "title": "",
    "section": "Assessing Coefficient Accuracy: Confidence Intervals",
    "text": "Assessing Coefficient Accuracy: Confidence Intervals\n\nConfidence Interval: A range of values that is likely to contain the true unknown value of a parameter, with a certain level of confidence (e.g., 95%).\nApproximate 95% confidence interval for \\(\\beta_1\\):\n\n\\[\n\\hat{\\beta_1} \\pm 2 \\cdot \\text{SE}(\\hat{\\beta_1})\n\\]\n\n\n\n\n\n\nThis means that if we were to repeatedly sample from the population and construct 95% confidence intervals, approximately 95% of those intervals would contain the true value of \\(\\beta_1\\)."
  },
  {
    "objectID": "qmd/islp3.html#hypothesis-testing",
    "href": "qmd/islp3.html#hypothesis-testing",
    "title": "",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nNull Hypothesis (H₀): There is no relationship between \\(X\\) and \\(Y\\) (\\(\\beta_1 = 0\\)).\nAlternative Hypothesis (Hₐ): There is some relationship between \\(X\\) and \\(Y\\) (\\(\\beta_1 \\neq 0\\)).\nt-statistic: Measures how many standard deviations \\(\\hat{\\beta_1}\\) is away from 0:\n\n\\[\nt = \\frac{\\hat{\\beta_1} - 0}{\\text{SE}(\\hat{\\beta_1})}\n\\]\n\np-value: The probability of observing a t-statistic as extreme as, or more extreme than, the one calculated, assuming \\(H_0\\) is true.\n\n\n\n\n\n\n\nA small p-value (typically &lt; 0.05) provides evidence against the null hypothesis, suggesting a relationship between X and Y."
  },
  {
    "objectID": "qmd/islp3.html#hypothesis-testing-example-advertising-data",
    "href": "qmd/islp3.html#hypothesis-testing-example-advertising-data",
    "title": "",
    "section": "Hypothesis Testing: Example (Advertising Data)",
    "text": "Hypothesis Testing: Example (Advertising Data)\n\n\n\nPredictor\nCoefficient\nStd. error\nt-statistic\np-value\n\n\n\n\nIntercept\n7.0325\n0.4578\n15.36\n&lt; 0.0001\n\n\nTV\n0.0475\n0.0027\n17.67\n&lt; 0.0001\n\n\n\n\n\n\n\n\n\nThe table shows the results of regressing sales on TV advertising. The very small p-value for TV provides strong evidence that \\(\\beta_1 \\neq 0\\), meaning there is a relationship between TV advertising and sales."
  },
  {
    "objectID": "qmd/islp3.html#assessing-model-accuracy-rse",
    "href": "qmd/islp3.html#assessing-model-accuracy-rse",
    "title": "",
    "section": "Assessing Model Accuracy: RSE",
    "text": "Assessing Model Accuracy: RSE\n\nResidual Standard Error (RSE): An estimate of the standard deviation of the error term \\(\\epsilon\\). It represents the average amount that the response will deviate from the true regression line.\n\n\\[\n\\text{RSE} = \\sqrt{\\frac{1}{n-2}\\text{RSS}} = \\sqrt{\\frac{1}{n-2}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}\n\\]\n\n\n\n\n\n\nLower RSE values indicate a better fit. The RSE is measured in the units of Y."
  },
  {
    "objectID": "qmd/islp3.html#assessing-model-accuracy-r²",
    "href": "qmd/islp3.html#assessing-model-accuracy-r²",
    "title": "",
    "section": "Assessing Model Accuracy: R²",
    "text": "Assessing Model Accuracy: R²\n\nR² Statistic: Measures the proportion of variance explained by the model. It always falls between 0 and 1.\n\n\\[\nR^2 = \\frac{\\text{TSS} - \\text{RSS}}{\\text{TSS}} = 1 - \\frac{\\text{RSS}}{\\text{TSS}}\n\\]\n\nTotal Sum of Squares (TSS): \\(\\sum(y_i - \\bar{y})^2\\) - Measures the total variance in the response \\(Y\\).\n\n\n\n\n\n\n\nR² closer to 1 indicates that a large proportion of the variability in the response is explained by the regression. R² is the square of the correlation between X and Y in simple linear regression."
  },
  {
    "objectID": "qmd/islp3.html#assessing-model-accuracy-example-advertising-data",
    "href": "qmd/islp3.html#assessing-model-accuracy-example-advertising-data",
    "title": "",
    "section": "Assessing Model Accuracy: Example (Advertising Data)",
    "text": "Assessing Model Accuracy: Example (Advertising Data)\n\n\n\nQuantity\nValue\n\n\n\n\nResidual standard error\n3.26\n\n\nR²\n0.612\n\n\nF-statistic\n312.1\n\n\n\n\n\n\n\n\n\nFor the regression of sales on TV, the RSE is 3.26 (thousands of units), and the R² is 0.612. This means that about 61.2% of the variability in sales is explained by TV advertising."
  },
  {
    "objectID": "qmd/islp3.html#multiple-linear-regression",
    "href": "qmd/islp3.html#multiple-linear-regression",
    "title": "",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nExtends simple linear regression to handle multiple predictors:\n\\[\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p + \\epsilon\n\\]\n\n\\(\\beta_j\\): The average effect on \\(Y\\) of a one-unit increase in \\(X_j\\), holding all other predictors fixed.\n\n\n\n\n\n\n\nEach predictor now has its own slope coefficient."
  },
  {
    "objectID": "qmd/islp3.html#multiple-linear-regression-example-advertising-data",
    "href": "qmd/islp3.html#multiple-linear-regression-example-advertising-data",
    "title": "",
    "section": "Multiple Linear Regression: Example (Advertising Data)",
    "text": "Multiple Linear Regression: Example (Advertising Data)\n\\[\n\\text{sales} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\beta_3 \\times \\text{newspaper} + \\epsilon\n\\]\n\n\n\nPredictor\nCoefficient\nStd. error\nt-statistic\np-value\n\n\n\n\nIntercept\n2.939\n0.3119\n9.42\n&lt; 0.0001\n\n\nTV\n0.046\n0.0014\n32.81\n&lt; 0.0001\n\n\nradio\n0.189\n0.0086\n21.89\n&lt; 0.0001\n\n\nnewspaper\n-0.001\n0.0059\n-0.18\n0.8599\n\n\n\n\n\n\n\n\n\nHolding TV and newspaper advertising fixed, spending an additional $1,000 on radio advertising is associated with an increase in sales of approximately 189 units. The newspaper coefficient is not statistically significant. The interpretation changes compared to simple linear regression due to correlations between the predictors."
  },
  {
    "objectID": "qmd/islp3.html#correlation-between-predictors",
    "href": "qmd/islp3.html#correlation-between-predictors",
    "title": "",
    "section": "Correlation Between Predictors",
    "text": "Correlation Between Predictors\n\n\n\n\nTV\nradio\nnewspaper\nsales\n\n\n\n\nTV\n1.0000\n0.0548\n0.0567\n0.7822\n\n\nradio\n0.0548\n1.0000\n0.3541\n0.5762\n\n\nnewspaper\n0.0567\n0.3541\n1.0000\n0.2283\n\n\nsales\n0.7822\n0.5762\n0.2283\n1.0000\n\n\n\n\n\n\n\n\n\nThis correlation matrix shows the pairwise correlations between the variables in the Advertising data. Notice the moderate correlation (0.35) between radio and newspaper. This explains the difference in the newspaper coefficient between the simple and multiple linear regressions."
  },
  {
    "objectID": "qmd/islp3.html#important-questions-in-multiple-linear-regression",
    "href": "qmd/islp3.html#important-questions-in-multiple-linear-regression",
    "title": "",
    "section": "Important Questions in Multiple Linear Regression",
    "text": "Important Questions in Multiple Linear Regression\n\nAny Useful Predictors? Is at least one predictor useful in predicting the response? (F-test)\nAll or Subset? Do all predictors help explain \\(Y\\), or only a subset? (Variable selection)\nModel Fit: How well does the model fit the data? (RSE, R²)\nPrediction: Given predictor values, what should we predict for the response, and how accurate is our prediction? (Prediction intervals, confidence intervals)"
  },
  {
    "objectID": "qmd/islp3.html#one-is-there-a-relationship-f-test",
    "href": "qmd/islp3.html#one-is-there-a-relationship-f-test",
    "title": "",
    "section": "One: Is There a Relationship? (F-test)",
    "text": "One: Is There a Relationship? (F-test)\n\nNull Hypothesis (H₀): All coefficients are zero (\\(\\beta_1 = \\beta_2 = \\dots = \\beta_p = 0\\)).\nAlternative Hypothesis (Hₐ): At least one coefficient is non-zero.\nF-statistic:\n\n\\[\nF = \\frac{(\\text{TSS} - \\text{RSS})/p}{\\text{RSS}/(n - p - 1)}\n\\]\n\n\n\n\n\n\nIf there’s no relationship between the response and predictors, the F-statistic will be close to 1. If Hₐ is true, F will be greater than 1."
  },
  {
    "objectID": "qmd/islp3.html#f-test-example-advertising-data",
    "href": "qmd/islp3.html#f-test-example-advertising-data",
    "title": "",
    "section": "F-test: Example (Advertising Data)",
    "text": "F-test: Example (Advertising Data)\n\n\n\nQuantity\nValue\n\n\n\n\nResidual standard error\n1.69\n\n\nR²\n0.897\n\n\nF-statistic\n570\n\n\n\n\n\n\n\n\n\nThe F-statistic for the multiple regression of sales on TV, radio, and newspaper is 570. This is much larger than 1, providing strong evidence against the null hypothesis. The p-value is essentially zero, indicating that at least one advertising medium is related to sales."
  },
  {
    "objectID": "qmd/islp3.html#two-deciding-on-important-variables-variable-selection",
    "href": "qmd/islp3.html#two-deciding-on-important-variables-variable-selection",
    "title": "",
    "section": "Two: Deciding on Important Variables (Variable Selection)",
    "text": "Two: Deciding on Important Variables (Variable Selection)\n\nGoal: Identify the subset of predictors that are most strongly related to the response.\nMethods:\n\nForward Selection: Start with the null model (intercept only) and add predictors one by one.\nBackward Selection: Start with all predictors and remove them one by one.\nMixed Selection: Combination of forward and backward selection.\n\n\n\n\n\n\n\n\nWe typically can’t try all possible subsets of predictors (there are 2ᵖ of them!), so we use these more efficient methods."
  },
  {
    "objectID": "qmd/islp3.html#three-model-fit-rse-and-r²",
    "href": "qmd/islp3.html#three-model-fit-rse-and-r²",
    "title": "",
    "section": "Three: Model Fit (RSE and R²)",
    "text": "Three: Model Fit (RSE and R²)\n\nRSE and R²: Same interpretations as in simple linear regression.\nImportant Note: R² will always increase when more variables are added to the model, even if those variables are only weakly associated with the response.\n\n\n\n\n\n\n\nAdding more variables always reduces the RSS on the training data, but may not improve predictions on new data."
  },
  {
    "objectID": "qmd/islp3.html#four-predictions",
    "href": "qmd/islp3.html#four-predictions",
    "title": "",
    "section": "Four: Predictions",
    "text": "Four: Predictions\nThree sources of uncertainty in predictions:\n\nCoefficient Uncertainty: The least squares plane is only an estimate of the true population regression plane. (Reducible error, addressed with confidence intervals)\nModel Bias: The linear model is likely an approximation. (Reducible error, addressed by considering more complex models)\nIrreducible Error: Even if we knew the true relationship, we couldn’t predict \\(Y\\) perfectly because of the random error \\(\\epsilon\\). (Addressed with prediction intervals)"
  },
  {
    "objectID": "qmd/islp3.html#confidence-vs.-prediction-intervals",
    "href": "qmd/islp3.html#confidence-vs.-prediction-intervals",
    "title": "",
    "section": "Confidence vs. Prediction Intervals",
    "text": "Confidence vs. Prediction Intervals\n\nConfidence Interval: Quantifies uncertainty around the average response value.\nPrediction Interval: Quantifies uncertainty around a single response value.\n\n\n\n\n\n\n\nPrediction intervals are always wider than confidence intervals because they account for both the uncertainty in estimating the population regression plane and the inherent variability of individual data points around that plane."
  },
  {
    "objectID": "qmd/islp3.html#qualitative-predictors",
    "href": "qmd/islp3.html#qualitative-predictors",
    "title": "",
    "section": "Qualitative Predictors",
    "text": "Qualitative Predictors\n\nQualitative Predictor (Factor): A variable with categorical values (levels).\nDummy Variable: A numerical variable used to represent a qualitative predictor in a regression model.\n\nFor a predictor with two levels: Create one dummy variable.\nFor a predictor with more than two levels: Create one fewer dummy variable than the number of levels.\nOne level will serve as a baseline.\n\n\n\n\n\n\n\n\nEach dummy variable is coded as 0 or 1, indicating the absence or presence of a particular level."
  },
  {
    "objectID": "qmd/islp3.html#qualitative-predictors-example-credit-data",
    "href": "qmd/islp3.html#qualitative-predictors-example-credit-data",
    "title": "",
    "section": "Qualitative Predictors: Example (Credit Data)",
    "text": "Qualitative Predictors: Example (Credit Data)\nWe want to predict balance using the own variable (whether someone owns a house).\n\nCreate a dummy variable:\n\n\\[\nx_i = \\begin{cases}\n1 & \\text{if person } i \\text{ owns a house} \\\\\n0 & \\text{if person } i \\text{ does not own a house}\n\\end{cases}\n\\]\n\nRegression model:\n\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i = \\begin{cases}\n\\beta_0 + \\beta_1 + \\epsilon_i & \\text{if person } i \\text{ owns a house} \\\\\n\\beta_0 + \\epsilon_i & \\text{if person } i \\text{ does not own a house}\n\\end{cases}\n\\]\n\n\n\n\n\n\n\\(\\beta_0\\) represents the average credit card balance for non-owners, and \\(\\beta_0 + \\beta_1\\) represents the average balance for owners. \\(\\beta_1\\) is the average difference in balance between owners and non-owners."
  },
  {
    "objectID": "qmd/islp3.html#interactions",
    "href": "qmd/islp3.html#interactions",
    "title": "",
    "section": "Interactions",
    "text": "Interactions\n\nAdditive Assumption: The effect of one predictor on the response does not depend on the values of other predictors.\nInteraction Effect (Synergy): The effect of one predictor on the response does depend on the values of other predictors.\nInteraction Term: Include the product of two predictors in the model.\n\n\n\n\n\n\n\nInteractions allow the relationship between a predictor and the response to vary depending on the values of other predictors."
  },
  {
    "objectID": "qmd/islp3.html#interactions-example-advertising-data",
    "href": "qmd/islp3.html#interactions-example-advertising-data",
    "title": "",
    "section": "Interactions: Example (Advertising Data)",
    "text": "Interactions: Example (Advertising Data)\n\\[\n\\text{sales} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\beta_3 \\times (\\text{TV} \\times \\text{radio}) + \\epsilon\n\\]\nThis can be rewritten as:\n\\[\n\\text{sales} = \\beta_0 + (\\beta_1 + \\beta_3 \\times \\text{radio}) \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\epsilon\n\\]\n\n\n\n\n\n\nNow, the slope for TV depends on the value of radio. The interaction term allows for synergy between the advertising media."
  },
  {
    "objectID": "qmd/islp3.html#non-linear-relationships-polynomial-regression",
    "href": "qmd/islp3.html#non-linear-relationships-polynomial-regression",
    "title": "",
    "section": "Non-linear Relationships: Polynomial Regression",
    "text": "Non-linear Relationships: Polynomial Regression\n\nLinearity Assumption: The relationship between the predictors and the response is linear.\nPolynomial Regression: Include polynomial terms (e.g., \\(X^2\\), \\(X^3\\)) of the predictors in the model to capture non-linear relationships.\n\n\\[\n\\text{mpg} = \\beta_0 + \\beta_1 \\times \\text{horsepower} + \\beta_2 \\times \\text{horsepower}^2 + \\epsilon\n\\]\n\n\n\n\n\n\nThis is still a linear regression model (linear in the coefficients), but it models a non-linear relationship between mpg and horsepower.\n\n\n\nPolynomial Regression"
  },
  {
    "objectID": "qmd/islp3.html#potential-problems-in-linear-regression",
    "href": "qmd/islp3.html#potential-problems-in-linear-regression",
    "title": "",
    "section": "Potential Problems in Linear Regression",
    "text": "Potential Problems in Linear Regression\n\nNon-linearity: The relationship between response and predictors is not linear. (Use residual plots to detect)\nCorrelation of Error Terms: Errors are not independent. (Common in time series data)\nNon-constant Variance of Error Terms (Heteroscedasticity): Variance of errors changes with the response. (Funnel shape in residual plot)\nOutliers: Observations with unusual response values.\nHigh Leverage Points: Observations with unusual predictor values.\nCollinearity: Predictors are highly correlated.\n\n\n\n\n\n\n\nThese problems can affect the accuracy and interpretability of the regression model."
  },
  {
    "objectID": "qmd/islp3.html#summary",
    "href": "qmd/islp3.html#summary",
    "title": "",
    "section": "Summary",
    "text": "Summary\n\nLinear regression is a fundamental and versatile tool for predicting a quantitative response.\nIt relies on assumptions about linearity, additivity, and the error terms.\nWe can assess model fit using RSE and R², and assess coefficient significance using t-statistics and p-values.\nMultiple linear regression allows for multiple predictors, and we can use the F-test to check for any relationship between the predictors and the response.\nExtensions like qualitative predictors, interaction terms, and polynomial regression increase the flexibility of the linear model."
  },
  {
    "objectID": "qmd/islp3.html#thoughts-and-discussion",
    "href": "qmd/islp3.html#thoughts-and-discussion",
    "title": "",
    "section": "Thoughts and Discussion",
    "text": "Thoughts and Discussion\n\nHow do we choose the “best” model among a set of possible models (e.g., different combinations of predictors, interactions, polynomial terms)?\nWhat are the limitations of linear regression, and when might other, more complex methods be more appropriate?\nHow can we effectively diagnose and address the potential problems in linear regression (non-linearity, collinearity, etc.)?\nHow can the insights from a linear regression model be used to inform real-world decisions (e.g., marketing strategies)?\nHow does the size and quality of data affect the model outcome?"
  },
  {
    "objectID": "qmd/islp5.html",
    "href": "qmd/islp5.html",
    "title": "",
    "section": "",
    "text": "Welcome to Chapter 5: Resampling Methods!\nThis chapter introduces powerful statistical techniques that help us better understand and utilize our models."
  },
  {
    "objectID": "qmd/islp5.html#introduction",
    "href": "qmd/islp5.html#introduction",
    "title": "",
    "section": "",
    "text": "Welcome to Chapter 5: Resampling Methods!\nThis chapter introduces powerful statistical techniques that help us better understand and utilize our models."
  },
  {
    "objectID": "qmd/islp5.html#what-are-resampling-methods",
    "href": "qmd/islp5.html#what-are-resampling-methods",
    "title": "",
    "section": "What are Resampling Methods?",
    "text": "What are Resampling Methods?\n\nResampling methods are indispensable tools in modern statistics.\nThey involve repeatedly drawing samples from a training set and refitting a model of interest on each sample.\n\nThis allows us to obtain additional information about the fitted model, such as the variability of a linear regression fit.\nThis kind of information might be difficult to get from the original training sample alone.\n\nResampling approaches can be computationally expensive but are usually not prohibitive because they involve fitting the same statistical method multiple times using different data subsets."
  },
  {
    "objectID": "qmd/islp5.html#key-concepts",
    "href": "qmd/islp5.html#key-concepts",
    "title": "",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nModel Assessment: Evaluating a model’s performance (e.g., estimating test error).\nModel Selection: Choosing the appropriate level of flexibility for a model.\nBootstrap: A method to measure the accuracy of a parameter estimate or a statistical learning method."
  },
  {
    "objectID": "qmd/islp5.html#why-resampling",
    "href": "qmd/islp5.html#why-resampling",
    "title": "",
    "section": "Why Resampling?",
    "text": "Why Resampling?\n\nImagine you fit a linear regression model. How confident are you in the coefficient estimates?\nResampling lets us answer this by:\n\nRepeatedly drawing different samples from the training data.\nFitting the model to each new sample.\nExamining how much the resulting fits differ.\n\nThis provides insights beyond a single model fit."
  },
  {
    "objectID": "qmd/islp5.html#two-main-resampling-methods",
    "href": "qmd/islp5.html#two-main-resampling-methods",
    "title": "",
    "section": "Two Main Resampling Methods",
    "text": "Two Main Resampling Methods\nWe will cover the two most commonly used resampling methods:\n\nCross-Validation:\n\nEstimating test error for model assessment.\nSelecting model complexity.\n\nBootstrap:\n\nQuantifying uncertainty (e.g., standard errors) of estimates.\nUseful when standard statistical software doesn’t provide the uncertainty information you need."
  },
  {
    "objectID": "qmd/islp5.html#cross-validation-introduction",
    "href": "qmd/islp5.html#cross-validation-introduction",
    "title": "",
    "section": "Cross-Validation: Introduction",
    "text": "Cross-Validation: Introduction\n\nRemember the distinction between test error and training error (from Chapter 2)?\n\nTraining Error: Calculated on the data used to train the model. It often underestimates the test error.\nTest Error: The average error on new, unseen data. This is what we really care about!\nWe want statistical learning methods that yield low test error.\n\nIdeally, we’d have a large, separate test set to estimate test error directly. Often, we don’t!\nCross-validation helps us estimate test error using only the available training data."
  },
  {
    "objectID": "qmd/islp5.html#the-core-idea-of-cross-validation",
    "href": "qmd/islp5.html#the-core-idea-of-cross-validation",
    "title": "",
    "section": "The Core Idea of Cross-Validation",
    "text": "The Core Idea of Cross-Validation\n\n\n\n\n\n\n\nHold Out Data: We hold out a subset of the training data.\nTrain and Predict: We train the model on the remaining data and then predict the held-out observations.\nEstimate Test Error: Because the held-out data wasn’t used for training, the prediction error on this subset gives us an estimate of the test error.\nThere are a few different ways to do this hold-out, leading to different cross-validation techniques.\n\n\n\n\n\n\n\n\n\nA schematic display of the validation set approach. A set of n observations are randomly split into a training set (shown in blue) and a validation set (shown in beige)."
  },
  {
    "objectID": "qmd/islp5.html#the-validation-set-approach",
    "href": "qmd/islp5.html#the-validation-set-approach",
    "title": "",
    "section": "5.1.1 The Validation Set Approach",
    "text": "5.1.1 The Validation Set Approach\n\nSimplest form of cross-validation.\nProcedure:\n\nRandomly divide the data into two parts:\n\nTraining set: Used to fit the model.\nValidation set (or hold-out set): Used to estimate test error.\n\nFit the model on the training set.\nPredict the responses for the observations in the validation set.\nCalculate the validation set error (e.g., MSE for regression). This is our estimate of the test error."
  },
  {
    "objectID": "qmd/islp5.html#validation-set-approach-example-auto-data",
    "href": "qmd/islp5.html#validation-set-approach-example-auto-data",
    "title": "",
    "section": "Validation Set Approach: Example (Auto Data)",
    "text": "Validation Set Approach: Example (Auto Data)\n\nRecall the Auto data set (Chapter 3). We saw a non-linear relationship between mpg and horsepower.\nWe can use the validation set approach to see if a quadratic or cubic model predicts mpg better than a linear model."
  },
  {
    "objectID": "qmd/islp5.html#validation-set-approach-example-auto-data-1",
    "href": "qmd/islp5.html#validation-set-approach-example-auto-data-1",
    "title": "",
    "section": "Validation Set Approach: Example (Auto Data)",
    "text": "Validation Set Approach: Example (Auto Data)\n\n\n\n\n\n\n\nLeft Panel: Shows the validation set MSE for a single random split of the data. The quadratic model has lower MSE than the linear model.\nRight Panel: Shows validation set MSE for ten different random splits. Notice the variability!"
  },
  {
    "objectID": "qmd/islp5.html#validation-set-approach-drawbacks",
    "href": "qmd/islp5.html#validation-set-approach-drawbacks",
    "title": "",
    "section": "Validation Set Approach: Drawbacks",
    "text": "Validation Set Approach: Drawbacks\n\n\n\n\n\n\n\nHigh Variability: The test error estimate can vary significantly depending on which observations end up in the training and validation sets (as seen in the previous slide).\nOverestimation of Test Error: Only a subset of the data is used for training. Models tend to perform worse with less data, so the validation set error may overestimate the test error of a model trained on the entire dataset."
  },
  {
    "objectID": "qmd/islp5.html#leave-one-out-cross-validation-loocv",
    "href": "qmd/islp5.html#leave-one-out-cross-validation-loocv",
    "title": "",
    "section": "5.1.2 Leave-One-Out Cross-Validation (LOOCV)",
    "text": "5.1.2 Leave-One-Out Cross-Validation (LOOCV)\n\nAddresses the drawbacks of the validation set approach.\nProcedure:\n\nFor each observation i in the dataset (from 1 to n):\n\nHold out observation i as the validation set.\nTrain the model on the remaining n-1 observations.\nPredict the response for observation i (using its predictor values).\nCalculate the error for observation i (e.g., (yi - ŷi)² for regression).\n\nCalculate the LOOCV estimate of the test MSE as the average of these n individual errors:\n\n\\[\nCV_{(n)} = \\frac{1}{n}\\sum_{i=1}^{n}MSE_i\n\\]"
  },
  {
    "objectID": "qmd/islp5.html#loocv-visualized",
    "href": "qmd/islp5.html#loocv-visualized",
    "title": "",
    "section": "LOOCV: Visualized",
    "text": "LOOCV: Visualized\n\n\n\n\n\n\n\n\n\n\nA schematic display of LOOCV. A set of n data points is repeatedly split into a training set (shown in blue) containing all but one observation, and a validation set that contains only that observation (shown in beige). The test error is then estimated by averaging the n resulting MSEs."
  },
  {
    "objectID": "qmd/islp5.html#loocv-advantages",
    "href": "qmd/islp5.html#loocv-advantages",
    "title": "",
    "section": "LOOCV: Advantages",
    "text": "LOOCV: Advantages\n\nLess Bias: LOOCV uses almost all the data (n-1 observations) for training in each iteration. This leads to a less biased estimate of the test error compared to the validation set approach.\nNo Randomness: Unlike the validation set approach, LOOCV always produces the same result because there’s no random splitting."
  },
  {
    "objectID": "qmd/islp5.html#loocv-auto-data-example",
    "href": "qmd/islp5.html#loocv-auto-data-example",
    "title": "",
    "section": "LOOCV: Auto Data Example",
    "text": "LOOCV: Auto Data Example\n\n\n\n\n\n\n\n\n\n\nLeft Panel: The LOOCV error curve for different polynomial models predicting mpg from horsepower.\nRight Panel: Shows multiple 10-fold CV curves."
  },
  {
    "objectID": "qmd/islp5.html#loocv-a-computational-shortcut",
    "href": "qmd/islp5.html#loocv-a-computational-shortcut",
    "title": "",
    "section": "LOOCV: A Computational Shortcut",
    "text": "LOOCV: A Computational Shortcut\n\nLOOCV can be computationally expensive, requiring n model fits.\nShortcut for Least Squares Linear/Polynomial Regression: There’s a magic formula!\n\n\\[\nCV_{(n)} = \\frac{1}{n}\\sum_{i=1}^{n}\\left( \\frac{y_i - \\hat{y}_i}{1 - h_i} \\right)^2\n\\]\n\n\\(\\hat{y}_i\\) is the ith fitted value from the original least squares fit.\n\\(h_i\\) is the leverage (a measure of how much an observation influences its own fit).\nKey Point: This formula lets us calculate LOOCV with the cost of just one model fit!\nThis shortcut does not generally apply to other models (like logistic regression)."
  },
  {
    "objectID": "qmd/islp5.html#k-fold-cross-validation",
    "href": "qmd/islp5.html#k-fold-cross-validation",
    "title": "",
    "section": "5.1.3 k-Fold Cross-Validation",
    "text": "5.1.3 k-Fold Cross-Validation\n\nA compromise between the validation set approach and LOOCV.\nProcedure:\n\nRandomly divide the data into k groups (or “folds”) of approximately equal size.\nFor each fold j (from 1 to k):\n\nTreat fold j as the validation set.\nTrain the model on the remaining k-1 folds.\nCompute the error (e.g., MSE) on the held-out fold j.\n\nCalculate the k-fold CV estimate as the average of the k errors:\n\n\\[\nCV_{(k)} = \\frac{1}{k}\\sum_{i=1}^{k}MSE_i\n\\]"
  },
  {
    "objectID": "qmd/islp5.html#k-fold-cv-visualized",
    "href": "qmd/islp5.html#k-fold-cv-visualized",
    "title": "",
    "section": "k-Fold CV: Visualized",
    "text": "k-Fold CV: Visualized\n\n\n\n\n\n\n\n\n\n\nA schematic display of 5-fold CV. A set of n observations is randomly split into five non-overlapping groups. Each of these fifths acts as a validation set (shown in beige), and the remainder as a training set (shown in blue). The test error is estimated by averaging the five resulting MSE estimates."
  },
  {
    "objectID": "qmd/islp5.html#k-fold-cv-choosing-k",
    "href": "qmd/islp5.html#k-fold-cv-choosing-k",
    "title": "",
    "section": "k-Fold CV: Choosing k",
    "text": "k-Fold CV: Choosing k\n\nCommon choices for k are 5 or 10.\nComputational Advantage: k-fold CV (with k &lt; n) is less computationally expensive than LOOCV. It only requires k model fits, not n."
  },
  {
    "objectID": "qmd/islp5.html#bias-variance-trade-off-for-k-fold-cv",
    "href": "qmd/islp5.html#bias-variance-trade-off-for-k-fold-cv",
    "title": "",
    "section": "5.1.4 Bias-Variance Trade-Off for k-Fold CV",
    "text": "5.1.4 Bias-Variance Trade-Off for k-Fold CV\n\nBias:\n\nLOOCV is nearly unbiased (uses almost all data for training).\nk-fold CV has slightly more bias (uses (k-1)n/k observations for training).\nValidation set approach has the most bias (uses roughly half the data).\n\nVariance:\n\nLOOCV has higher variance than k-fold CV. Why? The n fitted models in LOOCV are highly correlated (they share almost all their training data). Averaging highly correlated quantities has higher variance.\nk-fold CV averages k models with less overlap in training data, leading to lower variance.\n\nConclusion: 5-fold or 10-fold CV often achieves a good balance between bias and variance."
  },
  {
    "objectID": "qmd/islp5.html#cross-validation-on-simulated-data",
    "href": "qmd/islp5.html#cross-validation-on-simulated-data",
    "title": "",
    "section": "Cross-Validation on Simulated Data",
    "text": "Cross-Validation on Simulated Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlue: True test MSE.\nBlack Dashed: LOOCV estimate.\nOrange Solid: 10-fold CV estimate.\nCrosses: Minimum points of each curve.\nThe plots show that CV curves can sometimes underestimate the true test MSE, but they generally identify the correct level of flexibility."
  },
  {
    "objectID": "qmd/islp5.html#cross-validation-for-classification",
    "href": "qmd/islp5.html#cross-validation-for-classification",
    "title": "",
    "section": "5.1.5 Cross-Validation for Classification",
    "text": "5.1.5 Cross-Validation for Classification\n\nSo far, we’ve focused on regression (quantitative response).\nCross-validation works similarly for classification (qualitative response).\nInstead of MSE, we use the number of misclassified observations to quantify error.\nFor example, the LOOCV error rate is:\n\n\\[\nCV_{(n)} = \\frac{1}{n}\\sum_{i=1}^{n}Err_i\n\\]\nwhere \\(Err_i = I(y_i \\neq \\hat{y}_i)\\) (1 if misclassified, 0 otherwise)."
  },
  {
    "objectID": "qmd/islp5.html#cv-for-classification-example",
    "href": "qmd/islp5.html#cv-for-classification-example",
    "title": "",
    "section": "CV for Classification: Example",
    "text": "CV for Classification: Example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPurple Dashed Line: Bayes decision boundary.\nBlack Lines: Decision boundaries from logistic regression with different polynomial degrees."
  },
  {
    "objectID": "qmd/islp5.html#cv-for-classification-example-cont.",
    "href": "qmd/islp5.html#cv-for-classification-example-cont.",
    "title": "",
    "section": "CV for Classification: Example (cont.)",
    "text": "CV for Classification: Example (cont.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeft: Logistic regression with polynomial terms.\nRight: KNN classifier with different values of K.\nBrown: True test error.\nBlue: Training error.\nBlack: 10-fold CV error.\nCross-validation curves often underestimate the true test error but identify the minimum."
  },
  {
    "objectID": "qmd/islp5.html#the-bootstrap",
    "href": "qmd/islp5.html#the-bootstrap",
    "title": "",
    "section": "5.2 The Bootstrap",
    "text": "5.2 The Bootstrap\n\nA powerful and widely applicable tool to quantify the uncertainty of an estimator or statistical learning method.\nExample: Estimating the standard errors of regression coefficients. (Standard software does this for linear regression, but the bootstrap is useful in more general cases)."
  },
  {
    "objectID": "qmd/islp5.html#bootstrap-the-core-idea",
    "href": "qmd/islp5.html#bootstrap-the-core-idea",
    "title": "",
    "section": "Bootstrap: The Core Idea",
    "text": "Bootstrap: The Core Idea\n\nProblem: We usually can’t generate new samples from the population.\nBootstrap Solution: We repeatedly sample observations with replacement from the original data set to create bootstrap data sets.\n\nWith Replacement: The same observation can appear multiple times in a bootstrap data set.\n\nWe treat the original data set as if it were the population."
  },
  {
    "objectID": "qmd/islp5.html#bootstrap-visualized",
    "href": "qmd/islp5.html#bootstrap-visualized",
    "title": "",
    "section": "Bootstrap: Visualized",
    "text": "Bootstrap: Visualized\n\n\n\n\n\n\n\n\n\n\nA graphical illustration of the bootstrap approach on a small sample containing n = 3 observations. Each bootstrap data set contains n observations, sampled with replacement from the original data set."
  },
  {
    "objectID": "qmd/islp5.html#bootstrap-procedure",
    "href": "qmd/islp5.html#bootstrap-procedure",
    "title": "",
    "section": "Bootstrap: Procedure",
    "text": "Bootstrap: Procedure\n\nCreate B bootstrap data sets (each of size n) by sampling with replacement from the original data.\nFor each bootstrap data set, calculate the statistic of interest (e.g., a regression coefficient). This gives you B bootstrap estimates.\nEstimate the standard error of the statistic using the standard deviation of the B bootstrap estimates. \\[SE_B(\\hat{\\alpha}) = \\sqrt{\\frac{1}{B-1} \\sum_{r=1}^B \\left( \\hat{\\alpha}^{*r} - \\frac{1}{B}\\sum_{r'=1}^B \\hat{\\alpha}^{*r'} \\right)^2}\\]"
  },
  {
    "objectID": "qmd/islp5.html#bootstrap-investment-example",
    "href": "qmd/islp5.html#bootstrap-investment-example",
    "title": "",
    "section": "Bootstrap: Investment Example",
    "text": "Bootstrap: Investment Example\n\nWe want to invest in two assets, X and Y, and minimize the risk (variance) of our investment.\nThe optimal fraction (α) to invest in X is given by a formula involving the variances and covariance of X and Y returns.\n\n\\[\n\\alpha = \\frac{\\sigma_Y^2 - \\sigma_{XY}}{\\sigma_X^2 + \\sigma_Y^2 - 2\\sigma_{XY}}\n\\]\n\nIn practice, the population variances and covariance are unknown."
  },
  {
    "objectID": "qmd/islp5.html#bootstrap-example-continued",
    "href": "qmd/islp5.html#bootstrap-example-continued",
    "title": "",
    "section": "Bootstrap Example Continued",
    "text": "Bootstrap Example Continued\n\n\n\n\n\n\n\n\n\n\nEach panel shows 100 simulated returns for investments X and Y."
  },
  {
    "objectID": "qmd/islp5.html#bootstrap-example-cont.",
    "href": "qmd/islp5.html#bootstrap-example-cont.",
    "title": "",
    "section": "Bootstrap: Example (cont.)",
    "text": "Bootstrap: Example (cont.)\n\n\n\n\n\n\n\n\n\n\nLeft: Histogram of α estimates from 1,000 simulated data sets.\nCenter: Histogram of α estimates from 1,000 bootstrap samples from a single data set.\nRight: Boxplots comparing the two.\nThe bootstrap accurately estimates the variability of α!"
  },
  {
    "objectID": "qmd/islp5.html#summary",
    "href": "qmd/islp5.html#summary",
    "title": "",
    "section": "Summary",
    "text": "Summary\n\nResampling methods are crucial for:\n\nAssessing model performance (cross-validation).\nQuantifying uncertainty (bootstrap).\n\nCross-validation:\n\nEstimates test error by holding out data.\nValidation set, LOOCV, and k-fold CV are common techniques.\nk-fold CV often provides a good bias-variance trade-off.\n\nBootstrap:\n\nEstimates uncertainty by resampling with replacement from the original data.\nWidely applicable, especially when standard error formulas are unavailable."
  },
  {
    "objectID": "qmd/islp5.html#thoughts-and-discussion",
    "href": "qmd/islp5.html#thoughts-and-discussion",
    "title": "",
    "section": "Thoughts and Discussion 🤔",
    "text": "Thoughts and Discussion 🤔\n\nWhen might you prefer LOOCV over k-fold CV, despite the higher computational cost?\nCan you think of situations where the bootstrap would be particularly useful?\nHow do these resampling methods relate to the concepts of bias and variance we discussed in earlier chapters?\nWhat are the limitations of these methods? When might they not be appropriate?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "我是邱飞，这是我的个人网站，\n用于分享一些数据分析文章，还有个人的记录和资料。"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "统计学习导论",
    "section": "",
    "text": "教材采用著名的ISL(Introduction to Statistical Learning)\nhttps://www.statlearning.com/"
  },
  {
    "objectID": "index.html#教材",
    "href": "index.html#教材",
    "title": "统计学习导论",
    "section": "",
    "text": "教材采用著名的ISL(Introduction to Statistical Learning)\nhttps://www.statlearning.com/"
  },
  {
    "objectID": "index.html#ppt",
    "href": "index.html#ppt",
    "title": "统计学习导论",
    "section": "ppt",
    "text": "ppt\n 第一章 \n 第二章 \n 第三章 \n 第四章 \n 第五章 \n 第六章 \n 第七章 \n 第八章 \n 第九章 \n 第十章 \n 第十一章 \n 第十二章 \n 第十三章"
  },
  {
    "objectID": "qmd/islp6.html",
    "href": "qmd/islp6.html",
    "title": "Linear Model Selection and Regularization",
    "section": "",
    "text": "Last time, we explored the fundamentals of linear regression, a powerful tool for modeling relationships between variables.\n\n\nHowever, the simple linear model, while interpretable and often effective, has limitations. In this chapter, we extend the linear model. We will discuss:\n\n\n\n\n\nWays in which the simple linear model can be improved.\nAlternative fitting procedures instead of least squares.\n\n\n\nThe Goals:\n\n\n\n\n\nBetter Prediction Accuracy 💪\nImproved Model Interpretability 🧐"
  },
  {
    "objectID": "qmd/islp6.html#introduction-beyond-simple-linear-regression",
    "href": "qmd/islp6.html#introduction-beyond-simple-linear-regression",
    "title": "Linear Model Selection and Regularization",
    "section": "",
    "text": "Last time, we explored the fundamentals of linear regression, a powerful tool for modeling relationships between variables.\n\n\nHowever, the simple linear model, while interpretable and often effective, has limitations. In this chapter, we extend the linear model. We will discuss:\n\n\n\n\n\nWays in which the simple linear model can be improved.\nAlternative fitting procedures instead of least squares.\n\n\n\nThe Goals:\n\n\n\n\n\nBetter Prediction Accuracy 💪\nImproved Model Interpretability 🧐"
  },
  {
    "objectID": "qmd/islp6.html#why-go-beyond-least-squares",
    "href": "qmd/islp6.html#why-go-beyond-least-squares",
    "title": "Linear Model Selection and Regularization",
    "section": "Why Go Beyond Least Squares?",
    "text": "Why Go Beyond Least Squares?\n\n\n\n\n\n\nLet’s recall our standard linear model:\n\n\n\\[\nY = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p + \\epsilon\n\\] (eq: 6.1)\n\n\n\n\n\nWe usually use Least Squares to fit this model.\nLinear model has distinct advantages in terms of inference and competitive in relation to non-linear methods.\nBut, plain Least Squares has some limitations.\n\n\n\n\n\n\n\n\n\n\nWhen do we need to use another fitting procedure instead of least squares?"
  },
  {
    "objectID": "qmd/islp6.html#limitations-of-least-squares-prediction-accuracy",
    "href": "qmd/islp6.html#limitations-of-least-squares-prediction-accuracy",
    "title": "Linear Model Selection and Regularization",
    "section": "Limitations of Least Squares: Prediction Accuracy",
    "text": "Limitations of Least Squares: Prediction Accuracy\n\n\n\n\n\n\n\nLow Bias, Low Variance (Ideal): When the true relationship is approximately linear and you have many more observations (n) than predictors (p) (\\(n \\gg p\\)), least squares works great! The estimates have low bias and low variance.\nHigh Variance (Problem): If n is not much larger than p, the least squares fit can have high variability. This leads to overfitting 🤯 - the model fits the training data too closely and performs poorly on new data.\nNo Unique Solution (Big Problem): If p &gt; n (more predictors than observations), there’s no longer a unique least squares solution! Many possible coefficient values will fit perfectly, leading to huge variance and terrible predictions.\n\n\n\n\n\n\n\n\n\n\nOverfitting: A model that fits the training data too well, capturing noise and random fluctuations rather than the true underlying relationship. It won’t generalize well to new data."
  },
  {
    "objectID": "qmd/islp6.html#limitations-of-least-squares-model-interpretability",
    "href": "qmd/islp6.html#limitations-of-least-squares-model-interpretability",
    "title": "Linear Model Selection and Regularization",
    "section": "Limitations of Least Squares: Model Interpretability",
    "text": "Limitations of Least Squares: Model Interpretability\n\n\n\n\n\n\n\nIrrelevant Variables: Often, some predictors in your model aren’t actually related to the response. Including them adds unnecessary complexity. We’d like to remove these irrelevant variables.\nLeast Squares Doesn’t Zero Out: Least squares rarely sets coefficients exactly to zero. This makes it hard to identify the truly important variables.\nFeature/Variable Selection: We want methods that automatically perform feature selection (or variable selection) – excluding irrelevant variables to create a simpler, more interpretable model.\n\n\n\n\n\n\n\n\n\n\nA model with fewer, carefully selected variables is often easier to understand and explain. It highlights the key drivers of the response."
  },
  {
    "objectID": "qmd/islp6.html#three-classes-of-methods",
    "href": "qmd/islp6.html#three-classes-of-methods",
    "title": "Linear Model Selection and Regularization",
    "section": "Three Classes of Methods",
    "text": "Three Classes of Methods\nTo address these limitations, we explore three main classes of methods that offer alternatives to least squares:\n\nSubset Selection: Identify a subset of the p predictors that are most related to the response. Fit a model using least squares on this reduced set of variables.\nShrinkage (Regularization): Fit a model with all p predictors, but shrink the estimated coefficients towards zero. This reduces variance. Some methods (like the lasso) can even set coefficients exactly to zero, performing variable selection.\nDimension Reduction: Project the p predictors into an M-dimensional subspace (M &lt; p). This means creating M linear combinations (projections) of the original variables. Use these projections as predictors in a least squares model."
  },
  {
    "objectID": "qmd/islp6.html#subset-selection",
    "href": "qmd/islp6.html#subset-selection",
    "title": "Linear Model Selection and Regularization",
    "section": "1. Subset Selection",
    "text": "1. Subset Selection\nWe will introduce several methods to select subsets of predictors. Here we consider best subset and stepwise model selection procedures.\n\n1.1 Best Subset Selection\n\nThe Idea: Fit a separate least squares regression for every possible combination of the p predictors. Then, choose the “best” model from this set.\nExhaustive Search: If you have p predictors, you have 2p possible models! (e.g., 10 predictors = 1,024 models; 20 predictors = over 1 million models!)"
  },
  {
    "objectID": "qmd/islp6.html#best-subset-selection-algorithm",
    "href": "qmd/islp6.html#best-subset-selection-algorithm",
    "title": "Linear Model Selection and Regularization",
    "section": "Best Subset Selection Algorithm",
    "text": "Best Subset Selection Algorithm\n\n\n\n\n\n\nAlgorithm 6.1 Best subset selection\n\n\n\n\nNull Model (M0): A model with no predictors. It simply predicts the sample mean of the response for all observations.\nFor k = 1, 2, …, p: (where k is the number of predictors)\n\nFit all \\(\\binom{p}{k}\\) models that contain exactly k predictors.\nPick the “best” model among these \\(\\binom{p}{k}\\) models, and call it Mk. “Best” is defined as having the smallest Residual Sum of Squares (RSS) or, equivalently, the largest R2.\n\nSelect the ultimate best model: From the models M0, M1, …, Mp, choose the single best model using:\n\nValidation set error\nCross-validation error\nCp (AIC)\nBIC\nAdjusted R2 ## Best Subset Selection: Illustration\n\n\n\n\n\n\n\n\n\n\n\nCredit data: RSS and \\(R^2\\) for all possible models. The red frontier tracks the best model for each number of predictors.\n\n\n\n\n\nFigure 6.1: Shows RSS and R2 for all possible models on the Credit dataset.\nThe data contains ten predictors, but the x-axis ranges to 11. The reason is that one of the predictors is categorical, taking three values. It is split up into two dummy variables.\nThe red line connects the best models for each size (lowest RSS or highest R2).\nAs expected, RSS decreases and R2 increases as more variables are added. However, the improvements become very small after just a few variables."
  },
  {
    "objectID": "qmd/islp6.html#best-subset-selection-choosing-the-best-model",
    "href": "qmd/islp6.html#best-subset-selection-choosing-the-best-model",
    "title": "Linear Model Selection and Regularization",
    "section": "Best Subset Selection: Choosing the Best Model",
    "text": "Best Subset Selection: Choosing the Best Model\n\n\n\n\n\n\nThe RSS of these p + 1 models decreases monotonically, and the R² increases monotonically, as the number of features included in the models increases. So we can’t use them to select the best model!\n\n\n\n\nTraining Error vs. Test Error: Low RSS and high R2 indicate a good fit to the training data. But we want a model that performs well on new, unseen data (low test error). Training error is often much smaller than test error!\nNeed a Different Criterion: We can’t use RSS or R2 directly to select the best model. We need to estimate the test error."
  },
  {
    "objectID": "qmd/islp6.html#best-subset-selection-computational-limitations",
    "href": "qmd/islp6.html#best-subset-selection-computational-limitations",
    "title": "Linear Model Selection and Regularization",
    "section": "Best Subset Selection: Computational Limitations",
    "text": "Best Subset Selection: Computational Limitations\n\nExponential Growth: The number of possible models (2p) grows very quickly as p increases.\nInfeasible for Large p: Best subset selection becomes computationally infeasible for even moderately large values of p (e.g., p &gt; 40).\nStatistical Problems (Large p): With a huge search space, there’s a higher chance of finding models that fit the training data well by chance, even if they have no real predictive power. This leads to overfitting and high variance in the coefficient estimates.\n\n\n1.2 Stepwise Selection\nBest subset selection is often computationally infeasible for large p. Thus, stepwise methods are attractive alternatives.\n\nStepwise methods explore a far more restricted set of models.\n\n\n1.2.1 Forward Stepwise Selection\n\nThe Idea: Start with the null model (no predictors). Add predictors one-at-a-time, always choosing the variable that gives the greatest additional improvement to the fit.\n\n\n\n\n\n\n\nAlgorithm 6.2 Forward stepwise selection\n\n\n\n\nNull Model (M0): Start with the model containing no predictors.\nFor k = 0, 1, …, p-1:\n\nConsider all p - k models that add one additional predictor to the current model (Mk).\nChoose the “best” of these p - k models (smallest RSS or highest R2), and call it Mk+1.\n\nSelect the ultimate best model: Choose the single best model from M0, M1, …, Mp using validation set error, cross-validation, Cp, BIC, or adjusted R2."
  },
  {
    "objectID": "qmd/islp6.html#forward-stepwise-selection-computational-advantage",
    "href": "qmd/islp6.html#forward-stepwise-selection-computational-advantage",
    "title": "Linear Model Selection and Regularization",
    "section": "Forward Stepwise Selection: Computational Advantage",
    "text": "Forward Stepwise Selection: Computational Advantage\n\nMuch Fewer Models: Forward stepwise selection considers many fewer models than best subset selection.\n\nBest subset: 2p models.\nForward stepwise: 1 + p(p+1)/2 models.\nExample: If p = 20, best subset considers over 1 million models, while forward stepwise considers only 211.\n\nComputational Efficiency: This makes forward stepwise selection computationally feasible for much larger values of p."
  },
  {
    "objectID": "qmd/islp6.html#forward-stepwise-selection-limitations",
    "href": "qmd/islp6.html#forward-stepwise-selection-limitations",
    "title": "Linear Model Selection and Regularization",
    "section": "Forward Stepwise Selection: Limitations",
    "text": "Forward Stepwise Selection: Limitations\n\nNot Guaranteed Optimal: Forward stepwise selection is not guaranteed to find the best possible model out of all 2p possibilities. It’s a greedy algorithm – it makes the locally optimal choice at each step, which may not lead to the globally optimal solution.\nExample:\n\nSuppose the best 1-variable model contains X1.\nThe best 2-variable model might contain X2 and X3.\nForward stepwise won’t find this, because it must keep X1 in the 2-variable model."
  },
  {
    "objectID": "qmd/islp6.html#forward-stepwise-selection-vs.-best-subset-selection-an-example",
    "href": "qmd/islp6.html#forward-stepwise-selection-vs.-best-subset-selection-an-example",
    "title": "Linear Model Selection and Regularization",
    "section": "Forward Stepwise Selection vs. Best Subset Selection: An Example",
    "text": "Forward Stepwise Selection vs. Best Subset Selection: An Example\n\n\n\n\n\n\n\nComparison on the Credit dataset.\n\n\n\n\n\n\n\n# Variables\nBest Subset\nForward Stepwise\n\n\n\n\nOne\nrating\nrating\n\n\nTwo\nrating, income\nrating, income\n\n\nThree\nrating, income, student\nrating, income, student\n\n\nFour\ncards, income, student, limit\nrating, income, student, limit\n\n\n\n\n\n\nThe first three models selected are identical.\nThe fourth models differ.\nBut in this example, the four-variable models perform very similarly (see Figure 6.1), so the difference isn’t crucial."
  },
  {
    "objectID": "qmd/islp6.html#forward-stepwise-in-high-dimensions",
    "href": "qmd/islp6.html#forward-stepwise-in-high-dimensions",
    "title": "Linear Model Selection and Regularization",
    "section": "Forward Stepwise in High Dimensions",
    "text": "Forward Stepwise in High Dimensions\n\nn &lt; p Case: Forward stepwise selection can be used even when n &lt; p (more predictors than observations).\nLimitation: In this case, you can only build models up to size Mn-1, because least squares can’t fit a unique solution when p ≥ n.\n\n\n1.2.2 Backward Stepwise Selection\n\nThe Idea: Start with the full model (all p predictors). Remove the least useful predictor one-at-a-time.\n\n\n\n\n\n\n\nAlgorithm 6.3 Backward stepwise selection\n\n\n\n\nFull Model (Mp): Begin with the model containing all p predictors.\nFor k = p, p-1, …, 1:\n\nConsider all k models that remove one predictor from the current model (Mk).\nChoose the “best” of these k models (smallest RSS or highest R2), and call it Mk-1.\n\nSelect the ultimate best model: Select the single best model from M0, …, Mp using validation set error, cross-validation, Cp, BIC, or adjusted R2."
  },
  {
    "objectID": "qmd/islp6.html#backward-stepwise-selection-properties",
    "href": "qmd/islp6.html#backward-stepwise-selection-properties",
    "title": "Linear Model Selection and Regularization",
    "section": "Backward Stepwise Selection: Properties",
    "text": "Backward Stepwise Selection: Properties\n\nComputational Advantage: Like forward stepwise, backward stepwise considers only 1 + p(p+1)/2 models, making it computationally efficient.\nNot Guaranteed Optimal: Like forward stepwise, it’s not guaranteed to find the best possible model.\nRequirement: n &gt; p: Backward stepwise selection requires that n &gt; p (more observations than predictors) so that the full model can be fit.\n\n\n\n\n\n\n\nForward stepwise can be used even when n &lt; p, and so is the only viable subset method when p is very large.\n\n\n\n\n1.2.3 Hybrid Approaches\n\nCombine Forward and Backward: Hybrid methods combine aspects of forward and backward stepwise selection.\nAdd and Remove: Variables are added sequentially (like forward). But, after adding each new variable, the method may also remove any variables that no longer contribute significantly to the model fit.\nGoal: Try to mimic best subset selection while retaining the computational advantages of stepwise methods.\n\n\n\n1.3 Choosing the Optimal Model\n\n\n\n\n\n\nBest subset selection, forward selection, and backward selection result in the creation of a set of models, each of which contains a subset of the p predictors.\n\n\n\n\nThe Challenge: How do we choose the best model from among the set of models generated by subset selection or stepwise selection? We cannot simply use the model that has the smallest RSS and the largest R2!\nNeed to Estimate Test Error: We need to estimate the test error of each model.\nTwo Main Approaches:\n\nIndirectly Estimate Test Error: Adjust the training error to account for the bias due to overfitting.\nDirectly Estimate Test Error: Use a validation set or cross-validation."
  },
  {
    "objectID": "qmd/islp6.html#indirectly-estimating-test-error-cp-aic-bic-adjusted-r2",
    "href": "qmd/islp6.html#indirectly-estimating-test-error-cp-aic-bic-adjusted-r2",
    "title": "Linear Model Selection and Regularization",
    "section": "Indirectly Estimating Test Error: Cp, AIC, BIC, Adjusted R2",
    "text": "Indirectly Estimating Test Error: Cp, AIC, BIC, Adjusted R2\n\nTraining Error is Deceptive: The training set MSE (RSS/n) generally underestimates the test MSE. This is because least squares specifically minimizes the training RSS.\nAdjusting for Model Size: We need to adjust the training error to account for the fact that it tends to be too optimistic. Several techniques do this:\n\nCp\nAkaike Information Criterion (AIC)\nBayesian Information Criterion (BIC)\nAdjusted R2"
  },
  {
    "objectID": "qmd/islp6.html#cp-aic-bic-adjusted-r2-formulas",
    "href": "qmd/islp6.html#cp-aic-bic-adjusted-r2-formulas",
    "title": "Linear Model Selection and Regularization",
    "section": "Cp, AIC, BIC, Adjusted R2: Formulas",
    "text": "Cp, AIC, BIC, Adjusted R2: Formulas\nFor a least squares model with d predictors, these statistics are computed as:\n\nCp: \\[\nC_p = \\frac{1}{n}(RSS + 2d\\hat{\\sigma}^2)\n\\] (eq: 6.2)\n\n\\(\\hat{\\sigma}^2\\) is an estimate of the error variance.\nAdds a penalty proportional to the number of predictors.\n\nAIC: \\[\nAIC = \\frac{1}{n}(RSS + 2d\\hat{\\sigma}^2)\n\\]\n\nFor linear model with Gaussian errors, AIC is proportional to Cp.\n\nBIC: \\[\nBIC = \\frac{1}{n}(RSS + log(n)d\\hat{\\sigma}^2)\n\\] (eq: 6.3)\n\nSimilar to Cp, but the penalty for the number of predictors is multiplied by log(n).\nSince log(n) &gt; 2 for n &gt; 7, BIC generally penalizes models with more variables more heavily than Cp, leading to the selection of smaller models.\n\nAdjusted R2: \\[\nAdjusted \\ R^2 = 1 - \\frac{RSS/(n - d - 1)}{TSS/(n-1)}\n\\] (eq: 6.4)\nMaximizing the adjusted R² is equivalent to minimizing \\(\\frac{RSS}{n-d-1}\\)."
  },
  {
    "objectID": "qmd/islp6.html#cp-aic-bic-adjusted-r2-interpretation",
    "href": "qmd/islp6.html#cp-aic-bic-adjusted-r2-interpretation",
    "title": "Linear Model Selection and Regularization",
    "section": "Cp, AIC, BIC, Adjusted R2: Interpretation",
    "text": "Cp, AIC, BIC, Adjusted R2: Interpretation\n\nLow Values are Good (Cp, AIC, BIC): For Cp, AIC, and BIC, we choose the model with the lowest value.\nHigh Values are Good (Adjusted R2): For adjusted R2, we choose the model with the highest value.\nTheoretical Justification: Cp, AIC, and BIC have theoretical justifications (though they rely on assumptions that may not always hold). Adjusted R2 is more intuitive but less theoretically grounded."
  },
  {
    "objectID": "qmd/islp6.html#cp-aic-bic-adjusted-r2-example",
    "href": "qmd/islp6.html#cp-aic-bic-adjusted-r2-example",
    "title": "Linear Model Selection and Regularization",
    "section": "Cp, AIC, BIC, Adjusted R2: Example",
    "text": "Cp, AIC, BIC, Adjusted R2: Example\n\n\n\nCp, BIC, and adjusted R2 for the best models of each size on the Credit data.\n\n\n\nFigure 6.2: Shows these statistics for the Credit dataset.\nCp and BIC are estimates of test MSE.\nBIC selects a model with 4 variables (income, limit, cards, student).\nCp selects a 6-variable model.\nAdjusted R2 selects a 7-variable model."
  },
  {
    "objectID": "qmd/islp6.html#directly-estimating-test-error-validation-and-cross-validation",
    "href": "qmd/islp6.html#directly-estimating-test-error-validation-and-cross-validation",
    "title": "Linear Model Selection and Regularization",
    "section": "Directly Estimating Test Error: Validation and Cross-Validation",
    "text": "Directly Estimating Test Error: Validation and Cross-Validation\n\nDirect Estimation: Instead of adjusting the training error, we can directly estimate the test error using:\n\nValidation set approach\nCross-validation approach\n\nAdvantages:\n\nProvide a direct estimate of the test error.\nMake fewer assumptions about the true underlying model.\nCan be used in a wider range of model selection tasks.\n\nComputational Cost: Historically, cross-validation was computationally expensive. Now, with fast computers, this is less of a concern."
  },
  {
    "objectID": "qmd/islp6.html#validation-and-cross-validation-example",
    "href": "qmd/islp6.html#validation-and-cross-validation-example",
    "title": "Linear Model Selection and Regularization",
    "section": "Validation and Cross-Validation: Example",
    "text": "Validation and Cross-Validation: Example\n\n\n\nBIC, validation set error, and cross-validation error for the best models of each size on the Credit data.\n\n\n\nFigure 6.3: Shows BIC, validation set error, and cross-validation error for the Credit data.\nValidation and cross-validation both select a 6-variable model.\nAll three approaches suggest that models with 4, 5, or 6 variables are quite similar.\nOne-Standard-Error Rule: A practical rule for choosing among models with similar estimated test error.\n\nCalculate the standard error of the estimated test MSE for each model size.\nSelect the smallest model for which the estimated test error is within one standard error of the lowest point on the curve.\nRationale: Choose the simplest model among those that perform comparably.\n\nApply to this example, we may choose the three-variable model."
  },
  {
    "objectID": "qmd/islp6.html#shrinkage-methods",
    "href": "qmd/islp6.html#shrinkage-methods",
    "title": "Linear Model Selection and Regularization",
    "section": "2. Shrinkage Methods",
    "text": "2. Shrinkage Methods\n\nAlternative to Subset Selection: Instead of selecting a subset of variables, shrinkage methods (also called regularization methods) fit a model with all p predictors, but constrain or regularize the coefficient estimates.\nHow it Works: Shrinkage methods shrink the coefficient estimates towards zero.\nWhy Shrink?: Shrinking the coefficients can significantly reduce their variance.\nTwo Main Techniques:\n\nRidge regression\nLasso\n\n\n\n2.1 Ridge Regression\n\nRecall Least Squares: Least squares minimizes the Residual Sum of Squares (RSS):\n\n\\[\nRSS = \\sum_{i=1}^{n}(y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij})^2\n\\]\n\nRidge Regression: Ridge regression minimizes a slightly different quantity:\n\n\\[\n\\sum_{i=1}^{n}(y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij})^2 + \\lambda\\sum_{j=1}^{p}\\beta_j^2 = RSS + \\lambda\\sum_{j=1}^{p}\\beta_j^2\n\\] (eq: 6.5)\n\nλ (Tuning Parameter): λ ≥ 0 is a tuning parameter that controls the amount of shrinkage."
  },
  {
    "objectID": "qmd/islp6.html#ridge-regression-the-shrinkage-penalty",
    "href": "qmd/islp6.html#ridge-regression-the-shrinkage-penalty",
    "title": "Linear Model Selection and Regularization",
    "section": "Ridge Regression: The Shrinkage Penalty",
    "text": "Ridge Regression: The Shrinkage Penalty\n\\[\n\\sum_{i=1}^{n}(y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij})^2 + \\lambda\\sum_{j=1}^{p}\\beta_j^2 = RSS + \\lambda\\sum_{j=1}^{p}\\beta_j^2\n\\]\n\nTwo Parts:\n\nRSS: Measures how well the model fits the data.\nShrinkage Penalty (λΣβj2): Penalizes large coefficients. This term is small when β1, …, βp are close to zero.\n\nTuning Parameter (λ):\n\nλ = 0: No penalty. Ridge regression is the same as least squares.\nλ → ∞: Coefficients are shrunk all the way to zero (null model).\n0 &lt; λ &lt; ∞: Controls the trade-off between fitting the data well and shrinking the coefficients."
  },
  {
    "objectID": "qmd/islp6.html#ridge-regression-the-intercept",
    "href": "qmd/islp6.html#ridge-regression-the-intercept",
    "title": "Linear Model Selection and Regularization",
    "section": "Ridge Regression: The Intercept",
    "text": "Ridge Regression: The Intercept\n\nNo Shrinkage on Intercept: Notice that the shrinkage penalty is not applied to the intercept (β0).\nWhy?: We want to shrink the coefficients of the predictors, but not the intercept, which represents the average value of the response when all predictors are zero.\nCentering Predictors: If the predictors are centered (mean of zero) before performing ridge regression, then the estimated intercept will be the sample mean of the response: \\(\\hat{\\beta}_0 = \\bar{y}\\)."
  },
  {
    "objectID": "qmd/islp6.html#ridge-regression-example-on-credit-data",
    "href": "qmd/islp6.html#ridge-regression-example-on-credit-data",
    "title": "Linear Model Selection and Regularization",
    "section": "Ridge Regression: Example on Credit Data",
    "text": "Ridge Regression: Example on Credit Data\n\n\n\nStandardized ridge regression coefficients for the Credit data, as a function of λ and ||βλR||2 / ||β||2.\n\n\n\nFigure 6.4: Shows ridge regression coefficient estimates for the Credit data.\nLeft Panel: Coefficients plotted against λ.\n\nλ = 0: Coefficients are the same as least squares.\nAs λ increases, coefficients shrink towards zero.\n\nRight Panel: Coefficients plotted against ||βλR||2 / ||β||2\nThe x-axis can be seen as how much the ridge regression coefficient estimates have been shrunken towards zero."
  },
  {
    "objectID": "qmd/islp6.html#ridge-regression-standardization",
    "href": "qmd/islp6.html#ridge-regression-standardization",
    "title": "Linear Model Selection and Regularization",
    "section": "Ridge Regression: Standardization",
    "text": "Ridge Regression: Standardization\n\nScale Equivariance (Least Squares): Least squares coefficient estimates are scale equivariant. Multiplying a predictor by a constant c simply scales the corresponding coefficient by 1/c.\nScale Dependence (Ridge Regression): Ridge regression coefficients can change substantially when multiplying a predictor by a constant. This is because of the Σβj2 term in the penalty.\nStandardization: It’s best to apply ridge regression after standardizing the predictors:\n\n\\[\n\\tilde{x}_{ij} = \\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(x_{ij}-\\bar{x}_j)^2}}\n\\] (eq: 6.6)\n\nThis ensures that all predictors are on the same scale."
  },
  {
    "objectID": "qmd/islp6.html#why-does-ridge-regression-improve-over-least-squares",
    "href": "qmd/islp6.html#why-does-ridge-regression-improve-over-least-squares",
    "title": "Linear Model Selection and Regularization",
    "section": "Why Does Ridge Regression Improve Over Least Squares?",
    "text": "Why Does Ridge Regression Improve Over Least Squares?\n\nBias-Variance Trade-Off: Ridge regression’s advantage comes from the bias-variance trade-off.\n\nAs λ increases:\n\nFlexibility of the model decreases.\nVariance decreases.\nBias increases.\n\n\nFinding the Sweet Spot: The goal is to find a value of λ that reduces variance more than it increases bias, leading to a lower test MSE."
  },
  {
    "objectID": "qmd/islp6.html#ridge-regression-bias-variance-trade-off-illustrated",
    "href": "qmd/islp6.html#ridge-regression-bias-variance-trade-off-illustrated",
    "title": "Linear Model Selection and Regularization",
    "section": "Ridge Regression: Bias-Variance Trade-Off Illustrated",
    "text": "Ridge Regression: Bias-Variance Trade-Off Illustrated\n\n\n\nSquared bias, variance, and test MSE for ridge regression on a simulated dataset.\n\n\n\nFigure 6.5: Shows bias, variance, and test MSE for ridge regression on a simulated dataset.\nAs λ increases, variance decreases rapidly at first, with only a small increase in bias. This leads to a decrease in MSE.\nEventually, the decrease in variance slows, and the increase in bias accelerates, causing the MSE to increase.\nThe minimum MSE is achieved at a moderate value of λ."
  },
  {
    "objectID": "qmd/islp6.html#when-does-ridge-regression-work-well",
    "href": "qmd/islp6.html#when-does-ridge-regression-work-well",
    "title": "Linear Model Selection and Regularization",
    "section": "When Does Ridge Regression Work Well?",
    "text": "When Does Ridge Regression Work Well?\n\nHigh Variance in Least Squares: Ridge regression works best in situations where the least squares estimates have high variance. This often happens when:\n\nn is not much larger than p.\np is close to n.\np &gt; n (though least squares doesn’t have a unique solution in this case).\n\nComputational Advantage: Ridge regression is also computationally efficient, even for large p.\n\n\n2.2 The Lasso\n\nDisadvantage of Ridge Regression: Ridge regression includes all p predictors in the final model. The penalty shrinks coefficients towards zero, but it doesn’t set any of them exactly to zero (unless λ = ∞). This can make interpretation difficult when p is large.\nThe Lasso: An Alternative: The lasso is a more recent alternative to ridge regression that overcomes this disadvantage.\nLasso Penalty: The lasso uses a different penalty term:\n\n\\[\n\\sum_{i=1}^{n}(y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij})^2 + \\lambda\\sum_{j=1}^{p}|\\beta_j| = RSS + \\lambda\\sum_{j=1}^{p}|\\beta_j|\n\\] (eq: 6.7)\n\nAbsolute Value Penalty: The lasso uses an l1 penalty (absolute value of coefficients) instead of an l2 penalty (squared coefficients)."
  },
  {
    "objectID": "qmd/islp6.html#the-lasso-variable-selection",
    "href": "qmd/islp6.html#the-lasso-variable-selection",
    "title": "Linear Model Selection and Regularization",
    "section": "The Lasso: Variable Selection",
    "text": "The Lasso: Variable Selection\n\nShrinkage and Selection: Like ridge regression, the lasso shrinks coefficients towards zero.\nKey Difference: The l1 penalty has the effect of forcing some coefficients to be exactly zero when λ is sufficiently large.\nVariable Selection: This means the lasso performs variable selection!\nSparse Models: The lasso yields sparse models – models that involve only a subset of the variables."
  },
  {
    "objectID": "qmd/islp6.html#the-lasso-example-on-credit-data",
    "href": "qmd/islp6.html#the-lasso-example-on-credit-data",
    "title": "Linear Model Selection and Regularization",
    "section": "The Lasso: Example on Credit Data",
    "text": "The Lasso: Example on Credit Data\n\n\n\nStandardized lasso coefficients for the Credit data, as a function of λ and ||βλL||1 / ||β||1.\n\n\n\nFigure 6.6: Shows lasso coefficient estimates for the Credit data.\nAs λ increases, coefficients shrink towards zero. But unlike ridge regression, some coefficients are set exactly to zero.\nThis leads to a simpler, more interpretable model."
  },
  {
    "objectID": "qmd/islp6.html#another-formulation-for-ridge-regression-and-the-lasso",
    "href": "qmd/islp6.html#another-formulation-for-ridge-regression-and-the-lasso",
    "title": "Linear Model Selection and Regularization",
    "section": "Another Formulation for Ridge Regression and the Lasso",
    "text": "Another Formulation for Ridge Regression and the Lasso\n\nBoth ridge regression and the lasso can be formulated as constrained optimization problems.\n\nRidge Regression: \\[\n  \\underset{\\beta}{minimize} \\left\\{ \\sum_{i=1}^{n}(y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij})^2 \\right\\} \\quad subject \\ to \\ \\sum_{j=1}^{p}\\beta_j^2 \\le s\n  \\] (eq: 6.9)\nLasso: \\[\n  \\underset{\\beta}{minimize} \\left\\{ \\sum_{i=1}^{n}(y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij})^2 \\right\\} \\quad subject \\ to \\ \\sum_{j=1}^{p}|\\beta_j| \\le s\n  \\] (eq: 6.8)\n\nThe above formulations reveal a close connection between the lasso, ridge regression, and best subset selection."
  },
  {
    "objectID": "qmd/islp6.html#the-variable-selection-property-of-the-lasso",
    "href": "qmd/islp6.html#the-variable-selection-property-of-the-lasso",
    "title": "Linear Model Selection and Regularization",
    "section": "The Variable Selection Property of the Lasso",
    "text": "The Variable Selection Property of the Lasso\n\n\n\n\n\n\n\nWhy does the lasso set coefficients to zero, while ridge regression doesn’t?\n\nConsider the constraint regions (where the solution must lie):\n\nRidge regression: A circle (l2 constraint).\nLasso: A diamond (l1 constraint).\n\n\n\n\n\n\n\n\nContours of the error and constraint functions for the lasso (left) and ridge regression (right).\n\n\n\n\n\n\n\nThe solution is the first point where the “ellipse” (contour of constant RSS) touches the constraint region.\nBecause the lasso constraint has corners, the ellipse often intersects at an axis, setting one coefficient to zero.\nRidge regression’s circular constraint doesn’t have corners, so this rarely happens."
  },
  {
    "objectID": "qmd/islp6.html#comparing-the-lasso-and-ridge-regression",
    "href": "qmd/islp6.html#comparing-the-lasso-and-ridge-regression",
    "title": "Linear Model Selection and Regularization",
    "section": "Comparing the Lasso and Ridge Regression",
    "text": "Comparing the Lasso and Ridge Regression\n\nInterpretability: The lasso has a major advantage in terms of interpretability, producing simpler models with fewer variables.\nPrediction Accuracy: Which method is better for prediction depends on the true underlying relationship between the predictors and the response.\n\nFew Important Predictors: If only a few predictors are truly important (with large coefficients), the lasso tends to perform better.\nMany Important Predictors: If many predictors have small or moderate-sized coefficients, ridge regression tends to perform better.\n\nUnknown Truth: In practice, we don’t know which scenario is true. Cross-validation can help us choose the best approach for a particular dataset."
  },
  {
    "objectID": "qmd/islp6.html#comparing-the-lasso-and-ridge-regression-simulated-examples",
    "href": "qmd/islp6.html#comparing-the-lasso-and-ridge-regression-simulated-examples",
    "title": "Linear Model Selection and Regularization",
    "section": "Comparing the Lasso and Ridge Regression: Simulated Examples",
    "text": "Comparing the Lasso and Ridge Regression: Simulated Examples\n\n\n\n\n\n\n\n\n\nLasso and ridge regression on a simulated dataset where all predictors are related to the response.\n\n\n\n\n\nFigure 6.8: All 45 predictors are related to the response.\nRidge regression slightly outperforms the lasso.\nThe minimum MSE of ridge regression is slightly smaller than that of the lasso.\n\n\n\n\n\n\n\n\nLasso and ridge regression on a simulated dataset where only two predictors are related to the response.\n\n\n\n\n\nFigure 6.9: Only 2 of 45 predictors are related to the response.\nThe lasso tends to outperform ridge regression."
  },
  {
    "objectID": "qmd/islp6.html#a-simple-special-case-for-ridge-regression-and-the-lasso",
    "href": "qmd/islp6.html#a-simple-special-case-for-ridge-regression-and-the-lasso",
    "title": "Linear Model Selection and Regularization",
    "section": "A Simple Special Case for Ridge Regression and the Lasso",
    "text": "A Simple Special Case for Ridge Regression and the Lasso\nWe consider a simple situation: - \\(n=p\\). - \\(\\mathbf{X}\\) is a diagonal matrix with 1’s on the diagonal. - No intercept. Then, it can be shown: - Ridge regression shrinks each least squares coefficient estimate by the same proportion. \\[\n        \\hat{\\beta}_j^R = y_j / (1 + \\lambda)\n        \\] (eq: 6.14) - Lasso soft-threshold the least squares coefficient estimates. \\[\n        \\hat{\\beta}_j^L =\n        \\begin{cases}\n        y_j - \\lambda/2 & \\text{if } y_j &gt; \\lambda/2 \\\\\n        y_j + \\lambda/2 & \\text{if } y_j &lt; -\\lambda/2 \\\\\n        0 & \\text{if } |y_j| \\le \\lambda/2\n        \\end{cases}\n        \\] (eq: 6.15)"
  },
  {
    "objectID": "qmd/islp6.html#a-simple-special-case-for-ridge-regression-and-the-lasso-1",
    "href": "qmd/islp6.html#a-simple-special-case-for-ridge-regression-and-the-lasso-1",
    "title": "Linear Model Selection and Regularization",
    "section": "A Simple Special Case for Ridge Regression and the Lasso",
    "text": "A Simple Special Case for Ridge Regression and the Lasso\n\n\n\n\n\n\n\n\n\nRidge and lasso coefficient estimates in the simple case.\n\n\n\n\n\nFigure 6.10: It shows that:\nRidge regression shrinks each coefficient by same proportion.\nLasso shrinks all coefficients toward zero by a similar amount, and sufficiently small coefficients are shrunken all the way to zero.\n\n\n\n\n\n2.3 Selecting the Tuning Parameter\n\nCrucial Choice: Just like with subset selection, we need to choose the tuning parameter (λ) for ridge regression and the lasso.\nCross-Validation: Cross-validation is a powerful method for selecting λ.\n\nChoose a grid of λ values.\nCompute the cross-validation error for each value of λ.\nSelect the λ that gives the smallest cross-validation error.\nRe-fit the model using all of the data with the chosen λ."
  },
  {
    "objectID": "qmd/islp6.html#selecting-λ-example-for-ridge-regression",
    "href": "qmd/islp6.html#selecting-λ-example-for-ridge-regression",
    "title": "Linear Model Selection and Regularization",
    "section": "Selecting λ: Example for Ridge Regression",
    "text": "Selecting λ: Example for Ridge Regression\n\n\n\nCross-validation error and coefficient estimates for ridge regression on the Credit data.\n\n\n\nFigure 6.12: Shows cross-validation for ridge regression on the Credit data.\nThe optimal λ is relatively small, indicating a small amount of shrinkage.\nThe cross-validation error curve is quite flat, suggesting that a range of λ values would work similarly well."
  },
  {
    "objectID": "qmd/islp6.html#selecting-λ-example-for-lasso",
    "href": "qmd/islp6.html#selecting-λ-example-for-lasso",
    "title": "Linear Model Selection and Regularization",
    "section": "Selecting λ: Example for Lasso",
    "text": "Selecting λ: Example for Lasso\n\n\n\n\n\n\n\n\n\nCross-validation error and coefficient estimates for the lasso on the simulated data from Figure 6.9.\n\n\n\n\n\nFigure 6.13: Shows cross-validation for the lasso on the simulated data from Figure 6.9 (where only two predictors are truly related to the response).\nThe lasso correctly identifies the two signal variables (colored lines) and sets the coefficients of the noise variables (gray lines) to near zero.\nThe minimum cross-validation error occurs when only the signal variables have non-zero coefficients."
  },
  {
    "objectID": "qmd/islp6.html#dimension-reduction-methods",
    "href": "qmd/islp6.html#dimension-reduction-methods",
    "title": "Linear Model Selection and Regularization",
    "section": "3. Dimension Reduction Methods",
    "text": "3. Dimension Reduction Methods\n\nDifferent Approach: Instead of working directly with the original predictors (X1, …, Xp), dimension reduction methods transform the predictors and then fit a least squares model using the transformed variables.\nLinear Combinations: Create M linear combinations (Z1, …, ZM) of the original p predictors, where M &lt; p.\n\n\\[\nZ_m = \\sum_{j=1}^{p}\\phi_{jm}X_j\n\\] (eq: 6.16)\n-   φ&lt;sub&gt;jm&lt;/sub&gt; are constants.\n\nReduced Dimension: Fit a linear regression model using Z1, …, ZM as predictors:\n\n\\[\ny_i = \\theta_0 + \\sum_{m=1}^{M}\\theta_mz_{im} + \\epsilon_i\n\\] (eq: 6.17)\n-   This reduces the problem from estimating *p*+1 coefficients to estimating *M*+1 coefficients."
  },
  {
    "objectID": "qmd/islp6.html#dimension-reduction-why-it-works",
    "href": "qmd/islp6.html#dimension-reduction-why-it-works",
    "title": "Linear Model Selection and Regularization",
    "section": "Dimension Reduction: Why it Works",
    "text": "Dimension Reduction: Why it Works\n\nConstraint: The coefficients in the dimension-reduced model are constrained by the linear combinations:\n\n\\[\n\\beta_j = \\sum_{m=1}^{M}\\theta_m\\phi_{jm}\n\\] (eq: 6.18)\n\nBias-Variance Trade-Off: This constraint can introduce bias, but if p is large relative to n, choosing M &lt;&lt; p can significantly reduce the variance of the fitted coefficients.\nTwo Steps:\n\nObtain the transformed predictors (Z1, …, ZM).\nFit a least squares model using these M predictors.\n\nDifferent Methods: Different dimension reduction methods differ in how they choose the Zm (or, equivalently, the φjm).\nPrincipal components regression (PCR)\nPartial least squares (PLS)\n\n\n3.1 Principal Components Regression (PCR)\n\nPrincipal Components Analysis (PCA): PCA is a technique for deriving a low-dimensional set of features from a larger set of variables. (More detail in Chapter 12.)\nUnsupervised: PCA is an unsupervised method – it identifies linear combinations that best represent the predictors (X), without considering the response (Y).\n\n\nAn Overview of Principal Components Analysis\n\nPCA seeks to find the directions in the data along which the observations vary the most.\nFirst Principal Component: The first principal component is the direction in the data with the greatest variance."
  },
  {
    "objectID": "qmd/islp6.html#pca-example-on-advertising-data",
    "href": "qmd/islp6.html#pca-example-on-advertising-data",
    "title": "Linear Model Selection and Regularization",
    "section": "PCA: Example on Advertising Data",
    "text": "PCA: Example on Advertising Data\n\n\n\nPopulation size and ad spending for 100 cities. The first principal component is shown in green, and the second in blue.\n\n\n\nFigure 6.14: Shows population size (pop) and ad spending (ad) for 100 cities.\nThe green line is the first principal component direction.\nProjecting the observations onto this line would maximize the variance of the projected points."
  },
  {
    "objectID": "qmd/islp6.html#pca-finding-the-first-principal-component",
    "href": "qmd/islp6.html#pca-finding-the-first-principal-component",
    "title": "Linear Model Selection and Regularization",
    "section": "PCA: Finding the First Principal Component",
    "text": "PCA: Finding the First Principal Component\n\nMathematical Representation: The first principal component can be written as:\n\n\\[\nZ_1 = 0.839 \\times (pop - \\overline{pop}) + 0.544 \\times (ad - \\overline{ad})\n\\] (eq: 6.19)\n-   0.839 and 0.544 are the *principal component loadings*.\n-   $\\overline{pop}$ and $\\overline{ad}$ are the means of pop and ad, respectively.\n\nInterpretation: Z1 is almost an average of the two variables (since the loadings are positive and similar in size)."
  },
  {
    "objectID": "qmd/islp6.html#pca-principal-component-scores",
    "href": "qmd/islp6.html#pca-principal-component-scores",
    "title": "Linear Model Selection and Regularization",
    "section": "PCA: Principal Component Scores",
    "text": "PCA: Principal Component Scores\n\\[\nZ_{i1} = 0.839 \\times (pop_i - \\overline{pop}) + 0.544 \\times (ad_i - \\overline{ad})\n\\] (eq: 6.20)\n\nScores: The values zi1, …, zn1 are called the principal component scores. They represent the “coordinates” of the data points along the first principal component direction."
  },
  {
    "objectID": "qmd/islp6.html#pca-another-interpretation",
    "href": "qmd/islp6.html#pca-another-interpretation",
    "title": "Linear Model Selection and Regularization",
    "section": "PCA: Another Interpretation",
    "text": "PCA: Another Interpretation\n\nClosest Line: The first principal component vector defines the line that is as close as possible to the data (minimizing the sum of squared perpendicular distances).\n\n\n\n\n\n\n\n\n\n\nThe first principal component direction, with distances to the observations shown as dashed lines.\n\n\n\n\n\nFigure 6.15:\n\nLeft: Shows the perpendicular distances from each point to the first principal component line.\nRight: Rotates the plot so that the first principal component is horizontal. The x-coordinate of each point in this rotated plot is its principal component score."
  },
  {
    "objectID": "qmd/islp6.html#pca-capturing-information",
    "href": "qmd/islp6.html#pca-capturing-information",
    "title": "Linear Model Selection and Regularization",
    "section": "PCA: Capturing Information",
    "text": "PCA: Capturing Information\n\n\n\nPlots of the first principal component scores versus pop and ad.\n\n\n\nFigure 6.16: Shows the first principal component scores (zi1) plotted against pop and ad.\nStrong Relationship: There’s a strong relationship, indicating that the first principal component captures much of the information in the original two variables."
  },
  {
    "objectID": "qmd/islp6.html#pca-multiple-principal-components",
    "href": "qmd/islp6.html#pca-multiple-principal-components",
    "title": "Linear Model Selection and Regularization",
    "section": "PCA: Multiple Principal Components",
    "text": "PCA: Multiple Principal Components\n\nMore than One: You can construct up to p distinct principal components.\nSecond Principal Component: The second principal component (Z2) is:\n\nA linear combination of the variables.\nUncorrelated with Z1.\nHas the largest variance among all linear combinations uncorrelated with Z1.\nOrthogonal (perpendicular) to the first principal component.\n\nSuccessive Components: Each subsequent principal component captures the maximum remaining variance, subject to being uncorrelated with the previous components."
  },
  {
    "objectID": "qmd/islp6.html#pca-second-principal-component",
    "href": "qmd/islp6.html#pca-second-principal-component",
    "title": "Linear Model Selection and Regularization",
    "section": "PCA: Second Principal Component",
    "text": "PCA: Second Principal Component\n\n\n\nPlots of the second principal component scores versus pop and ad.\n\n\n\nFigure 6.17: Shows the second principal component scores (zi2) plotted against pop and ad.\nWeak Relationship: There’s very little relationship, indicating that the second principal component captures much less information than the first.\n\n\nThe Principal Components Regression Approach\n\nThe Idea: Use the first M principal components (Z1, …, ZM) as predictors in a linear regression model.\nAssumption: We assume that the directions in which X1, …, Xp show the most variation are the directions that are associated with Y.\nPotential for Improvement: If this assumption holds, PCR can outperform least squares, especially when M &lt;&lt; p, by reducing variance."
  },
  {
    "objectID": "qmd/islp6.html#pcr-example",
    "href": "qmd/islp6.html#pcr-example",
    "title": "Linear Model Selection and Regularization",
    "section": "PCR: Example",
    "text": "PCR: Example\n\n\n\nPCR applied to two simulated datasets.\n\n\n\nFigure 6.18: Shows PCR applied to the simulated datasets from Figures 6.8 and 6.9.\nPCR with an appropriate choice of M can improve substantially over least squares.\nHowever, in this example, PCR does not perform as well as ridge regression or the lasso. This is because the data were generated in a way that required many principal components to model the response well."
  },
  {
    "objectID": "qmd/islp6.html#pcr-when-it-works-well",
    "href": "qmd/islp6.html#pcr-when-it-works-well",
    "title": "Linear Model Selection and Regularization",
    "section": "PCR: When it Works Well",
    "text": "PCR: When it Works Well\n\nFirst Few Components are Key: PCR tends to work well when the first few principal components capture most of the variation in the predictors and the relationship with the response.\n\n\n\n\nPCR, ridge regression, and the lasso on a simulated dataset where the first five principal components of X contain all the information about the response Y.\n\n\n\nFigure 6.19: Shows an example where the response depends only on the first five principal components.\nPCR performs very well, achieving a low MSE with M = 5.\nPCR and ridge regression slightly outperform the lasso in this case."
  },
  {
    "objectID": "qmd/islp6.html#pcr-not-feature-selection",
    "href": "qmd/islp6.html#pcr-not-feature-selection",
    "title": "Linear Model Selection and Regularization",
    "section": "PCR: Not Feature Selection",
    "text": "PCR: Not Feature Selection\n\nLinear Combinations: PCR is not a feature selection method. Each principal component is a linear combination of all p original features.\nExample: In the advertising data, Z1 was a combination of both pop and ad.\nRelationship to Ridge Regression: PCR is more closely related to ridge regression than to the lasso."
  },
  {
    "objectID": "qmd/islp6.html#pcr-choosing-m-and-standardization",
    "href": "qmd/islp6.html#pcr-choosing-m-and-standardization",
    "title": "Linear Model Selection and Regularization",
    "section": "PCR: Choosing M and Standardization",
    "text": "PCR: Choosing M and Standardization\n\nChoosing M: The number of principal components (M) is typically chosen by cross-validation.\n\n\n\n\nPCR standardized coefficient estimates and cross-validation MSE on the Credit data.\n\n\n\nFigure 6.20: Shows cross-validation for PCR on the Credit data.\nThe lowest cross-validation error occurs with M = 10, which is almost no dimension reduction.\nStandardization: It’s generally recommended to standardize each predictor before performing PCA. This ensures that all variables are on the same scale.\n\n\n3.2 Partial Least Squares (PLS)\n\nSupervised Dimension Reduction: PLS is a supervised dimension reduction technique. Unlike PCR (which is unsupervised), PLS uses the response (Y) to help identify the new features (Z1, …, ZM).\nGoal: Find directions that explain both the response and the predictors."
  },
  {
    "objectID": "qmd/islp6.html#pls-computing-the-first-direction",
    "href": "qmd/islp6.html#pls-computing-the-first-direction",
    "title": "Linear Model Selection and Regularization",
    "section": "PLS: Computing the First Direction",
    "text": "PLS: Computing the First Direction\n\nStandardize Predictors: Standardize the p predictors.\nSimple Linear Regressions: Compute the coefficient from the simple linear regression of Y onto each Xj.\nFirst PLS Direction: Set each φj1 in the equation for Z1 (Equation 6.16) equal to this coefficient. This means PLS places the highest weight on variables that are most strongly related to the response."
  },
  {
    "objectID": "qmd/islp6.html#pls-example",
    "href": "qmd/islp6.html#pls-example",
    "title": "Linear Model Selection and Regularization",
    "section": "PLS: Example",
    "text": "PLS: Example\n\n\n\nFirst PLS direction (solid line) and first PCR direction (dotted line) for the advertising data.\n\n\n\nFigure 6.21: Shows the first PLS and PCR directions for a synthetic dataset with Sales as the response and Population Size and Advertising Spending as predictors.\nPLS chooses a direction that emphasizes Population Size more than Advertising Spending, because Population Size is more correlated with Sales."
  },
  {
    "objectID": "qmd/islp6.html#pls-subsequent-directions",
    "href": "qmd/islp6.html#pls-subsequent-directions",
    "title": "Linear Model Selection and Regularization",
    "section": "PLS: Subsequent Directions",
    "text": "PLS: Subsequent Directions\n\nIterative Process:\n\nAdjust for Z1: Regress each variable on Z1 and take the residuals. This removes the information already explained by Z1.\nCompute Z2: Compute Z2 using these orthogonalized data, in the same way as Z1 was computed.\nRepeat: Repeat this process M times to identify multiple PLS components (Z1, …, ZM).\n\nFinal Model: Fit a linear model using Z1, …, ZM as predictors, just like in PCR."
  },
  {
    "objectID": "qmd/islp6.html#pls-tuning-parameter-and-standardization",
    "href": "qmd/islp6.html#pls-tuning-parameter-and-standardization",
    "title": "Linear Model Selection and Regularization",
    "section": "PLS: Tuning Parameter and Standardization",
    "text": "PLS: Tuning Parameter and Standardization\n\nChoosing M: The number of PLS directions (M) is a tuning parameter, typically chosen by cross-validation.\nStandardization: It’s generally recommended to standardize both the predictors and the response before performing PLS.\nPerformance: In practice, PLS often performs no better than ridge regression or PCR. The supervised dimension reduction of PLS can reduce bias, but it also has the potential to increase variance."
  },
  {
    "objectID": "qmd/islp6.html#considerations-in-high-dimensions",
    "href": "qmd/islp6.html#considerations-in-high-dimensions",
    "title": "Linear Model Selection and Regularization",
    "section": "4. Considerations in High Dimensions",
    "text": "4. Considerations in High Dimensions\n\n4.1 High-Dimensional Data\n\nLow-Dimensional Setting: Most traditional statistical techniques are designed for the low-dimensional setting, where n (number of observations) is much greater than p (number of features).\nHigh-Dimensional Setting: In recent years, new technologies have led to a dramatic increase in the number of features that can be measured. We often encounter datasets where p is large, possibly even larger than n.\nExamples:\n\nGenomics: Measuring hundreds of thousands of single nucleotide polymorphisms (SNPs) to predict a trait.\nMarketing: Using all search terms entered by users of a search engine to understand online shopping patterns.\n\n\n\n\n4.2 What Goes Wrong in High Dimensions?\n\nLeast Squares Fails: When p is as large as or larger than n, least squares cannot be used (or should not be used).\nPerfect Fit, But Useless: Least squares will always find a set of coefficients that perfectly fit the training data (zero residuals), regardless of whether there’s a true relationship between the features and the response.\nOverfitting: This perfect fit is a result of overfitting. The model is too flexible and captures noise in the data, leading to terrible performance on new data.\nCurse of Dimensionality: Adding more features, even if unrelated to response, can easily lead to the model overfitting the training data."
  },
  {
    "objectID": "qmd/islp6.html#regression-in-high-dimensions",
    "href": "qmd/islp6.html#regression-in-high-dimensions",
    "title": "Linear Model Selection and Regularization",
    "section": "4.3 Regression in High Dimensions",
    "text": "4.3 Regression in High Dimensions\n\nLess Flexible Methods: Many of the methods we’ve discussed in this chapter – forward stepwise selection, ridge regression, the lasso, and PCR – are particularly useful in the high-dimensional setting.\nAvoiding Overfitting: These methods avoid overfitting by being less flexible than least squares."
  },
  {
    "objectID": "qmd/islp6.html#regression-in-high-dimensions-example-with-the-lasso",
    "href": "qmd/islp6.html#regression-in-high-dimensions-example-with-the-lasso",
    "title": "Linear Model Selection and Regularization",
    "section": "Regression in High Dimensions: Example with the Lasso",
    "text": "Regression in High Dimensions: Example with the Lasso\n\n\n\nThe lasso performed with varying numbers of features (p) and a fixed sample size (n).\n\n\n\nFigure 6.24: Lasso with n=100, p can be 20, 50 and 2000.\nAs the number of features increases, the test set error increases, highlights the curse of dimensionality.\nThree Key Points:\n\nRegularization is Crucial: Regularization (or shrinkage) is essential in high-dimensional problems.\nTuning Parameter Selection: Choosing the right tuning parameter (e.g., λ for the lasso) is critical for good performance.\nCurse of Dimensionality: The test error tends to increase as the dimensionality of the problem increases, unless the additional features are truly associated with the response.\n\n\n\n4.4 Interpreting Results in High Dimensions\n\nMulticollinearity is Extreme: In the high-dimensional setting, multicollinearity is extreme. Any variable can be written as a linear combination of all the other variables.\nCannot Identify True Predictors: This means we can never know exactly which variables (if any) are truly predictive of the outcome. We can only identify variables that are correlated with the true predictors.\nCaution in Reporting: Be very cautious when reporting results. Don’t overstate conclusions.\nNever Use Training Data for Evaluation: Never use training data measures (sum of squared errors, p-values, R2) as evidence of a good model fit in the high-dimensional setting. These measures will be misleadingly optimistic.\nUse Test Data or Cross-Validation: Always report results on an independent test set or using cross-validation."
  },
  {
    "objectID": "qmd/islp6.html#summary",
    "href": "qmd/islp6.html#summary",
    "title": "Linear Model Selection and Regularization",
    "section": "Summary",
    "text": "Summary\n\nBeyond Least Squares: We’ve explored several alternatives to least squares for linear regression:\n\nSubset selection (best subset, forward stepwise, backward stepwise)\nShrinkage methods (ridge regression, the lasso)\nDimension reduction methods (PCR, PLS)\n\nGoals: These methods aim to improve:\n\nPrediction accuracy (by reducing variance, often at the cost of a small increase in bias)\nModel interpretability (by selecting a subset of variables or shrinking coefficients)\n\nHigh-Dimensional Data: These methods are particularly important in the high-dimensional setting (p ≥ n), where least squares fails.\nCross-Validation: Cross-validation is a powerful tool for selecting tuning parameters and estimating the test error of different models.\nThe choice of modeling method, the choice of tuning parameter, and the choice of assessment metrics all become especially important in high dimensions."
  },
  {
    "objectID": "qmd/islp6.html#thoughts-and-discussion",
    "href": "qmd/islp6.html#thoughts-and-discussion",
    "title": "Linear Model Selection and Regularization",
    "section": "Thoughts and Discussion",
    "text": "Thoughts and Discussion\n\nThink about situations where you might encounter high-dimensional data. What are the challenges and opportunities?\nHow would you choose between the different methods we’ve discussed (subset selection, ridge regression, the lasso, PCR, PLS)? What factors would you consider?\nWhat are the ethical implications of using high-dimensional data for prediction, especially in sensitive areas like healthcare or finance? How can we mitigate potential biases and ensure fairness?"
  },
  {
    "objectID": "qmd/islp7.html",
    "href": "qmd/islp7.html",
    "title": "",
    "section": "",
    "text": "Linear models are relatively simple and interpretable, but they might not be powerful enough for prediction, because the linearity assumption is often an oversimplification.\n\n\n\n\n\n\n\n\nLinear Regression\nRidge Regression\nLasso\nPCR"
  },
  {
    "objectID": "qmd/islp7.html#introduction",
    "href": "qmd/islp7.html#introduction",
    "title": "",
    "section": "",
    "text": "Linear models are relatively simple and interpretable, but they might not be powerful enough for prediction, because the linearity assumption is often an oversimplification.\n\n\n\n\n\n\n\n\nLinear Regression\nRidge Regression\nLasso\nPCR"
  },
  {
    "objectID": "qmd/islp7.html#introduction-cont.",
    "href": "qmd/islp7.html#introduction-cont.",
    "title": "",
    "section": "Introduction (Cont.)",
    "text": "Introduction (Cont.)\nThis chapter will focus on relaxing the linearity assumption while trying to maintain interpretability. We will explore several non-linear approaches:\n\nPolynomial Regression: Introduces non-linearity by adding polynomial terms of the predictors.\nStep Functions: Divides the range of a variable into distinct regions, fitting a constant within each.\nRegression Splines: A more flexible approach, dividing the predictor’s range into regions and fitting polynomials within each, but with constraints for smoothness.\nSmoothing Splines: Minimizes a combination of the residual sum of squares and a penalty for function roughness.\nLocal Regression: Fits a model locally, using only nearby data points.\nGeneralized Additive Models (GAMs): Extends the above methods to multiple predictors."
  },
  {
    "objectID": "qmd/islp7.html#polynomial-regression",
    "href": "qmd/islp7.html#polynomial-regression",
    "title": "",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\nInstead of a linear model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\]\nWe use a polynomial function:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + ... + \\beta_d x_i^d + \\epsilon_i\n\\]\n\nThis is still a linear model in terms of the coefficients, so we can use least squares!\n\nThe coefficients themselves aren’t usually of direct interest, but we examine the entire fitted function.\nIt is unusual to use \\(d &gt; 3\\) or \\(4\\) as higher degrees can lead to overly flexible and strange shapes."
  },
  {
    "objectID": "qmd/islp7.html#polynomial-regression-example",
    "href": "qmd/islp7.html#polynomial-regression-example",
    "title": "",
    "section": "Polynomial Regression: Example",
    "text": "Polynomial Regression: Example\n\n\n\n\n\n\nWe fit a degree-4 polynomial to the Wage data. The solid blue curve shows the polynomial fit, and the dashed curves represent a 95% confidence interval.\n\n\nThe confidence interval is calculated by:\n\n\n\n\n\nComputing the fitted value at a specific age: \\(\\hat{f}(x_0)\\).\nEstimating the variance of the fit: \\(\\text{Var}[\\hat{f}(x_0)]\\).\nCalculating the pointwise standard error: \\(\\sqrt{\\text{Var}[\\hat{f}(x_0)]}\\).\nForming an approximate 95% confidence interval: \\(\\hat{f}(x_0) \\pm 2 \\cdot \\text{SE}[\\hat{f}(x_0)]\\).\n\n\n\n\n\n\n\nWage data with a degree-4 polynomial fit."
  },
  {
    "objectID": "qmd/islp7.html#polynomial-logistic-regression",
    "href": "qmd/islp7.html#polynomial-logistic-regression",
    "title": "",
    "section": "Polynomial Logistic Regression",
    "text": "Polynomial Logistic Regression\n\n\n\n\n\n\nWe can also use polynomial terms within logistic regression to model a binary outcome. Here, we model the probability that wage &gt; 250 given age.\n\n\n\\[\n\\text{Pr}(y_i &gt; 250 | x_i) = \\frac{\\exp(\\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + ... + \\beta_d x_i^d)}{1 + \\exp(\\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + ... + \\beta_d x_i^d)}\n\\]\n\n\n\n\n\n\nLogistic regression with a degree-4 polynomial.\n\n\n\n\nNote the wide confidence intervals, especially for older ages, due to fewer high earners in the dataset."
  },
  {
    "objectID": "qmd/islp7.html#step-functions",
    "href": "qmd/islp7.html#step-functions",
    "title": "",
    "section": "Step Functions",
    "text": "Step Functions\n\nPolynomial regression imposes a global structure.\nStep functions impose a local structure by:\n\nBreaking the range of \\(X\\) into bins (defined by cutpoints \\(c_1, c_2, ..., c_K\\)).\nFitting a different constant in each bin.\nCreating \\(K+1\\) new variables:\n\n\n\\[\\begin{aligned}\nC_0(X) &= I(X &lt; c_1), \\\\\nC_1(X) &= I(c_1 \\le X &lt; c_2), \\\\\nC_2(X) &= I(c_2 \\le X &lt; c_3), \\\\\n&\\vdots \\\\\nC_{K-1}(X) &= I(c_{K-1} \\le X &lt; c_K), \\\\\nC_K(X) &= I(c_K \\le X),\n\\end{aligned}\\]\nwhere \\(I(\\cdot)\\) is an indicator function."
  },
  {
    "objectID": "qmd/islp7.html#step-functions-cont.",
    "href": "qmd/islp7.html#step-functions-cont.",
    "title": "",
    "section": "Step Functions (Cont.)",
    "text": "Step Functions (Cont.)\n\n\nWe then use least squares with \\(C_1(X), C_2(X), ..., C_K(X)\\) as predictors:\n\\[y_i = \\beta_0 + \\beta_1C_1(x_i) + \\beta_2C_2(x_i) + \\dots + \\beta_KC_K(x_i) + \\epsilon_i\\]\nNote that \\(C_0(X)\\) is excluded to avoid multicollinearity (since \\(\\sum_{k=0}^{K} C_k(X) = 1\\)). \\(\\beta_0\\) is the mean value of \\(Y\\) for \\(X &lt; c_1\\).\n\\(\\beta_j\\) represents the average increase in the response for \\(X\\) in the range \\(c_j \\le X &lt; c_{j+1}\\) relative to \\(X &lt; c_1\\).\nWe essentially converted a continuous variable into an ordered categorical variable."
  },
  {
    "objectID": "qmd/islp7.html#step-functions-example",
    "href": "qmd/islp7.html#step-functions-example",
    "title": "",
    "section": "Step Functions: Example",
    "text": "Step Functions: Example\n\n\n\n\n\n\nHere’s an example of fitting step functions to the Wage data. The left panel shows a piecewise constant fit to wage, and the right panel shows the fitted probabilities from a logistic regression model for wage &gt; 250.\n\n\n\n\n\n\nPiecewise constant fit and logistic regression with step functions.\n\n\n\n\nStep functions can miss trends, like the initial increase of wage with age. They are popular in biostatistics and epidemiology (e.g., using 5-year age groups)."
  },
  {
    "objectID": "qmd/islp7.html#basis-functions",
    "href": "qmd/islp7.html#basis-functions",
    "title": "",
    "section": "Basis Functions",
    "text": "Basis Functions\n\nPolynomial and piecewise-constant regression are special cases of a basis function approach.\nWe have a family of functions/transformations to apply to a variable \\(X\\): \\(b_1(X), b_2(X), ..., b_K(X)\\).\nInstead of fitting a linear model in \\(X\\), we fit:\n\\[y_i = \\beta_0 + \\beta_1 b_1(x_i) + \\beta_2 b_2(x_i) + ... + \\beta_K b_K(x_i) + \\epsilon_i\\]\nBasis functions are fixed and known. We choose them.\nThis is still a linear model! We can use least squares.\nAll inference tools from linear models (Chapter 3) are still available!"
  },
  {
    "objectID": "qmd/islp7.html#regression-splines",
    "href": "qmd/islp7.html#regression-splines",
    "title": "",
    "section": "Regression Splines",
    "text": "Regression Splines\n\nRegression splines extend upon polynomial and piecewise-constant regression.\nThey provide more flexibility by combining the strengths of these two approaches.\n\n\nPiecewise Polynomials\n\nInstead of a high-degree polynomial over the entire range of \\(X\\), we fit separate low-degree polynomials over different regions of \\(X\\).\nExample: Piecewise cubic polynomial:\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\epsilon_i\\]\nwhere the coefficients \\(\\beta_0, \\beta_1, \\beta_2, \\beta_3\\) differ in different parts of the range of \\(X\\).\nKnots: Points where the coefficients change."
  },
  {
    "objectID": "qmd/islp7.html#piecewise-polynomials-example",
    "href": "qmd/islp7.html#piecewise-polynomials-example",
    "title": "",
    "section": "Piecewise Polynomials (Example)",
    "text": "Piecewise Polynomials (Example)\nPiecewise Cubic:\n\\[y_i = \\begin{cases}\n\\beta_{01} + \\beta_{11}x_i + \\beta_{21}x_i^2 + \\beta_{31}x_i^3 + \\epsilon_i & \\text{if } x_i &lt; c \\\\\n\\beta_{02} + \\beta_{12}x_i + \\beta_{22}x_i^2 + \\beta_{32}x_i^3 + \\epsilon_i & \\text{if } x_i \\ge c\n\\end{cases}\\]\n\n\n\n\n\n\n\n\n\nUnconstrained piecewise cubic polynomial\n\n\n\n\n\nThis fit has a problem: It’s discontinuous! It looks ridiculous! We have 8 degrees of freedom (2 sets of 4 parameters)"
  },
  {
    "objectID": "qmd/islp7.html#constraints-and-splines",
    "href": "qmd/islp7.html#constraints-and-splines",
    "title": "",
    "section": "Constraints and Splines",
    "text": "Constraints and Splines\nTo make the piecewise cubic more reasonable, we introduce constraints:\n\n\nContinuity: Make the function continuous at the knot.\nContinuous 1st Derivative: Make the function smooth (no sharp turns) at the knot.\nContinuous 2nd Derivative: Make the function even smoother.\n\n\n\n\n\n\n\n\n\n\n\nUnconstrained piecewise cubic polynomial\n\n\n\n\n\n\n\nContinuous Piecewise Cubic\n\n\n\n\n\n\n\nCubic Spline. Continuous, first and second contiuous derivatives"
  },
  {
    "objectID": "qmd/islp7.html#constraints-and-splines-cont.",
    "href": "qmd/islp7.html#constraints-and-splines-cont.",
    "title": "",
    "section": "Constraints and Splines (Cont.)",
    "text": "Constraints and Splines (Cont.)\n\nEach constraint reduces the degrees of freedom.\nA cubic spline with K knots uses 4 + K degrees of freedom.\n\n4: base cubic polynomial\nK: number of knots\n\nA linear spline is continuous, fitting a line in each region, with continuity at each knot.\n\n\n\n\nLinear Spline"
  },
  {
    "objectID": "qmd/islp7.html#the-spline-basis-representation",
    "href": "qmd/islp7.html#the-spline-basis-representation",
    "title": "",
    "section": "The Spline Basis Representation",
    "text": "The Spline Basis Representation\n\nA cubic spline with K knots can be represented using a basis function model:\n\\[y_i = \\beta_0 + \\beta_1 b_1(x_i) + \\beta_2 b_2(x_i) + ... + \\beta_{K+3} b_{K+3}(x_i) + \\epsilon_i\\]\nA simple basis is: \\(x, x^2, x^3\\), and then one truncated power basis function per knot:\n\\[h(x, \\xi) = (x - \\xi)^3_+ = \\begin{cases} (x-\\xi)^3 & \\text{if } x &gt; \\xi \\\\ 0 & \\text{otherwise} \\end{cases}\\]\nwhere \\(\\xi\\) is the knot.\nWe can fit this model using least squares."
  },
  {
    "objectID": "qmd/islp7.html#the-spline-basis-representation-cont.",
    "href": "qmd/islp7.html#the-spline-basis-representation-cont.",
    "title": "",
    "section": "The Spline Basis Representation (Cont.)",
    "text": "The Spline Basis Representation (Cont.)\n\nTo fit a cubic spline with K knots, use least squares with an intercept and 3 + K predictors: \\(X, X^2, X^3, h(X, \\xi_1), h(X, \\xi_2), ..., h(X, \\xi_K)\\).\nSplines can have high variance at the boundaries.\nA natural spline adds boundary constraints: the function is linear outside the boundary knots.\n\n\n\n\nCubic Spline and Natural Cubic Spline."
  },
  {
    "objectID": "qmd/islp7.html#choosing-the-number-and-locations-of-the-knots",
    "href": "qmd/islp7.html#choosing-the-number-and-locations-of-the-knots",
    "title": "",
    "section": "Choosing the Number and Locations of the Knots",
    "text": "Choosing the Number and Locations of the Knots\n\nPlace more knots where the function might vary rapidly, fewer where it’s stable.\nCommon practice: Place knots uniformly (e.g., at quantiles of the data).\n\nSpecify degrees of freedom.\nSoftware places knots automatically.\n\n\n\n\n\nNatural cubic spline with four degrees of freedom."
  },
  {
    "objectID": "qmd/islp7.html#choosing-the-number-and-locations-of-the-knotscont.",
    "href": "qmd/islp7.html#choosing-the-number-and-locations-of-the-knotscont.",
    "title": "",
    "section": "Choosing the Number and Locations of the Knots(Cont.)",
    "text": "Choosing the Number and Locations of the Knots(Cont.)\n\nHow many knots (or degrees of freedom)?\nCross-validation is a more objective approach.\n\nFit splines with varying numbers of knots.\nCompute cross-validated RSS.\nChoose the number of knots that minimizes the cross-validated RSS.\n\n\n\n\n\nTen-fold cross-validated mean squared errors"
  },
  {
    "objectID": "qmd/islp7.html#comparison-to-polynomial-regression",
    "href": "qmd/islp7.html#comparison-to-polynomial-regression",
    "title": "",
    "section": "Comparison to Polynomial Regression",
    "text": "Comparison to Polynomial Regression\n\n\n\n\n\n\n\nRegression splines often give better results than polynomial regression.\nSplines introduce flexibility by increasing knots, keeping degree fixed.\nPolynomials need high degrees for flexibility.\n\n\n\n\n\n\n\nNatural cubic spline versus high-degree polynomial.\n\n\n\n\nHigh-degree polynomials can show wild behavior, especially near boundaries."
  },
  {
    "objectID": "qmd/islp7.html#smoothing-splines",
    "href": "qmd/islp7.html#smoothing-splines",
    "title": "",
    "section": "Smoothing Splines",
    "text": "Smoothing Splines\n\nAn alternative approach that also produces a spline.\n\n\nAn Overview of Smoothing Splines\n\nWe want to find a function \\(g(x)\\) that fits the data well (small RSS) and is smooth.\nWe minimize:\n\\[\\sum_{i=1}^{n}(y_i - g(x_i))^2 + \\lambda \\int g''(t)^2 dt\\]\n\nLoss + Penalty\n\\(\\lambda\\): Non-negative tuning parameter.\n\\(\\int g''(t)^2 dt\\): Measures the roughness of the function. (Integral of the squared second derivative)."
  },
  {
    "objectID": "qmd/islp7.html#smoothing-splines-cont.",
    "href": "qmd/islp7.html#smoothing-splines-cont.",
    "title": "",
    "section": "Smoothing Splines (Cont.)",
    "text": "Smoothing Splines (Cont.)\n\n\\(\\lambda \\int g''(t)^2 dt\\) encourages \\(g\\) to be smooth.\n\nLarger \\(\\lambda\\) \\(\\Rightarrow\\) smoother \\(g\\).\n\\(\\lambda = 0\\): \\(g\\) interpolates the data (overfits).\n\\(\\lambda \\to \\infty\\): \\(g\\) becomes a linear least squares line.\n\n\\(\\lambda\\) controls the bias-variance trade-off.\nThe function \\(g(x)\\) that minimizes the above equation is a natural cubic spline with knots at every unique value of \\(x_i\\)."
  },
  {
    "objectID": "qmd/islp7.html#choosing-the-smoothing-parameter-lambda",
    "href": "qmd/islp7.html#choosing-the-smoothing-parameter-lambda",
    "title": "",
    "section": "Choosing the Smoothing Parameter \\(\\lambda\\)",
    "text": "Choosing the Smoothing Parameter \\(\\lambda\\)\n\nA smoothing spline has knots at every data point.\nBut, the tuning parameter \\(\\lambda\\) controls the effective degrees of freedom.\nAs \\(\\lambda\\) increases from 0 to \\(\\infty\\), the effective degrees of freedom (\\(df_{\\lambda}\\)) decrease from \\(n\\) to 2.\nLOOCV (Leave-One-Out Cross-Validation) can be used efficiently:\n\\[\\text{RSS}_{cv}(\\lambda) = \\sum_{i=1}^{n} \\left( \\frac{y_i - \\hat{g}_{\\lambda}^{(-i)}(x_i)}{1 - \\{\\mathbf{S}_{\\lambda}\\}_{ii}} \\right)^2\\]\n\\(\\hat{g}_{\\lambda}^{(-i)}(x_i)\\): Fit with observation \\(i\\) left out.\n\\(\\mathbf{S}_{\\lambda}\\): A matrix depending on \\(\\lambda\\).\nLOOCV lets us choose \\(\\lambda\\) efficiently."
  },
  {
    "objectID": "qmd/islp7.html#smoothing-splines-example",
    "href": "qmd/islp7.html#smoothing-splines-example",
    "title": "",
    "section": "Smoothing Splines: Example",
    "text": "Smoothing Splines: Example\n\n\n\n\n\n\nSmoothing spline fits to the Wage data. Red curve: 16 effective degrees of freedom. Blue curve: \\(\\lambda\\) chosen by LOOCV, resulting in 6.8 effective degrees of freedom.\n\n\n\n\n\n\nSmoothing spline fits with different effective degrees of freedom.\n\n\n\n\nSimpler models are preferred, unless there is strong evidence for a more complex model."
  },
  {
    "objectID": "qmd/islp7.html#local-regression",
    "href": "qmd/islp7.html#local-regression",
    "title": "",
    "section": "Local Regression",
    "text": "Local Regression\n\nFits a model locally, using only nearby training observations.\nAlgorithm:\n\nGather the fraction s = k/n of training points closest to \\(x_0\\).\nAssign a weight \\(K_{i0} = K(x_i, x_0)\\) to each point in this neighborhood.\n\nPoints farthest from \\(x_0\\) get weight zero.\nClosest points get the highest weight.\n\nFit a weighted least squares regression: \\[\\text{minimize } \\sum_{i=1}^{n} K_{i0} (y_i - \\beta_0 - \\beta_1 x_i)^2\\]\nThe fitted value at \\(x_0\\) is: \\(\\hat{f}(x_0) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0\\)."
  },
  {
    "objectID": "qmd/islp7.html#local-regression-cont.",
    "href": "qmd/islp7.html#local-regression-cont.",
    "title": "",
    "section": "Local Regression (Cont.)",
    "text": "Local Regression (Cont.)\n\n\n\nIllustration of local regression.\n\n\n\nKey Choice: The span, s.\n\nSmaller s: More local and wiggly fit.\nLarger s: Smoother, more global fit.\n\nWe can use cross-validation to choose s.\nLocal regression can be generalized.\n\nVarying coefficient model.\nMulti-dimension.\n\n\n\n\n\nLocal linear fits to the Wage data."
  },
  {
    "objectID": "qmd/islp7.html#generalized-additive-models-gams",
    "href": "qmd/islp7.html#generalized-additive-models-gams",
    "title": "",
    "section": "Generalized Additive Models (GAMs)",
    "text": "Generalized Additive Models (GAMs)\n\nExtends multiple linear regression to allow non-linear functions of predictors, while maintaining additivity.\n\n\\[y_i = \\beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + ... + f_p(x_{ip}) + \\epsilon_i\\]\n\nGAMs allow non-linear functions \\(f_j\\) for each \\(X_j\\).\nWe can use various building blocks: splines, local regression, polynomial regression, step functions.\nWe can fit GAM by “backfitting”"
  },
  {
    "objectID": "qmd/islp7.html#gams-example",
    "href": "qmd/islp7.html#gams-example",
    "title": "",
    "section": "GAMs: Example",
    "text": "GAMs: Example\n\nFor the Wage data:\nwage = β0 + f1(year) + f2(age) + f3(education) + ε\n\nyear, age: quantitative.\neducation: qualitative (5 levels).\n\nFit using natural splines (for year, age) and dummy variables (for education).\nThe entire model is a large regression on spline basis variables and dummy variables.\n\n\n\n\nGAM fit with natural splines."
  },
  {
    "objectID": "qmd/islp7.html#gams-example-smoothing-splines",
    "href": "qmd/islp7.html#gams-example-smoothing-splines",
    "title": "",
    "section": "GAMs: Example (Smoothing Splines)",
    "text": "GAMs: Example (Smoothing Splines)\n\n\n\nGAM fit with smoothing splines.\n\n\n\nWe can also fit using smoothing splines.\nIn most cases, results are similar between natural and smoothing splines."
  },
  {
    "objectID": "qmd/islp7.html#gams-for-classification-problems",
    "href": "qmd/islp7.html#gams-for-classification-problems",
    "title": "",
    "section": "GAMs for Classification Problems",
    "text": "GAMs for Classification Problems\n\nCan be used when Y is qualitative (e.g., binary).\nLogistic regression GAM:\n\\[\\log\\left( \\frac{p(X)}{1 - p(X)} \\right) = \\beta_0 + f_1(X_1) + f_2(X_2) + ... + f_p(X_p)\\]\nExample: Predict wage &gt; 250 using a GAM.\n\n\n\n\nLogistic regression GAM."
  },
  {
    "objectID": "qmd/islp7.html#pros-and-cons-of-gams",
    "href": "qmd/islp7.html#pros-and-cons-of-gams",
    "title": "",
    "section": "Pros and Cons of GAMs",
    "text": "Pros and Cons of GAMs\n\n\nPros:\n\nAutomatic modeling of non-linear relationships.\nPotentially more accurate predictions.\nAdditivity helps interpret individual effects.\nSmoothness can be summarized via degrees of freedom.\n\nCons:\n\nRestricted to be additive (can miss interactions).\nInteractions can be manually added."
  },
  {
    "objectID": "qmd/islp7.html#summary",
    "href": "qmd/islp7.html#summary",
    "title": "",
    "section": "Summary",
    "text": "Summary\n\nThis chapter explored techniques for moving beyond linearity in regression models.\nWe saw how to incorporate non-linear relationships using:\n\nPolynomial regression\nStep functions\nRegression splines\nSmoothing splines\nLocal regression\nGeneralized Additive Models (GAMs)\n\nThese methods allow for more flexibility while maintaining interpretability.\nCross-validation is crucial for choosing tuning parameters.\nGAMs extend these ideas to multiple predictors."
  },
  {
    "objectID": "qmd/islp7.html#thoughts-and-discussion",
    "href": "qmd/islp7.html#thoughts-and-discussion",
    "title": "",
    "section": "Thoughts and Discussion",
    "text": "Thoughts and Discussion\n\nWhen would you choose a linear model over a non-linear model, and vice-versa?\nHow do the different non-linear techniques compare in terms of flexibility and interpretability?\nCan you think of situations where a GAM might not be sufficient, and a more complex model (e.g., tree-based methods) would be needed?\nHow can you incorporate interactions into GAMs?\nDiscuss the trade-off between using smoothing splines and regression splines. When might one be preferred over the other?"
  },
  {
    "objectID": "qmd/islp2.html",
    "href": "qmd/islp2.html",
    "title": "",
    "section": "",
    "text": "Statistical learning is a vast set of tools for understanding data. These tools can be classified as supervised or unsupervised.\n\nSupervised statistical learning involves building a statistical model for predicting, or estimating, an output based on one or more inputs.\nWith unsupervised statistical learning, there are inputs but no supervising output; nevertheless, we can learn relationships and structure from such data.\n\nThis chapter introduces many of the key concepts of statistical learning, focusing on the fundamental ideas, which include data mining, machine learning and statistical learning relationship, estimating f, the trade-off between prediction accuracy and model interpretability, supervised versus unsupervised learning, and assessing model accuracy using mean square error, the bias-variance, Bayes error rate."
  },
  {
    "objectID": "qmd/islp2.html#introduction-to-statistical-learning",
    "href": "qmd/islp2.html#introduction-to-statistical-learning",
    "title": "",
    "section": "",
    "text": "Statistical learning is a vast set of tools for understanding data. These tools can be classified as supervised or unsupervised.\n\nSupervised statistical learning involves building a statistical model for predicting, or estimating, an output based on one or more inputs.\nWith unsupervised statistical learning, there are inputs but no supervising output; nevertheless, we can learn relationships and structure from such data.\n\nThis chapter introduces many of the key concepts of statistical learning, focusing on the fundamental ideas, which include data mining, machine learning and statistical learning relationship, estimating f, the trade-off between prediction accuracy and model interpretability, supervised versus unsupervised learning, and assessing model accuracy using mean square error, the bias-variance, Bayes error rate."
  },
  {
    "objectID": "qmd/islp2.html#data-mining-machine-learning-and-statistical-learning",
    "href": "qmd/islp2.html#data-mining-machine-learning-and-statistical-learning",
    "title": "",
    "section": "Data Mining, Machine Learning, and Statistical Learning",
    "text": "Data Mining, Machine Learning, and Statistical Learning\n\nLet’s clarify the relationship between these often-used terms:\n\nData Mining: The process of discovering patterns and insights from large datasets, often using computational techniques. It emphasizes finding any interesting pattern, even if it’s not directly related to a specific prediction task.\nMachine Learning: A field of computer science focused on algorithms that can learn from data. It’s heavily focused on prediction – enabling computers to make accurate predictions on new, unseen data.\nStatistical Learning: A subfield of statistics that focuses on developing and applying statistical models and methods for prediction and inference. It emphasizes understanding the relationships between variables and making inferences about the underlying data-generating process.\nAll the concepts are focusing on extracting information from data.\n\n\n\n\n\n\n\ngraph LR\n    A[Data Mining] --&gt; C(Common Ground)\n    B[Machine Learning] --&gt; C\n    D[Statistical Learning] --&gt; C\n    C --&gt; E[Insights & Predictions]"
  },
  {
    "objectID": "qmd/islp2.html#the-advertising-example",
    "href": "qmd/islp2.html#the-advertising-example",
    "title": "",
    "section": "The Advertising Example",
    "text": "The Advertising Example\n\nTo motivate our study, let’s consider a simple example. A company wants to understand how advertising spending affects product sales.\n\nData: Sales of a product in 200 different markets, along with advertising budgets for TV, radio, and newspaper.\nGoal: Build a model to predict sales based on the advertising budgets in the three media.\nWhy? The company can’t directly control sales, but can control advertising spending. A good model helps them optimize their advertising budget."
  },
  {
    "objectID": "qmd/islp2.html#input-and-output-variables",
    "href": "qmd/islp2.html#input-and-output-variables",
    "title": "",
    "section": "Input and Output Variables",
    "text": "Input and Output Variables\n\nIn the advertising example:\n\nInput variables (X): Advertising budgets (TV, radio, newspaper). These are also called predictors, independent variables, or features. We often denote them as X₁, X₂, X₃, …, Xₚ.\nOutput variable (Y): Sales. This is also called the response or dependent variable. We’re trying to predict or understand Y.\nStatistical learning will use all these terms interchangeably.\n\n\n\n\nExample:\n\nX₁ = TV budget\nX₂ = Radio budget\nX₃ = Newspaper budget\nY = Sales"
  },
  {
    "objectID": "qmd/islp2.html#visualizing-the-advertising-data",
    "href": "qmd/islp2.html#visualizing-the-advertising-data",
    "title": "",
    "section": "Visualizing the Advertising Data",
    "text": "Visualizing the Advertising Data\n\n\n\n\nAdvertising Data: Sales vs. Advertising Budgets\n\n\n\n\nEach plot shows sales versus one advertising medium.\nThe blue line represents a simple linear model (least squares fit) to predict sales using each medium individually. Least squares is an algorithm to fit data. We’ll learn more about this in Chapter 3.\nObservations: There’s a positive relationship between advertising spending and sales for TV and radio. The relationship with newspaper spending is less clear."
  },
  {
    "objectID": "qmd/islp2.html#the-general-model",
    "href": "qmd/islp2.html#the-general-model",
    "title": "",
    "section": "The General Model",
    "text": "The General Model\n\nMore generally, we assume a relationship between the response Y and predictors X:\n\\[\nY = f(X) + \\epsilon\n\\]\n\nY: The quantitative response variable we want to predict.\nX: (X₁, X₂, …, Xₚ), a vector of p predictors.\nf(X): An unknown function representing the systematic relationship between X and Y. This is what we want to estimate!\nε: A random error term, independent of X, with a mean of zero. It represents the variation in Y that cannot be explained by f(X).\n\n\n\nGoal of Statistical Learning: Estimate the unknown function f."
  },
  {
    "objectID": "qmd/islp2.html#understanding-the-error-term-ε",
    "href": "qmd/islp2.html#understanding-the-error-term-ε",
    "title": "",
    "section": "Understanding the Error Term (ε)",
    "text": "Understanding the Error Term (ε)\n\nThe error term, ε, captures all the factors that affect Y but are not included in our predictors X. This could include:\n\nUnmeasured variables: Factors influencing Y that we didn’t or couldn’t measure.\nMeasurement error: Inaccuracies in how we measured X or Y.\nRandomness: Inherent variability in Y that can’t be perfectly predicted.\n\n\n\n\n\n\n\nNote\n\n\n\nThe error term is crucial. It acknowledges that our models are approximations of reality. Even the “best” model won’t be perfect."
  },
  {
    "objectID": "qmd/islp2.html#example-income-vs.-education",
    "href": "qmd/islp2.html#example-income-vs.-education",
    "title": "",
    "section": "Example: Income vs. Education",
    "text": "Example: Income vs. Education\n\n\n\n\nIncome vs. Years of Education\n\n\n\nLeft: Observed income (in thousands of dollars) versus years of education for 30 individuals.\nRight: The true underlying relationship (blue curve), which is usually unknown (but known here because the data were simulated). The black line segments represent errors associated with each data.\nObservation: More years of education generally lead to higher income, but there’s variation (the error)."
  },
  {
    "objectID": "qmd/islp2.html#why-estimate-f",
    "href": "qmd/islp2.html#why-estimate-f",
    "title": "",
    "section": "Why Estimate f?",
    "text": "Why Estimate f?\n\nThere are two main reasons to estimate f:\n\nPrediction: We want to predict Y given a set of X values. We don’t necessarily care about the exact form of f, just that it gives accurate predictions (treat f as a “black box”).\n\\[\n\\hat{Y} = \\hat{f}(X)\n\\]\n\nŶ: The prediction of Y.\nf̂: Our estimate of f.\n\nInference: We want to understand the relationship between Y and X. We do care about the form of f. We want to answer questions like:\n\nWhich predictors are associated with the response?\nIs the relationship positive or negative?\nIs the relationship linear or more complex?"
  },
  {
    "objectID": "qmd/islp2.html#prediction-reducible-and-irreducible-error",
    "href": "qmd/islp2.html#prediction-reducible-and-irreducible-error",
    "title": "",
    "section": "Prediction: Reducible and Irreducible Error",
    "text": "Prediction: Reducible and Irreducible Error\n\nThe accuracy of our prediction, Ŷ, depends on two types of error:\n\nReducible Error: Error due to our estimate of f (f̂) not being perfect. We can reduce this error by choosing better statistical learning techniques.\nIrreducible Error: Error due to the random error term, ε. Even if we knew the true f, we cannot predict ε. This sets a limit on how accurate our predictions can be. \\[E(Y - \\hat{Y})^2 = \\underbrace{[f(X) - \\hat{f}(X)]^2}_{\\text{Reducible}} + \\underbrace{Var(\\epsilon)}_{\\text{Irreducible}}\\]\n\n\n\n\n\n\n\nNote\n\n\n\nOur goal is to minimize the reducible error."
  },
  {
    "objectID": "qmd/islp2.html#inference-understanding-the-relationship",
    "href": "qmd/islp2.html#inference-understanding-the-relationship",
    "title": "",
    "section": "Inference: Understanding the Relationship",
    "text": "Inference: Understanding the Relationship\n\nWhen our goal is inference, we want to understand how Y changes as a function of X₁, …, Xₚ. We’re interested in questions like:\n\nWhich predictors matter? Are all the Xᵢ related to Y, or only a subset?\nWhat’s the nature of the relationship? Is it positive, negative, linear, non-linear?\nCan we simplify the model? Can we get a good understanding with a simpler model (e.g., a linear model)?\n\nWe care about interpretability, the form of f, and statistical significance."
  },
  {
    "objectID": "qmd/islp2.html#example-modeling-for-prediction",
    "href": "qmd/islp2.html#example-modeling-for-prediction",
    "title": "",
    "section": "Example: Modeling for Prediction",
    "text": "Example: Modeling for Prediction\n\nScenario: A company wants to target a direct-marketing campaign to individuals likely to respond positively.\n\nPredictors (X): Demographic variables (age, income, location, etc.).\nResponse (Y): Response to the campaign (positive or negative).\nGoal: Accurately predict Y using X. The company doesn’t need to deeply understand why each predictor is related to the response, only that the prediction is accurate.\n\n\n\n\n\n\n\nNote\n\n\n\nThis is a classic prediction problem. The model is a “black box”."
  },
  {
    "objectID": "qmd/islp2.html#example-modeling-for-inference",
    "href": "qmd/islp2.html#example-modeling-for-inference",
    "title": "",
    "section": "Example: Modeling for Inference",
    "text": "Example: Modeling for Inference\n\nScenario: Analyze the Advertising data (Figure 2.1).\n\nPredictors (X): TV, radio, and newspaper advertising budgets.\nResponse (Y): Sales.\nGoal: Understand how each advertising medium affects sales. Questions to answer:\n\nWhich media are associated with sales?\nWhich media generate the biggest boost in sales?\nHow large is the effect of TV advertising on sales?\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is an inference problem. We want to understand the relationships."
  },
  {
    "objectID": "qmd/islp2.html#how-do-we-estimate-f",
    "href": "qmd/islp2.html#how-do-we-estimate-f",
    "title": "",
    "section": "How Do We Estimate f?",
    "text": "How Do We Estimate f?\n\nWe use training data to “teach” our statistical learning method how to estimate f.\n\nTraining data: A set of observed data points: {(x₁, y₁), (x₂, y₂), …, (xₙ, yₙ)}, where:\n\nxᵢ = (xᵢ₁, xᵢ₂, …, xᵢₚ)ᵀ is the vector of predictor values for the ith observation.\nyᵢ is the response value for the ith observation.\n\nGoal: Find a function, f̂, such that Y ≈ f̂(X) for any observation (X, Y).\nTwo broad approaches: Parametric and non-parametric methods."
  },
  {
    "objectID": "qmd/islp2.html#parametric-methods",
    "href": "qmd/islp2.html#parametric-methods",
    "title": "",
    "section": "Parametric Methods",
    "text": "Parametric Methods\n\nA two-step, model-based approach:\n\nAssume a functional form for f. For example, assume f is linear:\n\\[\nf(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p\n\\]\nThis reduces the problem to estimating the p + 1 coefficients (β₀, β₁, …, βₚ).\nUse training data to fit or train the model. Find the values of the parameters (β₀, β₁, …, βₚ) that best fit the data. A common method is (ordinary) least squares.\n\n\n\n\n\n\n\nNote\n\n\n\nParametric methods simplify the problem by assuming a specific form for f."
  },
  {
    "objectID": "qmd/islp2.html#example-linear-model-fit-to-income-data",
    "href": "qmd/islp2.html#example-linear-model-fit-to-income-data",
    "title": "",
    "section": "Example: Linear Model Fit to Income Data",
    "text": "Example: Linear Model Fit to Income Data\n\n\n\n\nLinear Model Fit\n\n\n\nA linear model (yellow plane) fit to the Income data (Figure 2.3).\nRed dots are the observed data points.\nThe model assumes: income ≈ β₀ + β₁ × education + β₂ × seniority.\nLinear model is relatively inflexible because it can generate linear functions"
  },
  {
    "objectID": "qmd/islp2.html#parametric-methods-advantages-and-disadvantages",
    "href": "qmd/islp2.html#parametric-methods-advantages-and-disadvantages",
    "title": "",
    "section": "Parametric Methods: Advantages and Disadvantages",
    "text": "Parametric Methods: Advantages and Disadvantages\n\n\nAdvantage: Simplifies the problem of estimating f. It’s easier to estimate a few parameters than an entirely arbitrary function.\nDisadvantage: The assumed form of f might be wrong. If the true f is very different from our assumed form, our estimate will be poor.\nOverfitting: If we use a very complex (flexible) model, we might overfit the data. This means the model follows the noise (random error) too closely, resulting in poor predictions on new data."
  },
  {
    "objectID": "qmd/islp2.html#non-parametric-methods",
    "href": "qmd/islp2.html#non-parametric-methods",
    "title": "",
    "section": "Non-parametric Methods",
    "text": "Non-parametric Methods\n\n\nDo not make explicit assumptions about the functional form of f.\nSeek an estimate of f that gets as close to the data points as possible, without being too rough or wiggly.\nAdvantage: Can accurately fit a wider range of possible shapes for f. Avoids the risk of making a wrong assumption about the form of f.\nDisadvantage: Requires a very large number of observations to get an accurate estimate of f."
  },
  {
    "objectID": "qmd/islp2.html#example-thin-plate-spline-fit-to-income-data",
    "href": "qmd/islp2.html#example-thin-plate-spline-fit-to-income-data",
    "title": "",
    "section": "Example: Thin-Plate Spline Fit to Income Data",
    "text": "Example: Thin-Plate Spline Fit to Income Data\n\n\n\n\nThin-Plate Spline Fit\n\n\n\nA thin-plate spline (yellow surface) fit to the Income data.\nThis is a non-parametric method. No pre-specified model is assumed.\nThe fit is much closer to the true f (Figure 2.3) than the linear fit.\nThis is a smooth fit."
  },
  {
    "objectID": "qmd/islp2.html#example-overfitting-with-a-rough-spline",
    "href": "qmd/islp2.html#example-overfitting-with-a-rough-spline",
    "title": "",
    "section": "Example: Overfitting with a Rough Spline",
    "text": "Example: Overfitting with a Rough Spline\n\n\n\n\nRough Spline Fit\n\n\n\nSame data, but a rougher thin-plate spline fit.\nThis fit perfectly matches the training data (zero error on training data!).\nBUT: This is an example of overfitting. The fit is too wiggly and will likely perform poorly on new data. It has captured the noise, not just the underlying pattern."
  },
  {
    "objectID": "qmd/islp2.html#the-trade-off-between-prediction-accuracy-and-model-interpretability",
    "href": "qmd/islp2.html#the-trade-off-between-prediction-accuracy-and-model-interpretability",
    "title": "",
    "section": "The Trade-Off Between Prediction Accuracy and Model Interpretability",
    "text": "The Trade-Off Between Prediction Accuracy and Model Interpretability\n\n\nFlexibility: How many different shapes of functions can the method fit?\n\nLess flexible (restrictive): Linear regression (only linear functions).\nMore flexible: Thin-plate splines, neural networks.\n\nInterpretability: How easy is it to understand the fitted model?\n\nMore interpretable: Linear regression (easy to understand coefficients).\nLess interpretable: Complex, non-linear models (hard to see how each predictor affects the response).\n\nGeneral rule: As flexibility increases, interpretability decreases.\nImportant Trade-off: We often have to choose between more accurate, but less interpretable models, and simpler, more interpretable models."
  },
  {
    "objectID": "qmd/islp2.html#why-choose-a-more-restrictive-method",
    "href": "qmd/islp2.html#why-choose-a-more-restrictive-method",
    "title": "",
    "section": "Why Choose a More Restrictive Method?",
    "text": "Why Choose a More Restrictive Method?\n\nEven if we only care about prediction, a more restrictive model (like linear regression) can sometimes outperform a more flexible model!\nReasons:\n\nInference: If we’re interested in understanding the relationship, restrictive models are more interpretable.\nOverfitting: Flexible models can overfit the training data, leading to poor predictions on new data. A simpler model might generalize better.\nCurse of Dimensionality: With many predictors, flexible models can be hard to fit well and require huge amounts of data."
  },
  {
    "objectID": "qmd/islp2.html#supervised-vs.-unsupervised-learning",
    "href": "qmd/islp2.html#supervised-vs.-unsupervised-learning",
    "title": "",
    "section": "Supervised vs. Unsupervised Learning",
    "text": "Supervised vs. Unsupervised Learning\n\n\nSupervised Learning: We have both predictors (X) and a response (Y) for each observation. We want to learn the relationship between X and Y.\n\nExamples: Regression, classification.\nMost of the methods in this book are supervised.\n\nUnsupervised Learning: We only have predictors (X), no response (Y). We want to find patterns and structure in the data.\n\nExample: Cluster analysis (grouping observations into clusters).\n\nSemi-supervised Learning: A mix. We have (X, Y) for some observations, but only X for others.\n\n\n\n\n\n\n\nNote\n\n\n\nThe distinction between supervised and unsupervised learning isn’t always clear-cut."
  },
  {
    "objectID": "qmd/islp2.html#example-cluster-analysis",
    "href": "qmd/islp2.html#example-cluster-analysis",
    "title": "",
    "section": "Example: Cluster Analysis",
    "text": "Example: Cluster Analysis\n\n\n\n\nCluster Analysis Example\n\n\n\n150 observations, two variables (X₁, X₂).\nLeft: Three well-separated groups (clusters). Clustering should easily identify these.\nRight: Overlapping groups. Clustering is much harder.\nGoal: Identify distinct groups without knowing the group labels beforehand.\nIn the examples shown, there are only two variables, and we can check the scatterplots to identify clusters. But in practice, we cannot do that, and we need to use clustering and other unsupervised learning approaches."
  },
  {
    "objectID": "qmd/islp2.html#regression-vs.-classification-problems",
    "href": "qmd/islp2.html#regression-vs.-classification-problems",
    "title": "",
    "section": "Regression vs. Classification Problems",
    "text": "Regression vs. Classification Problems\n\n\nRegression: The response variable (Y) is quantitative (numerical).\n\nExample: Predicting income, house price, stock return.\n\nClassification: The response variable (Y) is qualitative (categorical).\n\nExample: Predicting whether someone will default on a loan (yes/no), which brand of product they’ll buy (A/B/C), or a medical diagnosis (disease 1/disease 2/no disease).\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nSome methods are better suited to regression, others to classification. But many methods can be used for both.\nWhether the predictors are quantitative or qualitative is usually less important than the type of response."
  },
  {
    "objectID": "qmd/islp2.html#assessing-model-accuracy-regression",
    "href": "qmd/islp2.html#assessing-model-accuracy-regression",
    "title": "",
    "section": "Assessing Model Accuracy: Regression",
    "text": "Assessing Model Accuracy: Regression\n\n\nGoal: Quantify how well our predictions match the observed data.\nMean Squared Error (MSE): A common measure in regression:\n\\[\n\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{f}(x_i))^2\n\\]\n\nyᵢ: The true response value for the ith observation.\nf̂(xᵢ): The predicted response value for the ith observation.\nLower MSE is better (closer predictions).\n\nTraining MSE: Calculated using the training data.\nTest MSE: Calculated using new, unseen data (test data). This is what we really care about!"
  },
  {
    "objectID": "qmd/islp2.html#training-mse-vs.-test-mse",
    "href": "qmd/islp2.html#training-mse-vs.-test-mse",
    "title": "",
    "section": "Training MSE vs. Test MSE",
    "text": "Training MSE vs. Test MSE\n\n\nWe usually don’t care how well the model fits the training data. We care about how well it predicts new data (the test data).\nA model with low training MSE might have high test MSE (overfitting!).\nIdeally: We’d choose the model with the lowest test MSE.\nProblem: We often don’t have test data when building the model.\nSolution: Techniques like cross-validation (Chapter 5) can help us estimate the test MSE using the training data."
  },
  {
    "objectID": "qmd/islp2.html#example-training-and-test-mse-vs.-flexibility",
    "href": "qmd/islp2.html#example-training-and-test-mse-vs.-flexibility",
    "title": "",
    "section": "Example: Training and Test MSE vs. Flexibility",
    "text": "Example: Training and Test MSE vs. Flexibility\n\n\n\n\nMSE vs. Flexibility\n\n\n\nLeft: Data simulated from a non-linear f (black curve). Three fits: linear (orange), smooth spline (blue), wiggly spline (green).\nRight:\n\nTraining MSE (grey curve): Decreases as flexibility increases.\nTest MSE (red curve): U-shaped. Decreases, then increases (overfitting).\nDashed line: Minimum possible test MSE (irreducible error).\n\nObservation: The blue curve (moderate flexibility) has the lowest test MSE."
  },
  {
    "objectID": "qmd/islp2.html#the-bias-variance-trade-off",
    "href": "qmd/islp2.html#the-bias-variance-trade-off",
    "title": "",
    "section": "The Bias-Variance Trade-Off",
    "text": "The Bias-Variance Trade-Off\n\nThe U-shape in the test MSE curve is due to two competing properties:\n\nVariance: How much would our estimate of f (f̂) change if we used a different training set?\n\nHigh variance: f̂ changes a lot with different training sets (typical of flexible models).\nLow variance: f̂ is relatively stable (typical of less flexible models).\n\nBias: The error introduced by approximating a complex real-world problem with a simpler model.\n\nHigh bias: The model makes strong (and possibly wrong) assumptions about f (typical of less flexible models).\nLow bias: The model makes fewer assumptions (typical of flexible models).\n\nExpected test MSE at x0 can be decomposed to: \\[E(y_0 - \\hat{f}(x_0))^2 = Var(\\hat{f}(x_0)) + [Bias(\\hat{f}(x_0))]^2 + Var(\\epsilon)\\]"
  },
  {
    "objectID": "qmd/islp2.html#bias-variance-trade-off-illustration",
    "href": "qmd/islp2.html#bias-variance-trade-off-illustration",
    "title": "",
    "section": "Bias-Variance Trade-Off: Illustration",
    "text": "Bias-Variance Trade-Off: Illustration\n\n\n\n\nBias-Variance Trade-Off\n\n\n\nSquared bias (blue), variance (orange), irreducible error (dashed), and test MSE (red) for the three examples.\nLeft (non-linear f): Bias decreases rapidly, variance increases slowly.\nCenter (nearly linear f): Bias is low, variance increases quickly.\nRight (very non-linear f): Bias decreases dramatically, variance is low.\n\nKey takeaway: Good models need both low variance and low bias. This is a trade-off!"
  },
  {
    "objectID": "qmd/islp2.html#assessing-model-accuracy-classification",
    "href": "qmd/islp2.html#assessing-model-accuracy-classification",
    "title": "",
    "section": "Assessing Model Accuracy: Classification",
    "text": "Assessing Model Accuracy: Classification\n\n\nError Rate: The proportion of mistakes made by the classifier.\n\nTraining error rate:\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} I(y_i \\ne \\hat{y}_i)\n\\]\n\nyᵢ: True class label.\nŷᵢ: Predicted class label.\nI(yᵢ ≠ ŷᵢ): Indicator variable (1 if mistake, 0 if correct).\n\nTest error rate: Ave(I(y₀ ≠ ŷ₀)). This is what we care about!\n\nGoal: Choose the classifier with the lowest test error rate."
  },
  {
    "objectID": "qmd/islp2.html#the-bayes-classifier",
    "href": "qmd/islp2.html#the-bayes-classifier",
    "title": "",
    "section": "The Bayes Classifier",
    "text": "The Bayes Classifier\n\n\nThe “ideal” classifier: Assigns each observation to the most likely class, given its predictor values.\nConditional probability: Pr(Y = j | X = x₀) - the probability that Y = j (class j), given the predictor values x₀.\nBayes Classifier: Assigns an observation to the class j for which Pr(Y = j | X = x₀) is largest.\nBayes Decision Boundary: The points where the conditional probabilities for different classes are equal.\nBayes Error Rate: The lowest possible test error rate achievable. Analogous to the irreducible error."
  },
  {
    "objectID": "qmd/islp2.html#example-bayes-classifier",
    "href": "qmd/islp2.html#example-bayes-classifier",
    "title": "",
    "section": "Example: Bayes Classifier",
    "text": "Example: Bayes Classifier\n\n\n\n\nBayes Classifier Example\n\n\n\nSimulated data, two classes (orange, blue).\nPurple dashed line: Bayes decision boundary.\nOrange/blue shaded regions: Regions where the Bayes classifier would predict orange/blue.\nThe Bayes error rate is greater than zero because the classes overlap."
  },
  {
    "objectID": "qmd/islp2.html#k-nearest-neighbors-knn",
    "href": "qmd/islp2.html#k-nearest-neighbors-knn",
    "title": "",
    "section": "K-Nearest Neighbors (KNN)",
    "text": "K-Nearest Neighbors (KNN)\n\n\nProblem: In reality, we don’t know the conditional distribution of Y given X. So, we can’t directly use the Bayes classifier.\nKNN: A non-parametric method that estimates the conditional distribution and then classifies based on the estimate.\nHow it works:\n\nGiven a test observation, x₀, find the K closest training observations (the “neighborhood”).\nEstimate the conditional probability for class j as the fraction of neighbors in the neighborhood whose response value is j.\nClassify x₀ to the class with the highest estimated probability."
  },
  {
    "objectID": "qmd/islp2.html#example-knn",
    "href": "qmd/islp2.html#example-knn",
    "title": "",
    "section": "Example: KNN",
    "text": "Example: KNN\n\n\n\n\nKNN Example\n\n\n\nLeft: Small training set (6 blue, 6 orange). Black cross is the test observation. Circle shows the 3 nearest neighbors (K=3): 2 blue, 1 orange. KNN predicts “blue”.\nRight: KNN decision boundary (K=3) for all possible values of X₁ and X₂.\nKNN can produce a decision boundary and classifier that’s close to Bayes Classifier."
  },
  {
    "objectID": "qmd/islp2.html#knn-the-choice-of-k",
    "href": "qmd/islp2.html#knn-the-choice-of-k",
    "title": "",
    "section": "KNN: The Choice of K",
    "text": "KNN: The Choice of K\n\n\nThe choice of K (the number of neighbors) controls the flexibility of the KNN classifier.\n\nSmall K: More flexible, lower bias, higher variance (risk of overfitting).\nLarge K: Less flexible, higher bias, lower variance.\n\nExample: Figure 2.16 shows KNN fits with K=1 and K=100. K=1 overfits, K=100 is too inflexible.\nFinding the best K: We want to choose K to minimize the test error rate. Techniques like cross-validation can help."
  },
  {
    "objectID": "qmd/islp2.html#summary",
    "href": "qmd/islp2.html#summary",
    "title": "",
    "section": "Summary",
    "text": "Summary\n\n\nStatistical learning is about estimating relationships between variables, for prediction and/or inference.\nParametric methods assume a specific functional form; non-parametric methods don’t.\nThere’s a trade-off between model flexibility and interpretability.\nWe need to assess model accuracy using test data (or estimates of test error).\nThe bias-variance trade-off is fundamental: Good models need both low bias and low variance.\nIn classification, the Bayes classifier is optimal, but we often have to approximate it (e.g., with KNN).\nChoosing the right level of flexibility is crucial."
  },
  {
    "objectID": "qmd/islp2.html#thoughts-and-discussion",
    "href": "qmd/islp2.html#thoughts-and-discussion",
    "title": "",
    "section": "Thoughts and Discussion",
    "text": "Thoughts and Discussion\n\n\nThink about real-world problems you’re interested in. Would you approach them with a focus on prediction, inference, or both?\nCan you think of examples where a simple, interpretable model might be preferable to a more complex, “black box” model, even for prediction?\nHow might the “best” model (in terms of test error) depend on the amount of data available?\nHow does the concept of “overfitting” relate to the bias-variance trade-off?\nDiscuss the differences and similarities between supervised, unsupervised, and semi-supervised learning, and how they apply to real-world problems."
  },
  {
    "objectID": "qmd/islp4.html",
    "href": "qmd/islp4.html",
    "title": "",
    "section": "",
    "text": "So far, we’ve focused on regression problems, where the response variable is quantitative.\nNow, we shift gears to classification problems, where the response variable is qualitative (categorical).\nGoal: To predict a qualitative response – that is, to classify an observation into a category or class.\nExamples:\n\nPredicting whether a patient has a specific disease (yes/no).\nClassifying an email as spam or not spam.\nDetermining whether a credit card transaction is fraudulent."
  },
  {
    "objectID": "qmd/islp4.html#introduction-to-classification",
    "href": "qmd/islp4.html#introduction-to-classification",
    "title": "",
    "section": "",
    "text": "So far, we’ve focused on regression problems, where the response variable is quantitative.\nNow, we shift gears to classification problems, where the response variable is qualitative (categorical).\nGoal: To predict a qualitative response – that is, to classify an observation into a category or class.\nExamples:\n\nPredicting whether a patient has a specific disease (yes/no).\nClassifying an email as spam or not spam.\nDetermining whether a credit card transaction is fraudulent."
  },
  {
    "objectID": "qmd/islp4.html#what-is-classification",
    "href": "qmd/islp4.html#what-is-classification",
    "title": "",
    "section": "What is Classification?",
    "text": "What is Classification?\n\nClassification involves assigning an observation to a category, or class.\nIt’s like sorting things into different boxes. 📦\nMany methods first predict the probability of belonging to each category, and classify based on those probabilities."
  },
  {
    "objectID": "qmd/islp4.html#classification-vs.-regression",
    "href": "qmd/islp4.html#classification-vs.-regression",
    "title": "",
    "section": "Classification vs. Regression",
    "text": "Classification vs. Regression\n\n\n\nFeature\nRegression\nClassification\n\n\n\n\nResponse Variable\nQuantitative\nQualitative\n\n\nGoal\nPredict a numerical value\nPredict a category\n\n\nExample\nPredict house price\nPredict disease presence (yes/no)"
  },
  {
    "objectID": "qmd/islp4.html#why-not-linear-regression",
    "href": "qmd/islp4.html#why-not-linear-regression",
    "title": "",
    "section": "Why Not Linear Regression?",
    "text": "Why Not Linear Regression?\n\n\n\n\n\n\n\nText material pointed out linear regression is not appropriate in the case of a qualitative response.\nSuppose there are three possible diagnoses: stroke, drug overdose, and epileptic seizure.\nWe could try coding them numerically (e.g., 1=stroke, 2=drug overdose, 3=epileptic seizure).\n\n\n\n\n\n\nMedical conditions coding\n\n\n\n\n\n\n\nBut this imposes an order and equal differences that may not make sense!\nDifferent codings would give completely different models and predictions!\nFor binary (two-level) responses, linear regression can work (0/1 coding), but probabilities might fall outside [0,1]. Better options exist!"
  },
  {
    "objectID": "qmd/islp4.html#example-the-default-data",
    "href": "qmd/islp4.html#example-the-default-data",
    "title": "",
    "section": "Example: The Default Data",
    "text": "Example: The Default Data\n\nWe’ll use a simulated dataset called “Default”.\nGoal: Predict whether an individual will default on their credit card payment.\nPredictors:\n\nAnnual income.\nMonthly credit card balance.\n\nResponse: default (Yes/No)"
  },
  {
    "objectID": "qmd/islp4.html#visualizing-the-default-data",
    "href": "qmd/islp4.html#visualizing-the-default-data",
    "title": "",
    "section": "Visualizing the Default Data",
    "text": "Visualizing the Default Data\n\n\n\nThe Default data set\n\n\n\n\n\n\n\n\n\nLeft: Scatterplot of income and balance, color-coded by default status (orange = defaulted, blue = did not default).\nCenter: Boxplots of balance, separated by default status. Defaulters tend to have higher balances.\nRight: Boxplots of income, separated by default status. The relationship is less clear here.\n\n\n\n\n\n\n\n\n\n\nHigher credit card balances seem associated with a higher probability of default."
  },
  {
    "objectID": "qmd/islp4.html#logistic-regression-the-core-idea",
    "href": "qmd/islp4.html#logistic-regression-the-core-idea",
    "title": "",
    "section": "Logistic Regression: The Core Idea",
    "text": "Logistic Regression: The Core Idea\n\nInstead of modeling the response directly, logistic regression models the probability that Y belongs to a particular category.\nFor the Default data, we model: Pr(default = Yes | balance, income)\nThis probability will always be between 0 and 1. 👍\nOnce we have the probability, we can classify (e.g., predict “default=Yes” if the probability is &gt; 0.5)."
  },
  {
    "objectID": "qmd/islp4.html#the-logistic-model",
    "href": "qmd/islp4.html#the-logistic-model",
    "title": "",
    "section": "The Logistic Model",
    "text": "The Logistic Model\n\nWe need a function that outputs values between 0 and 1, for any input. The logistic function does this!\n\n\\[\np(X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}}\n\\]\n\n\n\n\n\n\n\np(X): Probability of the event (e.g., default) given predictor(s) X.\nβ₀ and β₁: Coefficients estimated from the data.\ne: The base of the natural logarithm (approximately 2.718).\n\n\n\n\n\nThis produces an S-shaped curve."
  },
  {
    "objectID": "qmd/islp4.html#linear-vs.-logistic-regression-on-default-data",
    "href": "qmd/islp4.html#linear-vs.-logistic-regression-on-default-data",
    "title": "",
    "section": "Linear vs. Logistic Regression on Default Data",
    "text": "Linear vs. Logistic Regression on Default Data\n\n\n\nClassification using the Default data\n\n\n\n\n\n\n\n\n\nLeft: Linear regression. Notice the negative probabilities and probabilities &gt; 1! 🙁\nRight: Logistic regression. Probabilities are always between 0 and 1. 🙂\n\n\n\n\n\n\n\n\n\n\nLogistic regression provides a more sensible fit for binary outcomes."
  },
  {
    "objectID": "qmd/islp4.html#the-logistic-model-continued",
    "href": "qmd/islp4.html#the-logistic-model-continued",
    "title": "",
    "section": "The Logistic Model (Continued)",
    "text": "The Logistic Model (Continued)\n\nA little algebra reveals a connection to odds:\n\n\\[\n\\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1X}\n\\]\n\nThe left side is the odds, which can range from 0 to ∞.\nTaking the logarithm of both sides:\n\n\\[\n\\log\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + \\beta_1X\n\\]\n\nThe left side is the log-odds or logit. This is linear in X!"
  },
  {
    "objectID": "qmd/islp4.html#interpreting-the-coefficients",
    "href": "qmd/islp4.html#interpreting-the-coefficients",
    "title": "",
    "section": "Interpreting the Coefficients",
    "text": "Interpreting the Coefficients\n\nIn linear regression, β₁ is the average change in Y for a one-unit increase in X.\nIn logistic regression, β₁ is the change in the log-odds for a one-unit increase in X.\nEquivalently, a one-unit increase in X multiplies the odds by eβ₁.\nThe amount p(X) changes depends on the current value of X, because the relationship is non-linear."
  },
  {
    "objectID": "qmd/islp4.html#estimating-the-coefficients",
    "href": "qmd/islp4.html#estimating-the-coefficients",
    "title": "",
    "section": "Estimating the Coefficients",
    "text": "Estimating the Coefficients\n\nWe use maximum likelihood estimation (MLE).\n\nThe goal is to find the coefficients (β₀, β₁, etc.) that make the observed data most likely.\nThe likelihood function for logistic regression:\n\n\\[\nl(\\beta_0, \\beta_1) = \\prod_{i:y_i=1} p(x_i) \\prod_{i':y_{i'}=0} (1 - p(x_{i'}))\n\\]\n\n\n\n\n\n\n\nWe multiply the probabilities of observing each data point, given the coefficients.\nFor defaulters (yᵢ=1), we use p(xᵢ). For non-defaulters (yᵢ’=0), we use 1-p(xᵢ’).\n\n\n\n\n\nSoftware (like R) handles the maximization for us."
  },
  {
    "objectID": "qmd/islp4.html#making-predictions",
    "href": "qmd/islp4.html#making-predictions",
    "title": "",
    "section": "Making Predictions",
    "text": "Making Predictions\n\nOnce we have the estimated coefficients, we can predict the probability of default for any given values of balance and income.\nExample (using coefficients from Table 4.1):\n\nbalance = $1,000: p(X) ≈ 0.00576 (less than 1% chance of default)\nbalance = $2,000: p(X) ≈ 0.586 (58.6% chance of default)\n\nWe can classify based on a threshold (e.g., classify as “default” if p(X) &gt; 0.5)."
  },
  {
    "objectID": "qmd/islp4.html#multiple-logistic-regression",
    "href": "qmd/islp4.html#multiple-logistic-regression",
    "title": "",
    "section": "Multiple Logistic Regression",
    "text": "Multiple Logistic Regression\n\nJust like with linear regression, we can include multiple predictors:\n\n\\[\n\\log\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p\n\\] \\[\np(X) = \\frac{e^{\\beta_0 + \\beta_1X_1 + \\dots + \\beta_pX_p}}{1 + e^{\\beta_0 + \\beta_1X_1 + \\dots + \\beta_pX_p}}\n\\] - We estimate the coefficients using MLE. - Interpretation: βⱼ represents the change in log-odds for a one-unit increase in Xⱼ, holding all other predictors constant."
  },
  {
    "objectID": "qmd/islp4.html#example-multiple-logistic-regression-on-default-data",
    "href": "qmd/islp4.html#example-multiple-logistic-regression-on-default-data",
    "title": "",
    "section": "Example: Multiple Logistic Regression on Default Data",
    "text": "Example: Multiple Logistic Regression on Default Data\n\nPredictors: balance, income, student (dummy variable: 1 if student, 0 if not).\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. error\nz-statistic\np-value\n\n\n\n\nIntercept\n-10.8690\n0.4923\n-22.08\n&lt; 0.0001\n\n\nbalance\n0.0057\n0.0002\n24.74\n&lt; 0.0001\n\n\nincome\n0.0030\n0.0082\n0.37\n0.7115\n\n\nstudent[Yes]\n-0.6468\n0.2362\n-2.74\n0.0062\n\n\n\n\nSurprising result: The coefficient for student is negative! This suggests students are less likely to default, holding balance and income constant."
  },
  {
    "objectID": "qmd/islp4.html#confounding",
    "href": "qmd/islp4.html#confounding",
    "title": "",
    "section": "Confounding",
    "text": "Confounding\n\n\n\nConfounding in the Default data.\n\n\n\n\n\n\n\n\n\nLeft: Default rates for students (orange) and non-students (blue) as a function of balance. Solid lines: default rate at each balance. Dashed lines: overall default rate.\nRight: Boxplots of balance for students and non-students.\n\n\n\n\n\nStudents have higher overall default rates (dashed lines), but lower default rates at each balance level (solid lines).\nReason: student and balance are correlated. Students tend to have higher balances, which are associated with higher default rates.\nThis is confounding: The effect of one predictor is mixed up with the effect of another."
  },
  {
    "objectID": "qmd/islp4.html#multinomial-logistic-regression",
    "href": "qmd/islp4.html#multinomial-logistic-regression",
    "title": "",
    "section": "Multinomial Logistic Regression",
    "text": "Multinomial Logistic Regression\n\nWhat if the response has more than two categories? (e.g., medical diagnosis: stroke, drug overdose, epileptic seizure)\nMultinomial logistic regression extends the two-class case.\nWe choose a baseline category (e.g., the Kth category).\nWe model the log-odds of each category relative to the baseline:\n\n\\[\n\\log\\left(\\frac{\\Pr(Y = k|X = x)}{\\Pr(Y = K|X = x)}\\right) = \\beta_{k0} + \\beta_{k1}x_1 + \\dots + \\beta_{kp}x_p\n\\]\n\nThere are K-1 sets of coefficients.\nThe choice of baseline is arbitrary; the predictions will be the same regardless."
  },
  {
    "objectID": "qmd/islp4.html#multinomial-logistic-regression-softmax-coding",
    "href": "qmd/islp4.html#multinomial-logistic-regression-softmax-coding",
    "title": "",
    "section": "Multinomial Logistic Regression: Softmax Coding",
    "text": "Multinomial Logistic Regression: Softmax Coding\n\nAn alternative, equivalent formulation is softmax coding.\nInstead of a baseline, we treat all K classes symmetrically:\n\n\\[\n\\Pr(Y = k|X = x) = \\frac{e^{\\beta_{k0} + \\beta_{k1}x_1 + \\dots + \\beta_{kp}x_p}}{\\sum_{l=1}^K e^{\\beta_{l0} + \\beta_{l1}x_1 + \\dots + \\beta_{lp}x_p}}\n\\]\n\nThis is often used in machine learning."
  },
  {
    "objectID": "qmd/islp4.html#generative-models-for-classification",
    "href": "qmd/islp4.html#generative-models-for-classification",
    "title": "",
    "section": "Generative Models for Classification",
    "text": "Generative Models for Classification\n\nLogistic regression directly models Pr(Y = k | X = x).\nNow we explore an indirect approach:\n\nModel the distribution of the predictors X separately in each response class (i.e., for each value of Y).\nUse Bayes’ theorem to “flip” these around into estimates of Pr(Y = k | X = x).\n\nThese are called generative models because they specify how the data is generated."
  },
  {
    "objectID": "qmd/islp4.html#bayes-theorem-for-classification",
    "href": "qmd/islp4.html#bayes-theorem-for-classification",
    "title": "",
    "section": "Bayes’ Theorem for Classification",
    "text": "Bayes’ Theorem for Classification\n\nLet πk be the prior probability that an observation comes from class k.\nLet fk(X) = Pr(X | Y = k) be the density function of X for an observation from class k.\nBayes’ theorem states:\n\n\\[\n\\Pr(Y = k|X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)}\n\\]\n\nTo use this, we need to estimate πk and fk(x). Estimating πk is usually easy (fraction of training observations in class k). Estimating fk(x) is harder."
  },
  {
    "objectID": "qmd/islp4.html#why-bother-with-generative-models",
    "href": "qmd/islp4.html#why-bother-with-generative-models",
    "title": "",
    "section": "Why Bother with Generative Models?",
    "text": "Why Bother with Generative Models?\n\n\n\n\n\n\nIf we already have logistic regression, why use this indirect approach?\n\n\n\nWhen there is substantial separation between classes, logistic regression parameters can be unstable. Generative models may be better.\nIf the distribution of predictors is approximately normal within each class, and the sample size is small, generative models may be more accurate.\nGenerative models can be easily extended to more than two response classes."
  },
  {
    "objectID": "qmd/islp4.html#linear-discriminant-analysis-lda",
    "href": "qmd/islp4.html#linear-discriminant-analysis-lda",
    "title": "",
    "section": "Linear Discriminant Analysis (LDA)",
    "text": "Linear Discriminant Analysis (LDA)\n\nAssumptions:\n\nfk(x) is normal (Gaussian).\nWe have a common variance across all K classes (σ²).\n\nFor a single predictor (p=1):\n\n\\[\nf_k(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2\\right)\n\\]\n\n\n\n\n\n\n\nμk: mean for class k\nσ²: common variance"
  },
  {
    "objectID": "qmd/islp4.html#lda-continued",
    "href": "qmd/islp4.html#lda-continued",
    "title": "",
    "section": "LDA (Continued)",
    "text": "LDA (Continued)\n\nPlugging fk(x) into Bayes’ theorem and simplifying, we classify an observation to the class for which this is largest:\n\n\\[\n\\delta_k(x) = x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k)\n\\]\n\nThis is the discriminant function. It’s linear in x. That’s why it’s called Linear Discriminant Analysis!\nIn practice, we don’t know the true parameters (πk, μk, σ²). We estimate them from the training data."
  },
  {
    "objectID": "qmd/islp4.html#lda-estimating-the-parameters",
    "href": "qmd/islp4.html#lda-estimating-the-parameters",
    "title": "",
    "section": "LDA: Estimating the Parameters",
    "text": "LDA: Estimating the Parameters\n\nμ̂k = (1/nk) Σi:yᵢ=k xᵢ (sample mean for class k)\nσ̂² = (1/(n-K)) Σk=1K Σi:yᵢ=k (xᵢ - μ̂k)² (pooled variance estimate)\nπ̂k = nk/n (sample proportion for class k)\nWe plug these estimates into the discriminant function."
  },
  {
    "objectID": "qmd/islp4.html#lda-example",
    "href": "qmd/islp4.html#lda-example",
    "title": "",
    "section": "LDA: Example",
    "text": "LDA: Example\n\n\n\nOne-dimensional normal density functions and LDA decision boundary\n\n\n\n\n\n\n\n\n\nLeft: Two normal density functions. Dashed line: Bayes decision boundary.\nRight: 20 observations from each class (histograms). Dashed line: Bayes boundary. Solid line: LDA boundary.\n\n\n\n\n\nLDA approximates the Bayes classifier."
  },
  {
    "objectID": "qmd/islp4.html#lda-with-multiple-predictors-p-1",
    "href": "qmd/islp4.html#lda-with-multiple-predictors-p-1",
    "title": "",
    "section": "LDA with Multiple Predictors (p > 1)",
    "text": "LDA with Multiple Predictors (p &gt; 1)\n\nWe assume X = (X₁, X₂, …, Xp) follows a multivariate Gaussian distribution, with a class-specific mean vector and a common covariance matrix.\nMultivariate Gaussian density:\n\n\\[\nf(x) = \\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x - \\mu)^T\\Sigma^{-1}(x - \\mu)\\right)\n\\]\n\n\n\n\n\n\n\nμ: mean vector\nΣ: covariance matrix\n|Σ|: determinant of Σ"
  },
  {
    "objectID": "qmd/islp4.html#lda-with-multiple-predictors-continued",
    "href": "qmd/islp4.html#lda-with-multiple-predictors-continued",
    "title": "",
    "section": "LDA with Multiple Predictors (Continued)",
    "text": "LDA with Multiple Predictors (Continued)\n\nPlugging the multivariate Gaussian density into Bayes’ theorem, we classify to the class for which this is largest:\n\n\\[\n\\delta_k(x) = x^T \\Sigma^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + \\log \\pi_k\n\\]\n\nAgain, this is linear in x.\nWe estimate the parameters (μk, Σ, πk) from the training data."
  },
  {
    "objectID": "qmd/islp4.html#lda-example-with-three-classes",
    "href": "qmd/islp4.html#lda-example-with-three-classes",
    "title": "",
    "section": "LDA: Example with Three Classes",
    "text": "LDA: Example with Three Classes\n ::: {.callout-note appearance=“simple”} - Left: Ellipses contain 95% of the probability for each class. Dashed lines: Bayes decision boundaries. - Right: 20 observations from each class. Solid lines: LDA decision boundaries. ::: - LDA approximates the Bayes decision boundaries."
  },
  {
    "objectID": "qmd/islp4.html#lda-on-the-default-data",
    "href": "qmd/islp4.html#lda-on-the-default-data",
    "title": "",
    "section": "LDA on the Default Data",
    "text": "LDA on the Default Data\n\nWe can apply LDA to the Default data (predicting default based on balance and student status).\nTraining error rate: 2.75%. Sounds good, but…\nTwo problems:\n\nTraining error rates are usually lower than test error rates.\nOnly 3.33% of individuals in the training data defaulted. A useless classifier that always predicts “no default” would have a 3.33% error rate! (This is the null classifier.)"
  },
  {
    "objectID": "qmd/islp4.html#confusion-matrix",
    "href": "qmd/islp4.html#confusion-matrix",
    "title": "",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\nA confusion matrix shows the types of errors being made.\n\n\n\n\n\n\n\n\n\n\n\nPredicted No Default\nPredicted Default\nTotal\n\n\n\n\nActual No Default\n9644\n23\n9667\n\n\nActual Default\n252\n81\n333\n\n\nTotal\n9896\n104\n10000\n\n\n\n\nLDA misclassifies 252/333 = 75.7% of defaulters! (Low sensitivity.)\nBut it correctly classifies (1 - 23/9667) = 99.8% of non-defaulters. (High specificity.)"
  },
  {
    "objectID": "qmd/islp4.html#modifying-the-threshold",
    "href": "qmd/islp4.html#modifying-the-threshold",
    "title": "",
    "section": "Modifying the Threshold",
    "text": "Modifying the Threshold\n\nLDA (and the Bayes classifier) uses a threshold of 0.5 for the posterior probability of default.\nIf we lower the threshold (e.g., to 0.2), we can increase sensitivity (correctly identify more defaulters), but at the cost of decreased specificity (more false positives).\nThe best threshold depends on the relative costs of the two types of errors."
  },
  {
    "objectID": "qmd/islp4.html#roc-curves",
    "href": "qmd/islp4.html#roc-curves",
    "title": "",
    "section": "ROC Curves",
    "text": "ROC Curves\n\n\n\nError rates as a function of threshold\n\n\n\n\n\n\n\n\n\nShows various error rates as the threshold changes.\nBlack solid line: overall error rate\nBlue dashed line: fraction of defaulters incorrectly classified\nOrange dotted line: fraction of errors among non-defaulters\n\n\n\n\n\nA Receiver Operating Characteristic (ROC) curve plots the true positive rate (sensitivity) versus the false positive rate (1 - specificity) for all possible thresholds."
  },
  {
    "objectID": "qmd/islp4.html#roc-curve-continued",
    "href": "qmd/islp4.html#roc-curve-continued",
    "title": "",
    "section": "ROC Curve (Continued)",
    "text": "ROC Curve (Continued)\n\n\n\nROC curve for LDA classifier on Default data\n\n\n\n\n\n\n\n\n\nIdeal curve hugs the top left corner (high true positive rate, low false positive rate).\nDotted line: “no information” classifier.\n\n\n\n\n\nThe area under the curve (AUC) summarizes performance. AUC = 1 is perfect; AUC = 0.5 is no better than chance."
  },
  {
    "objectID": "qmd/islp4.html#quadratic-discriminant-analysis-qda",
    "href": "qmd/islp4.html#quadratic-discriminant-analysis-qda",
    "title": "",
    "section": "Quadratic Discriminant Analysis (QDA)",
    "text": "Quadratic Discriminant Analysis (QDA)\n\nQDA relaxes the assumption of a common covariance matrix.\nEach class has its own covariance matrix, Σk.\nThe discriminant function becomes quadratic in x:\n\n\\[\n\\delta_k(x) = -\\frac{1}{2}x^T\\Sigma_k^{-1}x + x^T\\Sigma_k^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\Sigma_k^{-1}\\mu_k - \\frac{1}{2}\\log|\\Sigma_k| + \\log \\pi_k\n\\]\n\nWe estimate the parameters (μk, Σk, πk) from the training data."
  },
  {
    "objectID": "qmd/islp4.html#lda-vs.-qda-bias-variance-trade-off",
    "href": "qmd/islp4.html#lda-vs.-qda-bias-variance-trade-off",
    "title": "",
    "section": "LDA vs. QDA: Bias-Variance Trade-Off",
    "text": "LDA vs. QDA: Bias-Variance Trade-Off\n\nQDA is more flexible than LDA (more parameters to estimate).\nQDA has higher variance but potentially lower bias.\nLDA tends to be better when there are fewer training observations (reducing variance is crucial).\nQDA is recommended when the training set is large, or when the assumption of a common covariance matrix is clearly untenable."
  },
  {
    "objectID": "qmd/islp4.html#lda-vs.-qda-example",
    "href": "qmd/islp4.html#lda-vs.-qda-example",
    "title": "",
    "section": "LDA vs. QDA: Example",
    "text": "LDA vs. QDA: Example\n\n\n\nLDA and QDA decision boundaries\n\n\n\n\n\n\n\n\n\nLeft: Classes have common correlation. Bayes boundary (dashed) is linear. LDA (black dotted) is better than QDA (green solid).\nRight: Classes have different correlations. Bayes boundary is quadratic. QDA is better than LDA."
  },
  {
    "objectID": "qmd/islp4.html#naive-bayes",
    "href": "qmd/islp4.html#naive-bayes",
    "title": "",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nMakes a very strong assumption: Within each class, the p predictors are independent.\nfk(x) = fk1(x₁) × fk2(x₂) × … × fkp(xp)\n\nfkj is the density function of the jth predictor in class k.\n\nThis simplifies things a lot! We only need to estimate one-dimensional densities.\nWe plug this into Bayes’ theorem:\n\n\\[\n\\Pr(Y = k|X = x) = \\frac{\\pi_k \\times f_{k1}(x_1) \\times f_{k2}(x_2) \\times \\dots \\times f_{kp}(x_p)}{\\sum_{l=1}^K \\pi_l \\times f_{l1}(x_1) \\times f_{l2}(x_2) \\times \\dots \\times f_{lp}(x_p)}\n\\]"
  },
  {
    "objectID": "qmd/islp4.html#naive-bayes-estimating-the-one-dimensional-densities",
    "href": "qmd/islp4.html#naive-bayes-estimating-the-one-dimensional-densities",
    "title": "",
    "section": "Naive Bayes: Estimating the One-Dimensional Densities",
    "text": "Naive Bayes: Estimating the One-Dimensional Densities\n\nIf Xj is quantitative:\n\nWe can assume Xj | Y = k ~ N(μjk, σj²) (Gaussian).\nOr, we can use a non-parametric estimate (e.g., histogram, kernel density estimator).\n\nIf Xj is qualitative:\n\nWe can simply count the proportion of training observations for each level of the predictor within each class."
  },
  {
    "objectID": "qmd/islp4.html#naive-bayes-example",
    "href": "qmd/islp4.html#naive-bayes-example",
    "title": "",
    "section": "Naive Bayes: Example",
    "text": "Naive Bayes: Example\n\n\n\n\n\n\n\n\n\nf11\n\n\n\n\n\n\n\nf12\n\n\n\n\n\n\n\nf13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nf21\n\n\n\n\n\n\n\nf22\n\n\n\n\n\n\n\nf23\n\n\n\n\n\n\n\n\n\n\n\n\nTwo classes (K=2), three predictors (p=3). First two predictors are quantitative, third is qualitative (three levels).\nEstimated density functions fkj are shown.\n\n\n\n\n\nNaive Bayes often works surprisingly well, despite the strong independence assumption. It reduces variance, which can be beneficial."
  },
  {
    "objectID": "qmd/islp4.html#comparison-of-classification-methods-an-analytical-perspective",
    "href": "qmd/islp4.html#comparison-of-classification-methods-an-analytical-perspective",
    "title": "",
    "section": "Comparison of Classification Methods: An Analytical Perspective",
    "text": "Comparison of Classification Methods: An Analytical Perspective\n\nLogistic regression, LDA, QDA, and naive Bayes can all be expressed in terms of maximizing Pr(Y = k | X = x).\nEquivalently, we can maximize the log-odds relative to a baseline class (K):\n\n\\[\n\\log\\left(\\frac{\\Pr(Y = k|X = x)}{\\Pr(Y = K|X = x)}\\right)\n\\]\n\nThe form of this log-odds expression differs for each method.\n\n\n\n\n\n\n\n\nMethod\nLog-Odds Form\n\n\n\n\nLogistic Regression\nβk0 + Σj=1p βkjxj (linear)\n\n\nLDA\nak + Σj=1p bkjxj (linear)\n\n\nQDA\nak + Σj=1p bkjxj + Σj=1p Σl=1p ckjlxjxl (quadratic)\n\n\nNaive Bayes\nak + Σj=1p gkj(xj) (additive, where gkj is a function of xj)\n\n\n\n\nLDA is a special case of QDA.\nAny classifier with a linear decision boundary is a special case of naive Bayes.\nLogistic regression has the same linear form as LDA, but the coefficients are estimated differently.\nNone of the method dominate others universally."
  },
  {
    "objectID": "qmd/islp4.html#a-comparison-of-classification-methodsan-empirical-comparison",
    "href": "qmd/islp4.html#a-comparison-of-classification-methodsan-empirical-comparison",
    "title": "",
    "section": "A comparison of classification methods：An empirical comparison",
    "text": "A comparison of classification methods：An empirical comparison\n\nTest error rates of different methods are compared through six different scenarios.\n\n\n\n\n\n\n\n\n\n\n\nFIGURE 4.11\n\n\n\n\n\n\n\nFIGURE 4.12\n\n\n\n\n\n\n\n\n\n\n\n\nSix different scenarios are involved, scenarios 1-3 Bayes decision boundaries are linear, and scenarios 4-6 Bayes decision boundaries are non-linear.\nThere were p=2 quantitative predictors in each of the six scenarios.\nFor each scenario, 100 random training data sets were generated.\nScenario 1 to Scenario 6 are described in the text material page 31 and page 32.\n\n\n\n\n\nWhen the true decision boundaries are linear, LDA and logistic regression perform well.\nWhen boundaries are moderately non-linear, QDA or naive Bayes may be better.\nFor very complex boundaries, a non-parametric method like KNN can be superior, but the level of smoothness must be chosen carefully."
  },
  {
    "objectID": "qmd/islp4.html#generalized-linear-models-glms",
    "href": "qmd/islp4.html#generalized-linear-models-glms",
    "title": "",
    "section": "Generalized Linear Models (GLMs)",
    "text": "Generalized Linear Models (GLMs)\n\nSo far, the response Y is either quantitative(use least squares linear regression to predict) or qualitative(use classification methods).\nBut Y is neither qualitative nor quantitative is also possible, like the number of hourly users of a bike sharing program.\nTo predict the number of bike users, the least squares linear regression model shown defects(negative fitted values, variance increasing when mean number increasing, response not continuous-valued)\nPoisson regression could provide a much more natural and elegant approach."
  },
  {
    "objectID": "qmd/islp4.html#poisson-regression",
    "href": "qmd/islp4.html#poisson-regression",
    "title": "",
    "section": "Poisson Regression",
    "text": "Poisson Regression\n\nSuppose the random variable Y takes on nonnegative integer values and follows the Poisson distribution. The probability could be written as:\n\n\\[\n\\Pr(Y = k) = \\frac{e^{-\\lambda}\\lambda^k}{k!}  \\quad \\text{for } k = 0, 1, 2, \\dots\n\\]\n\n\n\n\n\n\n\nk! means k factorial, λ &gt; 0 is the expected value of Y, which also equals the variance of Y.\nLarger the mean of Y, larger the variance of Y.\n\n\n\n\n\nPoisson distribution is used to model counts.\nNumber of bike users could be modeled as Poisson distribution with mean value. The mean could vary as a function of the covariates."
  },
  {
    "objectID": "qmd/islp4.html#poisson-regression-model",
    "href": "qmd/islp4.html#poisson-regression-model",
    "title": "",
    "section": "Poisson Regression Model",
    "text": "Poisson Regression Model\n\\[\n\\log(\\lambda(X_1, \\dots, X_p)) = \\beta_0 + \\beta_1X_1 + \\dots + \\beta_pX_p\n\\] or equivalently\n\\[\n\\lambda(X_1, \\dots, X_p) = e^{\\beta_0 + \\beta_1X_1 + \\dots + \\beta_pX_p}\n\\]\n\nThe log of λ is linear in predictors.\nλ takes on nonnegative values for all values of covariates.\nWe use maximum likelihood to estimate parameters."
  },
  {
    "objectID": "qmd/islp4.html#comparing-poisson-regression-model-to-the-linear-regression-model",
    "href": "qmd/islp4.html#comparing-poisson-regression-model-to-the-linear-regression-model",
    "title": "",
    "section": "Comparing Poisson Regression Model to the linear regression model",
    "text": "Comparing Poisson Regression Model to the linear regression model\n\nInterpretation of the coefficients\nMean-variance relationship\nNonnegative fitted values\nPlease read text material for details."
  },
  {
    "objectID": "qmd/islp4.html#generalized-linear-models-in-greater-generality",
    "href": "qmd/islp4.html#generalized-linear-models-in-greater-generality",
    "title": "",
    "section": "Generalized Linear Models in Greater Generality",
    "text": "Generalized Linear Models in Greater Generality\n\nThree types of regression models have been discussed: linear, logistic and Poisson. They share some common characteristics:\n\nPredictors are used to predict response variable. Conditional on predictors, Y belongs to a certain family of distributions. We make different assumption on the family of distributions that Y belongs to, correspondingly, we have different regression models.\nModeling the mean of Y. Use different link function η, these regression models could be expressed as below:\n\n\n\\[\n\\eta(E(Y|X_1, \\dots, X_p)) = \\beta_0 + \\beta_1X_1 + \\dots + \\beta_pX_p\n\\]\n\nAll these regression models are examples of Generalized Linear Model(GLM)"
  },
  {
    "objectID": "qmd/islp4.html#summary",
    "href": "qmd/islp4.html#summary",
    "title": "",
    "section": "Summary",
    "text": "Summary\n\nClassification predicts qualitative responses.\nLogistic regression, LDA, QDA, and naive Bayes are common classification methods.\nLogistic regression models the probability of class membership using the logistic function.\nLDA and QDA assume the predictors follow a Gaussian distribution within each class. LDA assumes a common covariance matrix; QDA does not.\nNaive Bayes assumes independence of predictors within each class.\nThe choice of method depends on the data and the bias-variance trade-off.\nROC curves are useful for evaluating classifier performance across a range of thresholds.\nGeneralized linear models handle responses from non-normal distributions, Poisson regression model is an example."
  },
  {
    "objectID": "qmd/islp4.html#thoughts-and-discussion",
    "href": "qmd/islp4.html#thoughts-and-discussion",
    "title": "",
    "section": "Thoughts and Discussion 🤔",
    "text": "Thoughts and Discussion 🤔\n\n\nCan you think of real-world scenarios where each classification method (logistic regression, LDA, QDA, naive Bayes) might be most appropriate?\nHow would you choose the “best” classification method for a given dataset? What metrics would you consider?\nWhat are the limitations of each method? When might they fail?\nHow does the bias-variance trade-off play a role in choosing a classification method?\nCan we apply the knowledge of mean-variance relationship and fitted values to choose suitable regression model?"
  },
  {
    "objectID": "qmd/islp1.html",
    "href": "qmd/islp1.html",
    "title": "",
    "section": "",
    "text": "Welcome to Statistical Learning!\n\n\nThis chapter provides a gentle introduction to the exciting world of statistical learning, a powerful toolkit for understanding data. We will explore the core concepts, see real-world examples, and start our journey toward building predictive models."
  },
  {
    "objectID": "qmd/islp1.html#welcome",
    "href": "qmd/islp1.html#welcome",
    "title": "",
    "section": "",
    "text": "Welcome to Statistical Learning!\n\n\nThis chapter provides a gentle introduction to the exciting world of statistical learning, a powerful toolkit for understanding data. We will explore the core concepts, see real-world examples, and start our journey toward building predictive models."
  },
  {
    "objectID": "qmd/islp1.html#what-is-statistical-learning",
    "href": "qmd/islp1.html#what-is-statistical-learning",
    "title": "",
    "section": "What is Statistical Learning?",
    "text": "What is Statistical Learning?\nStatistical learning refers to a vast set of tools for understanding data. These tools can be broadly classified into two categories:\n\n\n1. Supervised Learning:\n\nBuilding a model to predict or estimate an output based on one or more inputs (also known as predictors, features, or independent variables).\nThink of it like teaching a computer to learn from examples where you provide both the questions (inputs) and the answers (outputs).\nExample: Predicting a house price based on its size, location, and number of bedrooms.\n\n\n2. Unsupervised Learning:\n\nDiscovering relationships and structure in data without a predefined output variable.\nHere, you’re letting the computer explore the data and find patterns on its own.\nExample: Grouping customers into different segments based on their purchasing behavior, without knowing in advance what those segments should be."
  },
  {
    "objectID": "qmd/islp1.html#supervised-vs.-unsupervised-a-visual-analogy",
    "href": "qmd/islp1.html#supervised-vs.-unsupervised-a-visual-analogy",
    "title": "",
    "section": "Supervised vs. Unsupervised: A Visual Analogy",
    "text": "Supervised vs. Unsupervised: A Visual Analogy\n\nImagine you have a basket of fruits.\n\nSupervised Learning: You tell a child, “This is an apple, this is a banana, this is an orange.” Then you show them a new fruit and ask, “What is this?”\nUnsupervised Learning: You give the child the basket and say, “Sort these fruits into groups however you think is best.” The child might group them by color, shape, or size, discovering inherent patterns without being told what to look for."
  },
  {
    "objectID": "qmd/islp1.html#data-mining-machine-learning-and-statistical-learning",
    "href": "qmd/islp1.html#data-mining-machine-learning-and-statistical-learning",
    "title": "",
    "section": "Data Mining, Machine Learning, and Statistical Learning",
    "text": "Data Mining, Machine Learning, and Statistical Learning\nThese terms are often used interchangeably, but there are subtle differences:\n\n\n\n\n\ngraph LR\n    A[Data Mining] --&gt; C(Common Ground)\n    B[Machine Learning] --&gt; C\n    D[Statistical Learning] --&gt; C\n    C --&gt; E[Insights & Predictions]\n\n\n\n\n\n\n\nData Mining: Focuses on discovering patterns and extracting knowledge from large datasets, often using techniques from database management and computer science.\nMachine Learning: Primarily concerned with building algorithms that can learn from and make predictions on data. Emphasizes predictive accuracy and computational efficiency.\nStatistical Learning: A subfield of statistics that emphasizes model interpretability and understanding the uncertainty associated with predictions. Provides a rigorous statistical framework for machine learning."
  },
  {
    "objectID": "qmd/islp1.html#real-world-applications",
    "href": "qmd/islp1.html#real-world-applications",
    "title": "",
    "section": "Real-World Applications",
    "text": "Real-World Applications\nLet’s explore some real-world data sets:\n\nWage Data: Analyzing factors that influence a person’s wage.\nStock Market Data: Predicting stock market movements.\nGene Expression Data: Identifying groups of genes with similar expression patterns.\n\nWe will use these data sets throughout the course to illustrate various statistical learning techniques."
  },
  {
    "objectID": "qmd/islp1.html#wage-data-understanding-income",
    "href": "qmd/islp1.html#wage-data-understanding-income",
    "title": "",
    "section": "Wage Data: Understanding Income",
    "text": "Wage Data: Understanding Income\nWe want to understand how a person’s wage is related to their:\n\nAge\nEducation\nYear (calendar year)\n\n\n\n\n\n\n\nNote\n\n\n\nThe goal is to build a model that can predict a person’s wage based on these factors."
  },
  {
    "objectID": "qmd/islp1.html#wage-data-wage-vs.-age",
    "href": "qmd/islp1.html#wage-data-wage-vs.-age",
    "title": "",
    "section": "Wage Data: Wage vs. Age",
    "text": "Wage Data: Wage vs. Age\n\n\n\nWage as a function of age\n\n\n\n\nLeft Panel: This scatterplot shows individual wages plotted against age. The blue line represents the average wage for a given age.\nTrend: Wage generally increases with age until around 60, then decreases. This suggests a non-linear relationship.\nVariability: There’s significant spread around the average, meaning age alone isn’t a perfect predictor."
  },
  {
    "objectID": "qmd/islp1.html#wage-data-wage-vs.-year-education",
    "href": "qmd/islp1.html#wage-data-wage-vs.-year-education",
    "title": "",
    "section": "Wage Data: Wage vs. Year & Education",
    "text": "Wage Data: Wage vs. Year & Education\n\n\n\nWage as a function of year and education\n\n\n\n\nCenter Panel: Shows wage versus year. There’s a gradual increase in average wage over time (2003-2009).\nRight Panel: Boxplots of wage for different education levels (1 = lowest, 5 = highest). Higher education generally leads to higher wages.\nConclusion: Combining age, year, and education will likely provide the most accurate wage prediction."
  },
  {
    "objectID": "qmd/islp1.html#stock-market-data-predicting-direction",
    "href": "qmd/islp1.html#stock-market-data-predicting-direction",
    "title": "",
    "section": "Stock Market Data: Predicting Direction",
    "text": "Stock Market Data: Predicting Direction\n\nGoal: Predict whether the S&P 500 stock index will increase or decrease on a given day.\nInput: Percentage changes in the index over the previous 5 days.\nOutput: Categorical (qualitative) – either “Up” or “Down”. This is a classification problem."
  },
  {
    "objectID": "qmd/islp1.html#stock-market-data-previous-days-change",
    "href": "qmd/islp1.html#stock-market-data-previous-days-change",
    "title": "",
    "section": "Stock Market Data: Previous Day’s Change",
    "text": "Stock Market Data: Previous Day’s Change\n\n\n\nBoxplots of previous day’s change\n\n\n\n\nLeft Panel: Boxplots show the distribution of the previous day’s percentage change, separated by whether the market went “Up” or “Down” today.\nObservation: The two boxplots are very similar, suggesting that yesterday’s performance is not a strong predictor of today’s direction.\nCenter and Right Panel:The boxplots show the 2 days previous and 3 days previous percentage change, separated by whether the market went “Up” or “Down” today. The two boxplots are very similar."
  },
  {
    "objectID": "qmd/islp1.html#stock-market-data-predicting-with-qda",
    "href": "qmd/islp1.html#stock-market-data-predicting-with-qda",
    "title": "",
    "section": "Stock Market Data: Predicting with QDA",
    "text": "Stock Market Data: Predicting with QDA\n\n\n\nQuadratic Discriminant Analysis\n\n\n\n\nA statistical learning method called quadratic discriminant analysis (QDA) was used to predict market direction.\nResult: The model correctly predicted the direction about 60% of the time. This is better than random guessing (50%), but still far from perfect. It suggests weak trends might exist."
  },
  {
    "objectID": "qmd/islp1.html#gene-expression-data-clustering",
    "href": "qmd/islp1.html#gene-expression-data-clustering",
    "title": "",
    "section": "Gene Expression Data: Clustering",
    "text": "Gene Expression Data: Clustering\n\nGoal: Identify groups (clusters) of cancer cell lines based on their gene expression measurements.\nInput: 6,830 gene expression measurements for each of 64 cancer cell lines.\nOutput: None. This is an unsupervised learning problem (specifically, a clustering problem)."
  },
  {
    "objectID": "qmd/islp1.html#gene-expression-data-principal-components",
    "href": "qmd/islp1.html#gene-expression-data-principal-components",
    "title": "",
    "section": "Gene Expression Data: Principal Components",
    "text": "Gene Expression Data: Principal Components\n\n\n\nPrincipal Components Analysis\n\n\n\n\nChallenge: Visualizing 6,830 dimensions is impossible!\nSolution: Principal Component Analysis (PCA) reduces the data to two dimensions (Z1 and Z2) that capture the most important information.\nLeft Panel: Each point represents a cell line, colored by a suggested cluster. At least four groups seems clear.\nRight Panel: Same plot, but points are colored by the actual cancer type (14 types).\nObservation: Cell lines with the same cancer type tend to cluster together, validating the unsupervised clustering. This means that even though we didn’t tell the algorithm the cancer types, it was able to discover them (to some extent) from the gene expression data."
  },
  {
    "objectID": "qmd/islp1.html#a-brief-history-of-statistical-learning",
    "href": "qmd/islp1.html#a-brief-history-of-statistical-learning",
    "title": "",
    "section": "A Brief History of Statistical Learning",
    "text": "A Brief History of Statistical Learning\n\n\nEarly Beginnings (19th Century): Least squares (linear regression).\nMid-20th Century (1930s-1970s): Linear discriminant analysis, logistic regression, generalized linear models.\nComputational Revolution (1980s onwards): Non-linear methods become feasible (e.g., trees, generalized additive models). Neural Networks and Support Vector Machine were proposed.\nModern Era: Statistical learning emerges as a distinct field, fueled by powerful software (like Python) and increasing data availability."
  },
  {
    "objectID": "qmd/islp1.html#notation",
    "href": "qmd/islp1.html#notation",
    "title": "",
    "section": "Notation",
    "text": "Notation\n\n\nn: Number of observations (data points).\np: Number of variables (features, predictors).\nxij: Value of the jth variable for the ith observation.\nX: A matrix (n x p) representing the data. Think of it as a spreadsheet.\nyi: the ith observation of the variable on which we wish to make predictions\nBold lowercase (e.g., a): A vector of length n.\nNormal lowercase (e.g., a): A scalar (single number) or a vector not of length n.\nBold capitals (e.g., A): A matrix."
  },
  {
    "objectID": "qmd/islp1.html#notation-example-wage-data",
    "href": "qmd/islp1.html#notation-example-wage-data",
    "title": "",
    "section": "Notation: Example (Wage Data)",
    "text": "Notation: Example (Wage Data)\n\n\nn = 3000 (3000 people)\np = 11 (variables like year, age, education, etc.)\nx23 would be the value of the 3rd variable (e.g., education) for the 2nd person.\nX is a 3000 x 11 matrix.\ny1: the first person’s wage."
  },
  {
    "objectID": "qmd/islp1.html#summary",
    "href": "qmd/islp1.html#summary",
    "title": "",
    "section": "Summary",
    "text": "Summary\n\n\nStatistical learning provides tools to understand data, both with and without a specific outcome to predict.\nSupervised learning aims to predict an output, while unsupervised learning explores data structure.\nData mining, machine learning, and statistical learning are related but have different emphases.\nReal-world applications include predicting wages, stock market movements, and clustering gene expression data.\nUnderstanding notation is crucial for following the rest of the course."
  },
  {
    "objectID": "qmd/islp1.html#thoughts-and-discussion",
    "href": "qmd/islp1.html#thoughts-and-discussion",
    "title": "",
    "section": "Thoughts and Discussion",
    "text": "Thoughts and Discussion\n\n\nCan you think of other examples of supervised and unsupervised learning problems in your field of interest?\nWhy is it important to understand the limitations of statistical learning models, even when they achieve good predictive accuracy?\nWhat are the potential ethical implications of using statistical learning models in areas like hiring, loan applications, or criminal justice?\nHow do you imagine the combination of increasing data availability, more powerful hardware, and user-friendly software will further transform the field of statistical learning, making more sophisticated machine learning available to even more researchers?"
  },
  {
    "objectID": "qmd/islp10.html",
    "href": "qmd/islp10.html",
    "title": "",
    "section": "",
    "text": "Deep Learning is a very active area of research.\nIt is a subfield of machine learning.\nThe cornerstone of deep learning is the neural network."
  },
  {
    "objectID": "qmd/islp10.html#introduction-to-deep-learning",
    "href": "qmd/islp10.html#introduction-to-deep-learning",
    "title": "",
    "section": "",
    "text": "Deep Learning is a very active area of research.\nIt is a subfield of machine learning.\nThe cornerstone of deep learning is the neural network."
  },
  {
    "objectID": "qmd/islp10.html#what-is-data-mining-machine-learning-and-statistical-learning",
    "href": "qmd/islp10.html#what-is-data-mining-machine-learning-and-statistical-learning",
    "title": "",
    "section": "What is Data Mining, Machine Learning and Statistical Learning",
    "text": "What is Data Mining, Machine Learning and Statistical Learning\n\n\n\n\n\n\nData Mining\n\nDiscovering patterns, anomalies, and insights from large datasets.\n\n\n\nMachine Learning\n\nAlgorithms that improve their performance as they are exposed to more data.\nFocuses on prediction and decision-making.\n\n\n\n\n\nStatistical Learning\n\nA subfield of statistics. Focuses on statistical models and methods for finding patterns in data.\nBridges the gap between traditional statistics and machine learning. Focus: Interpretable Models.\n\n\n\n\n\n\n\n\n\ngraph LR\n    A[Data Mining] --&gt; C(Common Ground)\n    B[Machine Learning] --&gt; C\n    D[Statistical Learning] --&gt; C\n    C --&gt; E[Insights & Predictions]"
  },
  {
    "objectID": "qmd/islp10.html#history-and-motivation",
    "href": "qmd/islp10.html#history-and-motivation",
    "title": "",
    "section": "History and Motivation",
    "text": "History and Motivation\n\nNeural networks gained prominence in the late 1980s.\nInitial excitement and hype, followed by a period of stabilization.\nDeclined in popularity due to the rise of SVMs, boosting, and random forests (more “automatic”, less “tinkering”).\nResurfaced after 2010 with the name “Deep Learning” and have achieved many successes in image, video, speech and text modeling.\nNew architectures.\nLarger datasets.\nMore computing power."
  },
  {
    "objectID": "qmd/islp10.html#neural-networks-the-basics",
    "href": "qmd/islp10.html#neural-networks-the-basics",
    "title": "",
    "section": "Neural Networks: The Basics",
    "text": "Neural Networks: The Basics\n\nInspired by the structure of the human brain.\nComposed of interconnected nodes (“neurons”) organized in layers.\nLearn complex, non-linear relationships between inputs and outputs.\nFoundation for most deep learning models."
  },
  {
    "objectID": "qmd/islp10.html#single-layer-neural-networks",
    "href": "qmd/islp10.html#single-layer-neural-networks",
    "title": "",
    "section": "Single Layer Neural Networks",
    "text": "Single Layer Neural Networks\n\n\n\n\n\n\n\nTakes an input vector \\(\\mathbf{X} = (X_1, X_2, \\dots, X_p)\\).\nBuilds a non-linear function \\(f(\\mathbf{X})\\) to predict a response \\(Y\\).\nUses a specific structure:\n\nInput Layer: The features \\(X_1, \\dots, X_p\\).\nHidden Layer: Computes activations \\(A_k = h_k(\\mathbf{X})\\). These are non-linear transformations of linear combinations of the inputs.\nOutput Layer: A linear model that uses the activations as inputs, producing \\(f(\\mathbf{X})\\).\n\nThe functions \\(h_k(\\cdot)\\) are learned during training."
  },
  {
    "objectID": "qmd/islp10.html#single-layer-neural-network-mathematical-formulation",
    "href": "qmd/islp10.html#single-layer-neural-network-mathematical-formulation",
    "title": "",
    "section": "Single Layer Neural Network: Mathematical Formulation",
    "text": "Single Layer Neural Network: Mathematical Formulation\n\nThe neural network model is:\n\\[\nf(\\mathbf{X}) = \\beta_0 + \\sum_{k=1}^K \\beta_k h_k(\\mathbf{X})\n\\]\nwhere\n\\[\nA_k = h_k(\\mathbf{X}) = g\\left(w_{k0} + \\sum_{j=1}^p w_{kj}X_j\\right)\n\\]\n\\(K\\) is the number of hidden units (we choose this).\n\\(g(\\cdot)\\) is the activation function (explained next).\n\\(w_{kj}\\) and \\(\\beta_k\\) are the weights (parameters to be learned).\n\\(A_k\\) are called activations."
  },
  {
    "objectID": "qmd/islp10.html#activation-functions",
    "href": "qmd/islp10.html#activation-functions",
    "title": "",
    "section": "Activation Functions",
    "text": "Activation Functions\n\n\n\n\n\n\n\nIntroduce non-linearity into the model. Without them, the neural network would collapse into a simple linear model.\nCommon choices:\n\nSigmoid: \\(g(z) = \\frac{1}{1 + e^{-z}}\\) (Historically popular, squashes values between 0 and 1.)\nReLU (Rectified Linear Unit): \\(g(z) = \\max(0, z)\\) (Currently very popular, computationally efficient.)"
  },
  {
    "objectID": "qmd/islp10.html#why-non-linearity-matters",
    "href": "qmd/islp10.html#why-non-linearity-matters",
    "title": "",
    "section": "Why Non-linearity Matters",
    "text": "Why Non-linearity Matters\n\n\nCapturing Complexity: Real-world relationships are rarely linear. Activation functions allow the model to learn complex patterns.\nInteractions: Non-linearities enable the model to capture interactions between input variables.\nExample: A simple example with \\(p=2\\) inputs and \\(K=2\\) hidden units, using \\(g(z) = z^2\\), demonstrates how interactions can be modeled:\n\n\n\\[\nf(\\mathbf{X}) =  X_1X_2\n\\]"
  },
  {
    "objectID": "qmd/islp10.html#multilayer-neural-networks",
    "href": "qmd/islp10.html#multilayer-neural-networks",
    "title": "",
    "section": "Multilayer Neural Networks",
    "text": "Multilayer Neural Networks\n\n\n\n\n\n\n\nModern neural networks have multiple hidden layers.\nEach layer builds upon the previous layer’s activations, creating increasingly complex representations.\nFigure 10.4 shows a multilayer network for the MNIST digit classification task.\nMNIST: handwritten digits, 28x28 grayscale images (784 pixels)."
  },
  {
    "objectID": "qmd/islp10.html#mnist-dataset",
    "href": "qmd/islp10.html#mnist-dataset",
    "title": "",
    "section": "MNIST Dataset",
    "text": "MNIST Dataset\n\n\n\n\n\n\n\nGoal: Classify images into their correct digit (0-9).\nOne-hot encoding: Output represented as a vector \\(\\mathbf{Y} = (Y_0, Y_1, \\dots, Y_9)\\) where \\(Y_i = 1\\) if the digit is \\(i\\), and 0 otherwise.\n60,000 training images, 10,000 test images."
  },
  {
    "objectID": "qmd/islp10.html#multilayer-neural-networks-key-differences-from-single-layer",
    "href": "qmd/islp10.html#multilayer-neural-networks-key-differences-from-single-layer",
    "title": "",
    "section": "Multilayer Neural Networks: Key Differences from Single Layer",
    "text": "Multilayer Neural Networks: Key Differences from Single Layer\n\n\nMultiple Hidden Layers: \\(L_1\\) (256 units), \\(L_2\\) (128 units), etc.\nMultiple Outputs: One for each class (e.g., 10 for MNIST).\nMultitask Learning: Can predict multiple different responses simultaneously.\nLoss Function: Tailored to the task (e.g., cross-entropy for classification)."
  },
  {
    "objectID": "qmd/islp10.html#multilayer-neural-networks-mathematical-formulation",
    "href": "qmd/islp10.html#multilayer-neural-networks-mathematical-formulation",
    "title": "",
    "section": "Multilayer Neural Networks: Mathematical Formulation",
    "text": "Multilayer Neural Networks: Mathematical Formulation\n\nFirst hidden layer (same as before, but with superscript (1)):\n\\[\nA_k^{(1)} = h_k^{(1)}(\\mathbf{X}) = g\\left(w_{k0}^{(1)} + \\sum_{j=1}^p w_{kj}^{(1)} X_j\\right)\n\\]\nSecond hidden layer (takes activations from the first layer as input):\n\\[\nA_l^{(2)} = h_l^{(2)}(\\mathbf{X}) = g\\left(w_{l0}^{(2)} + \\sum_{k=1}^{K_1} w_{lk}^{(2)} A_k^{(1)}\\right)\n\\]\nOutput layer (for multi-class classification, uses softmax):\n\\[\nf_m(\\mathbf{X}) = \\Pr(Y = m | \\mathbf{X}) = \\frac{e^{Z_m}}{\\sum_{m'=0}^9 e^{Z_{m'}}}\n\\]\nwhere \\(Z_m = \\beta_{m0} + \\sum_{l=1}^{K_2} \\beta_{ml} A_l^{(2)}\\)."
  },
  {
    "objectID": "qmd/islp10.html#loss-function-and-optimization",
    "href": "qmd/islp10.html#loss-function-and-optimization",
    "title": "",
    "section": "Loss Function and Optimization",
    "text": "Loss Function and Optimization\n\nCross-entropy loss (for multi-class classification):\n\n\\[\n-\\sum_{i=1}^n \\sum_{m=0}^9 y_{im} \\log(f_m(\\mathbf{x}_i))\n\\]\n\nGoal: Find the weights that minimize this loss function.\nOptimization is done using gradient descent (explained later).\nWeights refers to all trainable parameters, including coefficients and bias."
  },
  {
    "objectID": "qmd/islp10.html#comparison-with-linear-models-mnist",
    "href": "qmd/islp10.html#comparison-with-linear-models-mnist",
    "title": "",
    "section": "Comparison with Linear Models (MNIST)",
    "text": "Comparison with Linear Models (MNIST)\n\n\n\nMethod\nTest Error\n\n\n\n\nNeural Network + Ridge\n2.3%\n\n\nNeural Network + Dropout\n1.8%\n\n\nMultinomial Logistic Regression\n7.2%\n\n\nLinear Discriminant Analysis\n12.7%\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNeural networks significantly outperform linear methods on this task."
  },
  {
    "objectID": "qmd/islp10.html#convolutional-neural-networks-cnns",
    "href": "qmd/islp10.html#convolutional-neural-networks-cnns",
    "title": "",
    "section": "Convolutional Neural Networks (CNNs)",
    "text": "Convolutional Neural Networks (CNNs)\n\nSpecialized neural networks for image classification (and other tasks with spatial structure).\nInspired by how humans process visual information.\nKey Idea: Learn local features and combine them to recognize global patterns."
  },
  {
    "objectID": "qmd/islp10.html#cnn-architecture-key-components",
    "href": "qmd/islp10.html#cnn-architecture-key-components",
    "title": "",
    "section": "CNN Architecture: Key Components",
    "text": "CNN Architecture: Key Components\n\n\nConvolutional Layers:\n\nApply convolution filters (small templates) to the input.\nDetect local features (edges, textures, etc.).\nLearned filters, not predefined.\nWeight sharing: the same filter is applied across the entire image.\n\nPooling Layers:\n\nDownsample the feature maps.\nReduce dimensionality and provide some translation invariance.\nMax pooling: Takes the maximum value within a region.\n\nFlatten Layers: convert multi-dimension feature maps to vector\nFully Connected Layers: Standard neural network layers (like those described earlier)."
  },
  {
    "objectID": "qmd/islp10.html#cnn-example-tiger-classification",
    "href": "qmd/islp10.html#cnn-example-tiger-classification",
    "title": "",
    "section": "CNN Example: Tiger Classification",
    "text": "CNN Example: Tiger Classification\n\n\nInput: Image of a tiger.\nConvolutional layers identify low-level features (edges, stripes).\nPooling layers downsample and provide invariance.\nHigher layers combine features (eyes, ears, etc.).\nOutput: Probability of the image being a tiger."
  },
  {
    "objectID": "qmd/islp10.html#convolution-operation-detailed-explanation",
    "href": "qmd/islp10.html#convolution-operation-detailed-explanation",
    "title": "",
    "section": "Convolution Operation: Detailed Explanation",
    "text": "Convolution Operation: Detailed Explanation\n\n\n\n\n\n\n\nA convolution filter is a small matrix (e.g., 3x3).\nIt’s “slid” across the input image.\nAt each position, element-wise multiplication and summation are performed.\nThe result is a single value in the convolved image.\nDifferent filters detect different features."
  },
  {
    "objectID": "qmd/islp10.html#convolution-operation-example",
    "href": "qmd/islp10.html#convolution-operation-example",
    "title": "",
    "section": "Convolution Operation: Example",
    "text": "Convolution Operation: Example\n\nInput Image: \\[\nOriginal Image = \\begin{bmatrix}\na & b & c\\\\\nd & e & f\\\\\ng & h & i\\\\\nj & k & l\n\\end{bmatrix}\n\\]\nFilter: \\[\nConvolutionFilter = \\begin{bmatrix}\n\\alpha & \\beta\\\\\n\\gamma & \\delta\n\\end{bmatrix}\n\\]\nConvolved Image: \\[\nConvolved Image = \\begin{bmatrix}\naa + b\\beta + d\\gamma + e\\delta & ba + c\\beta + e\\gamma + f\\delta\\\\\nda + e\\beta + g\\gamma + h\\delta & ea + f\\beta + h\\gamma + i\\delta\\\\\nga + h\\beta + j\\gamma + k\\delta & ha + i\\beta + k\\gamma + l\\delta\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "qmd/islp10.html#convolutional-layer-details",
    "href": "qmd/islp10.html#convolutional-layer-details",
    "title": "",
    "section": "Convolutional Layer: Details",
    "text": "Convolutional Layer: Details\n\nMultiple Channels: Color images have 3 channels (Red, Green, Blue). Convolution filters also have multiple channels.\nMultiple Filters: A convolutional layer uses many filters to extract different features.\nReLU Activation: Typically applied after convolution.\nDetector Layer: The combination of convolution and ReLU is sometimes called a detector layer."
  },
  {
    "objectID": "qmd/islp10.html#pooling-layer",
    "href": "qmd/islp10.html#pooling-layer",
    "title": "",
    "section": "Pooling Layer",
    "text": "Pooling Layer\n\nPurpose: Reduce the spatial dimensions of the feature maps.\nMax Pooling: Takes the maximum value within a non-overlapping region (e.g., 2x2).\nBenefits:\n\nReduces computation.\nProvides some translation invariance."
  },
  {
    "objectID": "qmd/islp10.html#example-of-max-pooling",
    "href": "qmd/islp10.html#example-of-max-pooling",
    "title": "",
    "section": "Example of Max Pooling",
    "text": "Example of Max Pooling\nInput:\n\\[\n\\begin{bmatrix}\n1 & 2 & 5 & 3 \\\\\n3 & 0 & 1 & 2 \\\\\n2 & 1 & 3 & 4 \\\\\n1 & 1 & 2 & 0 \\\\\n\\end{bmatrix}\n\\]\nOutput (after max pooling with a 2x2 window):\n\\[\n\\begin{bmatrix}\n3 & 5 \\\\\n2 & 4\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "qmd/islp10.html#cnn-architecture-putting-it-together",
    "href": "qmd/islp10.html#cnn-architecture-putting-it-together",
    "title": "",
    "section": "CNN Architecture: Putting it Together",
    "text": "CNN Architecture: Putting it Together\n\n\nSequence of convolutional and pooling layers.\nConvolutional layers extract features.\nPooling layers downsample.\nFlatten layer: Converts the 3D feature maps into a 1D vector.\nFully connected layers: Perform classification.\nSoftmax output layer: Produces probabilities for each class."
  },
  {
    "objectID": "qmd/islp10.html#data-augmentation",
    "href": "qmd/islp10.html#data-augmentation",
    "title": "",
    "section": "Data Augmentation",
    "text": "Data Augmentation\n\n\n\n\n\n\n\nPurpose: Artificially increase the size of the training set.\nMethod: Apply random transformations to training images (rotation, zoom, shift, flip, etc.).\nBenefits:\n\nReduces overfitting.\nImproves generalization.\nActs as a form of regularization."
  },
  {
    "objectID": "qmd/islp10.html#pretrained-classifiers",
    "href": "qmd/islp10.html#pretrained-classifiers",
    "title": "",
    "section": "Pretrained Classifiers",
    "text": "Pretrained Classifiers\n\n\n\n\n\n\n\nLeverage models trained on massive datasets (e.g., ImageNet).\nExample: ResNet50.\nWeight Freezing: Use the pretrained convolutional layers as feature extractors and train only the final layers."
  },
  {
    "objectID": "qmd/islp10.html#pretrained-classifiers-table-10.10",
    "href": "qmd/islp10.html#pretrained-classifiers-table-10.10",
    "title": "",
    "section": "Pretrained Classifiers: Table 10.10",
    "text": "Pretrained Classifiers: Table 10.10\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nTrue Label\nPrediction 1\nProb 1\nPrediction 2\nProb 2\nPrediction 3\nProb 3\n\n\n\n\n(Flamingo)\nFlamingo\nFlamingo\n0.83\nSpoonbill\n0.17\nWhite stork\n0.00\n\n\n(Cooper’s Hawk)\nCooper’s Hawk\nKite\n0.60\nGreat grey owl\n0.09\nNail\n0.12\n\n\n(Cooper’s Hawk)\nCooper’s Hawk\nFountain\n0.35\nnail\n0.12\nhook\n0.07\n\n\n(Lhasa Apso)\nLhasa Apso\nTibetan terrier\n0.56\nLhasa\n0.32\ncocker spaniel\n0.03\n\n\n(Cat)\nCat\nOld English sheepdog\n0.82\nShih-Tzu\n0.04\nPersian cat\n0.04\n\n\n(Cape weaver)\nCape weaver\njacamar\n0.28\nmacaw\n0.12\nrobin\n0.12"
  },
  {
    "objectID": "qmd/islp10.html#document-classification",
    "href": "qmd/islp10.html#document-classification",
    "title": "",
    "section": "Document Classification",
    "text": "Document Classification\n\nAnother important application of deep learning.\nGoal: Predict attributes of documents (e.g., sentiment, topic).\nFeaturization: Converting text into numerical representations.\nExample: Sentiment analysis of IMDb movie reviews (positive or negative)."
  },
  {
    "objectID": "qmd/islp10.html#bag-of-words-model",
    "href": "qmd/islp10.html#bag-of-words-model",
    "title": "",
    "section": "Bag-of-Words Model",
    "text": "Bag-of-Words Model\n\nSimplest featurization method.\nRepresents a document as a vector indicating the presence/absence of words from a dictionary.\nIgnores word order and context.\nSparse representation (most entries are zero).\nBag-of-n-grams considers sequences of n words."
  },
  {
    "objectID": "qmd/islp10.html#recurrent-neural-networks-rnns",
    "href": "qmd/islp10.html#recurrent-neural-networks-rnns",
    "title": "",
    "section": "Recurrent Neural Networks (RNNs)",
    "text": "Recurrent Neural Networks (RNNs)\n\nDesigned for sequential data (text, time series, etc.).\nKey Idea: Process the input sequence one element at a time, maintaining a “hidden state” that captures information from previous elements.\n“Unrolling” the RNN reveals its sequential nature."
  },
  {
    "objectID": "qmd/islp10.html#rnn-architecture",
    "href": "qmd/islp10.html#rnn-architecture",
    "title": "",
    "section": "RNN Architecture",
    "text": "RNN Architecture\n\n\nInput: Sequence of vectors \\(\\mathbf{X} = \\{\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_L\\}\\).\nHidden state: \\(A_l\\) (captures information from previous steps).\nOutput: \\(O_l\\) (often only the final output \\(O_L\\) is used).\nWeight sharing: The same weights (\\(\\mathbf{W, U, B}\\)) are used at each step."
  },
  {
    "objectID": "qmd/islp10.html#rnn-mathematical-formulation",
    "href": "qmd/islp10.html#rnn-mathematical-formulation",
    "title": "",
    "section": "RNN: Mathematical Formulation",
    "text": "RNN: Mathematical Formulation\n\nHidden state update:\n\\[\n\\mathbf{A}_{lk} = g\\left(w_{k0} + \\sum_{j=1}^p w_{kj} \\mathbf{X}_{lj} + \\sum_{s=1}^K u_{ks} \\mathbf{A}_{l-1,s}\\right)\n\\]\nOutput:\n\\[\n\\mathbf{O}_l = \\beta_0 + \\sum_{k=1}^K \\beta_k \\mathbf{A}_{lk}\n\\]\n\\(g(\\cdot)\\) is the activation function (e.g., ReLU).\n\\(\\mathbf{W, U, B}\\) are the shared weight matrices."
  },
  {
    "objectID": "qmd/islp10.html#rnns-for-document-classification",
    "href": "qmd/islp10.html#rnns-for-document-classification",
    "title": "",
    "section": "RNNs for Document Classification",
    "text": "RNNs for Document Classification\n\nInstead of bag-of-words, use the sequence of words.\nWord Embeddings: Represent words as dense, low-dimensional vectors (e.g., using word2vec or GloVe).\nEmbedding Layer: Maps one-hot encoded words to their embedding vectors.\nProcess the sequence of word embeddings using the RNN."
  },
  {
    "objectID": "qmd/islp10.html#word-embeddings-example",
    "href": "qmd/islp10.html#word-embeddings-example",
    "title": "",
    "section": "Word Embeddings: Example",
    "text": "Word Embeddings: Example\n - One hot encoding vector length is the vocabulary size. - Word embeddings provide dense representation."
  },
  {
    "objectID": "qmd/islp10.html#rnns-for-time-series-forecasting",
    "href": "qmd/islp10.html#rnns-for-time-series-forecasting",
    "title": "",
    "section": "RNNs for Time Series Forecasting",
    "text": "RNNs for Time Series Forecasting\n\nExample: Predicting stock market trading volume.\nInput: Sequence of past values (volume, return, volatility).\nOutput: Predicted volume for the next day.\nAutocorrelation: Values in time series are often correlated with past values."
  },
  {
    "objectID": "qmd/islp10.html#time-series-data-example",
    "href": "qmd/islp10.html#time-series-data-example",
    "title": "",
    "section": "Time Series Data: Example",
    "text": "Time Series Data: Example\n\n\nDaily trading statistics from the NYSE.\nLog trading volume, Dow Jones return, log volatility."
  },
  {
    "objectID": "qmd/islp10.html#autocorrelation-function",
    "href": "qmd/islp10.html#autocorrelation-function",
    "title": "",
    "section": "Autocorrelation Function",
    "text": "Autocorrelation Function\n\n\nMeasures the correlation between a time series and its lagged values.\nShows how strongly related values are at different time lags."
  },
  {
    "objectID": "qmd/islp10.html#rnn-forecaster-setup",
    "href": "qmd/islp10.html#rnn-forecaster-setup",
    "title": "",
    "section": "RNN Forecaster: Setup",
    "text": "RNN Forecaster: Setup\n\nInput Sequence: \\(L\\) past observations (e.g., \\(L=5\\) days).\n\n\\[\n\\mathbf{X}_1 = \\begin{pmatrix} v_{t-L} \\\\ r_{t-L} \\\\ z_{t-L} \\end{pmatrix}, \\mathbf{X}_2 = \\begin{pmatrix} v_{t-L+1} \\\\ r_{t-L+1} \\\\ z_{t-L+1} \\end{pmatrix}, \\dots, \\mathbf{X}_L = \\begin{pmatrix} v_{t-1} \\\\ r_{t-1} \\\\ z_{t-1} \\end{pmatrix}\n\\]\n\nOutput: \\(Y = v_t\\) (trading volume on day \\(t\\)).\nCreate many such (X, Y) pairs from the historical data."
  },
  {
    "objectID": "qmd/islp10.html#rnn-forecasting-results",
    "href": "qmd/islp10.html#rnn-forecasting-results",
    "title": "",
    "section": "RNN Forecasting Results",
    "text": "RNN Forecasting Results\n\n\nRNN achieves \\(R^2 = 0.42\\) on the test data.\nOutperforms a simple baseline (using yesterday’s volume)."
  },
  {
    "objectID": "qmd/islp10.html#autoregression-ar-model",
    "href": "qmd/islp10.html#autoregression-ar-model",
    "title": "",
    "section": "Autoregression (AR) Model",
    "text": "Autoregression (AR) Model\n\nA traditional time series model.\nPredicts the current value based on a linear combination of past values.\nAR(L) model:\n\\[\n\\hat{v}_t = \\beta_0 + \\beta_1 v_{t-1} + \\beta_2 v_{t-2} + \\dots + \\beta_L v_{t-L}\n\\]\nCan include lagged values of other variables (e.g., return, volatility).\nRNN can be seen a non-linear extension of autoregression."
  },
  {
    "objectID": "qmd/islp10.html#long-short-term-memory-lstm",
    "href": "qmd/islp10.html#long-short-term-memory-lstm",
    "title": "",
    "section": "Long Short-Term Memory (LSTM)",
    "text": "Long Short-Term Memory (LSTM)\n\nA more elaborate type of RNN.\nAddresses the “vanishing gradient” problem in long sequences.\nMaintains two hidden states: short-term and long-term memory.\nOften improves performance compared to basic RNNs."
  },
  {
    "objectID": "qmd/islp10.html#fitting-neural-networks-overview",
    "href": "qmd/islp10.html#fitting-neural-networks-overview",
    "title": "",
    "section": "Fitting Neural Networks: Overview",
    "text": "Fitting Neural Networks: Overview\n\nComplex optimization problem (non-convex).\nMultiple local minima.\nKey Techniques:\n\nGradient Descent: Iteratively adjust the weights to minimize the loss function.\nBackpropagation: Efficiently compute the gradient of the loss function.\nRegularization: Prevent overfitting (ridge, lasso, dropout).\nStochastic Gradient Descent (SGD): Use small batches of data to update the weights.\nEarly Stopping: Stop training before the model begins over fitting."
  },
  {
    "objectID": "qmd/islp10.html#gradient-descent-illustration",
    "href": "qmd/islp10.html#gradient-descent-illustration",
    "title": "",
    "section": "Gradient Descent: Illustration",
    "text": "Gradient Descent: Illustration"
  },
  {
    "objectID": "qmd/islp10.html#gradient-descent",
    "href": "qmd/islp10.html#gradient-descent",
    "title": "",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nIterative optimization algorithm.\nGoal: Find the values of the parameters (\\(\\theta\\)) that minimize the loss function \\(R(\\theta)\\).\nSteps:\n\nInitialize \\(\\theta\\) (often randomly).\nRepeatedly update \\(\\theta\\) by moving in the opposite direction of the gradient:\n\\[\n\\theta^{(m+1)} \\leftarrow \\theta^{(m)} - \\rho \\nabla R(\\theta^{(m)})\n\\]\n\\(\\rho\\) is the learning rate (controls the step size)."
  },
  {
    "objectID": "qmd/islp10.html#backpropagation",
    "href": "qmd/islp10.html#backpropagation",
    "title": "",
    "section": "Backpropagation",
    "text": "Backpropagation\n\nEfficiently computes the gradient of the loss function with respect to the weights.\nUses the chain rule of calculus.\n“Propagates” the error signal backward through the network."
  },
  {
    "objectID": "qmd/islp10.html#regularization",
    "href": "qmd/islp10.html#regularization",
    "title": "",
    "section": "Regularization",
    "text": "Regularization\n\nPurpose: Prevent overfitting.\nMethods:\n\nRidge/Lasso: Add a penalty term to the loss function.\nDropout: Randomly “drop out” units during training.\nEarly Stopping: Monitor performance on a validation set and stop training when it starts to worsen."
  },
  {
    "objectID": "qmd/islp10.html#stochastic-gradient-descent-sgd",
    "href": "qmd/islp10.html#stochastic-gradient-descent-sgd",
    "title": "",
    "section": "Stochastic Gradient Descent (SGD)",
    "text": "Stochastic Gradient Descent (SGD)\n\nUse small batches of data (minibatches) to update the weights.\nFaster than using the entire dataset for each update.\nIntroduces randomness, which can help escape local minima.\nThe standard approach for training deep learning models."
  },
  {
    "objectID": "qmd/islp10.html#training-and-validation-errors",
    "href": "qmd/islp10.html#training-and-validation-errors",
    "title": "",
    "section": "Training and Validation Errors",
    "text": "Training and Validation Errors\n\n\nMonitor both training and validation error during training.\nEarly stopping: Stop training when validation error starts to increase."
  },
  {
    "objectID": "qmd/islp10.html#dropout-learning",
    "href": "qmd/islp10.html#dropout-learning",
    "title": "",
    "section": "Dropout Learning",
    "text": "Dropout Learning"
  },
  {
    "objectID": "qmd/islp10.html#network-tuning",
    "href": "qmd/islp10.html#network-tuning",
    "title": "",
    "section": "Network Tuning",
    "text": "Network Tuning\n\nChoosing the right architecture and hyperparameters is important.\nKey Considerations:\n\nNumber of hidden layers.\nNumber of units per layer.\nRegularization parameters (dropout rate, ridge/lasso strength).\nLearning rate, batch size, number of epochs.\n\nOften involves trial and error, and can be time-consuming."
  },
  {
    "objectID": "qmd/islp10.html#interpolation-and-double-descent",
    "href": "qmd/islp10.html#interpolation-and-double-descent",
    "title": "",
    "section": "Interpolation and Double Descent",
    "text": "Interpolation and Double Descent\n\n\nInterpolation: Fitting a model that perfectly fits the training data (zero training error).\nDouble Descent: A phenomenon where test error decreases again after increasing as model complexity increases beyond the interpolation threshold.\nObserved in some deep learning models.\nDoes not contradict the bias-variance tradeoff."
  },
  {
    "objectID": "qmd/islp10.html#double-descent-explanation",
    "href": "qmd/islp10.html#double-descent-explanation",
    "title": "",
    "section": "Double Descent: Explanation",
    "text": "Double Descent: Explanation\n\nThe phenomenon that the test error has U shape at first, then descent again after the interpolation point.\nIt does not conflict with the bias-variance trade-off.\nRegularization methods can still get great results without interpolating."
  },
  {
    "objectID": "qmd/islp10.html#when-to-use-deep-learning",
    "href": "qmd/islp10.html#when-to-use-deep-learning",
    "title": "",
    "section": "When to Use Deep Learning",
    "text": "When to Use Deep Learning\n\nLarge Datasets: Deep learning excels when you have a lot of data.\nComplex Relationships: When the relationship between inputs and outputs is highly non-linear.\nFeature Engineering is Difficult: Deep learning can automatically learn features.\nInterpretability is Less Important: Deep learning models are often “black boxes.”\nComputational Resources: Training deep learning models can be computationally expensive."
  },
  {
    "objectID": "qmd/islp10.html#summary",
    "href": "qmd/islp10.html#summary",
    "title": "",
    "section": "Summary",
    "text": "Summary\n\nDeep learning is a powerful set of techniques based on neural networks.\nCNNs are specialized for images, RNNs for sequential data.\nFitting involves complex optimization, but software tools simplify the process.\nRegularization is crucial to prevent overfitting.\nDeep learning excels with large datasets and complex relationships.\nConsider simpler models if they perform well and are more interpretable."
  },
  {
    "objectID": "qmd/islp10.html#thoughts-and-discussion",
    "href": "qmd/islp10.html#thoughts-and-discussion",
    "title": "",
    "section": "Thoughts and Discussion",
    "text": "Thoughts and Discussion\n\nWhat are the ethical implications of using deep learning in various applications (e.g., facial recognition, loan applications)?\nHow can we make deep learning models more interpretable?\nWhat are the limitations of deep learning, and when might other methods be more appropriate?\nHow might deep learning evolve in the future? What new architectures or applications might emerge?\nHow to determine model complexity? What is the impact of using a model that is too simple or too complex?\nWhat steps can be taken to ensure data quality and address biases in datasets used for training deep learning models?"
  }
]