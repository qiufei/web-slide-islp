[
  {
    "objectID": "qmd/islp12.html",
    "href": "qmd/islp12.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Statistical learning refers to a set of tools and techniques used to understand and extract insights from data. It‚Äôs a broad field encompassing both supervised and unsupervised learning approaches. The goal is to build models that can either predict an outcome (supervised) or discover patterns (unsupervised) in data. üìä\n\n\n\n\n\n\nKey Idea: Statistical learning provides a framework for using data to gain knowledge and make predictions."
  },
  {
    "objectID": "qmd/islp12.html#introduction-what-is-statistical-learning",
    "href": "qmd/islp12.html#introduction-what-is-statistical-learning",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Statistical learning refers to a set of tools and techniques used to understand and extract insights from data. It‚Äôs a broad field encompassing both supervised and unsupervised learning approaches. The goal is to build models that can either predict an outcome (supervised) or discover patterns (unsupervised) in data. üìä\n\n\n\n\n\n\nKey Idea: Statistical learning provides a framework for using data to gain knowledge and make predictions."
  },
  {
    "objectID": "qmd/islp12.html#supervised-vs.-unsupervised-learning-overview",
    "href": "qmd/islp12.html#supervised-vs.-unsupervised-learning-overview",
    "title": "Unsupervised Learning",
    "section": "Supervised vs.¬†Unsupervised Learning: Overview",
    "text": "Supervised vs.¬†Unsupervised Learning: Overview\nLet‚Äôs start by understanding the fundamental difference between supervised and unsupervised learning. Think of it like learning with a teacher versus learning on your own.\n\nSupervised Learning: We have a ‚Äúteacher‚Äù (the response variable, Y) guiding the learning process. üë®‚Äçüè´ The goal is to learn a function that maps inputs to outputs.\nUnsupervised Learning: We‚Äôre exploring the data ‚Äúwithout a teacher‚Äù to discover hidden patterns. üïµÔ∏è‚Äç‚ôÄÔ∏è There‚Äôs no explicit output variable to predict."
  },
  {
    "objectID": "qmd/islp12.html#supervised-learning",
    "href": "qmd/islp12.html#supervised-learning",
    "title": "Unsupervised Learning",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nSupervised Learning: We have a set of features (X1, X2, ‚Ä¶, Xp) and a response variable (Y). The goal is to predict Y using the Xs. We ‚Äúteach‚Äù the algorithm by providing examples of inputs and their corresponding outputs.\nExamples: Linear regression (predicting house prices), logistic regression (predicting customer churn), Support Vector Machine (SVM) (image classification).\n\n\n\n\n\n\ngraph LR\n    A[Features (X)] --&gt; B(Model);\n    C[Response (Y)] --&gt; B;\n    B --&gt; D[Predictions];\n    style A fill:#ccf,stroke:#333,stroke-width:2px\n    style C fill:#f9f,stroke:#333,stroke-width:2px\n    style B fill:#ccf,stroke:#333,stroke-width:2px\n    style D fill:#ccf,stroke:#333,stroke-width:2px\n\n\n\n\n\n\n\n\n\n\n\n\nThe model learns the relationship between features (X) and the response (Y)."
  },
  {
    "objectID": "qmd/islp12.html#unsupervised-learning",
    "href": "qmd/islp12.html#unsupervised-learning",
    "title": "Unsupervised Learning",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nUnsupervised Learning: We only have features (X1, X2, ‚Ä¶, Xp), without any response variable Y. No ‚Äúteaching‚Äù or ‚Äúsupervision‚Äù ‚Äì the algorithm explores the data on its own.\nGoal: Discover interesting patterns and structure in the data; find relationships among the features and/or observations.\nExamples: Principal Component Analysis (PCA) (dimensionality reduction), Clustering (finding groups of similar customers).\n\n\n\n\n\n\ngraph LR\n    A[Features (X)] --&gt; B(Model);\n    B --&gt; C[Patterns & Structure];\nstyle A fill:#f9f,stroke:#333,stroke-width:2px\nstyle B fill:#ccf,stroke:#333,stroke-width:2px\nstyle C fill:#ccf,stroke:#333,stroke-width:2px\n\n\n\n\n\n\n\n\n\n\n\n\nThe model identifies inherent structure within the features (X)."
  },
  {
    "objectID": "qmd/islp12.html#what-is-unsupervised-learning-detailed",
    "href": "qmd/islp12.html#what-is-unsupervised-learning-detailed",
    "title": "Unsupervised Learning",
    "section": "What is Unsupervised Learning? (Detailed)",
    "text": "What is Unsupervised Learning? (Detailed)\nUnsupervised learning is a collection of statistical methods used when we only have input data (features) and no corresponding output (response variable). Because there‚Äôs no ‚Äúcorrect answer‚Äù to guide the analysis, we call it ‚Äúunsupervised.‚Äù We‚Äôre essentially detectives, exploring the data to uncover hidden relationships and structures. üîç It‚Äôs about learning the underlying structure of the data itself."
  },
  {
    "objectID": "qmd/islp12.html#key-goals-of-unsupervised-learning",
    "href": "qmd/islp12.html#key-goals-of-unsupervised-learning",
    "title": "Unsupervised Learning",
    "section": "Key Goals of Unsupervised Learning",
    "text": "Key Goals of Unsupervised Learning\nUnsupervised learning serves several important purposes:\n\nData Visualization: Finding ways to represent complex, high-dimensional data in a visually intuitive manner, often by reducing its dimensionality. Think of it like creating a map of your data, making it easier to navigate. üó∫Ô∏è\nDiscover Subgroups: Identifying clusters or groups within the data. This could be groups of customers with similar buying habits, or groups of genes with related functions. üßë‚Äçü§ù‚Äçüßë This helps us understand the heterogeneity within the data.\nData Pre-processing: Preparing data for supervised learning techniques. For example, we might use unsupervised learning to reduce the number of features or to create new, more informative features before applying a supervised learning algorithm. ‚öôÔ∏è This can improve the performance and efficiency of supervised models."
  },
  {
    "objectID": "qmd/islp12.html#types-of-unsupervised-learning",
    "href": "qmd/islp12.html#types-of-unsupervised-learning",
    "title": "Unsupervised Learning",
    "section": "Types of Unsupervised Learning",
    "text": "Types of Unsupervised Learning\nWe‚Äôll focus on two primary types of unsupervised learning:\n\nPrincipal Components Analysis (PCA): Primarily used for data visualization and dimensionality reduction, making complex data easier to understand by finding the most important ‚Äúdirections‚Äù in the data.\nClustering: Used to discover unknown subgroups or clusters within a dataset, grouping similar observations together."
  },
  {
    "objectID": "qmd/islp12.html#supervised-vs.-unsupervised-learning-comparison-table",
    "href": "qmd/islp12.html#supervised-vs.-unsupervised-learning-comparison-table",
    "title": "Unsupervised Learning",
    "section": "Supervised vs.¬†Unsupervised Learning: Comparison Table",
    "text": "Supervised vs.¬†Unsupervised Learning: Comparison Table\nLet‚Äôs solidify our understanding with a side-by-side comparison:\n\n\n\n\n\n\n\n\nFeature\nSupervised Learning\nUnsupervised Learning\n\n\n\n\nGoal\nPredict a response variable (Y) based on input features (X).\nDiscover patterns, structure, and relationships within the data (X).\n\n\nData\nFeatures (X) and a corresponding response variable (Y).\nFeatures (X) only; no response variable.\n\n\nEvaluation\nClear metrics (e.g., accuracy, R-squared, precision, recall) to assess performance.\nMore subjective, harder to evaluate; often relies on visual inspection or domain knowledge.\n\n\nExamples\nRegression, classification, support vector machines.\nPCA, clustering, association rule mining.\n\n\n‚ÄúCorrect Answer‚Äù\nYes (the response variable provides the ‚Äúground truth‚Äù).\nNo (no response variable, so no ‚Äúground truth‚Äù)."
  },
  {
    "objectID": "qmd/islp12.html#supervised-vs.-unsupervised-a-note-of-caution",
    "href": "qmd/islp12.html#supervised-vs.-unsupervised-a-note-of-caution",
    "title": "Unsupervised Learning",
    "section": "Supervised vs.¬†Unsupervised: A Note of Caution",
    "text": "Supervised vs.¬†Unsupervised: A Note of Caution\n\n\n\n\n\n\nUnsupervised learning is often more challenging than supervised learning. Why? Because there‚Äôs no easy way to check our results! We don‚Äôt have a ‚Äúground truth‚Äù (like a response variable) to compare against. This means the process is more exploratory and subjective, requiring careful interpretation and domain expertise. ü§î"
  },
  {
    "objectID": "qmd/islp12.html#applications-of-unsupervised-learning",
    "href": "qmd/islp12.html#applications-of-unsupervised-learning",
    "title": "Unsupervised Learning",
    "section": "Applications of Unsupervised Learning",
    "text": "Applications of Unsupervised Learning\nUnsupervised learning has become incredibly valuable across numerous fields:\n\nGenomics: Researchers studying cancer might use unsupervised learning to analyze gene expression data. This can help identify different subtypes of cancer, leading to more targeted treatments. üß¨\nE-commerce: Online retailers use unsupervised learning to group customers with similar browsing and purchasing patterns. This allows for personalized product recommendations, increasing sales and customer satisfaction. üõçÔ∏è\nSearch Engines: Unsupervised learning can be used to cluster users based on their click histories, leading to more relevant search results. üîé\nMarketing: Identifying market segments (groups of customers with shared characteristics) for targeted advertising campaigns. üéØ"
  },
  {
    "objectID": "qmd/islp12.html#applications-of-unsupervised-learning-cont.",
    "href": "qmd/islp12.html#applications-of-unsupervised-learning-cont.",
    "title": "Unsupervised Learning",
    "section": "Applications of Unsupervised Learning (Cont.)",
    "text": "Applications of Unsupervised Learning (Cont.)\nThese are just a few examples. The power of unsupervised learning lies in its ability to extract insights from data without needing a predefined outcome, making it a versatile tool for discovery in various domains. The ability to uncover hidden patterns is what makes it so powerful."
  },
  {
    "objectID": "qmd/islp12.html#diving-into-principal-component-analysis-pca",
    "href": "qmd/islp12.html#diving-into-principal-component-analysis-pca",
    "title": "Unsupervised Learning",
    "section": "Diving into Principal Component Analysis (PCA)",
    "text": "Diving into Principal Component Analysis (PCA)\nNow, let‚Äôs explore our first unsupervised learning technique: Principal Component Analysis (PCA). PCA is like taking a high-dimensional dataset (many features) and finding the best way to ‚Äúflatten‚Äù it while preserving as much of its original information as possible. It‚Äôs a dimensionality reduction technique that finds the most important ‚Äúdirections‚Äù in your data."
  },
  {
    "objectID": "qmd/islp12.html#what-does-pca-do",
    "href": "qmd/islp12.html#what-does-pca-do",
    "title": "Unsupervised Learning",
    "section": "What does PCA do?",
    "text": "What does PCA do?\n\nDimensionality Reduction: PCA simplifies data by finding a smaller set of representative variables, called principal components. These components capture most of the variability in the original data.\nData Visualization: It allows us to visualize high-dimensional data in lower dimensions (e.g., 2D or 3D plots), making it easier to spot patterns and relationships that would be hidden in higher dimensions.\nUnsupervised: PCA only uses the features (X) and doesn‚Äôt rely on any response variable (Y). It focuses solely on the relationships between the features.\nFeature Space Directions: Identifies the directions in the feature space along which the original data varies most. These are the ‚Äúprincipal components‚Äù.\nData Pre-processing: PCA can create new, uncorrelated features that can be used in subsequent supervised learning models. This can improve model performance and reduce overfitting."
  },
  {
    "objectID": "qmd/islp12.html#what-are-principal-components-explained",
    "href": "qmd/islp12.html#what-are-principal-components-explained",
    "title": "Unsupervised Learning",
    "section": "What are Principal Components? (Explained)",
    "text": "What are Principal Components? (Explained)\nImagine you have a dataset with many variables (features). PCA helps you find new variables, called principal components, which are linear combinations of the original features. It‚Äôs like creating new ‚Äúsummary‚Äù variables from the original ones, but in a way that maximizes the captured variance."
  },
  {
    "objectID": "qmd/islp12.html#understanding-the-first-principal-component-z1",
    "href": "qmd/islp12.html#understanding-the-first-principal-component-z1",
    "title": "Unsupervised Learning",
    "section": "Understanding the First Principal Component (Z1)",
    "text": "Understanding the First Principal Component (Z1)\n\nFirst Principal Component (Z1): This is the most important principal component. It‚Äôs the normalized linear combination of the original features that captures the largest variance in the data. It represents the direction of greatest variability in the data cloud.\n\nNormalized: The sum of the squared coefficients (loadings) equals 1. This ensures that the variance isn‚Äôt artificially inflated by using large coefficients. It‚Äôs a mathematical constraint for uniqueness.\nLoadings: The coefficients (œÜ) in the linear combination are called loadings. They tell us how much each original feature contributes to the principal component. A large loading (in absolute value) means the feature has a strong influence on that component."
  },
  {
    "objectID": "qmd/islp12.html#subsequent-principal-components",
    "href": "qmd/islp12.html#subsequent-principal-components",
    "title": "Unsupervised Learning",
    "section": "Subsequent Principal Components",
    "text": "Subsequent Principal Components\n\nSubsequent Principal Components: These are also linear combinations of the original features, but they capture the most remaining variance, with the constraint that they are uncorrelated (orthogonal) to the previous components. Each component captures a different ‚Äúdirection‚Äù of variability in the data, and they are all perpendicular to each other.\n\n\n\n\n\n\n\nFormula for the first principal component: \\[Z_1 = \\phi_{11}X_1 + \\phi_{21}X_2 + ... + \\phi_{p1}X_p\\] where \\(\\sum_{j=1}^{p} \\phi_{j1}^2 = 1\\) (normalization constraint). The values \\(\\phi_{11},...,\\phi_{p1}\\) are the loadings of the first principal component."
  },
  {
    "objectID": "qmd/islp12.html#geometric-interpretation-of-pca",
    "href": "qmd/islp12.html#geometric-interpretation-of-pca",
    "title": "Unsupervised Learning",
    "section": "Geometric Interpretation of PCA",
    "text": "Geometric Interpretation of PCA\nThe principal component loading vectors define directions in the feature space.\n\nFirst Principal Component: Represents the direction along which the data points vary the most. It‚Äôs the line of best fit through the data, minimizing the squared distances from the points to the line.\nSecond Principal Component: The direction, orthogonal (perpendicular) to the first, that captures the next most variance. And so on for subsequent components."
  },
  {
    "objectID": "qmd/islp12.html#geometric-interpretation-of-pca-visual",
    "href": "qmd/islp12.html#geometric-interpretation-of-pca-visual",
    "title": "Unsupervised Learning",
    "section": "Geometric Interpretation of PCA (Visual)",
    "text": "Geometric Interpretation of PCA (Visual)\n\n\n\nalt text\n\n\n\n\n\n\n\n\nThis figure, from an advertising dataset, shows the first two principal components. The green solid line represents the first principal component direction (Z1). The blue dashed line represents the second (Z2). Because this is a two-dimensional example, we only have two components. The lines show the directions of greatest variability in the data. The origin is at the mean of each feature."
  },
  {
    "objectID": "qmd/islp12.html#example-usarrests-data",
    "href": "qmd/islp12.html#example-usarrests-data",
    "title": "Unsupervised Learning",
    "section": "Example: USArrests Data",
    "text": "Example: USArrests Data\nLet‚Äôs apply PCA to a real-world dataset: the USArrests dataset. This dataset contains crime statistics for each of the 50 US states.\n\nFeatures: Murder, Assault, UrbanPop, Rape (all measured per 100,000 residents).\nGoal: Visualize the data and identify patterns using PCA. We want to see if we can reduce these four crime variables down to a smaller number of ‚Äúsummary‚Äù variables."
  },
  {
    "objectID": "qmd/islp12.html#usarrests-data-pca-biplot---image",
    "href": "qmd/islp12.html#usarrests-data-pca-biplot---image",
    "title": "Unsupervised Learning",
    "section": "USArrests Data: PCA Biplot - Image",
    "text": "USArrests Data: PCA Biplot - Image\n\n\n\nFirst two principal components for the USArrests data."
  },
  {
    "objectID": "qmd/islp12.html#usarrests-data-pca-biplot---explanation-1",
    "href": "qmd/islp12.html#usarrests-data-pca-biplot---explanation-1",
    "title": "Unsupervised Learning",
    "section": "USArrests Data: PCA Biplot - Explanation 1",
    "text": "USArrests Data: PCA Biplot - Explanation 1\nThis plot is called a biplot. It shows both the principal component scores (blue state names) and the principal component loadings (orange arrows). The scores represent the projection of each state onto the plane defined by the first two principal components."
  },
  {
    "objectID": "qmd/islp12.html#usarrests-data-pca-biplot---explanation-2",
    "href": "qmd/islp12.html#usarrests-data-pca-biplot---explanation-2",
    "title": "Unsupervised Learning",
    "section": "USArrests Data: PCA Biplot - Explanation 2",
    "text": "USArrests Data: PCA Biplot - Explanation 2\nThe loadings (orange arrows) show the contribution of each original variable to the principal components. For example, the loading for Rape on the first component is 0.54, and on the second component is 0.17. The word Rape is centered at the point (0.54, 0.17). The length and direction of the arrows indicate the strength and direction of the relationship."
  },
  {
    "objectID": "qmd/islp12.html#usarrests-data-pca-biplot---explanation-3",
    "href": "qmd/islp12.html#usarrests-data-pca-biplot---explanation-3",
    "title": "Unsupervised Learning",
    "section": "USArrests Data: PCA Biplot - Explanation 3",
    "text": "USArrests Data: PCA Biplot - Explanation 3\nThe axes are labeled as PC1 and PC2. Each point represents a state, and its position is determined by its scores on the two principal components. States closer together in this plot have similar crime profiles."
  },
  {
    "objectID": "qmd/islp12.html#interpreting-the-usarrests-biplot-cont.",
    "href": "qmd/islp12.html#interpreting-the-usarrests-biplot-cont.",
    "title": "Unsupervised Learning",
    "section": "Interpreting the USArrests Biplot (Cont.)",
    "text": "Interpreting the USArrests Biplot (Cont.)\n\nFirst Principal Component (PC1): This component places roughly equal weight on Murder, Assault, and Rape, with much less weight on UrbanPop. This suggests that PC1 primarily captures the overall level of violent crime. States with high scores on PC1 have high crime rates across these three categories.\nSecond Principal Component (PC2): This component places most of its weight on UrbanPop, suggesting that it represents the degree of urbanization. States with high scores on PC2 are more urbanized.\nCorrelation: The proximity of Murder, Assault, and Rape in the biplot indicates that these variables are positively correlated. UrbanPop is farther away, indicating it‚Äôs less correlated with the other three."
  },
  {
    "objectID": "qmd/islp12.html#interpreting-the-usarrests-biplot-loading-table---image",
    "href": "qmd/islp12.html#interpreting-the-usarrests-biplot-loading-table---image",
    "title": "Unsupervised Learning",
    "section": "Interpreting the USArrests Biplot: Loading Table - Image",
    "text": "Interpreting the USArrests Biplot: Loading Table - Image\n\n\n\n\nPC1\nPC2\n\n\n\n\nMurder\n0.536\n-0.418\n\n\nAssault\n0.583\n-0.188\n\n\nUrbanPop\n0.278\n0.873\n\n\nRape\n0.543\n0.167"
  },
  {
    "objectID": "qmd/islp12.html#interpreting-the-usarrests-biplot-loading-table---explanation-1",
    "href": "qmd/islp12.html#interpreting-the-usarrests-biplot-loading-table---explanation-1",
    "title": "Unsupervised Learning",
    "section": "Interpreting the USArrests Biplot: Loading Table - Explanation 1",
    "text": "Interpreting the USArrests Biplot: Loading Table - Explanation 1\nThis table shows the numerical values of the loading vectors for each principal component. These numbers correspond to the lengths and directions of the arrows in the biplot."
  },
  {
    "objectID": "qmd/islp12.html#interpreting-the-usarrests-biplot-loading-table---explanation-2",
    "href": "qmd/islp12.html#interpreting-the-usarrests-biplot-loading-table---explanation-2",
    "title": "Unsupervised Learning",
    "section": "Interpreting the USArrests Biplot: Loading Table - Explanation 2",
    "text": "Interpreting the USArrests Biplot: Loading Table - Explanation 2\nStates with large positive scores on PC1 (e.g., California, Nevada, Florida) have high crime rates, as indicated by the biplot and the large positive loadings for Murder, Assault, and Rape on PC1."
  },
  {
    "objectID": "qmd/islp12.html#interpreting-the-usarrests-biplot-loading-table---explanation-3",
    "href": "qmd/islp12.html#interpreting-the-usarrests-biplot-loading-table---explanation-3",
    "title": "Unsupervised Learning",
    "section": "Interpreting the USArrests Biplot: Loading Table - Explanation 3",
    "text": "Interpreting the USArrests Biplot: Loading Table - Explanation 3\nStates with large positive scores on PC2 (e.g., California) have high urbanization, as indicated by the biplot and the large positive loading for UrbanPop on PC2."
  },
  {
    "objectID": "qmd/islp12.html#another-interpretation-of-pca-closest-linear-surfaces",
    "href": "qmd/islp12.html#another-interpretation-of-pca-closest-linear-surfaces",
    "title": "Unsupervised Learning",
    "section": "Another Interpretation of PCA: Closest Linear Surfaces",
    "text": "Another Interpretation of PCA: Closest Linear Surfaces\nPCA can also be understood as finding linear surfaces that are closest to the data points.\n\nFirst Principal Component: The line in p-dimensional space that is closest to the n observations (in terms of average squared Euclidean distance). It‚Äôs the best-fitting line in the sense of minimizing the sum of squared distances from the points to the line.\nFirst Two Principal Components: The plane that is closest to the observations. It‚Äôs the best-fitting plane in the same least-squares sense.\nAnd so on‚Ä¶ For higher dimensions, PCA finds the best-fitting hyperplanes (generalizations of planes to higher dimensions)."
  },
  {
    "objectID": "qmd/islp12.html#another-interpretation-of-pca-visual---image",
    "href": "qmd/islp12.html#another-interpretation-of-pca-visual---image",
    "title": "Unsupervised Learning",
    "section": "Another Interpretation of PCA: Visual - Image",
    "text": "Another Interpretation of PCA: Visual - Image\n\n\n\nNinety observations simulated in three dimensions."
  },
  {
    "objectID": "qmd/islp12.html#another-interpretation-of-pca-visual---explanation-left",
    "href": "qmd/islp12.html#another-interpretation-of-pca-visual---explanation-left",
    "title": "Unsupervised Learning",
    "section": "Another Interpretation of PCA: Visual - Explanation (Left)",
    "text": "Another Interpretation of PCA: Visual - Explanation (Left)\nLeft: The first two principal component directions (shown in green) span the plane that best fits the data. It‚Äôs like finding the ‚Äúflattest‚Äù plane that passes through the cloud of points, minimizing the distances from the points to the plane."
  },
  {
    "objectID": "qmd/islp12.html#another-interpretation-of-pca-visual---explanation-right",
    "href": "qmd/islp12.html#another-interpretation-of-pca-visual---explanation-right",
    "title": "Unsupervised Learning",
    "section": "Another Interpretation of PCA: Visual - Explanation (Right)",
    "text": "Another Interpretation of PCA: Visual - Explanation (Right)\nRight: The first two principal component score vectors give the coordinates of the projection of the 90 observations onto this plane. Projecting the data onto this plane gives us a lower-dimensional representation, capturing the most important aspects of the data‚Äôs variability."
  },
  {
    "objectID": "qmd/islp12.html#proportion-of-variance-explained-pve",
    "href": "qmd/islp12.html#proportion-of-variance-explained-pve",
    "title": "Unsupervised Learning",
    "section": "Proportion of Variance Explained (PVE)",
    "text": "Proportion of Variance Explained (PVE)\nHow much information do we lose when we project our data onto the first few principal components? The Proportion of Variance Explained (PVE) helps us answer this question. It quantifies the amount of information retained by each principal component."
  },
  {
    "objectID": "qmd/islp12.html#understanding-pve",
    "href": "qmd/islp12.html#understanding-pve",
    "title": "Unsupervised Learning",
    "section": "Understanding PVE",
    "text": "Understanding PVE\n\nTotal Variance: The sum of the variances of all the original features (assuming the features have been centered). This represents the total variability in the original dataset.\nVariance Explained by the m-th PC: The variance of the m-th principal component. This is the amount of variability captured by that single component.\nPVE of the m-th PC: The proportion of the total variance that is explained by the m-th principal component. It tells us how much information is retained by that component, expressed as a percentage of the total."
  },
  {
    "objectID": "qmd/islp12.html#pve-formula",
    "href": "qmd/islp12.html#pve-formula",
    "title": "Unsupervised Learning",
    "section": "PVE Formula",
    "text": "PVE Formula\n\n\n\n\n\n\nFormula for PVE of the m-th PC: \\[\\frac{\\sum_{i=1}^{n} z_{im}^2}{\\sum_{j=1}^{p} \\sum_{i=1}^{n} x_{ij}^2}\\] Where: - zim is the score of the i-th observation on the m-th principal component. - xij is the value of the j-th feature for the i-th observation (after centering). The denominator represents the total variance."
  },
  {
    "objectID": "qmd/islp12.html#pve-usarrests-example-visual---image",
    "href": "qmd/islp12.html#pve-usarrests-example-visual---image",
    "title": "Unsupervised Learning",
    "section": "PVE: USArrests Example (Visual) - Image",
    "text": "PVE: USArrests Example (Visual) - Image\n\n\n\nScree plot and cumulative PVE plot for the USArrests data."
  },
  {
    "objectID": "qmd/islp12.html#pve-usarrests-example-visual---explanation-left",
    "href": "qmd/islp12.html#pve-usarrests-example-visual---explanation-left",
    "title": "Unsupervised Learning",
    "section": "PVE: USArrests Example (Visual) - Explanation (Left)",
    "text": "PVE: USArrests Example (Visual) - Explanation (Left)\nLeft: A scree plot, showing the PVE of each principal component. Each bar represents a principal component, and the height of the bar indicates the proportion of variance explained by that component."
  },
  {
    "objectID": "qmd/islp12.html#pve-usarrests-example-visual---explanation-right",
    "href": "qmd/islp12.html#pve-usarrests-example-visual---explanation-right",
    "title": "Unsupervised Learning",
    "section": "PVE: USArrests Example (Visual) - Explanation (Right)",
    "text": "PVE: USArrests Example (Visual) - Explanation (Right)\nRight: The cumulative PVE. This plot shows the cumulative proportion of variance explained as we add more principal components. It helps us see how much variance is explained by the first few components together."
  },
  {
    "objectID": "qmd/islp12.html#pve-usarrests-example-interpretation",
    "href": "qmd/islp12.html#pve-usarrests-example-interpretation",
    "title": "Unsupervised Learning",
    "section": "PVE: USArrests Example (Interpretation)",
    "text": "PVE: USArrests Example (Interpretation)\n\nPC1: Explains 62.0% of the variance in the data.\nPC2: Explains 24.7% of the variance.\nTogether: PC1 and PC2 explain almost 87% of the total variance.\n\nThis means that the biplot (from earlier slides) provides a reasonably good two-dimensional summary of the data, capturing a large portion of its variability. The scree plot helps us decide how many components to keep. We often look for an ‚Äúelbow‚Äù in the plot ‚Äì a point where the PVE starts to drop off significantly. This suggests that adding further components adds little additional information."
  },
  {
    "objectID": "qmd/islp12.html#scaling-the-variables-a-crucial-step",
    "href": "qmd/islp12.html#scaling-the-variables-a-crucial-step",
    "title": "Unsupervised Learning",
    "section": "Scaling the Variables: A Crucial Step",
    "text": "Scaling the Variables: A Crucial Step\nBefore performing PCA, we usually scale the variables to have a standard deviation of one (also called standardization or z-scoring). This is a very important step in most cases!"
  },
  {
    "objectID": "qmd/islp12.html#why-scale",
    "href": "qmd/islp12.html#why-scale",
    "title": "Unsupervised Learning",
    "section": "Why Scale?",
    "text": "Why Scale?\n\nDifferent Units/Variances: If variables are measured in different units (e.g., meters and kilograms) or have vastly different variances (e.g., one variable ranges from 0 to 1, another from 0 to 1000), the variables with the largest variances will dominate the principal components, regardless of whether they are actually the most important or informative.\nEqual Weight: Scaling prevents this by putting all variables on a ‚Äúlevel playing field.‚Äù It ensures that each variable contributes equally to the principal components, based on its relative variability, not its absolute scale."
  },
  {
    "objectID": "qmd/islp12.html#when-not-to-scale",
    "href": "qmd/islp12.html#when-not-to-scale",
    "title": "Unsupervised Learning",
    "section": "When Not to Scale",
    "text": "When Not to Scale\n\nSame Units: If variables are measured in the same units (e.g., gene expression levels measured using the same technology), and the differences in variance are scientifically meaningful, we might not want to scale. In this case, the differences in variance might reflect real biological differences."
  },
  {
    "objectID": "qmd/islp12.html#scaling-usarrests-example-visual---image",
    "href": "qmd/islp12.html#scaling-usarrests-example-visual---image",
    "title": "Unsupervised Learning",
    "section": "Scaling: USArrests Example (Visual) - Image",
    "text": "Scaling: USArrests Example (Visual) - Image\n\n\n\nEffect of scaling on the USArrests biplot."
  },
  {
    "objectID": "qmd/islp12.html#scaling-usarrests-example-visual---explanation-left",
    "href": "qmd/islp12.html#scaling-usarrests-example-visual---explanation-left",
    "title": "Unsupervised Learning",
    "section": "Scaling: USArrests Example (Visual) - Explanation (Left)",
    "text": "Scaling: USArrests Example (Visual) - Explanation (Left)\nLeft: PCA with scaled variables (this is the same as the biplot we saw earlier). Notice that all variables contribute reasonably to PC1."
  },
  {
    "objectID": "qmd/islp12.html#scaling-usarrests-example-visual---explanation-right",
    "href": "qmd/islp12.html#scaling-usarrests-example-visual---explanation-right",
    "title": "Unsupervised Learning",
    "section": "Scaling: USArrests Example (Visual) - Explanation (Right)",
    "text": "Scaling: USArrests Example (Visual) - Explanation (Right)\nRight: PCA with unscaled variables. Notice how Assault dominates the first principal component in the unscaled version simply because it has the highest variance among the four variables. Scaling gives a more balanced representation, reflecting the relative importance of the variables."
  },
  {
    "objectID": "qmd/islp12.html#how-many-principal-components-to-use",
    "href": "qmd/islp12.html#how-many-principal-components-to-use",
    "title": "Unsupervised Learning",
    "section": "How Many Principal Components to Use?",
    "text": "How Many Principal Components to Use?\nThis is a common question, and unfortunately, there‚Äôs no single ‚Äúmagic number‚Äù that works for all datasets. The best approach depends on the specific context, the dataset, and the goals of the analysis."
  },
  {
    "objectID": "qmd/islp12.html#guidelines-for-choosing-the-number-of-components",
    "href": "qmd/islp12.html#guidelines-for-choosing-the-number-of-components",
    "title": "Unsupervised Learning",
    "section": "Guidelines for Choosing the Number of Components",
    "text": "Guidelines for Choosing the Number of Components\n\nScree Plot: Look for an ‚Äúelbow‚Äù in the scree plot ‚Äì a point where the PVE drops off significantly. This suggests that adding more components beyond that point doesn‚Äôt provide much additional information.\nInterpretation: Keep enough components to capture the interesting patterns in the data. If you can interpret the first few components in a meaningful way, that‚Äôs a good sign.\nAd Hoc: The process is inherently somewhat subjective and requires judgment.\nSupervised Learning: If PCA is used for pre-processing in a supervised learning context (e.g., Principal Components Regression), we can use cross-validation to select the optimal number of components. This involves trying different numbers of components and seeing which number leads to the best predictive performance on unseen data."
  },
  {
    "objectID": "qmd/islp12.html#clustering-methods-finding-subgroups",
    "href": "qmd/islp12.html#clustering-methods-finding-subgroups",
    "title": "Unsupervised Learning",
    "section": "Clustering Methods: Finding Subgroups",
    "text": "Clustering Methods: Finding Subgroups\nNow, let‚Äôs move on to the second major type of unsupervised learning: clustering. Clustering aims to find subgroups (clusters) within a dataset, grouping observations that are similar to each other."
  },
  {
    "objectID": "qmd/islp12.html#clustering-the-goal",
    "href": "qmd/islp12.html#clustering-the-goal",
    "title": "Unsupervised Learning",
    "section": "Clustering: The Goal",
    "text": "Clustering: The Goal\n\nGoal: Partition observations into groups (clusters) so that observations within a group are similar, and observations in different groups are dissimilar. It‚Äôs like sorting a collection of objects into meaningful categories based on their shared characteristics.\nSimilarity: What does ‚Äúsimilar‚Äù mean? This is a crucial, and often domain-specific, consideration. We need to define how we measure the similarity or dissimilarity between observations, and this choice can significantly affect the results.\nUnsupervised: We‚Äôre looking for structure without a predefined outcome or ‚Äúcorrect answer.‚Äù We don‚Äôt know the ‚Äútrue‚Äù clusters beforehand."
  },
  {
    "objectID": "qmd/islp12.html#two-main-types-of-clustering",
    "href": "qmd/islp12.html#two-main-types-of-clustering",
    "title": "Unsupervised Learning",
    "section": "Two Main Types of Clustering",
    "text": "Two Main Types of Clustering\nWe‚Äôll cover two main types of clustering:\n\nK-Means Clustering: Partitions data into a pre-specified number (K) of clusters. We have to tell the algorithm how many clusters we expect to find.\nHierarchical Clustering: Builds a hierarchy of clusters, represented by a dendrogram (a tree-like diagram). This approach doesn‚Äôt require us to pre-specify the number of clusters; we can decide after seeing the dendrogram."
  },
  {
    "objectID": "qmd/islp12.html#k-means-clustering-a-popular-choice",
    "href": "qmd/islp12.html#k-means-clustering-a-popular-choice",
    "title": "Unsupervised Learning",
    "section": "K-Means Clustering: A Popular Choice",
    "text": "K-Means Clustering: A Popular Choice\nK-means clustering is a simple, widely used, and efficient clustering algorithm.\n\nInput: A dataset and a desired number of clusters, K.\nOutput: Assigns each observation to exactly one of K clusters.\nGoal: Minimize the within-cluster variation. This means we want the observations within each cluster to be as close to each other as possible, forming tight, compact groups."
  },
  {
    "objectID": "qmd/islp12.html#k-means-clustering-visual---image",
    "href": "qmd/islp12.html#k-means-clustering-visual---image",
    "title": "Unsupervised Learning",
    "section": "K-Means Clustering (Visual) - Image",
    "text": "K-Means Clustering (Visual) - Image\n\n\n\nK-means clustering results on simulated data."
  },
  {
    "objectID": "qmd/islp12.html#k-means-clustering-visual---explanation-k2",
    "href": "qmd/islp12.html#k-means-clustering-visual---explanation-k2",
    "title": "Unsupervised Learning",
    "section": "K-Means Clustering (Visual) - Explanation (K=2)",
    "text": "K-Means Clustering (Visual) - Explanation (K=2)\nThis figure shows the results of applying K-means clustering with K=2 (two clusters). The color of each observation indicates the cluster to which it was assigned. The algorithm has separated the data into two distinct groups."
  },
  {
    "objectID": "qmd/islp12.html#k-means-clustering-visual---explanation-k3",
    "href": "qmd/islp12.html#k-means-clustering-visual---explanation-k3",
    "title": "Unsupervised Learning",
    "section": "K-Means Clustering (Visual) - Explanation (K=3)",
    "text": "K-Means Clustering (Visual) - Explanation (K=3)\nHere, K=3. The algorithm has identified three clusters. Notice how the cluster assignments change as we change the value of K."
  },
  {
    "objectID": "qmd/islp12.html#k-means-clustering-visual---explanation-k4",
    "href": "qmd/islp12.html#k-means-clustering-visual---explanation-k4",
    "title": "Unsupervised Learning",
    "section": "K-Means Clustering (Visual) - Explanation (K=4)",
    "text": "K-Means Clustering (Visual) - Explanation (K=4)\nWith K=4, the data is further divided. The choice of K significantly impacts the resulting clusters."
  },
  {
    "objectID": "qmd/islp12.html#the-k-means-algorithm-step-by-step",
    "href": "qmd/islp12.html#the-k-means-algorithm-step-by-step",
    "title": "Unsupervised Learning",
    "section": "The K-Means Algorithm: Step-by-Step",
    "text": "The K-Means Algorithm: Step-by-Step\n\nInitialization: Randomly assign each observation to one of the K clusters. This is our initial ‚Äúguess‚Äù at the cluster assignments. These are random starting points.\nIteration: Repeat the following steps until the cluster assignments stop changing (or until a maximum number of iterations is reached):\n\nCompute Centroids: For each cluster, calculate the centroid. The centroid is the mean vector of all the observations in that cluster. It represents the ‚Äúcenter‚Äù of the cluster in the feature space.\nReassign Observations: Assign each observation to the cluster whose centroid is closest (usually using Euclidean distance). This step refines the cluster assignments based on the current centroids, moving observations between clusters to minimize within-cluster distances."
  },
  {
    "objectID": "qmd/islp12.html#the-k-means-algorithm-local-optima",
    "href": "qmd/islp12.html#the-k-means-algorithm-local-optima",
    "title": "Unsupervised Learning",
    "section": "The K-Means Algorithm (Local Optima)",
    "text": "The K-Means Algorithm (Local Optima)\n\n\n\n\n\n\nThe K-means algorithm is guaranteed to decrease the within-cluster variation at each step. However, it finds a local optimum, not necessarily the global optimum. This means the final cluster assignments can depend on the initial random assignments. Different starting points can lead to different final clusters."
  },
  {
    "objectID": "qmd/islp12.html#k-means-an-illustrative-example",
    "href": "qmd/islp12.html#k-means-an-illustrative-example",
    "title": "Unsupervised Learning",
    "section": "K-Means: An Illustrative Example",
    "text": "K-Means: An Illustrative Example\n\n\n\n\n\ngraph LR\n    A[Data] --&gt; B(Step 1: Randomly Assign Clusters);\n    B --&gt; C(Iteration 1, Step 2a: Compute Centroids);\n    C --&gt; D(Iteration 1, Step 2b: Reassign Observations);\n    D --&gt; E(Iteration 2, Step 2a: Compute Centroids);\n     E --&gt; F(Iteration 2, Step 2b: Reassign Observations);\n    F --&gt; G(Continue Iterating Until Convergence);\n    G --&gt; H(Final Cluster Assignments);\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style H fill:#ccf,stroke:#333,stroke-width:4px\n\n\n\n\n\n\nThis flowchart illustrates the iterative nature of the K-means algorithm. The process continues until the cluster assignments no longer change, indicating that a local optimum has been reached."
  },
  {
    "objectID": "qmd/islp12.html#k-means-algorithm-in-action---image",
    "href": "qmd/islp12.html#k-means-algorithm-in-action---image",
    "title": "Unsupervised Learning",
    "section": "K-Means Algorithm in Action - Image",
    "text": "K-Means Algorithm in Action - Image\n\n\n\nProgress of the K-means algorithm."
  },
  {
    "objectID": "qmd/islp12.html#k-means-algorithm-in-action---explanation-first-row",
    "href": "qmd/islp12.html#k-means-algorithm-in-action---explanation-first-row",
    "title": "Unsupervised Learning",
    "section": "K-Means Algorithm in Action - Explanation (First Row)",
    "text": "K-Means Algorithm in Action - Explanation (First Row)\nThis figure shows the progress of the K-means algorithm over several iterations. The crosses represent the cluster centroids. The first row shows the random initialization of cluster assignments and the initial centroids."
  },
  {
    "objectID": "qmd/islp12.html#k-means-algorithm-in-action---explanation-second-row",
    "href": "qmd/islp12.html#k-means-algorithm-in-action---explanation-second-row",
    "title": "Unsupervised Learning",
    "section": "K-Means Algorithm in Action - Explanation (Second Row)",
    "text": "K-Means Algorithm in Action - Explanation (Second Row)\nThe second row shows the state after the first iteration. The centroids have moved, and some observations have been reassigned to different clusters."
  },
  {
    "objectID": "qmd/islp12.html#k-means-algorithm-in-action---explanation-third-and-fourth-rows",
    "href": "qmd/islp12.html#k-means-algorithm-in-action---explanation-third-and-fourth-rows",
    "title": "Unsupervised Learning",
    "section": "K-Means Algorithm in Action - Explanation (Third and Fourth Rows)",
    "text": "K-Means Algorithm in Action - Explanation (Third and Fourth Rows)\nThe third and fourth rows show further iterations. The algorithm continues to update the centroids and reassign observations until convergence."
  },
  {
    "objectID": "qmd/islp12.html#k-means-the-problem-of-local-optima",
    "href": "qmd/islp12.html#k-means-the-problem-of-local-optima",
    "title": "Unsupervised Learning",
    "section": "K-Means: The Problem of Local Optima",
    "text": "K-Means: The Problem of Local Optima\nBecause K-means finds a local optimum, the results can vary depending on the initial random assignment of observations to clusters. Different starting points can lead to different final cluster assignments, and some solutions may be better (lower within-cluster variation) than others."
  },
  {
    "objectID": "qmd/islp12.html#dealing-with-local-optima",
    "href": "qmd/islp12.html#dealing-with-local-optima",
    "title": "Unsupervised Learning",
    "section": "Dealing with Local Optima",
    "text": "Dealing with Local Optima\n\nRecommendation: Run K-means multiple times with different initializations and choose the solution with the lowest within-cluster variation. This helps to mitigate the problem of getting stuck in a poor local optimum. It increases the chances of finding a good solution, although it doesn‚Äôt guarantee finding the global optimum."
  },
  {
    "objectID": "qmd/islp12.html#k-means-local-optima-visual---image",
    "href": "qmd/islp12.html#k-means-local-optima-visual---image",
    "title": "Unsupervised Learning",
    "section": "K-Means: Local Optima (Visual) - Image",
    "text": "K-Means: Local Optima (Visual) - Image\n\n\n\nK-means clustering with different initializations."
  },
  {
    "objectID": "qmd/islp12.html#k-means-local-optima-visual---explanation-1",
    "href": "qmd/islp12.html#k-means-local-optima-visual---explanation-1",
    "title": "Unsupervised Learning",
    "section": "K-Means: Local Optima (Visual) - Explanation 1",
    "text": "K-Means: Local Optima (Visual) - Explanation 1\nThis figure shows K-means clustering performed six times on the same data, each with a different random initialization."
  },
  {
    "objectID": "qmd/islp12.html#k-means-local-optima-visual---explanation-2",
    "href": "qmd/islp12.html#k-means-local-optima-visual---explanation-2",
    "title": "Unsupervised Learning",
    "section": "K-Means: Local Optima (Visual) - Explanation 2",
    "text": "K-Means: Local Optima (Visual) - Explanation 2\nThree different local optima were obtained. This highlights the variability in the results due to the random initialization."
  },
  {
    "objectID": "qmd/islp12.html#k-means-local-optima-visual---explanation-3",
    "href": "qmd/islp12.html#k-means-local-optima-visual---explanation-3",
    "title": "Unsupervised Learning",
    "section": "K-Means: Local Optima (Visual) - Explanation 3",
    "text": "K-Means: Local Optima (Visual) - Explanation 3\nOne of these local optima (bottom right) resulted in a better separation between the clusters. This emphasizes the importance of running K-means multiple times and comparing the results."
  },
  {
    "objectID": "qmd/islp12.html#hierarchical-clustering-a-different-approach",
    "href": "qmd/islp12.html#hierarchical-clustering-a-different-approach",
    "title": "Unsupervised Learning",
    "section": "Hierarchical Clustering: A Different Approach",
    "text": "Hierarchical Clustering: A Different Approach\nHierarchical clustering offers an alternative to K-means, building a hierarchy of clusters. It provides a more nuanced view of the relationships between observations."
  },
  {
    "objectID": "qmd/islp12.html#advantages-of-hierarchical-clustering",
    "href": "qmd/islp12.html#advantages-of-hierarchical-clustering",
    "title": "Unsupervised Learning",
    "section": "Advantages of Hierarchical Clustering",
    "text": "Advantages of Hierarchical Clustering\n\nNo Need to Pre-specify K: Unlike K-means, we don‚Äôt need to pre-specify the number of clusters (K). We can choose the number of clusters after the algorithm has run by examining the resulting dendrogram. This provides more flexibility.\nDendrogram: The output of hierarchical clustering is a dendrogram, a tree-like diagram that visually represents the hierarchy of clusters and the relationships between observations. It provides a visual summary of the clustering process."
  },
  {
    "objectID": "qmd/islp12.html#agglomerative-clustering-bottom-up",
    "href": "qmd/islp12.html#agglomerative-clustering-bottom-up",
    "title": "Unsupervised Learning",
    "section": "Agglomerative Clustering (Bottom-Up)",
    "text": "Agglomerative Clustering (Bottom-Up)\n\nAgglomerative (Bottom-Up): We‚Äôll focus on agglomerative clustering, which is the most common type of hierarchical clustering. It starts with each observation as its own cluster and successively merges the most similar clusters until only one cluster remains, building the hierarchy from the bottom up."
  },
  {
    "objectID": "qmd/islp12.html#hierarchical-clustering-dendrogram---image",
    "href": "qmd/islp12.html#hierarchical-clustering-dendrogram---image",
    "title": "Unsupervised Learning",
    "section": "Hierarchical Clustering: Dendrogram - Image",
    "text": "Hierarchical Clustering: Dendrogram - Image\n\n\n\nDendrogram of hierarchically clustering the data."
  },
  {
    "objectID": "qmd/islp12.html#hierarchical-clustering-dendrogram---explanation",
    "href": "qmd/islp12.html#hierarchical-clustering-dendrogram---explanation",
    "title": "Unsupervised Learning",
    "section": "Hierarchical Clustering: Dendrogram - Explanation",
    "text": "Hierarchical Clustering: Dendrogram - Explanation\nThis dendrogram visually represents the hierarchy of clusters. Each leaf represents an observation, and the branches show how clusters are merged."
  },
  {
    "objectID": "qmd/islp12.html#interpreting-a-dendrogram",
    "href": "qmd/islp12.html#interpreting-a-dendrogram",
    "title": "Unsupervised Learning",
    "section": "Interpreting a Dendrogram",
    "text": "Interpreting a Dendrogram\n\nLeaves: The leaves at the bottom of the dendrogram represent individual observations.\nFusions: As you move up the tree, leaves and branches fuse together. These fusions represent the merging of similar clusters. Earlier fusions indicate greater similarity.\nHeight of Fusion: The height at which two clusters fuse indicates their dissimilarity. Lower fusions mean the merged clusters are more similar. Higher fusions mean the clusters are more dissimilar. The height provides a measure of the distance between merged clusters.\nCutting the Dendrogram: A horizontal cut across the dendrogram gives a specific number of clusters. The height of the cut determines the number of clusters obtained. This is how we can choose the number of clusters after running the algorithm."
  },
  {
    "objectID": "qmd/islp12.html#interpreting-a-dendrogram-visual---image",
    "href": "qmd/islp12.html#interpreting-a-dendrogram-visual---image",
    "title": "Unsupervised Learning",
    "section": "Interpreting a Dendrogram (Visual) - Image",
    "text": "Interpreting a Dendrogram (Visual) - Image\n\n\n\nInterpreting a dendrogram."
  },
  {
    "objectID": "qmd/islp12.html#interpreting-a-dendrogram-visual---explanation-left",
    "href": "qmd/islp12.html#interpreting-a-dendrogram-visual---explanation-left",
    "title": "Unsupervised Learning",
    "section": "Interpreting a Dendrogram (Visual) - Explanation (Left)",
    "text": "Interpreting a Dendrogram (Visual) - Explanation (Left)\nLeft: A dendrogram generated using Euclidean distance and complete linkage. Observations 5 and 7 are quite similar, as indicated by the low height of their fusion. Observations 1 and 6 are also quite similar."
  },
  {
    "objectID": "qmd/islp12.html#interpreting-a-dendrogram-visual---explanation-right",
    "href": "qmd/islp12.html#interpreting-a-dendrogram-visual---explanation-right",
    "title": "Unsupervised Learning",
    "section": "Interpreting a Dendrogram (Visual) - Explanation (Right)",
    "text": "Interpreting a Dendrogram (Visual) - Explanation (Right)\nRight: The raw data that was used to generate the dendrogram. This allows us to see how the dendrogram reflects the relationships in the original data. The dendrogram accurately captures the similarity between observations 5 and 7, and between observations 1 and 6."
  },
  {
    "objectID": "qmd/islp12.html#the-hierarchical-clustering-algorithm-step-by-step",
    "href": "qmd/islp12.html#the-hierarchical-clustering-algorithm-step-by-step",
    "title": "Unsupervised Learning",
    "section": "The Hierarchical Clustering Algorithm: Step-by-Step",
    "text": "The Hierarchical Clustering Algorithm: Step-by-Step\n\nInitialization: Begin with each observation as its own cluster (n clusters). Calculate all pairwise dissimilarities between the observations (e.g., using Euclidean distance).\nIteration: For i = n, n-1, ‚Ä¶, 2:\n\nFind Most Similar Clusters: Identify the two most similar clusters (the two clusters with the smallest dissimilarity).\nMerge Clusters: Fuse these two clusters into a single cluster. The dissimilarity between these two clusters is represented by the height in the dendrogram where their branches fuse.\nUpdate Dissimilarities: Calculate the new pairwise inter-cluster dissimilarities between the remaining i-1 clusters. This is where different linkage methods come into play. The choice of linkage affects how the distances between clusters are calculated."
  },
  {
    "objectID": "qmd/islp12.html#the-key-question-linkage",
    "href": "qmd/islp12.html#the-key-question-linkage",
    "title": "Unsupervised Learning",
    "section": "The Key Question: Linkage",
    "text": "The Key Question: Linkage\n\n\n\n\n\n\nKey Question: How do we define the dissimilarity between clusters (groups of observations), not just between individual observations? We know how to calculate the distance between two points, but how do we calculate the distance between two sets of points? This is where linkage comes in."
  },
  {
    "objectID": "qmd/islp12.html#linkage-defining-inter-cluster-dissimilarity",
    "href": "qmd/islp12.html#linkage-defining-inter-cluster-dissimilarity",
    "title": "Unsupervised Learning",
    "section": "Linkage: Defining Inter-Cluster Dissimilarity",
    "text": "Linkage: Defining Inter-Cluster Dissimilarity\nLinkage defines how we measure the dissimilarity between two groups of observations (clusters). It‚Äôs a crucial choice in hierarchical clustering. There are several different linkage methods:"
  },
  {
    "objectID": "qmd/islp12.html#linkage-methods-a-table",
    "href": "qmd/islp12.html#linkage-methods-a-table",
    "title": "Unsupervised Learning",
    "section": "Linkage Methods: A Table",
    "text": "Linkage Methods: A Table\n\n\n\n\n\n\n\nLinkage\nDescription\n\n\n\n\nComplete\nMaximal intercluster dissimilarity. Calculates the dissimilarity between the most dissimilar points in the two clusters (the largest pairwise distance).\n\n\nSingle\nMinimal intercluster dissimilarity. Calculates the dissimilarity between the most similar points in the two clusters (the smallest pairwise distance).\n\n\nAverage\nMean intercluster dissimilarity. Calculates the average dissimilarity between all pairs of points in the two clusters.\n\n\nCentroid\nDissimilarity between the centroids (means) of the two clusters."
  },
  {
    "objectID": "qmd/islp12.html#linkage-recommendations",
    "href": "qmd/islp12.html#linkage-recommendations",
    "title": "Unsupervised Learning",
    "section": "Linkage: Recommendations",
    "text": "Linkage: Recommendations\n\n\n\n\n\n\nAverage and complete linkage are generally preferred over single and centroid linkage. Single linkage can lead to ‚Äúchaining,‚Äù where clusters become elongated and stringy. Centroid linkage can sometimes lead to undesirable inversions in the dendrogram, where clusters merge at a lower height than their individual members, making interpretation difficult."
  },
  {
    "objectID": "qmd/islp12.html#choice-of-dissimilarity-measure",
    "href": "qmd/islp12.html#choice-of-dissimilarity-measure",
    "title": "Unsupervised Learning",
    "section": "Choice of Dissimilarity Measure",
    "text": "Choice of Dissimilarity Measure\nIn addition to choosing a linkage method, we also need to choose a dissimilarity measure between individual observations. This is the foundation upon which the clustering is built."
  },
  {
    "objectID": "qmd/islp12.html#common-dissimilarity-measures",
    "href": "qmd/islp12.html#common-dissimilarity-measures",
    "title": "Unsupervised Learning",
    "section": "Common Dissimilarity Measures",
    "text": "Common Dissimilarity Measures\n\nEuclidean Distance: The most common choice. Measures the straight-line distance between two points in the feature space: \\[\\sqrt{\\sum_{j=1}^{p}(x_{ij} - x_{i'j})^2}\\]. Suitable when features are continuous and on similar scales (or have been scaled).\nCorrelation-Based Distance: Considers two observations to be similar if their features are highly correlated, even if their absolute values are far apart in terms of Euclidean distance. Useful when we‚Äôre interested in the shape of the feature profiles, rather than their magnitude. It‚Äôs calculated as 1 - correlation."
  },
  {
    "objectID": "qmd/islp12.html#dissimilarity-measures-an-example---image",
    "href": "qmd/islp12.html#dissimilarity-measures-an-example---image",
    "title": "Unsupervised Learning",
    "section": "Dissimilarity Measures: An Example - Image",
    "text": "Dissimilarity Measures: An Example - Image\n\n\n\nEuclidean vs.¬†correlation-based distance."
  },
  {
    "objectID": "qmd/islp12.html#dissimilarity-measures-an-example---explanation-top",
    "href": "qmd/islp12.html#dissimilarity-measures-an-example---explanation-top",
    "title": "Unsupervised Learning",
    "section": "Dissimilarity Measures: An Example - Explanation (Top)",
    "text": "Dissimilarity Measures: An Example - Explanation (Top)\nThe top panel shows three observations with two features. In terms of Euclidean distance, observations 1 and 3 are most similar, and observations 2 and 3 are most dissimilar."
  },
  {
    "objectID": "qmd/islp12.html#dissimilarity-measures-an-example---explanation-bottom",
    "href": "qmd/islp12.html#dissimilarity-measures-an-example---explanation-bottom",
    "title": "Unsupervised Learning",
    "section": "Dissimilarity Measures: An Example - Explanation (Bottom)",
    "text": "Dissimilarity Measures: An Example - Explanation (Bottom)\nThe bottom panel shows the same observations, but now we consider correlation-based distance. Observations 1 and 2 have a perfect correlation of 1, so their correlation-based distance is 0. Observation 3 is negatively correlated with observations 1 and 2.\nThe choice of dissimilarity measure depends on the type of data and the scientific question being addressed. Euclidean distance focuses on the magnitude of differences, while correlation-based distance focuses on the pattern of changes across features."
  },
  {
    "objectID": "qmd/islp12.html#practical-issues-in-clustering",
    "href": "qmd/islp12.html#practical-issues-in-clustering",
    "title": "Unsupervised Learning",
    "section": "Practical Issues in Clustering",
    "text": "Practical Issues in Clustering\nClustering, while powerful, comes with some practical challenges that require careful consideration.\n\nScaling: Should we scale the variables before clustering? (Usually yes, for the same reasons as in PCA: to give equal weight to each variable and prevent variables with large variances from dominating the results.)\nSmall Decisions, Big Consequences: Seemingly small choices, such as the dissimilarity measure, linkage method, and scaling, can have a substantial impact on the clustering results. There‚Äôs no single ‚Äúcorrect‚Äù set of choices, and different choices can lead to very different clusters.\nValidating Clusters: It‚Äôs difficult to definitively know if the clusters found are real and meaningful, or just an artifact of the clustering process. There‚Äôs no ‚Äúground truth‚Äù to compare against in most unsupervised learning scenarios.\nRobustness: Clustering methods are often not very robust. Small changes in the data (e.g., adding or removing a few observations) can lead to significantly different cluster assignments."
  },
  {
    "objectID": "qmd/islp12.html#clustering-recommendations-and-cautions",
    "href": "qmd/islp12.html#clustering-recommendations-and-cautions",
    "title": "Unsupervised Learning",
    "section": "Clustering: Recommendations and Cautions",
    "text": "Clustering: Recommendations and Cautions\nRecommendations:\n\nExperiment: Try different choices of dissimilarity measure, linkage (for hierarchical clustering), and scaling. Don‚Äôt rely on a single clustering result.\nConsistency: Look for consistent patterns across different clustering results. If different methods produce similar clusters, this increases confidence in the findings.\nDomain Knowledge: Use domain knowledge to assess the plausibility and interpretability of the clusters. Do the clusters make sense in the context of the problem?\n\nCaution: Clustering should be viewed as a starting point for further investigation, not as the final answer. It‚Äôs an exploratory technique that can generate hypotheses, but these hypotheses should be validated using other methods or domain knowledge. Don‚Äôt over-interpret clustering results."
  },
  {
    "objectID": "qmd/islp12.html#data-mining-machine-learning-and-statistical-learning",
    "href": "qmd/islp12.html#data-mining-machine-learning-and-statistical-learning",
    "title": "Unsupervised Learning",
    "section": "Data Mining, Machine Learning and Statistical Learning",
    "text": "Data Mining, Machine Learning and Statistical Learning\n\n\n\n\n\ngraph LR\n    A[Data Mining] --&gt; C(Common Ground)\n    B[Machine Learning] --&gt; C\n    D[Statistical Learning] --&gt; C\n    C --&gt; E[Insights & Predictions]\n    style A fill:#ccf,stroke:#333,stroke-width:2px\n    style B fill:#ccf,stroke:#333,stroke-width:2px\n    style D fill:#ccf,stroke:#333,stroke-width:2px\n     style C fill:#f9f,stroke:#333,stroke-width:4px\n      style E fill:#ccf,stroke:#333,stroke-width:4px\n\n\n\n\n\n\n\nData Mining: Focuses on discovering patterns, anomalies, and insights from large datasets, often using techniques from both machine learning and statistical learning.\nMachine Learning: Emphasizes the development of algorithms that can learn from data and make predictions without explicit programming.\nStatistical Learning: A subfield of statistics that focuses on developing and understanding models and methods for learning from data, with a strong emphasis on statistical inference and uncertainty quantification.\n\nAll three fields share the common goal of extracting knowledge and making predictions from data, but they differ in their emphasis and approaches."
  },
  {
    "objectID": "qmd/islp12.html#summary",
    "href": "qmd/islp12.html#summary",
    "title": "Unsupervised Learning",
    "section": "Summary",
    "text": "Summary\n\nUnsupervised learning is about finding patterns and structure in data without a response variable (no ‚Äúteacher‚Äù to guide the learning). It‚Äôs about discovering hidden relationships.\nPCA reduces dimensionality by finding linear combinations of features (principal components) that capture the most variance. It‚Äôs useful for visualization and pre-processing data for other analyses.\nClustering aims to find subgroups (clusters) within the data, grouping similar observations together.\n\nK-means requires pre-specifying the number of clusters (K). It‚Äôs an iterative algorithm that minimizes within-cluster variation. It‚Äôs sensitive to initialization.\nHierarchical clustering builds a hierarchy of clusters, represented by a dendrogram. It doesn‚Äôt require pre-specifying K, and it provides a visual representation of the relationships between observations.\n\nChoices of dissimilarity measure, linkage (for hierarchical clustering), and scaling can significantly affect clustering results. These choices should be made carefully and thoughtfully.\nClustering is a powerful, but often subjective and non-robust, technique. It‚Äôs best used for exploration and hypothesis generation, not for definitive conclusions. Always consider the limitations."
  },
  {
    "objectID": "qmd/islp12.html#thoughts-and-discussion",
    "href": "qmd/islp12.html#thoughts-and-discussion",
    "title": "Unsupervised Learning",
    "section": "Thoughts and Discussion",
    "text": "Thoughts and Discussion\n\nCan you think of other real-world applications where unsupervised learning might be useful? Consider areas like image analysis (grouping similar images), anomaly detection (identifying unusual transactions), or natural language processing (clustering documents by topic).\nWhat are the potential limitations of relying too heavily on clustering results without further validation? How could you try to validate the clusters you find? (e.g., using external data, domain expertise, or comparing results across different methods).\nHow might you combine supervised and unsupervised learning techniques in a single analysis? For example, could you use clustering to identify subgroups and then build separate supervised models for each subgroup (this is called ‚Äúcluster-then-predict‚Äù)? Or could you use PCA to reduce dimensionality before applying a supervised learning algorithm?\nHow do you understand the differences and connections between data mining, machine learning, and statistical learning? Can you give examples of techniques used in each field?\nWhat is the biggest difference do you think supervised and unsupervised learning?"
  },
  {
    "objectID": "qmd/islp9.html",
    "href": "qmd/islp9.html",
    "title": "Support Vector Machines",
    "section": "",
    "text": "Welcome to the fascinating world of Support Vector Machines (SVMs)! ü§ñ In this chapter, we embark on a journey to understand these powerful supervised learning models. SVMs are renowned for their ability to perform both classification and regression tasks, and they originated in the computer science community back in the 1990s. What makes them stand out? Their exceptional ‚Äúout-of-the-box‚Äù performance ‚Äì meaning they often work remarkably well with minimal need for fine-tuning. üéØ"
  },
  {
    "objectID": "qmd/islp9.html#introduction",
    "href": "qmd/islp9.html#introduction",
    "title": "Support Vector Machines",
    "section": "",
    "text": "Welcome to the fascinating world of Support Vector Machines (SVMs)! ü§ñ In this chapter, we embark on a journey to understand these powerful supervised learning models. SVMs are renowned for their ability to perform both classification and regression tasks, and they originated in the computer science community back in the 1990s. What makes them stand out? Their exceptional ‚Äúout-of-the-box‚Äù performance ‚Äì meaning they often work remarkably well with minimal need for fine-tuning. üéØ"
  },
  {
    "objectID": "qmd/islp9.html#what-is-data-mining",
    "href": "qmd/islp9.html#what-is-data-mining",
    "title": "Support Vector Machines",
    "section": "What is Data Mining?",
    "text": "What is Data Mining?\n\nImagine you‚Äôre a treasure hunter ü™ô, but instead of searching for gold in caves, you‚Äôre sifting through mountains of data! ‚õ∞Ô∏è That‚Äôs essentially what data mining is all about. It‚Äôs the art and science of discovering hidden patterns, trends, and valuable insights from massive datasets. üîç Data mining cleverly combines techniques from diverse fields like statistics and computer science to unearth knowledge that can empower decision-making.\n\n\n\n\n\n\n\nNote\n\n\n\nData mining uses techniques from various fields, including statistics and computer science."
  },
  {
    "objectID": "qmd/islp9.html#data-mining-a-deeper-dive",
    "href": "qmd/islp9.html#data-mining-a-deeper-dive",
    "title": "Support Vector Machines",
    "section": "Data Mining: A Deeper Dive",
    "text": "Data Mining: A Deeper Dive\n\nData mining isn‚Äôt just about finding any information; it‚Äôs about finding useful information. Think of it like this:"
  },
  {
    "objectID": "qmd/islp9.html#data-mining-a-deeper-dive---pattern-discovery",
    "href": "qmd/islp9.html#data-mining-a-deeper-dive---pattern-discovery",
    "title": "Support Vector Machines",
    "section": "Data Mining: A Deeper Dive - Pattern Discovery",
    "text": "Data Mining: A Deeper Dive - Pattern Discovery\n\n\nPattern Discovery: Identifying recurring sequences or relationships. For example, finding that customers who buy product A also tend to buy product B."
  },
  {
    "objectID": "qmd/islp9.html#data-mining-a-deeper-dive---anomaly-detection",
    "href": "qmd/islp9.html#data-mining-a-deeper-dive---anomaly-detection",
    "title": "Support Vector Machines",
    "section": "Data Mining: A Deeper Dive - Anomaly Detection",
    "text": "Data Mining: A Deeper Dive - Anomaly Detection\n\n\nAnomaly Detection: Spotting outliers or unusual events. This could be crucial for fraud detection or identifying system failures."
  },
  {
    "objectID": "qmd/islp9.html#data-mining-a-deeper-dive---association-rule-learning",
    "href": "qmd/islp9.html#data-mining-a-deeper-dive---association-rule-learning",
    "title": "Support Vector Machines",
    "section": "Data Mining: A Deeper Dive - Association Rule Learning",
    "text": "Data Mining: A Deeper Dive - Association Rule Learning\n\n\nAssociation Rule Learning: Uncovering rules that describe how data points relate to each other."
  },
  {
    "objectID": "qmd/islp9.html#data-mining-a-deeper-dive---clustering",
    "href": "qmd/islp9.html#data-mining-a-deeper-dive---clustering",
    "title": "Support Vector Machines",
    "section": "Data Mining: A Deeper Dive - Clustering",
    "text": "Data Mining: A Deeper Dive - Clustering\n\n\nClustering: Grouping similar data points together. This is useful for customer segmentation or identifying different types of network traffic."
  },
  {
    "objectID": "qmd/islp9.html#data-mining-a-deeper-dive---classification",
    "href": "qmd/islp9.html#data-mining-a-deeper-dive---classification",
    "title": "Support Vector Machines",
    "section": "Data Mining: A Deeper Dive - Classification",
    "text": "Data Mining: A Deeper Dive - Classification\n\n\nClassification: Assigning data points to predefined categories. This is what SVMs excel at!"
  },
  {
    "objectID": "qmd/islp9.html#what-is-machine-learning",
    "href": "qmd/islp9.html#what-is-machine-learning",
    "title": "Support Vector Machines",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\n\nMachine learning (ML) is a branch of artificial intelligence (AI) that empowers computers to learn from data without the need for explicit programming. ü§ñ Think of it as teaching a computer to learn by example, much like we teach children! üë∂ Algorithms in machine learning are designed to improve their performance on specific tasks as they are exposed to more data.\n\n\n\n\n\n\n\nNote\n\n\n\nMachine Learning involves algorithms that can improve their performance on a task as they are exposed to more data."
  },
  {
    "objectID": "qmd/islp9.html#machine-learning-key-concepts",
    "href": "qmd/islp9.html#machine-learning-key-concepts",
    "title": "Support Vector Machines",
    "section": "Machine Learning: Key Concepts",
    "text": "Machine Learning: Key Concepts\n\nLet‚Äôs explore some fundamental concepts in Machine Learning."
  },
  {
    "objectID": "qmd/islp9.html#machine-learning-supervised-learning",
    "href": "qmd/islp9.html#machine-learning-supervised-learning",
    "title": "Support Vector Machines",
    "section": "Machine Learning: Supervised Learning",
    "text": "Machine Learning: Supervised Learning\n\n\nSupervised Learning: The algorithm learns from labeled data (input-output pairs). This is the category SVMs fall into."
  },
  {
    "objectID": "qmd/islp9.html#machine-learning-unsupervised-learning",
    "href": "qmd/islp9.html#machine-learning-unsupervised-learning",
    "title": "Support Vector Machines",
    "section": "Machine Learning: Unsupervised Learning",
    "text": "Machine Learning: Unsupervised Learning\n\n\nUnsupervised Learning: The algorithm learns from unlabeled data, discovering patterns and structures on its own."
  },
  {
    "objectID": "qmd/islp9.html#machine-learning-reinforcement-learning",
    "href": "qmd/islp9.html#machine-learning-reinforcement-learning",
    "title": "Support Vector Machines",
    "section": "Machine Learning: Reinforcement Learning",
    "text": "Machine Learning: Reinforcement Learning\n\n\nReinforcement Learning: The algorithm learns through trial and error, receiving rewards or penalties for its actions."
  },
  {
    "objectID": "qmd/islp9.html#machine-learning-features",
    "href": "qmd/islp9.html#machine-learning-features",
    "title": "Support Vector Machines",
    "section": "Machine Learning: Features",
    "text": "Machine Learning: Features\n\n\nFeatures: The input variables used for learning."
  },
  {
    "objectID": "qmd/islp9.html#machine-learning-model",
    "href": "qmd/islp9.html#machine-learning-model",
    "title": "Support Vector Machines",
    "section": "Machine Learning: Model",
    "text": "Machine Learning: Model\n\n\nModel: The mathematical representation learned from the data."
  },
  {
    "objectID": "qmd/islp9.html#machine-learning-training",
    "href": "qmd/islp9.html#machine-learning-training",
    "title": "Support Vector Machines",
    "section": "Machine Learning: Training",
    "text": "Machine Learning: Training\n\n\nTraining: The process of fitting a model to the training data."
  },
  {
    "objectID": "qmd/islp9.html#machine-learning-testing",
    "href": "qmd/islp9.html#machine-learning-testing",
    "title": "Support Vector Machines",
    "section": "Machine Learning: Testing",
    "text": "Machine Learning: Testing\n\n\nTesting: Evaluating the model‚Äôs performance on unseen data."
  },
  {
    "objectID": "qmd/islp9.html#what-is-statistical-learning",
    "href": "qmd/islp9.html#what-is-statistical-learning",
    "title": "Support Vector Machines",
    "section": "What is Statistical Learning?",
    "text": "What is Statistical Learning?\n\nStatistical learning is a fascinating blend of statistics and machine learning. ü§î It‚Äôs all about developing and applying models and algorithms for prediction and inference. However, it places a strong emphasis on statistical properties and interpretability. Think of it as bridging the gap between theoretical statistics and practical machine learning, providing a robust framework to understand why certain models excel and how to enhance them.\n\n\n\n\n\n\n\nNote\n\n\n\nStatistical learning focuses on developing and applying models and algorithms for prediction and inference."
  },
  {
    "objectID": "qmd/islp9.html#statistical-learning-the-core-principles",
    "href": "qmd/islp9.html#statistical-learning-the-core-principles",
    "title": "Support Vector Machines",
    "section": "Statistical Learning: The Core Principles",
    "text": "Statistical Learning: The Core Principles\n\nHere are the core ideas driving Statistical Learning."
  },
  {
    "objectID": "qmd/islp9.html#statistical-learning-statistical-models",
    "href": "qmd/islp9.html#statistical-learning-statistical-models",
    "title": "Support Vector Machines",
    "section": "Statistical Learning: Statistical Models",
    "text": "Statistical Learning: Statistical Models\n\n\nStatistical Models: Using statistical models (like linear regression, logistic regression, etc.) to represent relationships in data."
  },
  {
    "objectID": "qmd/islp9.html#statistical-learning-inference",
    "href": "qmd/islp9.html#statistical-learning-inference",
    "title": "Support Vector Machines",
    "section": "Statistical Learning: Inference",
    "text": "Statistical Learning: Inference\n\n\nInference: Drawing conclusions about the population from the sample data."
  },
  {
    "objectID": "qmd/islp9.html#statistical-learning-prediction",
    "href": "qmd/islp9.html#statistical-learning-prediction",
    "title": "Support Vector Machines",
    "section": "Statistical Learning: Prediction",
    "text": "Statistical Learning: Prediction\n\n\nPrediction: Estimating future outcomes based on the learned model."
  },
  {
    "objectID": "qmd/islp9.html#statistical-learning-bias-variance-trade-off",
    "href": "qmd/islp9.html#statistical-learning-bias-variance-trade-off",
    "title": "Support Vector Machines",
    "section": "Statistical Learning: Bias-Variance Trade-off",
    "text": "Statistical Learning: Bias-Variance Trade-off\n\n\nBias-Variance Trade-off: A fundamental concept in statistical learning. Balancing the model‚Äôs ability to fit the training data (low bias) with its ability to generalize to new data (low variance)."
  },
  {
    "objectID": "qmd/islp9.html#statistical-learning-model-selection",
    "href": "qmd/islp9.html#statistical-learning-model-selection",
    "title": "Support Vector Machines",
    "section": "Statistical Learning: Model Selection",
    "text": "Statistical Learning: Model Selection\n\n\nModel Selection: Choosing the best model from a set of candidate models."
  },
  {
    "objectID": "qmd/islp9.html#statistical-learning-regularization",
    "href": "qmd/islp9.html#statistical-learning-regularization",
    "title": "Support Vector Machines",
    "section": "Statistical Learning: Regularization",
    "text": "Statistical Learning: Regularization\n\n\nRegularization: Techniques to prevent overfitting by adding a penalty to the model‚Äôs complexity."
  },
  {
    "objectID": "qmd/islp9.html#relationship-between-concepts",
    "href": "qmd/islp9.html#relationship-between-concepts",
    "title": "Support Vector Machines",
    "section": "Relationship Between Concepts",
    "text": "Relationship Between Concepts\n\n\n\n\n\ngraph LR\n    A[Data Mining] --&gt; C(Common Ground)\n    B[Machine Learning] --&gt; C\n    D[Statistical Learning] --&gt; C\n    C --&gt; E[Insights & Predictions]"
  },
  {
    "objectID": "qmd/islp9.html#relationship-between-concepts-explained",
    "href": "qmd/islp9.html#relationship-between-concepts-explained",
    "title": "Support Vector Machines",
    "section": "Relationship Between Concepts (Explained)",
    "text": "Relationship Between Concepts (Explained)\n\nLet‚Äôs break down the diagram:\n\nData Mining is the overarching field, encompassing the entire process of knowledge discovery from data. Think of it as the big picture. üñºÔ∏è\nMachine Learning provides the algorithms that enable computers to learn from data. It‚Äôs your trusty toolbox. üß∞\nStatistical Learning offers a theoretical framework for understanding and improving these algorithms. It emphasizes statistical rigor and interpretability. It‚Äôs the blueprint that guides your work. üìê\nThey all converge on a Common Ground: leveraging data to generate insights and make predictions. They‚Äôre all working towards the same goal!"
  },
  {
    "objectID": "qmd/islp9.html#support-vector-machines-overview",
    "href": "qmd/islp9.html#support-vector-machines-overview",
    "title": "Support Vector Machines",
    "section": "Support Vector Machines: Overview",
    "text": "Support Vector Machines: Overview\n\nSVM is a generalization of a simpler classifier called the maximal margin classifier. We will systematically explore the following concepts, each building upon the previous:"
  },
  {
    "objectID": "qmd/islp9.html#support-vector-machines-overview---maximal-margin-classifier",
    "href": "qmd/islp9.html#support-vector-machines-overview---maximal-margin-classifier",
    "title": "Support Vector Machines",
    "section": "Support Vector Machines: Overview - Maximal Margin Classifier",
    "text": "Support Vector Machines: Overview - Maximal Margin Classifier\n\n\nMaximal Margin Classifier: This is the foundation, but it has a limitation ‚Äì it only works when data can be perfectly separated by a straight line (or a hyperplane in higher dimensions)."
  },
  {
    "objectID": "qmd/islp9.html#support-vector-machines-overview---support-vector-classifier",
    "href": "qmd/islp9.html#support-vector-machines-overview---support-vector-classifier",
    "title": "Support Vector Machines",
    "section": "Support Vector Machines: Overview - Support Vector Classifier",
    "text": "Support Vector Machines: Overview - Support Vector Classifier\n\n\nSupport Vector Classifier: This is an extension that allows for some mistakes (a ‚Äúsoft margin‚Äù). This makes it applicable to a wider range of datasets, even those that aren‚Äôt perfectly separable."
  },
  {
    "objectID": "qmd/islp9.html#support-vector-machines-overview---support-vector-machine",
    "href": "qmd/islp9.html#support-vector-machines-overview---support-vector-machine",
    "title": "Support Vector Machines",
    "section": "Support Vector Machines: Overview - Support Vector Machine",
    "text": "Support Vector Machines: Overview - Support Vector Machine\n\n\nSupport Vector Machine: This is a further extension that uses a clever trick called ‚Äúkernels‚Äù to handle situations where the boundary between classes is not a straight line.\n\n\n\n\nNote: People often use ‚Äúsupport vector machines‚Äù as a blanket term. We‚Äôll be precise in distinguishing between the three concepts."
  },
  {
    "objectID": "qmd/islp9.html#maximal-margin-classifier",
    "href": "qmd/islp9.html#maximal-margin-classifier",
    "title": "Support Vector Machines",
    "section": "9.1 Maximal Margin Classifier",
    "text": "9.1 Maximal Margin Classifier\n\n9.1.1 What is a Hyperplane?\n\nA hyperplane is a flat, affine subspace with a dimension one less than its surrounding space. It‚Äôs a generalization of familiar concepts like lines and planes. Think of it as a ‚Äúdivider‚Äù in higher dimensions. It separates the space into two halves."
  },
  {
    "objectID": "qmd/islp9.html#hyperplane-dimensions",
    "href": "qmd/islp9.html#hyperplane-dimensions",
    "title": "Support Vector Machines",
    "section": "Hyperplane: Dimensions",
    "text": "Hyperplane: Dimensions\n\nLet‚Äôs understand Hyperplanes across different dimensions."
  },
  {
    "objectID": "qmd/islp9.html#hyperplane-2d",
    "href": "qmd/islp9.html#hyperplane-2d",
    "title": "Support Vector Machines",
    "section": "Hyperplane: 2D",
    "text": "Hyperplane: 2D\n\n\nIn 2D: A hyperplane is simply a line."
  },
  {
    "objectID": "qmd/islp9.html#hyperplane-3d",
    "href": "qmd/islp9.html#hyperplane-3d",
    "title": "Support Vector Machines",
    "section": "Hyperplane: 3D",
    "text": "Hyperplane: 3D\n\n\nIn 3D: A hyperplane is a plane."
  },
  {
    "objectID": "qmd/islp9.html#hyperplane-p-dimensions",
    "href": "qmd/islp9.html#hyperplane-p-dimensions",
    "title": "Support Vector Machines",
    "section": "Hyperplane: p Dimensions",
    "text": "Hyperplane: p Dimensions\n\n\nIn p dimensions: A hyperplane is a (p-1)-dimensional flat subspace, which divides the space into two half-spaces."
  },
  {
    "objectID": "qmd/islp9.html#hyperplane-mathematical-definition",
    "href": "qmd/islp9.html#hyperplane-mathematical-definition",
    "title": "Support Vector Machines",
    "section": "Hyperplane: Mathematical Definition",
    "text": "Hyperplane: Mathematical Definition\n\nA hyperplane in p-dimensional space is defined by the equation:\n\n\\[\n\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p = 0\n\\]"
  },
  {
    "objectID": "qmd/islp9.html#hyperplane-mathematical-definition---explanation",
    "href": "qmd/islp9.html#hyperplane-mathematical-definition---explanation",
    "title": "Support Vector Machines",
    "section": "Hyperplane: Mathematical Definition - Explanation",
    "text": "Hyperplane: Mathematical Definition - Explanation\n\n\n\\(X = (X_1, X_2, ..., X_p)^T\\) represents a point in p-dimensional space.\n\\(\\beta_0, \\beta_1, ..., \\beta_p\\) are the parameters (coefficients) of the hyperplane. These parameters determine the orientation and position of the hyperplane. Changing these values changes the hyperplane."
  },
  {
    "objectID": "qmd/islp9.html#hyperplane-sides",
    "href": "qmd/islp9.html#hyperplane-sides",
    "title": "Support Vector Machines",
    "section": "Hyperplane: Sides",
    "text": "Hyperplane: Sides\n\nThe hyperplane equation neatly divides the space into two regions:"
  },
  {
    "objectID": "qmd/islp9.html#hyperplane-sides---greater-than-zero",
    "href": "qmd/islp9.html#hyperplane-sides---greater-than-zero",
    "title": "Support Vector Machines",
    "section": "Hyperplane: Sides - Greater than Zero",
    "text": "Hyperplane: Sides - Greater than Zero\n\n\n\\(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p &gt; 0\\): Points on one side of the hyperplane."
  },
  {
    "objectID": "qmd/islp9.html#hyperplane-sides---less-than-zero",
    "href": "qmd/islp9.html#hyperplane-sides---less-than-zero",
    "title": "Support Vector Machines",
    "section": "Hyperplane: Sides - Less than Zero",
    "text": "Hyperplane: Sides - Less than Zero\n\n\n\\(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p &lt; 0\\): Points on the other side of the hyperplane."
  },
  {
    "objectID": "qmd/islp9.html#hyperplane-sides---sign-interpretation",
    "href": "qmd/islp9.html#hyperplane-sides---sign-interpretation",
    "title": "Support Vector Machines",
    "section": "Hyperplane: Sides - Sign Interpretation",
    "text": "Hyperplane: Sides - Sign Interpretation\n\n\nThe sign of the left-hand side tells you which side a point is on."
  },
  {
    "objectID": "qmd/islp9.html#hyperplane-example-2d",
    "href": "qmd/islp9.html#hyperplane-example-2d",
    "title": "Support Vector Machines",
    "section": "Hyperplane Example (2D)",
    "text": "Hyperplane Example (2D)\n\n\n\nHyperplane in 2D\n\n\n\n\nFIGURE 9.1: The blue area shows where \\(1 + 2X_1 + 3X_2 &gt; 0\\)."
  },
  {
    "objectID": "qmd/islp9.html#hyperplane-example-2d-1",
    "href": "qmd/islp9.html#hyperplane-example-2d-1",
    "title": "Support Vector Machines",
    "section": "Hyperplane Example (2D)",
    "text": "Hyperplane Example (2D)\n\n\n\nHyperplane in 2D\n\n\n\n\nThe purple area shows where \\(1 + 2X_1 + 3X_2 &lt; 0\\)."
  },
  {
    "objectID": "qmd/islp9.html#hyperplane-example-2d-2",
    "href": "qmd/islp9.html#hyperplane-example-2d-2",
    "title": "Support Vector Machines",
    "section": "Hyperplane Example (2D)",
    "text": "Hyperplane Example (2D)\n\n\n\nHyperplane in 2D\n\n\n\n\nThe solid line is the hyperplane, defined by \\(1 + 2X_1 + 3X_2 = 0\\). Points on the line satisfy the equation.\n\n\n\n9.1.2 Classification Using a Separating Hyperplane\n\nLet‚Äôs say we have training data with n observations and p features. These observations fall into two classes, which we‚Äôll label as -1 and 1:\n\n\\[\nX = \\begin{bmatrix}\nx_{11} & \\cdots & x_{1p} \\\\\n\\vdots & \\ddots & \\vdots \\\\\nx_{n1} & \\cdots & x_{np}\n\\end{bmatrix},\n\\quad\ny = \\begin{bmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix},\n\\quad y_i \\in \\{-1, 1\\}\n\\]"
  },
  {
    "objectID": "qmd/islp9.html#classification-using-a-separating-hyperplane---explanation",
    "href": "qmd/islp9.html#classification-using-a-separating-hyperplane---explanation",
    "title": "Support Vector Machines",
    "section": "Classification Using a Separating Hyperplane - Explanation",
    "text": "Classification Using a Separating Hyperplane - Explanation\n\n\n\\(X\\) is the feature matrix (our input data).\n\\(y\\) is the vector of class labels (what we want to predict)."
  },
  {
    "objectID": "qmd/islp9.html#separating-hyperplane-condition",
    "href": "qmd/islp9.html#separating-hyperplane-condition",
    "title": "Support Vector Machines",
    "section": "Separating Hyperplane Condition",
    "text": "Separating Hyperplane Condition\n\nIf a separating hyperplane exists (meaning the classes are linearly separable ‚Äì we can draw a straight line/plane to divide them), it must satisfy this condition:\n\\[\ny_i(\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_px_{ip}) &gt; 0\n\\]\nfor all i = 1, ‚Ä¶, n."
  },
  {
    "objectID": "qmd/islp9.html#separating-hyperplane-condition---explained",
    "href": "qmd/islp9.html#separating-hyperplane-condition---explained",
    "title": "Support Vector Machines",
    "section": "Separating Hyperplane Condition - Explained",
    "text": "Separating Hyperplane Condition - Explained\n\nThis is a crucial condition! It means that all points are correctly classified. The hyperplane perfectly separates the two classes. The \\(y_i\\) term ensures that the sign is always positive, regardless of the class."
  },
  {
    "objectID": "qmd/islp9.html#classification-rule",
    "href": "qmd/islp9.html#classification-rule",
    "title": "Support Vector Machines",
    "section": "Classification Rule",
    "text": "Classification Rule\n\nWe can classify a new test observation \\(x^* = (x_1^*, x_2^*, ..., x_p^*)^T\\) by simply looking at the sign of:\n\\[\nf(x^*) = \\beta_0 + \\beta_1x_1^* + \\beta_2x_2^* + \\dots + \\beta_px_p^*\n\\]"
  },
  {
    "objectID": "qmd/islp9.html#classification-rule---explanation",
    "href": "qmd/islp9.html#classification-rule---explanation",
    "title": "Support Vector Machines",
    "section": "Classification Rule - Explanation",
    "text": "Classification Rule - Explanation\n\n\nIf \\(f(x^*)\\)&gt; 0, we assign the observation to class 1.\nIf \\(f(x^*)\\)&lt; 0, we assign the observation to class -1.\nThe magnitude (absolute value) of \\(f(x^*)\\) indicates the confidence of the classification. A larger magnitude means the point is farther from the hyperplane, and we‚Äôre more confident in our classification. üí™"
  },
  {
    "objectID": "qmd/islp9.html#separating-hyperplanes-visual---multiple-hyperplanes",
    "href": "qmd/islp9.html#separating-hyperplanes-visual---multiple-hyperplanes",
    "title": "Support Vector Machines",
    "section": "Separating Hyperplanes (Visual) - Multiple Hyperplanes",
    "text": "Separating Hyperplanes (Visual) - Multiple Hyperplanes\n\n\n\nSeparating Hyperplanes\n\n\n\nFIGURE 9.2 Left: When data is linearly separable, there are many possible separating hyperplanes that can perfectly divide the classes."
  },
  {
    "objectID": "qmd/islp9.html#separating-hyperplanes-visual---question",
    "href": "qmd/islp9.html#separating-hyperplanes-visual---question",
    "title": "Support Vector Machines",
    "section": "Separating Hyperplanes (Visual) - Question",
    "text": "Separating Hyperplanes (Visual) - Question\n\n\n\nSeparating Hyperplanes\n\n\n\nThis raises a critical question: Which one should we choose? ü§î We don‚Äôt want just any separating hyperplane; we want the best one."
  },
  {
    "objectID": "qmd/islp9.html#separating-hyperplanes-visual---decision-boundary",
    "href": "qmd/islp9.html#separating-hyperplanes-visual---decision-boundary",
    "title": "Support Vector Machines",
    "section": "Separating Hyperplanes (Visual) - Decision Boundary",
    "text": "Separating Hyperplanes (Visual) - Decision Boundary\n\n\n\nSeparating Hyperplanes\n\n\n\nFIGURE 9.2 Right: The decision boundary created by a separating hyperplane."
  },
  {
    "objectID": "qmd/islp9.html#separating-hyperplanes-visual---classification-grid",
    "href": "qmd/islp9.html#separating-hyperplanes-visual---classification-grid",
    "title": "Support Vector Machines",
    "section": "Separating Hyperplanes (Visual) - Classification Grid",
    "text": "Separating Hyperplanes (Visual) - Classification Grid\n\n\n\nSeparating Hyperplanes\n\n\n\nThe blue and purple grids show how test observations would be classified, based on which side of the hyperplane they fall on.\n\n\n9.1.3 The Maximal Margin Classifier\n\nIf our data can be perfectly separated by a hyperplane, we have an infinite number of choices. The maximal margin classifier provides a principled way to choose the best one. It selects the hyperplane that maximizes the margin."
  },
  {
    "objectID": "qmd/islp9.html#maximal-margin-classifier---margin-definition",
    "href": "qmd/islp9.html#maximal-margin-classifier---margin-definition",
    "title": "Support Vector Machines",
    "section": "Maximal Margin Classifier - Margin Definition",
    "text": "Maximal Margin Classifier - Margin Definition\n\n\nMargin: The smallest distance from the hyperplane to any training observation. It‚Äôs like creating the widest possible ‚Äústreet‚Äù üõ£Ô∏è separating the classes, with the hyperplane running down the middle."
  },
  {
    "objectID": "qmd/islp9.html#maximal-margin-classifier---hyperplane-definition",
    "href": "qmd/islp9.html#maximal-margin-classifier---hyperplane-definition",
    "title": "Support Vector Machines",
    "section": "Maximal Margin Classifier - Hyperplane Definition",
    "text": "Maximal Margin Classifier - Hyperplane Definition\n\n\nMaximal Margin Hyperplane: The separating hyperplane that achieves the largest possible margin. This is the hyperplane we want!"
  },
  {
    "objectID": "qmd/islp9.html#maximal-margin-intuition",
    "href": "qmd/islp9.html#maximal-margin-intuition",
    "title": "Support Vector Machines",
    "section": "Maximal Margin Intuition",
    "text": "Maximal Margin Intuition\n\nThe maximal margin hyperplane is the ‚Äúmid-line‚Äù of the widest ‚Äúslab‚Äù (or ‚Äústreet‚Äù) that we can fit between the two classes without touching any data points."
  },
  {
    "objectID": "qmd/islp9.html#maximal-margin-intuition---generalization",
    "href": "qmd/islp9.html#maximal-margin-intuition---generalization",
    "title": "Support Vector Machines",
    "section": "Maximal Margin Intuition - Generalization",
    "text": "Maximal Margin Intuition - Generalization\n\nThe intuition is that a larger margin on the training data will likely lead to a larger margin on the test data, resulting in better classification performance. We want the biggest ‚Äúbuffer zone‚Äù we can get."
  },
  {
    "objectID": "qmd/islp9.html#maximal-margin-classifier-visual",
    "href": "qmd/islp9.html#maximal-margin-classifier-visual",
    "title": "Support Vector Machines",
    "section": "Maximal Margin Classifier (Visual)",
    "text": "Maximal Margin Classifier (Visual)\n\n\n\nMaximal Margin Classifier\n\n\n\nFIGURE 9.3: The maximal margin hyperplane is shown as a solid line."
  },
  {
    "objectID": "qmd/islp9.html#maximal-margin-classifier-visual---margin",
    "href": "qmd/islp9.html#maximal-margin-classifier-visual---margin",
    "title": "Support Vector Machines",
    "section": "Maximal Margin Classifier (Visual) - Margin",
    "text": "Maximal Margin Classifier (Visual) - Margin\n\n\n\nMaximal Margin Classifier\n\n\n\nThe dashed lines define the margin."
  },
  {
    "objectID": "qmd/islp9.html#maximal-margin-classifier-visual---support-vectors",
    "href": "qmd/islp9.html#maximal-margin-classifier-visual---support-vectors",
    "title": "Support Vector Machines",
    "section": "Maximal Margin Classifier (Visual) - Support Vectors",
    "text": "Maximal Margin Classifier (Visual) - Support Vectors\n\n\n\nMaximal Margin Classifier\n\n\n\nThe points touching the dashed lines are the support vectors ‚Äì they are crucial!"
  },
  {
    "objectID": "qmd/islp9.html#support-vectors",
    "href": "qmd/islp9.html#support-vectors",
    "title": "Support Vector Machines",
    "section": "Support Vectors",
    "text": "Support Vectors\n\n\nSupport Vectors: The training observations that lie exactly on the margin (the dashed lines in the previous figure). These are the ‚Äúcritical‚Äù points.\nThese points support the maximal margin hyperplane, meaning they determine its position and orientation. If these points move, the hyperplane must move! üîÑ\nA key property of the maximal margin hyperplane is that it depends only on the support vectors, not on any other observations. This is a very important characteristic!\n\n\n\n9.1.4 Construction of the Maximal Margin Classifier\n\nThe maximal margin hyperplane is found by solving a specific optimization problem:\n\n\\[\n\\begin{aligned}\n&\\underset{\\beta_0, \\beta_1, \\dots, \\beta_p, M}{\\text{maximize}} && M \\\\\n&\\text{subject to} && \\sum_{j=1}^p \\beta_j^2 = 1, \\\\\n& && y_i(\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_px_{ip}) \\geq M \\quad \\forall i = 1, \\dots, n.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "qmd/islp9.html#maximal-margin-classifier-optimization-problem-explained---m",
    "href": "qmd/islp9.html#maximal-margin-classifier-optimization-problem-explained---m",
    "title": "Support Vector Machines",
    "section": "Maximal Margin Classifier: Optimization Problem Explained - M",
    "text": "Maximal Margin Classifier: Optimization Problem Explained - M\n\n\nM: The margin width, which we want to maximize. We‚Äôre trying to find the widest possible ‚Äústreet.‚Äù"
  },
  {
    "objectID": "qmd/islp9.html#maximal-margin-classifier-optimization-problem-explained---constraint-1",
    "href": "qmd/islp9.html#maximal-margin-classifier-optimization-problem-explained---constraint-1",
    "title": "Support Vector Machines",
    "section": "Maximal Margin Classifier: Optimization Problem Explained - Constraint 1",
    "text": "Maximal Margin Classifier: Optimization Problem Explained - Constraint 1\n\n\n\\(\\sum_{j=1}^p \\beta_j^2 = 1\\): This constraint ensures a unique solution. It doesn‚Äôt restrict the hyperplane itself (we can always rescale the coefficients), but it scales the coefficients so we get a specific solution."
  },
  {
    "objectID": "qmd/islp9.html#maximal-margin-classifier-optimization-problem-explained---constraint-2",
    "href": "qmd/islp9.html#maximal-margin-classifier-optimization-problem-explained---constraint-2",
    "title": "Support Vector Machines",
    "section": "Maximal Margin Classifier: Optimization Problem Explained - Constraint 2",
    "text": "Maximal Margin Classifier: Optimization Problem Explained - Constraint 2\n\n\n\\(y_i(\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_px_{ip}) \\geq M\\): This is the most important constraint. It ensures that all observations are on the correct side of the hyperplane and at least a distance M away (i.e., outside the margin). This is what guarantees correct classification and a margin of at least M.\n\n\n\n9.1.5 The Non-separable Case\n\nThe maximal margin classifier has a significant limitation: it works only if a separating hyperplane exists. If the classes overlap, even a little bit, no such hyperplane exists, and the optimization problem has no solution. üôÅ This is a major drawback in real-world scenarios."
  },
  {
    "objectID": "qmd/islp9.html#non-separable-data-visual",
    "href": "qmd/islp9.html#non-separable-data-visual",
    "title": "Support Vector Machines",
    "section": "Non-separable Data (Visual)",
    "text": "Non-separable Data (Visual)\n\n\n\nNon-separable Data\n\n\n\nFIGURE 9.4: An example where the classes are not linearly separable."
  },
  {
    "objectID": "qmd/islp9.html#non-separable-data-visual---explanation",
    "href": "qmd/islp9.html#non-separable-data-visual---explanation",
    "title": "Support Vector Machines",
    "section": "Non-separable Data (Visual) - Explanation",
    "text": "Non-separable Data (Visual) - Explanation\n\n\n\nNon-separable Data\n\n\n\nThere‚Äôs no straight line we can draw to perfectly separate the blue and purple points. The maximal margin classifier cannot be used here."
  },
  {
    "objectID": "qmd/islp9.html#support-vector-classifiers",
    "href": "qmd/islp9.html#support-vector-classifiers",
    "title": "Support Vector Machines",
    "section": "9.2 Support Vector Classifiers",
    "text": "9.2 Support Vector Classifiers\n\n9.2.1 Overview of the Support Vector Classifier\n\nTo address the limitations of the maximal margin classifier (its inability to handle non-separable data and its sensitivity to individual observations), we introduce the support vector classifier, also known as the soft margin classifier."
  },
  {
    "objectID": "qmd/islp9.html#support-vector-classifier---the-key-idea",
    "href": "qmd/islp9.html#support-vector-classifier---the-key-idea",
    "title": "Support Vector Machines",
    "section": "Support Vector Classifier - The Key Idea",
    "text": "Support Vector Classifier - The Key Idea\n\nThe key idea is to allow some observations to be on the wrong side of the margin or even the wrong side of the hyperplane. This makes the margin ‚Äúsoft‚Äù ‚òÅÔ∏è, allowing for some flexibility."
  },
  {
    "objectID": "qmd/islp9.html#why-soft-margins",
    "href": "qmd/islp9.html#why-soft-margins",
    "title": "Support Vector Machines",
    "section": "Why Soft Margins?",
    "text": "Why Soft Margins?\n\nLet‚Äôs see why we need soft margins."
  },
  {
    "objectID": "qmd/islp9.html#why-soft-margins---non-separable-data",
    "href": "qmd/islp9.html#why-soft-margins---non-separable-data",
    "title": "Support Vector Machines",
    "section": "Why Soft Margins? - Non-Separable Data",
    "text": "Why Soft Margins? - Non-Separable Data\n\n\nNon-Separable Data: It‚Äôs absolutely necessary for overlapping classes, where perfect separation is simply impossible."
  },
  {
    "objectID": "qmd/islp9.html#why-soft-margins---robustness",
    "href": "qmd/islp9.html#why-soft-margins---robustness",
    "title": "Support Vector Machines",
    "section": "Why Soft Margins? - Robustness",
    "text": "Why Soft Margins? - Robustness\n\n\nRobustness: It makes the classifier less sensitive to individual observations, which helps to reduce overfitting. A single outlier shouldn‚Äôt drastically change the decision boundary."
  },
  {
    "objectID": "qmd/islp9.html#sensitivity-to-outliers-visual",
    "href": "qmd/islp9.html#sensitivity-to-outliers-visual",
    "title": "Support Vector Machines",
    "section": "Sensitivity to Outliers (Visual)",
    "text": "Sensitivity to Outliers (Visual)\n\n\n\nSensitivity to Outliers\n\n\n\nFIGURE 9.5: This figure demonstrates the sensitivity of the maximal margin classifier."
  },
  {
    "objectID": "qmd/islp9.html#sensitivity-to-outliers-visual---explanation",
    "href": "qmd/islp9.html#sensitivity-to-outliers-visual---explanation",
    "title": "Support Vector Machines",
    "section": "Sensitivity to Outliers (Visual) - Explanation",
    "text": "Sensitivity to Outliers (Visual) - Explanation\n\n\n\nSensitivity to Outliers\n\n\n\nAdding just one outlier (right panel) dramatically changes the maximal margin hyperplane. This highlights the need for a more robust approach."
  },
  {
    "objectID": "qmd/islp9.html#soft-margin-example-visual---margin-violations",
    "href": "qmd/islp9.html#soft-margin-example-visual---margin-violations",
    "title": "Support Vector Machines",
    "section": "Soft Margin Example (Visual) - Margin Violations",
    "text": "Soft Margin Example (Visual) - Margin Violations\n\n\n\nSoft Margin Example\n\n\n\nFIGURE 9.6 Left: In the soft margin classifier, most points are correctly classified and outside the margin."
  },
  {
    "objectID": "qmd/islp9.html#soft-margin-example-visual---margin-violations-explanation",
    "href": "qmd/islp9.html#soft-margin-example-visual---margin-violations-explanation",
    "title": "Support Vector Machines",
    "section": "Soft Margin Example (Visual) - Margin Violations Explanation",
    "text": "Soft Margin Example (Visual) - Margin Violations Explanation\n\n\n\nSoft Margin Example\n\n\n\nHowever, some points violate the margin ‚Äì they are allowed to be inside the ‚Äústreet.‚Äù"
  },
  {
    "objectID": "qmd/islp9.html#soft-margin-example-visual---misclassifications",
    "href": "qmd/islp9.html#soft-margin-example-visual---misclassifications",
    "title": "Support Vector Machines",
    "section": "Soft Margin Example (Visual) - Misclassifications",
    "text": "Soft Margin Example (Visual) - Misclassifications\n\n\n\nSoft Margin Example\n\n\n\nFIGURE 9.6 Right: Some points are even misclassified (they are on the wrong side of the hyperplane)."
  },
  {
    "objectID": "qmd/islp9.html#soft-margin-example-visual---misclassifications-explanation",
    "href": "qmd/islp9.html#soft-margin-example-visual---misclassifications-explanation",
    "title": "Support Vector Machines",
    "section": "Soft Margin Example (Visual) - Misclassifications Explanation",
    "text": "Soft Margin Example (Visual) - Misclassifications Explanation\n\n\n\nSoft Margin Example\n\n\n\nThe soft margin allows for this to happen, but the extent of these violations is controlled by a tuning parameter.\n\n\n9.2.2 Details of the Support Vector Classifier\n\nThe support vector classifier solves a modified optimization problem that allows for margin violations:\n\n\\[\n\\begin{aligned}\n&\\underset{\\beta_0, \\beta_1, \\dots, \\beta_p, \\epsilon_1, \\dots, \\epsilon_n, M}{\\text{maximize}} && M \\\\\n&\\text{subject to} && \\sum_{j=1}^p \\beta_j^2 = 1, \\\\\n& && y_i(\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_px_{ip}) \\geq M(1 - \\epsilon_i), \\\\\n& && \\epsilon_i \\geq 0, \\quad \\sum_{i=1}^n \\epsilon_i \\leq C.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "qmd/islp9.html#support-vector-classifier-optimization-problem-explained---slack-variables",
    "href": "qmd/islp9.html#support-vector-classifier-optimization-problem-explained---slack-variables",
    "title": "Support Vector Machines",
    "section": "Support Vector Classifier: Optimization Problem Explained - Slack Variables",
    "text": "Support Vector Classifier: Optimization Problem Explained - Slack Variables\n\n\n\\(\\epsilon_1, \\dots, \\epsilon_n\\): These are called slack variables. They allow observations to be on the wrong side of the margin or hyperplane. Each observation gets its own slack variable."
  },
  {
    "objectID": "qmd/islp9.html#support-vector-classifier-optimization-problem-explained---tuning-parameter",
    "href": "qmd/islp9.html#support-vector-classifier-optimization-problem-explained---tuning-parameter",
    "title": "Support Vector Machines",
    "section": "Support Vector Classifier: Optimization Problem Explained - Tuning Parameter",
    "text": "Support Vector Classifier: Optimization Problem Explained - Tuning Parameter\n\n\nC: A non-negative tuning parameter. Think of it as a ‚Äúbudget‚Äù for violations. It controls the trade-off between maximizing the margin width and minimizing the number and severity of violations."
  },
  {
    "objectID": "qmd/islp9.html#support-vector-classifier-optimization-problem-explained---other-parts",
    "href": "qmd/islp9.html#support-vector-classifier-optimization-problem-explained---other-parts",
    "title": "Support Vector Machines",
    "section": "Support Vector Classifier: Optimization Problem Explained - Other Parts",
    "text": "Support Vector Classifier: Optimization Problem Explained - Other Parts\n\n\nThe other parts (M and the first constraint) are the same as in the maximal margin classifier."
  },
  {
    "objectID": "qmd/islp9.html#slack-variables-explained---no-violation",
    "href": "qmd/islp9.html#slack-variables-explained---no-violation",
    "title": "Support Vector Machines",
    "section": "Slack Variables Explained - No Violation",
    "text": "Slack Variables Explained - No Violation\n\n\n\\(\\epsilon_i = 0\\): The ith observation is on the correct side of the margin (no violation)."
  },
  {
    "objectID": "qmd/islp9.html#slack-variables-explained---margin-violation",
    "href": "qmd/islp9.html#slack-variables-explained---margin-violation",
    "title": "Support Vector Machines",
    "section": "Slack Variables Explained - Margin Violation",
    "text": "Slack Variables Explained - Margin Violation\n\n\n\\(\\epsilon_i &gt; 0\\): The ith observation is on the wrong side of the margin (a margin violation)."
  },
  {
    "objectID": "qmd/islp9.html#slack-variables-explained---misclassification",
    "href": "qmd/islp9.html#slack-variables-explained---misclassification",
    "title": "Support Vector Machines",
    "section": "Slack Variables Explained - Misclassification",
    "text": "Slack Variables Explained - Misclassification\n\n\n\\(\\epsilon_i &gt; 1\\): The ith observation is on the wrong side of the hyperplane (it‚Äôs misclassified)."
  },
  {
    "objectID": "qmd/islp9.html#tuning-parameter-c---zero-budget",
    "href": "qmd/islp9.html#tuning-parameter-c---zero-budget",
    "title": "Support Vector Machines",
    "section": "Tuning Parameter C - Zero Budget",
    "text": "Tuning Parameter C - Zero Budget\n\n\nC = 0: No budget for violations at all. This reduces to the maximal margin classifier (if the data is separable; otherwise, there‚Äôs no solution)."
  },
  {
    "objectID": "qmd/islp9.html#tuning-parameter-c---small-c",
    "href": "qmd/islp9.html#tuning-parameter-c---small-c",
    "title": "Support Vector Machines",
    "section": "Tuning Parameter C - Small C",
    "text": "Tuning Parameter C - Small C\n\n\nSmall C: We prioritize a narrow margin and allow fewer violations. This can lead to a model that is less prone to overfitting but might have higher bias."
  },
  {
    "objectID": "qmd/islp9.html#tuning-parameter-c---large-c",
    "href": "qmd/islp9.html#tuning-parameter-c---large-c",
    "title": "Support Vector Machines",
    "section": "Tuning Parameter C - Large C",
    "text": "Tuning Parameter C - Large C\n\n\nLarge C: We allow a wider margin and more violations. This can lead to a model that might have lower bias but is potentially more prone to overfitting."
  },
  {
    "objectID": "qmd/islp9.html#tuning-parameter-c---cross-validation",
    "href": "qmd/islp9.html#tuning-parameter-c---cross-validation",
    "title": "Support Vector Machines",
    "section": "Tuning Parameter C - Cross-Validation",
    "text": "Tuning Parameter C - Cross-Validation\n\n\nC is typically chosen using cross-validation, a technique for evaluating model performance on unseen data."
  },
  {
    "objectID": "qmd/islp9.html#impact-of-c-visual",
    "href": "qmd/islp9.html#impact-of-c-visual",
    "title": "Support Vector Machines",
    "section": "Impact of C (Visual)",
    "text": "Impact of C (Visual)\n\n\n\nImpact of C\n\n\n\nFIGURE 9.7: This figure shows the effect of different C values."
  },
  {
    "objectID": "qmd/islp9.html#impact-of-c-visual---explanation",
    "href": "qmd/islp9.html#impact-of-c-visual---explanation",
    "title": "Support Vector Machines",
    "section": "Impact of C (Visual) - Explanation",
    "text": "Impact of C (Visual) - Explanation\n\n\n\nImpact of C\n\n\n\nA larger C results in a wider margin and more support vectors (because more points are violating the margin)."
  },
  {
    "objectID": "qmd/islp9.html#support-vectors-revisited",
    "href": "qmd/islp9.html#support-vectors-revisited",
    "title": "Support Vector Machines",
    "section": "Support Vectors (Revisited)",
    "text": "Support Vectors (Revisited)\n\nIn the support vector classifier, the support vectors are the observations that:"
  },
  {
    "objectID": "qmd/islp9.html#support-vectors-revisited---on-margin",
    "href": "qmd/islp9.html#support-vectors-revisited---on-margin",
    "title": "Support Vector Machines",
    "section": "Support Vectors (Revisited) - On Margin",
    "text": "Support Vectors (Revisited) - On Margin\n\n\nLie exactly on the margin (\\(\\epsilon_i = 0\\) and correctly classified)"
  },
  {
    "objectID": "qmd/islp9.html#support-vectors-revisited---wrong-side-of-margin",
    "href": "qmd/islp9.html#support-vectors-revisited---wrong-side-of-margin",
    "title": "Support Vector Machines",
    "section": "Support Vectors (Revisited) - Wrong Side of Margin",
    "text": "Support Vectors (Revisited) - Wrong Side of Margin\n\n\nLie on the wrong side of the margin (\\(0 &lt; \\epsilon_i \\leq 1\\))"
  },
  {
    "objectID": "qmd/islp9.html#support-vectors-revisited---wrong-side-of-hyperplane",
    "href": "qmd/islp9.html#support-vectors-revisited---wrong-side-of-hyperplane",
    "title": "Support Vector Machines",
    "section": "Support Vectors (Revisited) - Wrong Side of Hyperplane",
    "text": "Support Vectors (Revisited) - Wrong Side of Hyperplane\n\n\nLie on the wrong side of the hyperplane (\\(\\epsilon_i &gt; 1\\))"
  },
  {
    "objectID": "qmd/islp9.html#support-vectors-revisited---influence",
    "href": "qmd/islp9.html#support-vectors-revisited---influence",
    "title": "Support Vector Machines",
    "section": "Support Vectors (Revisited) - Influence",
    "text": "Support Vectors (Revisited) - Influence\n\nOnly the support vectors affect the hyperplane‚Äôs position and orientation. Observations that are on the correct side of the margin and sufficiently far away have no influence. This contributes to the robustness of the SVM."
  },
  {
    "objectID": "qmd/islp9.html#support-vector-machines",
    "href": "qmd/islp9.html#support-vector-machines",
    "title": "Support Vector Machines",
    "section": "9.3 Support Vector Machines",
    "text": "9.3 Support Vector Machines\n\n9.3.1 Classification with Non-Linear Decision Boundaries\n\nThe support vector classifier finds linear decision boundaries (hyperplanes). But what if the true boundary between classes is non-linear? ü§î"
  },
  {
    "objectID": "qmd/islp9.html#non-linear-decision-boundaries---feature-expansion",
    "href": "qmd/islp9.html#non-linear-decision-boundaries---feature-expansion",
    "title": "Support Vector Machines",
    "section": "Non-Linear Decision Boundaries - Feature Expansion",
    "text": "Non-Linear Decision Boundaries - Feature Expansion\n\nOne approach is to enlarge the feature space by adding polynomial features (e.g., \\(X_1^2\\), \\(X_1X_2\\), etc.). This can make the data linearly separable in the enlarged feature space. However, this can be computationally expensive."
  },
  {
    "objectID": "qmd/islp9.html#non-linear-data-visual",
    "href": "qmd/islp9.html#non-linear-data-visual",
    "title": "Support Vector Machines",
    "section": "Non-linear Data (Visual)",
    "text": "Non-linear Data (Visual)\n\n\n\nNon-linear Data\n\n\n\nFIGURE 9.8 Left: It‚Äôs clear that a non-linear boundary is needed to separate these classes effectively."
  },
  {
    "objectID": "qmd/islp9.html#non-linear-data-visual---linear-ineffectiveness",
    "href": "qmd/islp9.html#non-linear-data-visual---linear-ineffectiveness",
    "title": "Support Vector Machines",
    "section": "Non-linear Data (Visual) - Linear Ineffectiveness",
    "text": "Non-linear Data (Visual) - Linear Ineffectiveness\n\n\n\nNon-linear Data\n\n\n\nA straight line simply won‚Äôt do."
  },
  {
    "objectID": "qmd/islp9.html#linear-classifier-on-non-linear-data",
    "href": "qmd/islp9.html#linear-classifier-on-non-linear-data",
    "title": "Support Vector Machines",
    "section": "Linear Classifier on Non-linear Data",
    "text": "Linear Classifier on Non-linear Data\n\n\n\nNon-linear Data\n\n\n\nFIGURE 9.8 Right: A linear classifier performs poorly on this data."
  },
  {
    "objectID": "qmd/islp9.html#linear-classifier-on-non-linear-data---explanation",
    "href": "qmd/islp9.html#linear-classifier-on-non-linear-data---explanation",
    "title": "Support Vector Machines",
    "section": "Linear Classifier on Non-linear Data - Explanation",
    "text": "Linear Classifier on Non-linear Data - Explanation\n\n\n\nNon-linear Data\n\n\n\nIt cannot capture the non-linear relationship between the features and the class labels.\n\n\nFeature Expansion: A Potential Problem\n\nExplicitly enlarging the feature space by adding polynomial features or other transformations can become computationally expensive (or even impossible for very high-dimensional or infinite-dimensional feature spaces). üò© We need a more efficient way to handle non-linearity.\n\n\n\n9.3.2 The Support Vector Machine\n\nThe support vector machine (SVM) is a powerful extension of the support vector classifier. It uses kernels to implicitly enlarge the feature space without explicitly calculating the transformed features. This is known as the ‚Äúkernel trick‚Äù! ‚ú® It‚Äôs a brilliant mathematical technique."
  },
  {
    "objectID": "qmd/islp9.html#support-vector-machine---key-idea",
    "href": "qmd/islp9.html#support-vector-machine---key-idea",
    "title": "Support Vector Machines",
    "section": "Support Vector Machine - Key Idea",
    "text": "Support Vector Machine - Key Idea\n\nKey Idea: The solution to the support vector classifier depends only on the inner products of the observations, not on the observations themselves. This is the foundation of the kernel trick."
  },
  {
    "objectID": "qmd/islp9.html#inner-product",
    "href": "qmd/islp9.html#inner-product",
    "title": "Support Vector Machines",
    "section": "Inner Product",
    "text": "Inner Product\n\nInner Product: The inner product between two vectors, \\(x_i\\) and \\(x_{i'}\\), is calculated as:\n\n\\[\n\\langle x_i, x_{i'} \\rangle = \\sum_{j=1}^p x_{ij}x_{i'j}\n\\]"
  },
  {
    "objectID": "qmd/islp9.html#inner-product---explanation",
    "href": "qmd/islp9.html#inner-product---explanation",
    "title": "Support Vector Machines",
    "section": "Inner Product - Explanation",
    "text": "Inner Product - Explanation\n\nThe inner product is a measure of similarity between the two vectors."
  },
  {
    "objectID": "qmd/islp9.html#kernels",
    "href": "qmd/islp9.html#kernels",
    "title": "Support Vector Machines",
    "section": "Kernels",
    "text": "Kernels\n\nA kernel is a function that quantifies the similarity between two observations:\n\n\\[\nK(x_i, x_{i'})\n\\]"
  },
  {
    "objectID": "qmd/islp9.html#kernels---linear-kernel",
    "href": "qmd/islp9.html#kernels---linear-kernel",
    "title": "Support Vector Machines",
    "section": "Kernels - Linear Kernel",
    "text": "Kernels - Linear Kernel\n\n\nLinear Kernel: \\(K(x_i, x_{i'}) = \\langle x_i, x_{i'} \\rangle = \\sum_{j=1}^p x_{ij}x_{i'j}\\). Using this kernel gives you the standard support vector classifier."
  },
  {
    "objectID": "qmd/islp9.html#kernels---polynomial-kernel",
    "href": "qmd/islp9.html#kernels---polynomial-kernel",
    "title": "Support Vector Machines",
    "section": "Kernels - Polynomial Kernel",
    "text": "Kernels - Polynomial Kernel\n\n\nPolynomial Kernel: \\(K(x_i, x_{i'}) = (1 + \\langle x_i, x_{i'} \\rangle)^d\\). This implicitly uses polynomial features of degree d. You don‚Äôt have to explicitly calculate the polynomial features!"
  },
  {
    "objectID": "qmd/islp9.html#kernels---radial-kernel",
    "href": "qmd/islp9.html#kernels---radial-kernel",
    "title": "Support Vector Machines",
    "section": "Kernels - Radial Kernel",
    "text": "Kernels - Radial Kernel\n\n\nRadial Kernel: \\(K(x_i, x_{i'}) = \\exp(-\\gamma \\sum_{j=1}^p (x_{ij} - x_{i'j})^2)\\). This is a very popular choice and corresponds to an infinite-dimensional feature space! It‚Äôs incredibly powerful."
  },
  {
    "objectID": "qmd/islp9.html#the-svm-using-kernels",
    "href": "qmd/islp9.html#the-svm-using-kernels",
    "title": "Support Vector Machines",
    "section": "The SVM: Using Kernels",
    "text": "The SVM: Using Kernels\n\nBy replacing the inner product \\(\\langle x_i, x_{i'} \\rangle\\) with a kernel \\(K(x_i, x_{i'})\\) in the support vector classifier algorithm, we obtain the SVM. The resulting decision function is:\n\n\\[\nf(x) = \\beta_0 + \\sum_{i \\in S} \\alpha_i K(x, x_i)\n\\]"
  },
  {
    "objectID": "qmd/islp9.html#the-svm-using-kernels---explanation",
    "href": "qmd/islp9.html#the-svm-using-kernels---explanation",
    "title": "Support Vector Machines",
    "section": "The SVM: Using Kernels - Explanation",
    "text": "The SVM: Using Kernels - Explanation\n\nwhere S is the set of support vector indices, and the \\(\\alpha_i\\) are parameters learned during training. Notice that the decision function depends only on the support vectors and the kernel function."
  },
  {
    "objectID": "qmd/islp9.html#svm-with-polynomial-kernel-visual",
    "href": "qmd/islp9.html#svm-with-polynomial-kernel-visual",
    "title": "Support Vector Machines",
    "section": "SVM with Polynomial Kernel (Visual)",
    "text": "SVM with Polynomial Kernel (Visual)\n\n\n\nSVM with Polynomial Kernel\n\n\n\nFIGURE 9.9 Left: An SVM with a polynomial kernel (degree 3) fits the non-linear data much better than a linear classifier could."
  },
  {
    "objectID": "qmd/islp9.html#svm-with-polynomial-kernel-visual---decision-boundary",
    "href": "qmd/islp9.html#svm-with-polynomial-kernel-visual---decision-boundary",
    "title": "Support Vector Machines",
    "section": "SVM with Polynomial Kernel (Visual) - Decision Boundary",
    "text": "SVM with Polynomial Kernel (Visual) - Decision Boundary\n\n\n\nSVM with Polynomial Kernel\n\n\n\nThe decision boundary is curved."
  },
  {
    "objectID": "qmd/islp9.html#svm-with-radial-kernel-visual",
    "href": "qmd/islp9.html#svm-with-radial-kernel-visual",
    "title": "Support Vector Machines",
    "section": "SVM with Radial Kernel (Visual)",
    "text": "SVM with Radial Kernel (Visual)\n\n\n\nSVM with Radial Kernel\n\n\n\nFIGURE 9.9 Right: An SVM with a radial kernel also captures the non-linear boundary effectively."
  },
  {
    "objectID": "qmd/islp9.html#radial-kernel-intuition",
    "href": "qmd/islp9.html#radial-kernel-intuition",
    "title": "Support Vector Machines",
    "section": "Radial Kernel Intuition",
    "text": "Radial Kernel Intuition\n\n\nThe radial kernel has local behavior.\nIf a test observation \\(x^*\\) is far from a training observation \\(x_i\\), then \\(K(x^*, x_i)\\) is very small. This means that \\(x_i\\) has very little influence on the prediction for \\(x^*\\).\nOnly nearby training observations have a significant impact on the prediction.\n\\(\\gamma\\) is a tuning parameter that controls the ‚Äúreach‚Äù or ‚Äúwidth‚Äù of the kernel. A larger \\(\\gamma\\) makes the kernel more ‚Äúlocal‚Äù (only very close points have influence), while a smaller \\(\\gamma\\) makes it more ‚Äúglobal‚Äù (more distant points can have some influence)."
  },
  {
    "objectID": "qmd/islp9.html#computational-advantage-of-kernels",
    "href": "qmd/islp9.html#computational-advantage-of-kernels",
    "title": "Support Vector Machines",
    "section": "Computational Advantage of Kernels",
    "text": "Computational Advantage of Kernels\n\nThe crucial advantage of kernels is that we only need to compute \\(K(x_i, x_{i'})\\) for all pairs of training observations. We never need to explicitly compute the (potentially infinite-dimensional!) feature mapping. This makes the computation feasible, even for incredibly complex feature spaces. This is the magic of the ‚Äúkernel trick‚Äù!\n\n\n9.3.3 An Application to the Heart Disease Data\n\nThe text compares SVM to LDA on the Heart Disease Data, using ROC curves for both training and testing."
  },
  {
    "objectID": "qmd/islp9.html#roc-curve-on-training-set",
    "href": "qmd/islp9.html#roc-curve-on-training-set",
    "title": "Support Vector Machines",
    "section": "ROC Curve on Training Set",
    "text": "ROC Curve on Training Set\n\n\n\nROC_train\n\n\n\nFIGURE 9.10 Left: On the training data, the support vector classifier (which is equivalent to an SVM with a linear kernel) performs slightly better than LDA (Linear Discriminant Analysis)."
  },
  {
    "objectID": "qmd/islp9.html#roc-curve-on-training-set-overfitting",
    "href": "qmd/islp9.html#roc-curve-on-training-set-overfitting",
    "title": "Support Vector Machines",
    "section": "ROC Curve on Training Set: Overfitting?",
    "text": "ROC Curve on Training Set: Overfitting?\n\n\n\nROC_train\n\n\n\nFIGURE 9.10 Right: The SVM using a radial basis kernel with \\(\\gamma = 10^{-1}\\) shows almost perfect performance on the training set."
  },
  {
    "objectID": "qmd/islp9.html#roc-curve-on-training-set-overfitting---explanation",
    "href": "qmd/islp9.html#roc-curve-on-training-set-overfitting---explanation",
    "title": "Support Vector Machines",
    "section": "ROC Curve on Training Set: Overfitting? - Explanation",
    "text": "ROC Curve on Training Set: Overfitting? - Explanation\n\n\n\nROC_train\n\n\n\nThis is a strong warning sign of overfitting! The model is likely too complex and has memorized the training data instead of learning the underlying patterns."
  },
  {
    "objectID": "qmd/islp9.html#roc-curve-on-test-set",
    "href": "qmd/islp9.html#roc-curve-on-test-set",
    "title": "Support Vector Machines",
    "section": "ROC Curve on Test Set",
    "text": "ROC Curve on Test Set\n\n\n\nROC_test\n\n\n\nFIGURE 9.11 Left: On the test data, the support vector classifier continues to have a *small advantage over LDA."
  },
  {
    "objectID": "qmd/islp9.html#roc-curve-on-test-set---explanation",
    "href": "qmd/islp9.html#roc-curve-on-test-set---explanation",
    "title": "Support Vector Machines",
    "section": "ROC Curve on Test Set - Explanation",
    "text": "ROC Curve on Test Set - Explanation\n\n\n\nROC_test\n\n\n\nThis is a more reliable measure of performance than the training set results."
  },
  {
    "objectID": "qmd/islp9.html#roc-curve-on-test-set-overfitting-confirmed",
    "href": "qmd/islp9.html#roc-curve-on-test-set-overfitting-confirmed",
    "title": "Support Vector Machines",
    "section": "ROC Curve on Test Set: Overfitting Confirmed",
    "text": "ROC Curve on Test Set: Overfitting Confirmed\n\n\n\nROC_test\n\n\n\nFIGURE 9.11 Right: On the test data, the SVM with the radial basis kernel and \\(\\gamma = 10^{-1}\\) (which had near-perfect training performance) now performs the worst."
  },
  {
    "objectID": "qmd/islp9.html#roc-curve-on-test-set-overfitting-confirmed-explanation",
    "href": "qmd/islp9.html#roc-curve-on-test-set-overfitting-confirmed-explanation",
    "title": "Support Vector Machines",
    "section": "ROC Curve on Test Set: Overfitting Confirmed Explanation",
    "text": "ROC Curve on Test Set: Overfitting Confirmed Explanation\n\n\n\nROC_test\n\n\n\nThis confirms our suspicion of overfitting. The model that looked best on the training data is actually the worst on new data! The SVMs with \\(\\gamma = 10^{-2}\\) and \\(\\gamma = 10^{-3}\\) perform similarly to the support vector classifier (linear kernel), suggesting that a less complex model is better for this dataset."
  },
  {
    "objectID": "qmd/islp9.html#svms-with-more-than-two-classes",
    "href": "qmd/islp9.html#svms-with-more-than-two-classes",
    "title": "Support Vector Machines",
    "section": "9.4 SVMs with More than Two Classes",
    "text": "9.4 SVMs with More than Two Classes\n\nSVMs are naturally designed for binary classification (handling only two classes). To extend them to situations with K &gt; 2 classes, two common strategies are employed:"
  },
  {
    "objectID": "qmd/islp9.html#svms-with-more-than-two-classes---one-versus-one-ovo",
    "href": "qmd/islp9.html#svms-with-more-than-two-classes---one-versus-one-ovo",
    "title": "Support Vector Machines",
    "section": "SVMs with More than Two Classes - One-versus-One (OvO)",
    "text": "SVMs with More than Two Classes - One-versus-One (OvO)\n\n\nOne-versus-One (OvO): We construct \\(\\binom{K}{2}\\) SVMs, each comparing a pair of classes. For example, if we have classes A, B, and C, we build SVMs for A vs.¬†B, A vs.¬†C, and B vs.¬†C. To classify a test observation, we use all the SVMs and assign the observation to the class that wins the most ‚Äúvotes.‚Äù üó≥Ô∏è"
  },
  {
    "objectID": "qmd/islp9.html#svms-with-more-than-two-classes---one-versus-all-ova",
    "href": "qmd/islp9.html#svms-with-more-than-two-classes---one-versus-all-ova",
    "title": "Support Vector Machines",
    "section": "SVMs with More than Two Classes - One-versus-All (OvA)",
    "text": "SVMs with More than Two Classes - One-versus-All (OvA)\n\n\nOne-versus-All (OvA): We fit K separate SVMs, each comparing one class to the rest of the classes. For example, with classes A, B, and C, we build SVMs for A vs.¬†(B and C), B vs.¬†(A and C), and C vs.¬†(A and B). We classify a test observation to the class for which the decision function value is the highest."
  },
  {
    "objectID": "qmd/islp9.html#one-versus-one-ovo-and-one-versus-all-ova-comparison",
    "href": "qmd/islp9.html#one-versus-one-ovo-and-one-versus-all-ova-comparison",
    "title": "Support Vector Machines",
    "section": "One-versus-One (OvO) and One-versus-All (OvA) Comparison",
    "text": "One-versus-One (OvO) and One-versus-All (OvA) Comparison\n\n\n\n\n\n\n\n\n\nMethod\nNumber of Classifiers\nComplexity\nPotential Issues\n\n\n\n\nOne-versus-One\n\\(\\binom{K}{2}\\)\nQuadratic in the number of classes\nCan be computationally expensive for large K\n\n\nOne-versus-All\n\\(K\\)\nLinear in the number of classes\nClass imbalance can affect performance\n\n\n\n\nGenerally, if the number of classes K is large, one-versus-all may be preferred for computational reasons."
  },
  {
    "objectID": "qmd/islp9.html#relationship-to-logistic-regression",
    "href": "qmd/islp9.html#relationship-to-logistic-regression",
    "title": "Support Vector Machines",
    "section": "9.5 Relationship to Logistic Regression",
    "text": "9.5 Relationship to Logistic Regression\n\nIt turns out that SVMs are closely related to logistic regression! The support vector classifier can be rewritten in a ‚ÄúLoss + Penalty‚Äù form, which reveals this connection:\n\n\\[\n\\underset{\\beta_0, \\beta_1, \\dots, \\beta_p}{\\text{minimize}} \\left\\{ \\sum_{i=1}^n \\max[0, 1 - y_i(\\beta_0 + \\beta_1x_{i1} + \\dots + \\beta_px_{ip})] + \\lambda \\sum_{j=1}^p \\beta_j^2 \\right\\}\n\\]"
  },
  {
    "objectID": "qmd/islp9.html#relationship-to-logistic-regression---explanation",
    "href": "qmd/islp9.html#relationship-to-logistic-regression---explanation",
    "title": "Support Vector Machines",
    "section": "Relationship to Logistic Regression - Explanation",
    "text": "Relationship to Logistic Regression - Explanation\n\n\nThe loss function, \\(\\sum_{i=1}^n \\max[0, 1 - y_i(\\beta_0 + \\beta_1x_{i1} + \\dots + \\beta_px_{ip})]\\), is called the hinge loss. It penalizes misclassifications and margin violations.\nThe penalty term, \\(\\lambda \\sum_{j=1}^p \\beta_j^2\\), is a ridge penalty (also known as L2 regularization). It encourages smaller coefficient values, which helps to prevent overfitting."
  },
  {
    "objectID": "qmd/islp9.html#hinge-loss-vs.-logistic-regression-loss",
    "href": "qmd/islp9.html#hinge-loss-vs.-logistic-regression-loss",
    "title": "Support Vector Machines",
    "section": "Hinge Loss vs.¬†Logistic Regression Loss",
    "text": "Hinge Loss vs.¬†Logistic Regression Loss\n\n\n\nLoss compare\n\n\n\nFIGURE 9.12: This figure compares the hinge loss (used in SVMs) and the loss function used in logistic regression."
  },
  {
    "objectID": "qmd/islp9.html#hinge-loss-vs.-logistic-regression-loss---similarity",
    "href": "qmd/islp9.html#hinge-loss-vs.-logistic-regression-loss---similarity",
    "title": "Support Vector Machines",
    "section": "Hinge Loss vs.¬†Logistic Regression Loss - Similarity",
    "text": "Hinge Loss vs.¬†Logistic Regression Loss - Similarity\n\n\n\nLoss compare\n\n\n\nThey are remarkably similar!"
  },
  {
    "objectID": "qmd/islp9.html#hinge-loss-vs.-logistic-regression-loss---hinge-loss-behavior",
    "href": "qmd/islp9.html#hinge-loss-vs.-logistic-regression-loss---hinge-loss-behavior",
    "title": "Support Vector Machines",
    "section": "Hinge Loss vs.¬†Logistic Regression Loss - Hinge Loss Behavior",
    "text": "Hinge Loss vs.¬†Logistic Regression Loss - Hinge Loss Behavior\n\n\n\nLoss compare\n\n\n\n\nHinge Loss: It‚Äôs exactly zero for observations that are correctly classified and lie beyond the margin."
  },
  {
    "objectID": "qmd/islp9.html#hinge-loss-vs.-logistic-regression-loss---logistic-regression-loss-behavior",
    "href": "qmd/islp9.html#hinge-loss-vs.-logistic-regression-loss---logistic-regression-loss-behavior",
    "title": "Support Vector Machines",
    "section": "Hinge Loss vs.¬†Logistic Regression Loss - Logistic Regression Loss Behavior",
    "text": "Hinge Loss vs.¬†Logistic Regression Loss - Logistic Regression Loss Behavior\n\n\n\nLoss compare\n\n\n\n\nLogistic Regression Loss: It‚Äôs never exactly zero, but it can become very close to zero for confident, correct predictions."
  },
  {
    "objectID": "qmd/islp9.html#svm-vs.-logistic-regression-conclusion",
    "href": "qmd/islp9.html#svm-vs.-logistic-regression-conclusion",
    "title": "Support Vector Machines",
    "section": "SVM vs.¬†Logistic Regression: Conclusion",
    "text": "SVM vs.¬†Logistic Regression: Conclusion\n\nBecause of the similarity between their loss functions, SVM and logistic regression often produce similar results. However, there are some general guidelines:"
  },
  {
    "objectID": "qmd/islp9.html#svm-vs.-logistic-regression-svm-preference",
    "href": "qmd/islp9.html#svm-vs.-logistic-regression-svm-preference",
    "title": "Support Vector Machines",
    "section": "SVM vs.¬†Logistic Regression: SVM Preference",
    "text": "SVM vs.¬†Logistic Regression: SVM Preference\n\n\nSVM: Tends to perform better when the classes are well-separated."
  },
  {
    "objectID": "qmd/islp9.html#svm-vs.-logistic-regression-logistic-regression-preference",
    "href": "qmd/islp9.html#svm-vs.-logistic-regression-logistic-regression-preference",
    "title": "Support Vector Machines",
    "section": "SVM vs.¬†Logistic Regression: Logistic Regression Preference",
    "text": "SVM vs.¬†Logistic Regression: Logistic Regression Preference\n\n\nLogistic Regression: Often preferred when classes overlap or when probabilistic outputs are desired (logistic regression naturally provides probabilities, while SVMs don‚Äôt)."
  },
  {
    "objectID": "qmd/islp9.html#summary",
    "href": "qmd/islp9.html#summary",
    "title": "Support Vector Machines",
    "section": "Summary",
    "text": "Summary\n\n\nSupport Vector Machines (SVMs) are powerful and versatile tools for classification.\nMaximal Margin Classifier: The fundamental concept, applicable only to linearly separable data.\nSupport Vector Classifier (Soft Margin): Handles non-separable data and enhances robustness by allowing margin violations.\nSupport Vector Machine (Kernel Trick): Efficiently handles non-linear decision boundaries by using kernels to implicitly map data to high-dimensional spaces.\nKernels: Functions that quantify the similarity between observations. The radial kernel is a particularly popular and powerful choice.\nSupport Vectors: The crucial observations that define the decision boundary. Only these points influence the model.\nTuning Parameters: C (in the soft margin classifier) and kernel parameters (like \\(\\gamma\\) for the radial kernel) control the bias-variance trade-off.\nRelationship to Logistic Regression: SVMs are closely related to logistic regression, with the hinge loss being similar to the logistic regression loss function."
  },
  {
    "objectID": "qmd/islp9.html#thoughts-and-discussion",
    "href": "qmd/islp9.html#thoughts-and-discussion",
    "title": "Support Vector Machines",
    "section": "Thoughts and Discussion",
    "text": "Thoughts and Discussion\n\nLet‚Äôs discuss some thought-provoking questions related to SVMs."
  },
  {
    "objectID": "qmd/islp9.html#thoughts-and-discussion---question-1",
    "href": "qmd/islp9.html#thoughts-and-discussion---question-1",
    "title": "Support Vector Machines",
    "section": "Thoughts and Discussion - Question 1",
    "text": "Thoughts and Discussion - Question 1\n\n\nWhy are support vectors so important? What does this tell us about the robustness of SVMs? Hint: Consider which points have an influence on the decision boundary and which points don‚Äôt."
  },
  {
    "objectID": "qmd/islp9.html#thoughts-and-discussion---question-2",
    "href": "qmd/islp9.html#thoughts-and-discussion---question-2",
    "title": "Support Vector Machines",
    "section": "Thoughts and Discussion - Question 2",
    "text": "Thoughts and Discussion - Question 2\n\n\nHow does the choice of kernel and its parameters affect the SVM‚Äôs decision boundary? Specifically, think about the \\(\\gamma\\) parameter in the radial kernel. Hint: Consider the concepts of ‚Äúlocal‚Äù versus ‚Äúglobal‚Äù behavior. How does \\(\\gamma\\) control this?"
  },
  {
    "objectID": "qmd/islp9.html#thoughts-and-discussion---question-3",
    "href": "qmd/islp9.html#thoughts-and-discussion---question-3",
    "title": "Support Vector Machines",
    "section": "Thoughts and Discussion - Question 3",
    "text": "Thoughts and Discussion - Question 3\n\n\nWhen might you prefer logistic regression over an SVM, and vice-versa? Consider the characteristics of your data and the assumptions of each method. Hint: Think about the degree of class separability and whether you need probabilistic outputs."
  },
  {
    "objectID": "qmd/islp9.html#thoughts-and-discussion---question-4",
    "href": "qmd/islp9.html#thoughts-and-discussion---question-4",
    "title": "Support Vector Machines",
    "section": "Thoughts and Discussion - Question 4",
    "text": "Thoughts and Discussion - Question 4\n\n\nHow does the one-versus-one method compare to the one-versus-all approach in terms of computational complexity and performance? Hint: Consider how many classifiers are needed and the potential impact of class imbalance."
  },
  {
    "objectID": "qmd/islp9.html#thoughts-and-discussion---question-5",
    "href": "qmd/islp9.html#thoughts-and-discussion---question-5",
    "title": "Support Vector Machines",
    "section": "Thoughts and Discussion - Question 5",
    "text": "Thoughts and Discussion - Question 5\n\n\nCan we apply the kernel trick to other linear models besides the support vector classifier? If so, how? Hint: Think about other models that rely on inner products between observations."
  },
  {
    "objectID": "qmd/islp13.html",
    "href": "qmd/islp13.html",
    "title": "Multiple Testing",
    "section": "",
    "text": "Statistical learning is a set of tools for modeling and understanding complex datasets. It is a vital area with applications in many fields. Hypothesis testing, a core concept in statistical inference, allows us to use data to answer ‚Äúyes-or-no‚Äù questions, moving beyond simply estimating values or making predictions."
  },
  {
    "objectID": "qmd/islp13.html#introduction-statistical-learning-and-hypothesis-testing",
    "href": "qmd/islp13.html#introduction-statistical-learning-and-hypothesis-testing",
    "title": "Multiple Testing",
    "section": "",
    "text": "Statistical learning is a set of tools for modeling and understanding complex datasets. It is a vital area with applications in many fields. Hypothesis testing, a core concept in statistical inference, allows us to use data to answer ‚Äúyes-or-no‚Äù questions, moving beyond simply estimating values or making predictions."
  },
  {
    "objectID": "qmd/islp13.html#the-challenge-of-big-data",
    "href": "qmd/islp13.html#the-challenge-of-big-data",
    "title": "Multiple Testing",
    "section": "The Challenge of Big Data",
    "text": "The Challenge of Big Data\n\n\n\n\n\n\n\n\n\nIn today‚Äôs world of ‚Äúbig data,‚Äù we‚Äôre often faced with testing hundreds or even thousands of hypotheses at once. This is called multiple testing. The main challenge is to avoid ‚Äúfalse positives‚Äù ‚Äî incorrectly rejecting a true null hypothesis. We want to make sure our ‚Äúdiscoveries‚Äù are real, not just random noise! üîç"
  },
  {
    "objectID": "qmd/islp13.html#review-of-hypothesis-testing",
    "href": "qmd/islp13.html#review-of-hypothesis-testing",
    "title": "Multiple Testing",
    "section": "Review of Hypothesis Testing",
    "text": "Review of Hypothesis Testing\nHypothesis testing gives us a formal way to use data to answer specific questions. It provides a structured framework to evaluate evidence and make informed decisions.\n\nGoal: Decide if there‚Äôs enough evidence to reject a pre-defined assumption (the null hypothesis).\nExamples:\n\nIn linear regression, is a specific coefficient significantly different from zero? This tells us if a predictor truly influences the outcome.\nIn a clinical trial, does a new drug actually lower blood pressure compared to a placebo?"
  },
  {
    "objectID": "qmd/islp13.html#steps-in-hypothesis-testing",
    "href": "qmd/islp13.html#steps-in-hypothesis-testing",
    "title": "Multiple Testing",
    "section": "Steps in Hypothesis Testing",
    "text": "Steps in Hypothesis Testing\nWe generally follow these steps:\n\nDefine Hypotheses:\n\nNull Hypothesis (H‚ÇÄ): The default assumption ‚Äì usually representing ‚Äúno effect‚Äù or ‚Äúno difference.‚Äù\nAlternative Hypothesis (H‚Çê): What we suspect might be true if H‚ÇÄ isn‚Äôt.\n\nConstruct Test Statistic: Calculate a value that summarizes how strongly the data contradicts H‚ÇÄ."
  },
  {
    "objectID": "qmd/islp13.html#steps-in-hypothesis-testing-continued",
    "href": "qmd/islp13.html#steps-in-hypothesis-testing-continued",
    "title": "Multiple Testing",
    "section": "Steps in Hypothesis Testing (Continued)",
    "text": "Steps in Hypothesis Testing (Continued)\n\nCompute p-value: Find the probability of seeing data as extreme (or more extreme) than what we observed, assuming H‚ÇÄ is true.\nDecide: Based on the p-value (and a pre-set threshold), decide whether to reject H‚ÇÄ."
  },
  {
    "objectID": "qmd/islp13.html#defining-the-hypotheses",
    "href": "qmd/islp13.html#defining-the-hypotheses",
    "title": "Multiple Testing",
    "section": "Defining the Hypotheses",
    "text": "Defining the Hypotheses\n\n\n\n\n\n\nNull Hypothesis (H‚ÇÄ): The baseline assumption. We look for evidence against this. Think of it like the ‚Äústatus quo‚Äù or ‚Äúinnocent until proven guilty.‚Äù\n\n\n\n\n\n\n\n\n\nAlternative Hypothesis (H‚Çê): What we think might be true if H‚ÇÄ is false. It‚Äôs often the opposite of H‚ÇÄ. For instance, if H‚ÇÄ is ‚ÄúA=B,‚Äù then H‚Çê is ‚ÄúA‚â†B.‚Äù"
  },
  {
    "objectID": "qmd/islp13.html#constructing-the-test-statistic",
    "href": "qmd/islp13.html#constructing-the-test-statistic",
    "title": "Multiple Testing",
    "section": "Constructing the Test Statistic",
    "text": "Constructing the Test Statistic\n\nThe test statistic measures how far the data is from what we‚Äôd expect if H‚ÇÄ were true.\nThe exact formula depends on the specific hypothesis.\nExample: If we‚Äôre comparing the average values (means) of two groups (like treatment and control), we often use a two-sample t-statistic."
  },
  {
    "objectID": "qmd/islp13.html#two-sample-t-statistic-formula",
    "href": "qmd/islp13.html#two-sample-t-statistic-formula",
    "title": "Multiple Testing",
    "section": "Two-Sample t-statistic: Formula",
    "text": "Two-Sample t-statistic: Formula\nThe two-sample t-statistic is calculated like this:\n\\[\nT = \\frac{\\hat{\\mu}_t - \\hat{\\mu}_c}{s \\sqrt{\\frac{1}{n_t} + \\frac{1}{n_c}}}\n\\]\nwhere:\n\n$ _t, _c$: The sample means of the treatment and control groups. These are our best guesses for the true average values.\n$ n_t, n_c $: The number of observations in each group.\n$ s $: The ‚Äúpooled standard deviation‚Äù (explained next)."
  },
  {
    "objectID": "qmd/islp13.html#pooled-standard-deviation",
    "href": "qmd/islp13.html#pooled-standard-deviation",
    "title": "Multiple Testing",
    "section": "Pooled Standard Deviation",
    "text": "Pooled Standard Deviation\nThe pooled standard deviation, s, combines the variability within each group:\n\\[ s = \\sqrt{\\frac{(n_t - 1)s_t^2 + (n_c - 1)s_c^2}{n_t + n_c - 2}} \\]\n\n\\(s_t^2\\) and \\(s_c^2\\) are the sample variances of the treatment and control groups. They measure how spread out the data is within each group.\nA large absolute value of T (|T|) suggests that the group means are significantly different, providing evidence against the null hypothesis."
  },
  {
    "objectID": "qmd/islp13.html#computing-the-p-value",
    "href": "qmd/islp13.html#computing-the-p-value",
    "title": "Multiple Testing",
    "section": "Computing the p-value",
    "text": "Computing the p-value\n\np-value: The probability of observing a test statistic as extreme as (or more extreme than) the one we calculated, if the null hypothesis were actually true.\nA small p-value means the data is unlikely under H‚ÇÄ, providing strong evidence against it."
  },
  {
    "objectID": "qmd/islp13.html#p-value-important-clarification",
    "href": "qmd/islp13.html#p-value-important-clarification",
    "title": "Multiple Testing",
    "section": "p-value: Important Clarification",
    "text": "p-value: Important Clarification\n\n\n\n\n\n\nThe p-value is not the probability that H‚ÇÄ is true. It‚Äôs the probability of seeing the observed data (or more extreme data), given that H‚ÇÄ is true. This is a very important distinction! üßê"
  },
  {
    "objectID": "qmd/islp13.html#example-p-value-interpretation",
    "href": "qmd/islp13.html#example-p-value-interpretation",
    "title": "Multiple Testing",
    "section": "Example: p-value Interpretation",
    "text": "Example: p-value Interpretation\n\n\n\nDensity function for N(0,1)\n\n\nThis figure shows the probability density function of a standard normal distribution, often called a ‚Äúbell curve.‚Äù The vertical line marks a value of 2.33."
  },
  {
    "objectID": "qmd/islp13.html#example-p-value-interpretation-continued",
    "href": "qmd/islp13.html#example-p-value-interpretation-continued",
    "title": "Multiple Testing",
    "section": "Example: p-value Interpretation (Continued)",
    "text": "Example: p-value Interpretation (Continued)\n\nOnly 1% of the area under the curve is to the right of 2.33.\nThis means there‚Äôs only a 2% chance of getting a value from this distribution that‚Äôs either greater than 2.33 or less than -2.33.\nIf our test statistic follows this distribution when H‚ÇÄ is true, and we observe a test statistic of T = 2.33, the p-value would be 0.02."
  },
  {
    "objectID": "qmd/islp13.html#null-distribution",
    "href": "qmd/islp13.html#null-distribution",
    "title": "Multiple Testing",
    "section": "Null Distribution",
    "text": "Null Distribution\n\nThe null distribution is the probability distribution of the test statistic assuming the null hypothesis is true. It tells us what values of the test statistic are likely (or unlikely) if there‚Äôs no real effect.\nCommon examples:\n\nNormal distribution (bell curve)\nt-distribution (similar to normal, but used for smaller samples)\nœá¬≤-distribution (Chi-squared distribution)\nF-distribution\n\nWe need to know the null distribution to calculate p-values."
  },
  {
    "objectID": "qmd/islp13.html#decision-making",
    "href": "qmd/islp13.html#decision-making",
    "title": "Multiple Testing",
    "section": "Decision Making",
    "text": "Decision Making\n\nWe reject H‚ÇÄ if the p-value is smaller than a pre-defined ‚Äúsignificance level‚Äù (Œ±).\nSignificance level (Œ±): The cutoff for rejecting H‚ÇÄ. A common choice is Œ± = 0.05 (5%).\nDecision Rule:\n\nIf p-value &lt; Œ±, we reject H‚ÇÄ.\nIf p-value ‚â• Œ±, we fail to reject H‚ÇÄ."
  },
  {
    "objectID": "qmd/islp13.html#decision-making-important-note",
    "href": "qmd/islp13.html#decision-making-important-note",
    "title": "Multiple Testing",
    "section": "Decision Making: Important Note",
    "text": "Decision Making: Important Note\n\n\n\n\n\n\nWe never ‚Äúaccept‚Äù H‚ÇÄ. We only ‚Äúreject‚Äù or ‚Äúfail to reject‚Äù it. Failing to reject doesn‚Äôt mean H‚ÇÄ is definitely true, just that we don‚Äôt have enough evidence to say it‚Äôs false."
  },
  {
    "objectID": "qmd/islp13.html#type-i-and-type-ii-errors-a-table",
    "href": "qmd/islp13.html#type-i-and-type-ii-errors-a-table",
    "title": "Multiple Testing",
    "section": "Type I and Type II Errors: A Table",
    "text": "Type I and Type II Errors: A Table\n\n\n\n\nH‚ÇÄ True\nH‚ÇÄ False\n\n\n\n\nReject H‚ÇÄ\nType I Error\nCorrect Decision\n\n\nFail to Reject H‚ÇÄ\nCorrect Decision\nType II Error\n\n\n\n\nType I error: Rejecting H‚ÇÄ when it‚Äôs actually true (a ‚Äúfalse positive‚Äù). The probability of this is Œ± (the significance level).\nType II error: Failing to reject H‚ÇÄ when it‚Äôs actually false (a ‚Äúfalse negative‚Äù)."
  },
  {
    "objectID": "qmd/islp13.html#type-i-and-type-ii-errors-definitions",
    "href": "qmd/islp13.html#type-i-and-type-ii-errors-definitions",
    "title": "Multiple Testing",
    "section": "Type I and Type II Errors: Definitions",
    "text": "Type I and Type II Errors: Definitions\n\nPower: The probability of correctly rejecting H‚ÇÄ when it‚Äôs false (1 - Probability of Type II error). We want high power! üí™\nOur goal is to control the chance of Type I errors (Œ±) while maximizing power (minimizing Type II errors)."
  },
  {
    "objectID": "qmd/islp13.html#the-challenge-of-multiple-testing",
    "href": "qmd/islp13.html#the-challenge-of-multiple-testing",
    "title": "Multiple Testing",
    "section": "The Challenge of Multiple Testing",
    "text": "The Challenge of Multiple Testing\n\nWhen testing a single hypothesis, it‚Äôs relatively easy to control the Type I error rate (Œ±).\nProblem: When we test many hypotheses at the same time, the chance of making at least one Type I error goes way up, even if each individual test has a low Œ±."
  },
  {
    "objectID": "qmd/islp13.html#multiple-testing-an-analogy",
    "href": "qmd/islp13.html#multiple-testing-an-analogy",
    "title": "Multiple Testing",
    "section": "Multiple Testing: An Analogy",
    "text": "Multiple Testing: An Analogy\n\nAnalogy: Imagine flipping a fair coin many times. Each flip has a 50/50 chance, but if you flip enough times, you‚Äôre bound to see some ‚Äúunlikely‚Äù streaks (like all heads) just by random chance.\nSimilarly, when testing many hypotheses, some will look ‚Äúsignificant‚Äù (low p-value) just by chance, even if all the null hypotheses are true."
  },
  {
    "objectID": "qmd/islp13.html#illustration-the-stockbroker",
    "href": "qmd/islp13.html#illustration-the-stockbroker",
    "title": "Multiple Testing",
    "section": "Illustration: The Stockbroker",
    "text": "Illustration: The Stockbroker\n\nA stockbroker claims they can predict whether Apple‚Äôs stock will go up or down for the next 10 days.\nThey email 1024 potential clients, each getting a different sequence of predictions (2¬π‚Å∞ = 1024 possible sequences).\nJust by chance, one client will get a perfect prediction! They might think the stockbroker is amazing, but it‚Äôs just random luck.\nThis shows how making many ‚Äúguesses‚Äù (tests) can lead to seemingly significant results purely by chance."
  },
  {
    "objectID": "qmd/islp13.html#family-wise-error-rate-fwer",
    "href": "qmd/islp13.html#family-wise-error-rate-fwer",
    "title": "Multiple Testing",
    "section": "Family-Wise Error Rate (FWER)",
    "text": "Family-Wise Error Rate (FWER)\n\nFWER: The probability of making at least one Type I error (false positive) among all the hypothesis tests we do.\nIf all m tests are independent and all null hypotheses are true, then:\n\n\\[ \\text{FWER}(\\alpha) = 1 - (1 - \\alpha)^m \\]\n\nThis formula shows that the FWER increases quickly as m (the number of tests) increases."
  },
  {
    "objectID": "qmd/islp13.html#fwer-example-visualized",
    "href": "qmd/islp13.html#fwer-example-visualized",
    "title": "Multiple Testing",
    "section": "FWER Example: Visualized",
    "text": "FWER Example: Visualized\n\n\n\nFWER vs.¬†Number of Hypotheses\n\n\nThis graph shows how the FWER changes as we test more hypotheses (the x-axis is on a log scale), for different values of Œ±."
  },
  {
    "objectID": "qmd/islp13.html#fwer-example-interpretation",
    "href": "qmd/islp13.html#fwer-example-interpretation",
    "title": "Multiple Testing",
    "section": "FWER Example: Interpretation",
    "text": "FWER Example: Interpretation\n\nThe dashed line is at 0.05.\nTo keep the FWER at 0.05 when testing 50 null hypotheses, we need to control the Type I error for each individual hypothesis at a much lower level (around Œ± = 0.001).\nAs we test more hypotheses, we need to be more and more strict with our individual significance levels to maintain the same overall FWER."
  },
  {
    "objectID": "qmd/islp13.html#controlling-the-fwer",
    "href": "qmd/islp13.html#controlling-the-fwer",
    "title": "Multiple Testing",
    "section": "Controlling the FWER",
    "text": "Controlling the FWER\n\nTo control the FWER at a specific level (Œ±), we need stricter rules for rejecting each hypothesis.\nCommon methods:\n\nBonferroni Correction: Very simple, but often too conservative.\nHolm‚Äôs Method: Less conservative than Bonferroni."
  },
  {
    "objectID": "qmd/islp13.html#bonferroni-correction",
    "href": "qmd/islp13.html#bonferroni-correction",
    "title": "Multiple Testing",
    "section": "Bonferroni Correction",
    "text": "Bonferroni Correction\n\nRule: Reject H‚ÇÄj if its p-value is less than Œ±/m. Divide the desired overall significance level (Œ±) by the number of tests (m).\nGuarantee: FWER(Œ±/m) ‚â§ m √ó Œ±/m = Œ±. The Bonferroni correction ensures the FWER is no larger than Œ±.\nAdvantages:\n\nEasy to do.\nGuaranteed FWER control.\n\nDisadvantage:\n\nCan be very conservative (low power), especially when m is large. We might miss real effects."
  },
  {
    "objectID": "qmd/islp13.html#holms-method",
    "href": "qmd/islp13.html#holms-method",
    "title": "Multiple Testing",
    "section": "Holm‚Äôs Method",
    "text": "Holm‚Äôs Method\nHolm‚Äôs method is a step-down procedure, which is less conservative than Bonferroni:\n\nSpecify Œ±: Choose the desired overall FWER (e.g., Œ± = 0.05).\nCompute p-values: Calculate p-values (p1, ‚Ä¶, pm) for all m hypotheses.\nOrder p-values: Sort the p-values from smallest to largest: p(1) ‚â§ p(2) ‚â§ ‚Ä¶ ‚â§ p(m).\nFind L: \\[ L = \\min\\big\\{j:p_{(j)} &gt; \\frac{\\alpha}{m+1-j} \\big\\} \\]"
  },
  {
    "objectID": "qmd/islp13.html#holms-method-continued",
    "href": "qmd/islp13.html#holms-method-continued",
    "title": "Multiple Testing",
    "section": "Holm‚Äôs Method (Continued)",
    "text": "Holm‚Äôs Method (Continued)\n\nReject: Reject all null hypotheses H0j for which pj &lt; p(L).\n\n\nAdvantages:\n\nControls the FWER.\nLess conservative than Bonferroni (higher power). Holm will always reject at least as many hypotheses as Bonferroni, and usually more."
  },
  {
    "objectID": "qmd/islp13.html#holms-method-illustration",
    "href": "qmd/islp13.html#holms-method-illustration",
    "title": "Multiple Testing",
    "section": "Holm‚Äôs method illustration",
    "text": "Holm‚Äôs method illustration\n\n\n\nHolm‚Äôs and Bonferroni\n\n\nEach panel shows sorted p-values from simulations with 10 hypotheses. Black dots are true null hypotheses (2 out of 10), and red dots are false null hypotheses."
  },
  {
    "objectID": "qmd/islp13.html#holms-method-illustration-continued",
    "href": "qmd/islp13.html#holms-method-illustration-continued",
    "title": "Multiple Testing",
    "section": "Holm‚Äôs method illustration (Continued)",
    "text": "Holm‚Äôs method illustration (Continued)\n\nThe black line is the Bonferroni threshold (Œ±/m).\nThe blue line is the Holm threshold (Œ±/(m+1-j)).\nThe area between the lines shows hypotheses rejected by Holm but not Bonferroni.\nHolm rejects more hypotheses, giving it higher power."
  },
  {
    "objectID": "qmd/islp13.html#other-fwer-control-methods",
    "href": "qmd/islp13.html#other-fwer-control-methods",
    "title": "Multiple Testing",
    "section": "Other FWER Control Methods",
    "text": "Other FWER Control Methods\n\nTukey‚Äôs Method: Specifically for comparing all possible pairs of group means in ANOVA (Analysis of Variance).\nScheff√©‚Äôs Method: Allows testing any linear combination of group means, even after seeing the data. Very flexible, but also very conservative.\nThese are more powerful than Bonferroni/Holm in their specific situations, but less general."
  },
  {
    "objectID": "qmd/islp13.html#trade-off-fwer-and-power",
    "href": "qmd/islp13.html#trade-off-fwer-and-power",
    "title": "Multiple Testing",
    "section": "Trade-Off: FWER and Power",
    "text": "Trade-Off: FWER and Power\n\n\n\nFWER and Power\n\n\nThis plot shows how FWER and power are related in a simulation where 90% of the null hypotheses are true."
  },
  {
    "objectID": "qmd/islp13.html#trade-off-fwer-and-power-continued",
    "href": "qmd/islp13.html#trade-off-fwer-and-power-continued",
    "title": "Multiple Testing",
    "section": "Trade-Off: FWER and Power (Continued)",
    "text": "Trade-Off: FWER and Power (Continued)\n\nAs m (number of hypotheses) increases, power goes down for a fixed FWER.\nControlling the FWER very strictly (low Œ±) means lower power (more Type II errors). We‚Äôre less likely to find true effects.\nThe dashed vertical line is at FWER = 0.05.\nThis shows the key trade-off: being more conservative (controlling FWER) reduces our ability to find real effects."
  },
  {
    "objectID": "qmd/islp13.html#false-discovery-rate-fdr",
    "href": "qmd/islp13.html#false-discovery-rate-fdr",
    "title": "Multiple Testing",
    "section": "False Discovery Rate (FDR)",
    "text": "False Discovery Rate (FDR)\n\nFDR: The expected proportion of false positives (Type I errors) among all the rejected null hypotheses.\n\n\\[ \\text{FDR} = E\\left(\\frac{\\text{Number of Type I Errors}}{\\text{Number of Rejected Hypotheses}}\\right) = E\\left(\\frac{V}{R}\\right) \\]\n\nWhere V is the number of Type I Errors and R is the number of Rejected Hypotheses.\nControlling the FDR is less strict than controlling the FWER. It allows for some false positives, but aims to keep their proportion low."
  },
  {
    "objectID": "qmd/islp13.html#fdr-motivation",
    "href": "qmd/islp13.html#fdr-motivation",
    "title": "Multiple Testing",
    "section": "FDR: Motivation",
    "text": "FDR: Motivation\n\nMotivation: In exploratory studies with many hypotheses (m), we might be okay with some false positives to discover more true effects. We‚Äôre prioritizing discovery over absolute certainty.\nFDR control is good when we‚Äôre more interested in finding potential leads than in making rock-solid claims."
  },
  {
    "objectID": "qmd/islp13.html#benjamini-hochberg-bh-procedure",
    "href": "qmd/islp13.html#benjamini-hochberg-bh-procedure",
    "title": "Multiple Testing",
    "section": "Benjamini-Hochberg (BH) Procedure",
    "text": "Benjamini-Hochberg (BH) Procedure\nThe Benjamini-Hochberg (BH) procedure is a popular way to control the FDR at a desired level, q:\n\nSpecify q: Choose the desired FDR level (e.g., q = 0.10).\nCompute p-values: Get p-values (p1, ‚Ä¶, pm) for all m hypotheses.\nOrder p-values: Sort them from smallest to largest: p(1) ‚â§ p(2) ‚â§ ‚Ä¶ ‚â§ p(m).\nFind L: \\[L = \\max\\{j : p_{(j)} \\leq qj/m\\}.\\]\nReject: Reject all null hypotheses H0j for which pj ‚â§ p(L)."
  },
  {
    "objectID": "qmd/islp13.html#benjamini-hochberg-bh-procedure-continued",
    "href": "qmd/islp13.html#benjamini-hochberg-bh-procedure-continued",
    "title": "Multiple Testing",
    "section": "Benjamini-Hochberg (BH) Procedure (Continued)",
    "text": "Benjamini-Hochberg (BH) Procedure (Continued)\n\nGuarantee: The BH procedure guarantees that FDR ‚â§ q if the p-values are independent (or have some types of mild dependence).\nKey Idea: The cutoff for rejecting a hypothesis depends on all the p-values (through L), not just its own. This adaptive cutoff makes it less conservative than Bonferroni or Holm."
  },
  {
    "objectID": "qmd/islp13.html#example-bh-and-bonferroni",
    "href": "qmd/islp13.html#example-bh-and-bonferroni",
    "title": "Multiple Testing",
    "section": "Example: BH and Bonferroni",
    "text": "Example: BH and Bonferroni\n\n\n\nBH and Bonferroni\n\n\nThis figure shows the same set of 2000 ordered p-values, comparing Bonferroni (FWER control) and BH (FDR control) thresholds."
  },
  {
    "objectID": "qmd/islp13.html#example-bh-and-bonferroni-continued",
    "href": "qmd/islp13.html#example-bh-and-bonferroni-continued",
    "title": "Multiple Testing",
    "section": "Example: BH and Bonferroni (Continued)",
    "text": "Example: BH and Bonferroni (Continued)\n\nGreen lines are Bonferroni thresholds for different Œ± levels.\nOrange lines are BH thresholds for different q levels.\nBlue dots show p-values of rejected null hypotheses under BH.\nBH rejects many more hypotheses than Bonferroni for the same level of error control (e.g., compare Œ±=0.1 and q=0.1). This shows the increased power of FDR control."
  },
  {
    "objectID": "qmd/islp13.html#re-sampling-approaches",
    "href": "qmd/islp13.html#re-sampling-approaches",
    "title": "Multiple Testing",
    "section": "Re-Sampling Approaches",
    "text": "Re-Sampling Approaches\n\nSo far, we‚Äôve assumed we know the theoretical null distribution of the test statistic (like the t-distribution or F-distribution).\nProblem: Sometimes, the theoretical null distribution is unknown, or the assumptions needed for it to be valid aren‚Äôt met (e.g., data isn‚Äôt normally distributed, small sample size).\nSolution: Use re-sampling methods (like permutation tests) to estimate the null distribution from the data itself."
  },
  {
    "objectID": "qmd/islp13.html#permutation-test-example",
    "href": "qmd/islp13.html#permutation-test-example",
    "title": "Multiple Testing",
    "section": "Permutation Test: Example",
    "text": "Permutation Test: Example\n\nSuppose we want to compare the average values of two groups (X and Y).\nNull Hypothesis (H‚ÇÄ): The distributions of X and Y are the same. This means the group labels (‚ÄúX‚Äù or ‚ÄúY‚Äù) are essentially random.\nIdea: If H‚ÇÄ is true, then randomly swapping observations between the groups shouldn‚Äôt systematically change the test statistic.\nWe can permute (shuffle) the data many times, calculate the test statistic each time, and build an empirical null distribution."
  },
  {
    "objectID": "qmd/islp13.html#permutation-test-steps",
    "href": "qmd/islp13.html#permutation-test-steps",
    "title": "Multiple Testing",
    "section": "Permutation Test: Steps",
    "text": "Permutation Test: Steps\n\nCalculate the test statistic: Compute the test statistic (e.g., the t-statistic) on the original data.\nRepeat many times:\n\nRandomly shuffle the observations between the two groups.\nCalculate the test statistic on the shuffled data.\n\nCalculate the p-value: The p-value is the proportion of shuffled test statistics that are as extreme as (or more extreme than) the original test statistic."
  },
  {
    "objectID": "qmd/islp13.html#resampling-for-fdr-control",
    "href": "qmd/islp13.html#resampling-for-fdr-control",
    "title": "Multiple Testing",
    "section": "Resampling for FDR Control",
    "text": "Resampling for FDR Control\n\nWe can also use re-sampling to estimate the FDR directly, without calculating p-values first.\nThe idea is to approximate:\n\nE(V): The expected number of false positives. We estimate this by shuffling the data (simulating the null hypothesis) and counting how many hypotheses we‚Äôd reject.\nR: The total number of rejections on the original data.\n\n\n\\[FDR = E(\\frac{V}{R}) \\approx \\frac{E(V)}{R} \\]\n\nUsing re-sampling to estimate the null distribution can combine information across multiple hypotheses."
  },
  {
    "objectID": "qmd/islp13.html#when-is-re-sampling-useful",
    "href": "qmd/islp13.html#when-is-re-sampling-useful",
    "title": "Multiple Testing",
    "section": "When is Re-Sampling Useful?",
    "text": "When is Re-Sampling Useful?\n\nWhen the theoretical null distribution is unknown or unreliable.\nWhen assumptions for the theoretical distribution are violated (e.g., data isn‚Äôt normally distributed, sample size is small).\nProvides a flexible way to do hypothesis testing in complex situations where standard theoretical results don‚Äôt apply."
  },
  {
    "objectID": "qmd/islp13.html#re-sampling-example-comparison-with-theoretical-null",
    "href": "qmd/islp13.html#re-sampling-example-comparison-with-theoretical-null",
    "title": "Multiple Testing",
    "section": "Re-Sampling Example: Comparison with Theoretical Null",
    "text": "Re-Sampling Example: Comparison with Theoretical Null\n\n\n\nTheoretical and resampling null distribution comparison\n\n\nThis shows the theoretical and re-sampling null distributions for the 11th gene in the Khan dataset. They‚Äôre very similar. The theoretical p-value is 0.041, and the re-sampling p-value is 0.042."
  },
  {
    "objectID": "qmd/islp13.html#re-sampling-example-different-distributions",
    "href": "qmd/islp13.html#re-sampling-example-different-distributions",
    "title": "Multiple Testing",
    "section": "Re-Sampling Example: Different Distributions",
    "text": "Re-Sampling Example: Different Distributions\n\n\n\nTheoretical and resampling null distribution comparison\n\n\nThis shows the theoretical and re-sampling null distributions for the 877th gene in the Khan dataset. They‚Äôre quite different. The theoretical p-value is 0.571, and the re-sampling p-value is 0.673. This shows how re-sampling can be more accurate when the theoretical distribution isn‚Äôt a good fit."
  },
  {
    "objectID": "qmd/islp13.html#summary",
    "href": "qmd/islp13.html#summary",
    "title": "Multiple Testing",
    "section": "Summary",
    "text": "Summary\n\nMultiple testing: Testing many hypotheses at once increases the risk of false positives (Type I errors).\nFWER control: Strict methods like Bonferroni and Holm control the chance of any false positives, but can have low power.\nFDR control: Less strict, allowing some false positives to increase discovery. The Benjamini-Hochberg (BH) procedure is a common method.\nRe-sampling: A flexible way to estimate null distributions and do inference when theoretical assumptions are violated or the null distribution is unknown."
  },
  {
    "objectID": "qmd/islp13.html#thoughts-for-discussion",
    "href": "qmd/islp13.html#thoughts-for-discussion",
    "title": "Multiple Testing",
    "section": "Thoughts for Discussion ü§î",
    "text": "Thoughts for Discussion ü§î\n\nHow do you choose between controlling the FWER and the FDR in practice? What factors affect your decision?\nWhat are the trade-offs in choosing a stricter (FWER) versus a more lenient (FDR) approach?\nCan you think of real-world situations where multiple testing is important (e.g., genomics, A/B testing, finance)?\nWhen would you prefer to use a re-sampling approach (like a permutation test) instead of a theoretical null distribution?"
  },
  {
    "objectID": "qmd/islp8.html",
    "href": "qmd/islp8.html",
    "title": "Tree-Based Methods",
    "section": "",
    "text": "This chapter introduces tree-based methods for regression and classification. These methods involve segmenting the predictor space into simpler, more manageable regions. Think of it like creating a flowchart to make decisions!\n\n\n\n\n\n\nNote\n\n\n\nTree-based methods are simple and easy to understand (interpretable). However, they often aren‚Äôt as accurate as some other machine learning techniques. We‚Äôll explore ways to make them better, like bagging, random forests, and boosting, which combine many trees to improve performance."
  },
  {
    "objectID": "qmd/islp8.html#introduction-tree-based-methods",
    "href": "qmd/islp8.html#introduction-tree-based-methods",
    "title": "Tree-Based Methods",
    "section": "",
    "text": "This chapter introduces tree-based methods for regression and classification. These methods involve segmenting the predictor space into simpler, more manageable regions. Think of it like creating a flowchart to make decisions!\n\n\n\n\n\n\nNote\n\n\n\nTree-based methods are simple and easy to understand (interpretable). However, they often aren‚Äôt as accurate as some other machine learning techniques. We‚Äôll explore ways to make them better, like bagging, random forests, and boosting, which combine many trees to improve performance."
  },
  {
    "objectID": "qmd/islp8.html#core-concepts-data-mining",
    "href": "qmd/islp8.html#core-concepts-data-mining",
    "title": "Tree-Based Methods",
    "section": "Core Concepts: Data Mining",
    "text": "Core Concepts: Data Mining\nLet‚Äôs start with the foundational concepts. First up: Data Mining.\nData mining is the process of discovering patterns, anomalies, and insights from large datasets. It utilizes a multidisciplinary approach, blending techniques from:\n\nStatistics: For analyzing data and drawing inferences.\nMachine Learning: For building predictive models.\nDatabase Management: For efficiently storing and retrieving large datasets.\n\nIt‚Äôs like being a detective üïµÔ∏è‚Äç‚ôÄÔ∏è, sifting through raw data to find valuable knowledge and clues. The goal is to extract useful information, not just any information."
  },
  {
    "objectID": "qmd/islp8.html#core-concepts-machine-learning",
    "href": "qmd/islp8.html#core-concepts-machine-learning",
    "title": "Tree-Based Methods",
    "section": "Core Concepts: Machine Learning",
    "text": "Core Concepts: Machine Learning\nNext, we have Machine Learning.\nMachine learning is a subfield of artificial intelligence (AI). It focuses on creating algorithms that allow computers to learn from data without being explicitly programmed. This means we don‚Äôt give the computer a set of rules; instead, we give it data and it figures out the rules itself!\nKey aspects:\n\nLearning from Data: The core principle. The algorithm improves its performance as it is exposed to more data.\nBuilding Models: These models are mathematical representations of the patterns in the data. They can be used to make predictions or decisions about new, unseen data.\nNo Explicit Programming: The algorithm learns the patterns and relationships within the data without being explicitly told what those patterns are."
  },
  {
    "objectID": "qmd/islp8.html#core-concepts-statistical-learning",
    "href": "qmd/islp8.html#core-concepts-statistical-learning",
    "title": "Tree-Based Methods",
    "section": "Core Concepts: Statistical Learning",
    "text": "Core Concepts: Statistical Learning\nFinally, let‚Äôs define Statistical Learning.\nStatistical learning refers to a set of tools for modeling and understanding complex datasets. It‚Äôs a relatively new field within statistics that intersects with advances in computer science, particularly machine learning. It emphasizes both building predictive models and gaining insights into the underlying data-generating process.\nKey aspects:\n\nModeling and Understanding: Focuses on both building accurate models and gaining insights into the relationships between variables.\nConceptual and Practical: Provides both theoretical understanding of the methods and practical tools for applying them.\nInterdisciplinary: Combines statistical theory and computational techniques from computer science."
  },
  {
    "objectID": "qmd/islp8.html#relationships-of-concepts",
    "href": "qmd/islp8.html#relationships-of-concepts",
    "title": "Tree-Based Methods",
    "section": "Relationships of Concepts",
    "text": "Relationships of Concepts\nThe following diagram illustrates how these concepts relate to one another:\n\n\n\n\n\ngraph LR\n    A[Data Mining] --&gt; C(Common Ground)\n    B[Machine Learning] --&gt; C\n    D[Statistical Learning] --&gt; C\n    C --&gt; E[Insights & Predictions]\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nData mining, machine learning, and statistical learning all overlap. They are all about extracting insights and making predictions from data, but with different emphases and approaches. Think of them as different tools in the same toolbox üß∞, each with its own strengths."
  },
  {
    "objectID": "qmd/islp8.html#decision-trees-the-basics",
    "href": "qmd/islp8.html#decision-trees-the-basics",
    "title": "Tree-Based Methods",
    "section": "Decision Trees: The Basics üå≤",
    "text": "Decision Trees: The Basics üå≤\nDecision trees are a fundamental tree-based method. They work by dividing the predictor space (the space of all possible input values) using a series of splitting rules. These rules are organized in a hierarchical tree structure, making them easy to visualize and understand. They recursively partition the data based on the values of the predictor variables.\nKey Features:\n\nSimple Interpretation: Easy to understand and explain, even to non-experts. The decision-making process is transparent.\nNon-linear Relationships: Can capture complex, non-linear patterns in the data. This is a major advantage over linear models, which assume a straight-line relationship.\nRegression & Classification: Can be used for both predicting continuous values (regression) and categories (classification)."
  },
  {
    "objectID": "qmd/islp8.html#regression-trees-a-simple-example",
    "href": "qmd/islp8.html#regression-trees-a-simple-example",
    "title": "Tree-Based Methods",
    "section": "Regression Trees: A Simple Example",
    "text": "Regression Trees: A Simple Example\nLet‚Äôs illustrate with a regression tree example. We‚Äôll use the Hitters dataset (commonly used in statistical learning examples) to predict a baseball player‚Äôs salary. This dataset contains information about Major League Baseball players.\n\nPredictors: These are the input variables we‚Äôll use to make predictions.\n\nYears: Number of years the player has played in the major leagues.\nHits: Number of hits the player made in the previous year.\n\nResponse: This is the variable we want to predict.\n\nLog Salary: We use the logarithm of salary to make the distribution more bell-shaped (normal), which often improves model performance. This transformation helps to stabilize the variance and make the data more suitable for modeling."
  },
  {
    "objectID": "qmd/islp8.html#regression-tree-for-hitters-data",
    "href": "qmd/islp8.html#regression-tree-for-hitters-data",
    "title": "Tree-Based Methods",
    "section": "Regression Tree for Hitters Data",
    "text": "Regression Tree for Hitters Data\nThis image shows a regression tree built from the Hitters data:\n\n\n\nRegression tree for Hitters data\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis tree predicts the logarithm of a player‚Äôs salary based on their Years of experience and Hits in the previous year. The numbers at the bottom (in the ‚Äúleaves‚Äù) are the average log(Salary) for players who fall into that specific category. These values represent the predicted log salary for players within each region."
  },
  {
    "objectID": "qmd/islp8.html#interpreting-the-regression-tree-top-split",
    "href": "qmd/islp8.html#interpreting-the-regression-tree-top-split",
    "title": "Tree-Based Methods",
    "section": "Interpreting the Regression Tree: Top Split",
    "text": "Interpreting the Regression Tree: Top Split\nLet‚Äôs break down how to interpret this tree. The most important split is at the very top:\n\n\n\nRegression tree for Hitters data\n\n\n\nTop Split: Years &lt; 4.5\n\nThis tells us that the number of years a player has been in the major leagues is the single most important factor in determining their salary (or rather, the log of their salary). Less experience generally means a lower salary. This variable explains the most variance in the response variable."
  },
  {
    "objectID": "qmd/islp8.html#interpreting-the-regression-tree-internal-nodes-branches",
    "href": "qmd/islp8.html#interpreting-the-regression-tree-internal-nodes-branches",
    "title": "Tree-Based Methods",
    "section": "Interpreting the Regression Tree: Internal Nodes & Branches",
    "text": "Interpreting the Regression Tree: Internal Nodes & Branches\n\n\n\nRegression tree for Hitters data\n\n\n\nInternal Nodes: These represent further splits in the predictor space. For example, the node Hits &lt; 117.5 divides players with less than 4.5 years of experience based on their number of hits. Each internal node represents a decision point based on a predictor variable.\nBranches: The lines connecting the nodes. They represent the ‚Äúyes‚Äù or ‚Äúno‚Äù answers to the splitting rule questions. Each branch corresponds to a specific outcome of the splitting rule."
  },
  {
    "objectID": "qmd/islp8.html#interpreting-the-regression-tree-terminal-nodes-leaves",
    "href": "qmd/islp8.html#interpreting-the-regression-tree-terminal-nodes-leaves",
    "title": "Tree-Based Methods",
    "section": "Interpreting the Regression Tree: Terminal Nodes (Leaves)",
    "text": "Interpreting the Regression Tree: Terminal Nodes (Leaves)\n\n\n\nRegression tree for Hitters data\n\n\n\nTerminal Nodes (Leaves): These are the final boxes at the bottom of the tree. They represent the predicted values ‚Äì in this case, the mean log(Salary) for players who fall into that specific region of the predictor space. These are the final prediction points of the tree.\nExample: The left branch means If a player played less than 4.5 years, his predicted log salary is 5.11."
  },
  {
    "objectID": "qmd/islp8.html#regions-of-predictor-space",
    "href": "qmd/islp8.html#regions-of-predictor-space",
    "title": "Tree-Based Methods",
    "section": "Regions of Predictor Space",
    "text": "Regions of Predictor Space\nThe decision tree divides the predictor space (the combination of Years and Hits) into rectangular regions. This image shows how the tree‚Äôs splits correspond to these regions:\n\n\n\nThree-region partition\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe tree divides the space into three rectangular regions (R1, R2, R3). Each region corresponds to a leaf (terminal node) in the decision tree. Each rectangle represents a group of players with similar predicted salaries."
  },
  {
    "objectID": "qmd/islp8.html#regions-of-predictor-space-region-details",
    "href": "qmd/islp8.html#regions-of-predictor-space-region-details",
    "title": "Tree-Based Methods",
    "section": "Regions of Predictor Space: Region Details",
    "text": "Regions of Predictor Space: Region Details\n\n\n\nThree-region partition\n\n\n\nR1: Players with Years &lt; 4.5. This region corresponds to the leftmost leaf in the tree.\nR2: Players with Years &gt;= 4.5 and Hits &lt; 117.5. This region corresponds to the middle leaf.\nR3: Players with Years &gt;= 4.5 and Hits &gt;= 117.5. This region corresponds to the rightmost leaf."
  },
  {
    "objectID": "qmd/islp8.html#building-a-regression-tree-key-steps",
    "href": "qmd/islp8.html#building-a-regression-tree-key-steps",
    "title": "Tree-Based Methods",
    "section": "Building a Regression Tree: Key Steps",
    "text": "Building a Regression Tree: Key Steps\nHere‚Äôs a summary of how to build a regression tree:\n\nDivide Predictor Space: Split the predictor space into J distinct, non-overlapping regions (R1, R2, ‚Ä¶, RJ). These regions should not overlap, and each observation should belong to exactly one region. The goal is to find regions that minimize the prediction error.\nPrediction: For any observation that falls into region Rj, we predict the response value to be the mean of the response values of all the training observations that also fall into Rj. This is the average of the response values for all observations within that region."
  },
  {
    "objectID": "qmd/islp8.html#constructing-the-regions-minimizing-rss",
    "href": "qmd/islp8.html#constructing-the-regions-minimizing-rss",
    "title": "Tree-Based Methods",
    "section": "Constructing the Regions: Minimizing RSS",
    "text": "Constructing the Regions: Minimizing RSS\nThe key question is: how do we decide where to make the splits and create these regions? The goal is to find the regions that minimize the Residual Sum of Squares (RSS). RSS is a measure of the overall prediction error of the tree.\nThe RSS formula is:\n\\[\n\\text{RSS} = \\sum_{j=1}^{J} \\sum_{i \\in R_j} (y_i - \\hat{y}_{R_j})^2\n\\]\nWhere:\n\n\\(\\hat{y}_{R_j}\\): The mean response for the training observations within region Rj. This is the predicted value for all observations in region Rj.\n\\(y_i\\): The actual response value for the ith observation.\n\n\n\n\n\n\n\nTip\n\n\n\nFinding the absolute best set of regions to minimize RSS is computationally very expensive (infeasible in most cases). It would require checking every possible partition of the feature space. Therefore, we use a top-down, greedy approach called recursive binary splitting."
  },
  {
    "objectID": "qmd/islp8.html#recursive-binary-splitting-overview",
    "href": "qmd/islp8.html#recursive-binary-splitting-overview",
    "title": "Tree-Based Methods",
    "section": "Recursive Binary Splitting: Overview",
    "text": "Recursive Binary Splitting: Overview\nRecursive binary splitting is a clever way to build the tree efficiently. It‚Äôs a heuristic algorithm that finds a good solution, though not necessarily the optimal solution. It has these key characteristics:\n\nTop-Down: We start with all observations in a single region (the entire predictor space) and successively split the predictor space.\nGreedy: At each step, we make the best split at that particular moment. We don‚Äôt look ahead to see if a different split might be better later on. This means we choose the split that minimizes the RSS at the current step.\nBinary: Each split divides a region into exactly two sub-regions. This creates a binary tree structure."
  },
  {
    "objectID": "qmd/islp8.html#recursive-binary-splitting-the-process",
    "href": "qmd/islp8.html#recursive-binary-splitting-the-process",
    "title": "Tree-Based Methods",
    "section": "Recursive Binary Splitting: The Process",
    "text": "Recursive Binary Splitting: The Process\nHere‚Äôs how recursive binary splitting works step-by-step:\n\nSelect Predictor and Cutpoint:\n\nWe consider all predictors (X1, X2, ‚Ä¶, Xp) and all possible cutpoints (values of the predictor) for each predictor.\nWe choose the predictor (Xj) and the cutpoint (s) that lead to the greatest possible reduction in RSS when we split the region into two new regions:\n\nR1(j, s) = {X | Xj &lt; s} (all observations where Xj is less than s)\nR2(j, s) = {X | Xj ‚â• s} (all observations where Xj is greater than or equal to s)"
  },
  {
    "objectID": "qmd/islp8.html#recursive-binary-splitting-minimization",
    "href": "qmd/islp8.html#recursive-binary-splitting-minimization",
    "title": "Tree-Based Methods",
    "section": "Recursive Binary Splitting: Minimization",
    "text": "Recursive Binary Splitting: Minimization\n\nMinimize: We find the specific values of j (the predictor index) and s (the cutpoint) that minimize the following expression:\n\\[\n\\sum_{i: x_i \\in R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 + \\sum_{i: x_i \\in R_2(j,s)} (y_i - \\hat{y}_{R_2})^2\n\\]\nThis is just the sum of the RSS for the two new regions. The goal is to find the predictor and cutpoint that result in the lowest combined RSS for the two resulting regions."
  },
  {
    "objectID": "qmd/islp8.html#recursive-binary-splitting-repetition",
    "href": "qmd/islp8.html#recursive-binary-splitting-repetition",
    "title": "Tree-Based Methods",
    "section": "Recursive Binary Splitting: Repetition",
    "text": "Recursive Binary Splitting: Repetition\n\nRepeat: We take one of the newly created regions and repeat steps 1 and 2. We continue this process, splitting regions into smaller and smaller sub-regions, until a stopping criterion is met.\n\nA common stopping criterion is to stop splitting when a region contains fewer than a certain number of observations (e.g., 5 observations). Other stopping criteria could include reaching a maximum tree depth or a minimum improvement in RSS."
  },
  {
    "objectID": "qmd/islp8.html#visualizing-recursive-binary-splitting",
    "href": "qmd/islp8.html#visualizing-recursive-binary-splitting",
    "title": "Tree-Based Methods",
    "section": "Visualizing Recursive Binary Splitting",
    "text": "Visualizing Recursive Binary Splitting\nThis figure illustrates the process of recursive binary splitting:\n\n\n\nRecursive binary splitting\n\n\n\nTop Left: The first split is made, dividing the space horizontally based on the value of X1."
  },
  {
    "objectID": "qmd/islp8.html#visualizing-recursive-binary-splitting-1",
    "href": "qmd/islp8.html#visualizing-recursive-binary-splitting-1",
    "title": "Tree-Based Methods",
    "section": "Visualizing Recursive Binary Splitting",
    "text": "Visualizing Recursive Binary Splitting\n\n\n\nRecursive binary splitting\n\n\n\nTop Middle: The next two splits are made, one in each of the previously created regions."
  },
  {
    "objectID": "qmd/islp8.html#visualizing-recursive-binary-splitting-2",
    "href": "qmd/islp8.html#visualizing-recursive-binary-splitting-2",
    "title": "Tree-Based Methods",
    "section": "Visualizing Recursive Binary Splitting",
    "text": "Visualizing Recursive Binary Splitting\n\n\n\nRecursive binary splitting\n\n\n\nTop Right: Shows the final result of recursive binary splitting in two dimensions (like our Hitters example). The lines represent the splits. The predictor space is fully partitioned into rectangles."
  },
  {
    "objectID": "qmd/islp8.html#visualizing-recursive-binary-splitting-3",
    "href": "qmd/islp8.html#visualizing-recursive-binary-splitting-3",
    "title": "Tree-Based Methods",
    "section": "Visualizing Recursive Binary Splitting",
    "text": "Visualizing Recursive Binary Splitting\n\n\n\nRecursive binary splitting\n\n\n\nBottom Left: Shows the corresponding decision tree. Each split in the top right panel corresponds to an internal node in the tree."
  },
  {
    "objectID": "qmd/islp8.html#visualizing-recursive-binary-splitting-4",
    "href": "qmd/islp8.html#visualizing-recursive-binary-splitting-4",
    "title": "Tree-Based Methods",
    "section": "Visualizing Recursive Binary Splitting",
    "text": "Visualizing Recursive Binary Splitting\n\n\n\nRecursive binary splitting\n\n\n\nBottom Right: Presents a perspective plot of the prediction surface. This gives you a 3D view of how the predicted value (e.g., log salary) changes across the predictor space. The predicted value is constant within each rectangle."
  },
  {
    "objectID": "qmd/islp8.html#tree-pruning",
    "href": "qmd/islp8.html#tree-pruning",
    "title": "Tree-Based Methods",
    "section": "Tree Pruning ‚úÇÔ∏è",
    "text": "Tree Pruning ‚úÇÔ∏è\nThe recursive binary splitting process, if allowed to continue unchecked, will often lead to a very large and complex tree. This tree will likely overfit the training data. Overfitting means the tree is too closely tailored to the training data (including noise) and won‚Äôt generalize well to new, unseen data. It will have low bias but high variance.\nA smaller tree with fewer splits can have:\n\nLower Variance: Less sensitive to the specific details of the training data. It will be more stable and generalize better.\nBetter Interpretability: Easier to understand and explain. A simpler model is often preferred for its clarity.\n\n\n\n\n\n\n\nTip\n\n\n\nCost complexity pruning (also known as weakest link pruning) is a technique to find the best subtree of the original, large tree. It helps us find a simpler tree that balances accuracy and complexity. It balances the trade-off between model fit and model size."
  },
  {
    "objectID": "qmd/islp8.html#cost-complexity-pruning",
    "href": "qmd/islp8.html#cost-complexity-pruning",
    "title": "Tree-Based Methods",
    "section": "Cost Complexity Pruning",
    "text": "Cost Complexity Pruning\n\nGoal: Find a subtree T (which is a smaller version of the full tree, T0) that minimizes a penalized version of the RSS:\n\\[\n\\sum_{m=1}^{|T|} \\sum_{x_i \\in R_m} (y_i - \\hat{y}_{R_m})^2 + \\alpha|T|\n\\]\n|T|: The number of terminal nodes (leaves) in the subtree T. This is a measure of the tree‚Äôs complexity.\nŒ±: A tuning parameter that controls the trade-off between the subtree‚Äôs complexity (number of leaves) and how well it fits the training data (RSS). A larger Œ± leads to a larger penalty for complexity.\n\n\n\n\n\n\n\nNote\n\n\n\nAs we increase the value of Œ±, the penalty for having many terminal nodes increases. This leads to smaller and smaller subtrees. The tuning parameter Œ± controls the balance between a complex, well-fitting tree and a simple, less-well-fitting tree."
  },
  {
    "objectID": "qmd/islp8.html#algorithm-8.1-building-a-regression-tree",
    "href": "qmd/islp8.html#algorithm-8.1-building-a-regression-tree",
    "title": "Tree-Based Methods",
    "section": "Algorithm 8.1: Building a Regression Tree",
    "text": "Algorithm 8.1: Building a Regression Tree\nHere‚Äôs the complete algorithm for building a regression tree, including pruning:\n\nGrow a Large Tree: Use recursive binary splitting to grow a large initial tree (T0) on the training data. Stop splitting when each terminal node has fewer than some pre-specified minimum number of observations.\nCost Complexity Pruning: Apply cost complexity pruning to the large tree (T0) to obtain a sequence of best subtrees, as a function of the tuning parameter Œ±. For each value of Œ±, there is a corresponding subtree that minimizes the penalized RSS.\nChoose Œ±: Use K-fold cross-validation to choose the optimal value of Œ±.\n\nDivide the training data into K folds (typically K=5 or K=10).\nFor each fold, repeat steps 1 and 2 using the other K-1 folds as the training data.\nEvaluate the mean squared prediction error on the held-out fold (the fold that wasn‚Äôt used for training).\nAverage the results over all K folds, and pick the value of Œ± that minimizes the average error.\n\nReturn Subtree: Return the subtree from Step 2 that corresponds to the chosen Œ±. This is your final, pruned tree. This subtree gives the best balance between fit and complexity, as determined by cross-validation."
  },
  {
    "objectID": "qmd/islp8.html#classification-trees",
    "href": "qmd/islp8.html#classification-trees",
    "title": "Tree-Based Methods",
    "section": "Classification Trees",
    "text": "Classification Trees\nClassification trees are very similar to regression trees, but they are used to predict a qualitative response (a category or class label) rather than a continuous value.\n\nPrediction: Instead of predicting the mean response value in a region, we predict the most commonly occurring class in that region. This is the class with the highest proportion of training observations in that region.\nInterpretation: We also consider the class proportions within each region (the percentage of observations belonging to each class). This gives us an idea of how ‚Äúpure‚Äù each region is."
  },
  {
    "objectID": "qmd/islp8.html#growing-a-classification-tree",
    "href": "qmd/islp8.html#growing-a-classification-tree",
    "title": "Tree-Based Methods",
    "section": "Growing a Classification Tree",
    "text": "Growing a Classification Tree\n\nWe still use recursive binary splitting, but we cannot use RSS as the splitting criterion because we‚Äôre dealing with classes, not numerical values. RSS is suitable for regression, not classification.\nAlternatives to RSS: We need splitting criteria that measure the impurity of a node (how mixed the classes are within the node).\n\nClassification error rate\nGini index\nEntropy"
  },
  {
    "objectID": "qmd/islp8.html#splitting-criteria-classification-error-rate",
    "href": "qmd/islp8.html#splitting-criteria-classification-error-rate",
    "title": "Tree-Based Methods",
    "section": "Splitting Criteria: Classification Error Rate",
    "text": "Splitting Criteria: Classification Error Rate\nThe classification error rate is the simplest option:\n\\[\nE = 1 - \\max_k (\\hat{p}_{mk})\n\\]\n\n\\(\\hat{p}_{mk}\\): The proportion of training observations in the mth region that belong to the kth class. This is the fraction of training observations in region m that are from class k.\nProblem: The classification error rate is not sensitive enough for tree growing. It doesn‚Äôt always lead to the best splits. It is possible for different splits to result in the same classification error rate, even if one split produces more homogenous regions."
  },
  {
    "objectID": "qmd/islp8.html#splitting-criteria-gini-index",
    "href": "qmd/islp8.html#splitting-criteria-gini-index",
    "title": "Tree-Based Methods",
    "section": "Splitting Criteria: Gini Index",
    "text": "Splitting Criteria: Gini Index\nThe Gini index is a more sensitive measure of node purity:\n\\[\nG = \\sum_{k=1}^{K} \\hat{p}_{mk}(1 - \\hat{p}_{mk})\n\\]\n\nIt measures the total variance across all K classes.\nA small Gini index indicates that the node is pure ‚Äì most of the observations in that node belong to the same class. (If all \\(\\hat{p}_{mk}\\) are close to 0 or 1, the Gini index will be small). A Gini index of 0 indicates perfect purity."
  },
  {
    "objectID": "qmd/islp8.html#splitting-criteria-entropy",
    "href": "qmd/islp8.html#splitting-criteria-entropy",
    "title": "Tree-Based Methods",
    "section": "Splitting Criteria: Entropy",
    "text": "Splitting Criteria: Entropy\nEntropy is another measure of node impurity:\n\\[\nD = -\\sum_{k=1}^{K} \\hat{p}_{mk} \\log \\hat{p}_{mk}\n\\]\n\nEntropy will also take on a value near zero if the node is pure (if the \\(\\hat{p}_{mk}\\) values are close to 0 or 1). An entropy of 0 indicates perfect purity. We use the convention that 0 log 0 = 0.\nThe Gini index and entropy are numerically very similar. Both are more sensitive to changes in node purity than the classification error rate.\n\n\n\n\n\n\n\nTip\n\n\n\nThe Gini index and entropy are preferred over the classification error rate for growing the tree because they are more sensitive to node purity. For pruning the tree, any of the three criteria can be used, but the classification error rate is often preferred because it directly relates to the final prediction accuracy."
  },
  {
    "objectID": "qmd/islp8.html#example-heart-data",
    "href": "qmd/islp8.html#example-heart-data",
    "title": "Tree-Based Methods",
    "section": "Example: Heart Data",
    "text": "Example: Heart Data\nThis figure shows an unpruned classification tree built on the Heart dataset:\n\n\n\nHeart data unpruned tree\n\n\n\nGoal: Predict whether a patient has heart disease (Yes or No) based on 13 predictors."
  },
  {
    "objectID": "qmd/islp8.html#example-heart-data---predictors",
    "href": "qmd/islp8.html#example-heart-data---predictors",
    "title": "Tree-Based Methods",
    "section": "Example: Heart Data - Predictors",
    "text": "Example: Heart Data - Predictors\n\n\n\nHeart data unpruned tree\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that qualitative predictors (like ChestPain) can be handled directly by decision trees, without needing to create dummy variables. The tree automatically handles the different categories of qualitative predictors. For example, the ChestPain variable has four levels: ‚Äútypical angina,‚Äù ‚Äúatypical angina,‚Äù ‚Äúnon-anginal pain,‚Äù and ‚Äúasymptomatic.‚Äù"
  },
  {
    "objectID": "qmd/islp8.html#trees-vs.-linear-models",
    "href": "qmd/islp8.html#trees-vs.-linear-models",
    "title": "Tree-Based Methods",
    "section": "Trees vs.¬†Linear Models",
    "text": "Trees vs.¬†Linear Models\n\nLinear Regression: Assumes a linear relationship between the predictors and the response: \\(f(X) = \\beta_0 + \\sum_{j=1}^{p} X_j \\beta_j\\). The model assumes a constant change in the response for a unit change in each predictor.\nRegression Trees: Assume a piecewise constant model: \\(f(X) = \\sum_{m=1}^{M} c_m \\cdot 1(X \\in R_m)\\). The predicted value is constant within each region.\n\n\n\n\n\n\n\nTip\n\n\n\nThe best model depends on the true underlying relationship between the predictors and the response. If the relationship is close to linear, linear regression will likely outperform a decision tree. If the relationship is highly non-linear and complex, decision trees may be better."
  },
  {
    "objectID": "qmd/islp8.html#trees-vs.-linear-models---visual-comparison",
    "href": "qmd/islp8.html#trees-vs.-linear-models---visual-comparison",
    "title": "Tree-Based Methods",
    "section": "Trees vs.¬†Linear Models - Visual Comparison",
    "text": "Trees vs.¬†Linear Models - Visual Comparison\nThis figure compares decision boundaries of linear models and trees:\n\n\n\nTrees vs.¬†Linear Models\n\n\n\nTop Row: The true decision boundary is linear."
  },
  {
    "objectID": "qmd/islp8.html#trees-vs.-linear-models---visual-comparison-linear-boundary",
    "href": "qmd/islp8.html#trees-vs.-linear-models---visual-comparison-linear-boundary",
    "title": "Tree-Based Methods",
    "section": "Trees vs.¬†Linear Models - Visual Comparison (Linear Boundary)",
    "text": "Trees vs.¬†Linear Models - Visual Comparison (Linear Boundary)\n\n\n\nTrees vs.¬†Linear Models\n\n\n\nTop Left: The linear model (left) fits the data well, as expected. The decision boundary is a straight line.\nTop Right: The decision tree (right) attempts to approximate the linear boundary with a series of rectangular regions, resulting in a less accurate fit."
  },
  {
    "objectID": "qmd/islp8.html#trees-vs.-linear-models---visual-comparison-non-linear-boundary",
    "href": "qmd/islp8.html#trees-vs.-linear-models---visual-comparison-non-linear-boundary",
    "title": "Tree-Based Methods",
    "section": "Trees vs.¬†Linear Models - Visual Comparison (Non-linear Boundary)",
    "text": "Trees vs.¬†Linear Models - Visual Comparison (Non-linear Boundary)\n\n\n\nTrees vs.¬†Linear Models\n\n\n\nBottom Row: The true decision boundary is non-linear."
  },
  {
    "objectID": "qmd/islp8.html#trees-vs.-linear-models---visual-comparison-non-linear-boundary-1",
    "href": "qmd/islp8.html#trees-vs.-linear-models---visual-comparison-non-linear-boundary-1",
    "title": "Tree-Based Methods",
    "section": "Trees vs.¬†Linear Models - Visual Comparison (Non-linear Boundary)",
    "text": "Trees vs.¬†Linear Models - Visual Comparison (Non-linear Boundary)\n\n\n\nTrees vs.¬†Linear Models\n\n\n\nBottom Left: The linear model (left) is unable to capture the non-linear relationship, resulting in a poor fit.\nBottom Right: The decision tree (right) captures the non-linearity much better than the linear model (left) by partitioning the space into appropriate regions."
  },
  {
    "objectID": "qmd/islp8.html#advantages-of-trees",
    "href": "qmd/islp8.html#advantages-of-trees",
    "title": "Tree-Based Methods",
    "section": "Advantages of Trees üëç",
    "text": "Advantages of Trees üëç\n\nEasy to Explain: Decision trees are much simpler to explain than even linear regression. They are intuitive and can be easily understood by non-experts.\nHuman Decision-Making: Some people believe that decision trees more closely mirror human decision-making processes. They resemble a series of ‚Äúif-then‚Äù rules.\nGraphical Representation: They can be easily displayed graphically, making them easy to interpret. Visualizations aid in understanding the model‚Äôs logic.\nQualitative Predictors: They can handle qualitative predictors directly, without the need to create dummy variables (one-hot encoding). This simplifies the data preparation process."
  },
  {
    "objectID": "qmd/islp8.html#disadvantages-of-trees",
    "href": "qmd/islp8.html#disadvantages-of-trees",
    "title": "Tree-Based Methods",
    "section": "Disadvantages of Trees üëé",
    "text": "Disadvantages of Trees üëé\n\nLower Predictive Accuracy: Single decision trees generally don‚Äôt have the same level of predictive accuracy as some other machine learning methods, like support vector machines or neural networks.\nNon-Robust: Small changes in the data can cause large changes in the final estimated tree. This means they can be unstable and sensitive to noise in the data.\n\n\n\n\n\n\n\nTip\n\n\n\nAggregating multiple decision trees (using techniques like bagging, random forests, and boosting) can significantly improve predictive performance and robustness. This is the idea behind ensemble methods, which combine the predictions of multiple models to create a more accurate and stable model."
  },
  {
    "objectID": "qmd/islp8.html#ensemble-methods-combining-multiple-models",
    "href": "qmd/islp8.html#ensemble-methods-combining-multiple-models",
    "title": "Tree-Based Methods",
    "section": "Ensemble Methods: Combining Multiple Models",
    "text": "Ensemble Methods: Combining Multiple Models\nAn ensemble method combines multiple ‚Äúweak learner‚Äù models (like decision trees) to create a more powerful model. Think of it like ‚Äúwisdom of the crowds‚Äù Crowd wisdom ‚Äì the combined judgment of a group is often better than the judgment of any individual member.\n\nWeak Learner: A simple model that makes only mediocre predictions (slightly better than random guessing). A single decision tree is often a weak learner.\nEnsemble: A combination of many weak learners, which together form a strong learner. The ensemble leverages the diversity of the individual models to improve overall performance."
  },
  {
    "objectID": "qmd/islp8.html#bagging-bootstrap-aggregation",
    "href": "qmd/islp8.html#bagging-bootstrap-aggregation",
    "title": "Tree-Based Methods",
    "section": "Bagging (Bootstrap Aggregation)",
    "text": "Bagging (Bootstrap Aggregation)\n\nGoal: Reduce the variance of a statistical learning method. This is especially useful for decision trees, which tend to have high variance (they are sensitive to the specific training data).\nIdea: Take many (hundreds or thousands) of ‚Äúbootstrapped‚Äù samples from the training data, build a separate decision tree on each bootstrapped sample, and then average the predictions of all these trees. This averaging process reduces the variance."
  },
  {
    "objectID": "qmd/islp8.html#bagging-the-process",
    "href": "qmd/islp8.html#bagging-the-process",
    "title": "Tree-Based Methods",
    "section": "Bagging: The Process",
    "text": "Bagging: The Process\n\nBootstrap: Generate B different bootstrapped training datasets. Bootstrapping means taking repeated random samples with replacement from the original training data. Each bootstrapped dataset has the same size as the original dataset, but some observations will be repeated, and others will be left out.\nTrain: Train a decision tree on each of the B bootstrapped datasets. Typically, these trees are grown deep (large trees) and are not pruned. Growing deep trees allows them to capture complex patterns, and averaging reduces the risk of overfitting.\nAverage: For a given test observation:\n\nRegression: Average the predictions from all B trees.\nClassification: Take a majority vote (the class predicted by the most trees is the final prediction)."
  },
  {
    "objectID": "qmd/islp8.html#bagging-out-of-bag-oob-error",
    "href": "qmd/islp8.html#bagging-out-of-bag-oob-error",
    "title": "Tree-Based Methods",
    "section": "Bagging: Out-of-Bag (OOB) Error",
    "text": "Bagging: Out-of-Bag (OOB) Error\n\nOOB Observations: For each tree, the observations that were not included in the bootstrapped sample are called the ‚Äúout-of-bag‚Äù (OOB) observations. On average, about one-third of the observations are OOB for each tree.\nOOB Prediction: We can predict the response for each observation using only the trees in which that observation was OOB. This provides a way to estimate the prediction error without using a separate validation set.\nOOB Error: The OOB error is a valid estimate of the test error of the bagged model. It‚Äôs a very convenient way to assess performance, as it doesn‚Äôt require setting aside a separate validation set."
  },
  {
    "objectID": "qmd/islp8.html#bagging-variable-importance",
    "href": "qmd/islp8.html#bagging-variable-importance",
    "title": "Tree-Based Methods",
    "section": "Bagging: Variable Importance",
    "text": "Bagging: Variable Importance\n\nInterpretability Loss: Bagging improves prediction accuracy, but it sacrifices some interpretability because we no longer have a single tree to examine. The combined model is more of a ‚Äúblack box.‚Äù\nVariable Importance Measures: We can still get an overall summary of the importance of each predictor.\n\nRegression: For each predictor, record the total amount that the RSS decreases due to splits on that predictor, averaged over all B trees. A larger average decrease indicates a more important predictor.\nClassification: For each predictor, record the total amount that the Gini index decreases due to splits on that predictor, averaged over all B trees. A larger average decrease indicates a more important predictor."
  },
  {
    "objectID": "qmd/islp8.html#random-forests",
    "href": "qmd/islp8.html#random-forests",
    "title": "Tree-Based Methods",
    "section": "Random Forests",
    "text": "Random Forests\n\nImprovement over Bagging: Random forests provide an improvement over bagged trees by introducing a small ‚Äútweak‚Äù that decorrelates the trees. This further reduces variance and improves prediction accuracy.\nRandom Subset of Predictors: At each split in the tree-growing process, a random sample of m predictors is chosen as split candidates from the full set of p predictors. Typically, \\(m \\approx \\sqrt{p}\\). This means that at each split, some predictors aren‚Äôt even considered! This introduces further randomness and diversity among the trees."
  },
  {
    "objectID": "qmd/islp8.html#random-forests-rationale",
    "href": "qmd/islp8.html#random-forests-rationale",
    "title": "Tree-Based Methods",
    "section": "Random Forests: Rationale",
    "text": "Random Forests: Rationale\n\nStrong Predictor Problem: In bagging, if there‚Äôs one very strong predictor, most of the trees will use that predictor in the top split. This makes the trees very similar to each other (highly correlated). The predictions from highly correlated trees will be similar, and averaging them won‚Äôt reduce the variance as much as averaging predictions from uncorrelated trees.\nDecorrelation: By limiting the predictors considered at each split, random forests give other predictors a chance to be chosen. This leads to less correlated trees, and when we average the predictions of less correlated trees, we get lower variance and better overall performance."
  },
  {
    "objectID": "qmd/islp8.html#boosting",
    "href": "qmd/islp8.html#boosting",
    "title": "Tree-Based Methods",
    "section": "Boosting",
    "text": "Boosting\n\nSequential Tree Growth: Unlike bagging and random forests, where trees are grown independently, boosting grows trees sequentially. Each tree is grown using information from previously grown trees. This allows boosting to focus on observations that were poorly predicted by previous trees.\nSlow Learning: Boosting ‚Äúlearns slowly‚Äù by fitting small trees to the residuals (the differences between the observed values and the current prediction). It gradually improves the model by focusing on the errors made by previous trees.\nNo Bootstrapping: Boosting doesn‚Äôt use bootstrapped samples. Instead, it uses a modified version of the original data at each step."
  },
  {
    "objectID": "qmd/islp8.html#boosting-the-process",
    "href": "qmd/islp8.html#boosting-the-process",
    "title": "Tree-Based Methods",
    "section": "Boosting: The Process",
    "text": "Boosting: The Process\n\nInitialize: Set the initial prediction \\(\\hat{f}(x)\\) to 0 for all observations, and set the residuals \\(r_i\\) equal to the observed response values \\(y_i\\).\nIterate (for b = 1 to B):\n\nFit a small decision tree (with d splits) to the residuals (not the original response values!). This tree is denoted as \\(\\hat{f}^b(x)\\).\nUpdate the fitted function by adding a shrunken version of the new tree: \\[\\hat{f}(x) \\leftarrow \\hat{f}(x) + \\lambda \\hat{f}^b(x)\\]\nUpdate the residuals: \\[r_i \\leftarrow r_i - \\lambda \\hat{f}^b(x_i)\\]\n\nOutput: The boosted model is the sum of all the trees: \\[\\hat{f}(x) = \\sum_{b=1}^{B} \\lambda \\hat{f}^b(x)\\]"
  },
  {
    "objectID": "qmd/islp8.html#boosting-tuning-parameters",
    "href": "qmd/islp8.html#boosting-tuning-parameters",
    "title": "Tree-Based Methods",
    "section": "Boosting: Tuning Parameters",
    "text": "Boosting: Tuning Parameters\n\nB (Number of Trees): Boosting can overfit if B is too large (although overfitting tends to occur slowly). Use cross-validation to choose B.\nŒª (Shrinkage Parameter): A small positive number (e.g., 0.01 or 0.001) that controls the learning rate of the boosting process. Smaller values of Œª typically require larger values of B to achieve good performance. A smaller Œª leads to slower, more gradual learning.\nd (Number of Splits): Controls the complexity of each tree. Often d = 1 (decision stumps, trees with just a single split) works well, leading to an additive model. d is also referred as interaction depth."
  },
  {
    "objectID": "qmd/islp8.html#bayesian-additive-regression-trees-bart",
    "href": "qmd/islp8.html#bayesian-additive-regression-trees-bart",
    "title": "Tree-Based Methods",
    "section": "Bayesian Additive Regression Trees (BART)",
    "text": "Bayesian Additive Regression Trees (BART)\nBART, similar to other ensemble methods, employs decision trees as its foundational components. However, BART distinguishes itself through several key characteristics:\n\nRandom Tree Structure: Like random forests, BART incorporates randomness into the construction of its trees. The structure of each tree is not predetermined.\nSequential Updates: Similar to boosting, BART refines its model iteratively. The model is updated in a sequential manner, using information from previous iterations.\nTree Perturbation: Instead of fitting entirely new trees at each step, BART modifies existing trees from previous iterations. This modification is key to BART‚Äôs regularization properties."
  },
  {
    "objectID": "qmd/islp8.html#bart-core-idea",
    "href": "qmd/islp8.html#bart-core-idea",
    "title": "Tree-Based Methods",
    "section": "BART: Core Idea",
    "text": "BART: Core Idea\n\nInitialization: All trees begin with a single root node, predicting the mean of the response variable.\nIteration: For each tree, BART randomly perturbs the tree from the previous iteration. This perturbation involves:\n\nChanging the tree structure (adding or pruning branches, changing splitting rules).\nChanging the predictions in the terminal nodes.\n\nOutput: A collection of prediction models (one for each iteration). The final prediction is typically the average of the predictions from all trees after a ‚Äúburn-in‚Äù period (discarding the initial iterations). This averaging helps to stabilize the predictions."
  },
  {
    "objectID": "qmd/islp8.html#bart-key-features",
    "href": "qmd/islp8.html#bart-key-features",
    "title": "Tree-Based Methods",
    "section": "BART: Key Features",
    "text": "BART: Key Features\n\nGuards Against Overfitting: Perturbing trees instead of fitting entirely new ones limits how aggressively the model can fit the training data. This helps to prevent overfitting. The random perturbations act as a form of regularization.\nSmall Trees: Individual trees in BART are usually kept small. This further contributes to regularization.\nBayesian Interpretation: BART can be viewed as a Bayesian approach, where the tree perturbations represent draws from a posterior distribution. This provides a natural way to quantify uncertainty in the predictions (e.g., by constructing credible intervals)."
  },
  {
    "objectID": "qmd/islp8.html#summary-of-tree-ensemble-methods",
    "href": "qmd/islp8.html#summary-of-tree-ensemble-methods",
    "title": "Tree-Based Methods",
    "section": "Summary of Tree Ensemble Methods",
    "text": "Summary of Tree Ensemble Methods\nThis table summarizes the key characteristics of the ensemble methods we‚Äôve discussed:\n\n\n\n\n\n\n\n\n\nMethod\nTree Growth\nData Sampling\nKey Idea\n\n\n\n\nBagging\nIndependent\nBootstrapped\nAverage many trees to reduce variance.\n\n\nRandom Forests\nIndependent\nBootstrapped +\nDecorrelate trees by limiting the predictors considered at each split.\n\n\n\n\nRandom Subset\n\n\n\nBoosting\nSequential\nNone (Modified Data)\nLearn slowly by fitting small trees to the residuals.\n\n\nBART\nSequential,\nNone\nPerturb trees (modify existing trees) to avoid local optima and provide a Bayesian interpretation.\n\n\n\nPerturbed"
  },
  {
    "objectID": "qmd/islp8.html#summary",
    "href": "qmd/islp8.html#summary",
    "title": "Tree-Based Methods",
    "section": "Summary",
    "text": "Summary\n\nTree-Based Methods: Powerful and versatile tools for both regression and classification.\nInterpretability vs.¬†Accuracy: Single decision trees offer high interpretability but may sacrifice predictive accuracy.\nEnsemble Methods: Enhance performance by combining multiple trees. Bagging, random forests, boosting, and BART represent different strategies for creating ensembles.\nChoosing the Right Method: The best ensemble method depends on the specific dataset and problem. Consider factors like data structure, desired interpretability, and computational resources."
  },
  {
    "objectID": "qmd/islp8.html#thoughts-and-discussion",
    "href": "qmd/islp8.html#thoughts-and-discussion",
    "title": "Tree-Based Methods",
    "section": "Thoughts and Discussion ü§î",
    "text": "Thoughts and Discussion ü§î\n\nInterpretability: When might a single decision tree be preferred over an ensemble method, even if the ensemble method has slightly higher accuracy? (Hint: Think about situations where explaining the model to stakeholders is crucial, such as in medical diagnosis or loan applications.)\nMethod Selection: How might you choose between bagging, random forests, and boosting for a particular problem? What factors would you consider? (Hint: Think about the size of the dataset, the number of predictors, and the potential for strong predictors to dominate.)\nReal-World Applications: Can you think of real-world scenarios where tree-based methods would be particularly well-suited? (Hint: Consider applications like fraud detection, customer churn prediction, or image classification.)\nLimitations: What are some limitations of tree-based methods, even with ensemble techniques? When might they not be the best choice? (Hint: Think about situations with very high-dimensional data or where the underlying relationship is very smooth and linear.)"
  },
  {
    "objectID": "qmd/islp11.html",
    "href": "qmd/islp11.html",
    "title": "Survival Analysis",
    "section": "",
    "text": "Welcome to the fascinating world of survival analysis! üï∞Ô∏è"
  },
  {
    "objectID": "qmd/islp11.html#introduction",
    "href": "qmd/islp11.html#introduction",
    "title": "Survival Analysis",
    "section": "",
    "text": "Welcome to the fascinating world of survival analysis! üï∞Ô∏è"
  },
  {
    "objectID": "qmd/islp11.html#introduction-1",
    "href": "qmd/islp11.html#introduction-1",
    "title": "Survival Analysis",
    "section": "Introduction",
    "text": "Introduction\nIn this chapter, we delve into analyzing a unique type of data: the time until an event occurs.\n\n\n\n\n\n\nThis is different from typical regression problems because we are not just predicting a value, but the time it takes for something to happen."
  },
  {
    "objectID": "qmd/islp11.html#introduction-the-challenge-of-censoring",
    "href": "qmd/islp11.html#introduction-the-challenge-of-censoring",
    "title": "Survival Analysis",
    "section": "Introduction: The Challenge of Censoring",
    "text": "Introduction: The Challenge of Censoring\nThe key challenge in survival analysis is censoring.\n\n\n\n\n\n\nCensoring means we don‚Äôt always know the exact time the event occurs."
  },
  {
    "objectID": "qmd/islp11.html#introduction-censoring-example",
    "href": "qmd/islp11.html#introduction-censoring-example",
    "title": "Survival Analysis",
    "section": "Introduction: Censoring Example",
    "text": "Introduction: Censoring Example\nThink of a medical study tracking patient survival after cancer treatment. üè•\n\nSome patients might still be alive at the study‚Äôs end. üßë‚Äç‚öïÔ∏è\n\n\nWe know they survived at least that long, but not their exact survival time. This is censored data. ü§î"
  },
  {
    "objectID": "qmd/islp11.html#introduction-censoring-example---visual",
    "href": "qmd/islp11.html#introduction-censoring-example---visual",
    "title": "Survival Analysis",
    "section": "Introduction: Censoring Example - Visual",
    "text": "Introduction: Censoring Example - Visual\n\n\n\nCensored Data Example\n\n\n\n\n\n\n\n\nThis image illustrates different types of censoring. We‚Äôll focus on right-censoring, where we only know the event happened after a certain time."
  },
  {
    "objectID": "qmd/islp11.html#key-concepts",
    "href": "qmd/islp11.html#key-concepts",
    "title": "Survival Analysis",
    "section": "Key Concepts",
    "text": "Key Concepts\nLet‚Äôs define some essential terms.\n\n\n\nSurvival Analysis: Statistical methods for analyzing time-to-event data.\nCensored Data: Observations where the event of interest has not occurred for all subjects by the end of the observation period.\nEvent: The outcome of interest (e.g., death, recovery, machine failure, customer churn).\nSurvival Time: The time until the event occurs.\n\n\n\n\n\nTime to event concept."
  },
  {
    "objectID": "qmd/islp11.html#key-concepts-survival-analysis",
    "href": "qmd/islp11.html#key-concepts-survival-analysis",
    "title": "Survival Analysis",
    "section": "Key Concepts: Survival Analysis",
    "text": "Key Concepts: Survival Analysis\nSurvival Analysis: Statistical methods for analyzing time-to-event data.\n\n\n\n\n\n\nSurvival analysis focuses on the distribution of time until an event. It‚Äôs not just about whether an event happens, but when. It‚Äôs like being a detective, but instead of solving a crime, you‚Äôre solving for time! üïµÔ∏è‚Äç‚ôÄÔ∏è‚è∞"
  },
  {
    "objectID": "qmd/islp11.html#key-concepts-censored-data",
    "href": "qmd/islp11.html#key-concepts-censored-data",
    "title": "Survival Analysis",
    "section": "Key Concepts: Censored Data",
    "text": "Key Concepts: Censored Data\nCensored Data: Observations where the event of interest has not occurred for all subjects by the end of the observation period.\n\n\n\n\n\n\nCensoring means we have incomplete information about the event time. We only know a lower bound (right-censoring), upper bound (left-censoring), or an interval (interval-censoring). Think of it like reading a book with missing pages ‚Äì you know something happened, but not the full story. üìñ‚úÇÔ∏è"
  },
  {
    "objectID": "qmd/islp11.html#key-concepts-event",
    "href": "qmd/islp11.html#key-concepts-event",
    "title": "Survival Analysis",
    "section": "Key Concepts: Event",
    "text": "Key Concepts: Event\nEvent: The outcome of interest (e.g., death, recovery, machine failure, customer churn).\n\n\n\n\n\n\nThe ‚Äúevent‚Äù can be anything we‚Äôre interested in tracking the time to. It doesn‚Äôt have to be something negative! It could be a machine breaking down üõ†Ô∏è, a customer leaving üèÉ, or even a plant flowering! üå∏"
  },
  {
    "objectID": "qmd/islp11.html#key-concepts-survival-time",
    "href": "qmd/islp11.html#key-concepts-survival-time",
    "title": "Survival Analysis",
    "section": "Key Concepts: Survival Time",
    "text": "Key Concepts: Survival Time\nSurvival Time: The time until the event occurs.\n\n\n\n\n\n\nThis is the variable we‚Äôre ultimately trying to understand and predict. We often denote it with the letter T. It‚Äôs the unknown we‚Äôre trying to uncover! üîç"
  },
  {
    "objectID": "qmd/islp11.html#introduction-summary",
    "href": "qmd/islp11.html#introduction-summary",
    "title": "Survival Analysis",
    "section": "Introduction: Summary",
    "text": "Introduction: Summary\nWe will explore how to deal with censoring and effectively extract information using tools like survival analysis. This is like learning a new language to decipher incomplete data! üó£Ô∏è"
  },
  {
    "objectID": "qmd/islp11.html#why-survival-analysis",
    "href": "qmd/islp11.html#why-survival-analysis",
    "title": "Survival Analysis",
    "section": "Why Survival Analysis?",
    "text": "Why Survival Analysis?\nSurvival analysis isn‚Äôt limited to medical studies. It‚Äôs a versatile tool with broad applications! üåç"
  },
  {
    "objectID": "qmd/islp11.html#why-survival-analysis---medicine",
    "href": "qmd/islp11.html#why-survival-analysis---medicine",
    "title": "Survival Analysis",
    "section": "Why Survival Analysis? - Medicine",
    "text": "Why Survival Analysis? - Medicine\n\nMedicine: Predicting patient survival times, time to disease recurrence.\n\n\n\n\n\n\n\nSurvival analysis helps doctors estimate prognosis and evaluate treatment effectiveness. It allows them to make informed decisions and provide better care. It‚Äôs like giving doctors a crystal ball, but based on data! üîÆü©∫"
  },
  {
    "objectID": "qmd/islp11.html#why-survival-analysis---business",
    "href": "qmd/islp11.html#why-survival-analysis---business",
    "title": "Survival Analysis",
    "section": "Why Survival Analysis? - Business",
    "text": "Why Survival Analysis? - Business\n\nBusiness: Modeling customer churn (time until a customer cancels a subscription).\n\n\n\n\n\n\n\nUnderstanding churn allows businesses to take proactive steps to retain customers. Knowing when a customer might leave is as valuable as knowing why. It helps businesses keep their customers happy! üòäüíº"
  },
  {
    "objectID": "qmd/islp11.html#why-survival-analysis---engineering",
    "href": "qmd/islp11.html#why-survival-analysis---engineering",
    "title": "Survival Analysis",
    "section": "Why Survival Analysis? - Engineering",
    "text": "Why Survival Analysis? - Engineering\n\nEngineering: Assessing the reliability of components (time until failure).\n\n\n\n\n\n\n\nReliability analysis helps engineers design more durable and dependable products. It‚Äôs about building things that last! Think of it as making sure your bridge doesn‚Äôt collapse! üåâüí™"
  },
  {
    "objectID": "qmd/islp11.html#why-survival-analysis---finance",
    "href": "qmd/islp11.html#why-survival-analysis---finance",
    "title": "Survival Analysis",
    "section": "Why Survival Analysis? - Finance",
    "text": "Why Survival Analysis? - Finance\n\nFinance: Evaluating credit risk (time until default).\n\n\n\n\n\n\n\nSurvival models can predict the likelihood of loan defaults, informing lending decisions. This helps banks and lenders make smarter choices. It‚Äôs like having a financial advisor who can see into the future! üí∞üîÆ"
  },
  {
    "objectID": "qmd/islp11.html#why-survival-analysis---beyond-time",
    "href": "qmd/islp11.html#why-survival-analysis---beyond-time",
    "title": "Survival Analysis",
    "section": "Why Survival Analysis? - Beyond Time",
    "text": "Why Survival Analysis? - Beyond Time\n\nEven beyond time: Modeling weight when scales have upper limits. Any weight above that limit is censored.\n\n\n\n\n\n\n\nThe concept of censoring extends to situations with measurement limitations. If your scale only goes up to 300 lbs, and someone weighs more, you only know their weight is at least 300 lbs. It‚Äôs like measuring with a ruler that‚Äôs too short! üìèü§î"
  },
  {
    "objectID": "qmd/islp11.html#key-insight",
    "href": "qmd/islp11.html#key-insight",
    "title": "Survival Analysis",
    "section": "Key Insight",
    "text": "Key Insight\nKey Insight: Survival analysis techniques allow us to work with incomplete information.\n\nWe don‚Äôt need to observe the event for every individual to gain valuable insights. This is like solving a puzzle with missing pieces! üß©"
  },
  {
    "objectID": "qmd/islp11.html#survival-and-censoring-times",
    "href": "qmd/islp11.html#survival-and-censoring-times",
    "title": "Survival Analysis",
    "section": "Survival and Censoring Times",
    "text": "Survival and Censoring Times\nLet‚Äôs define some core concepts mathematically."
  },
  {
    "objectID": "qmd/islp11.html#survival-time-t",
    "href": "qmd/islp11.html#survival-time-t",
    "title": "Survival Analysis",
    "section": "Survival Time (T)",
    "text": "Survival Time (T)\n\nSurvival Time (T): The true time until the event of interest occurs. Also called failure time or event time.\n\n\n\n\n\n\n\nThis is the underlying quantity we‚Äôre interested in, even if we don‚Äôt always observe it directly. It‚Äôs the ‚Äútrue‚Äù time, even if we don‚Äôt know it yet."
  },
  {
    "objectID": "qmd/islp11.html#censoring-time-c",
    "href": "qmd/islp11.html#censoring-time-c",
    "title": "Survival Analysis",
    "section": "Censoring Time (C)",
    "text": "Censoring Time (C)\n\nCensoring Time (C): The time at which observation ends, either because the study ends, the patient drops out, or the event occurs.\n\n\n\n\n\n\n\nThis represents the point at which we lose track of the individual. It‚Äôs like the end of a movie, but you don‚Äôt know what happens next! üé¨‚ùì"
  },
  {
    "objectID": "qmd/islp11.html#observed-time-y",
    "href": "qmd/islp11.html#observed-time-y",
    "title": "Survival Analysis",
    "section": "Observed Time (Y)",
    "text": "Observed Time (Y)\n\nObserved Time (Y): What we actually see: either the survival time or the censoring time. Mathematically, Y = min(T, C).\n\n\n\n\n\n\n\nThis is the data we have to work with ‚Äì the minimum of the true survival time and the censoring time. It‚Äôs what we actually observe."
  },
  {
    "objectID": "qmd/islp11.html#status-indicator-Œ¥",
    "href": "qmd/islp11.html#status-indicator-Œ¥",
    "title": "Survival Analysis",
    "section": "Status Indicator (Œ¥)",
    "text": "Status Indicator (Œ¥)\n\nStatus Indicator (Œ¥): Tells us whether we observed the event (Œ¥ = 1) or the observation was censored (Œ¥ = 0).\n\n\\[\n\\delta =\n\\begin{cases}\n1 & \\text{if } T \\leq C \\text{ (event observed)} \\\\\n0 & \\text{if } T &gt; C \\text{ (censored)}\n\\end{cases}\n\\]\n\n\n\n\n\n\nThis indicator variable tells us whether Y represents a true survival time or a censoring time. It‚Äôs like a flag that tells us if we have complete information or not. üö©"
  },
  {
    "objectID": "qmd/islp11.html#survival-and-censoring-times---summary",
    "href": "qmd/islp11.html#survival-and-censoring-times---summary",
    "title": "Survival Analysis",
    "section": "Survival and Censoring Times - Summary",
    "text": "Survival and Censoring Times - Summary\nThese variables (T, C, Y, and Œ¥) form the basis of survival analysis. They are the building blocks of our analysis! üß±"
  },
  {
    "objectID": "qmd/islp11.html#visualizing-censored-data",
    "href": "qmd/islp11.html#visualizing-censored-data",
    "title": "Survival Analysis",
    "section": "Visualizing Censored Data",
    "text": "Visualizing Censored Data\nLet‚Äôs consider a simple example to illustrate these concepts.\n\n\n\nVisualizing Censored Data"
  },
  {
    "objectID": "qmd/islp11.html#visualizing-censored-data-patients-1-3",
    "href": "qmd/islp11.html#visualizing-censored-data-patients-1-3",
    "title": "Survival Analysis",
    "section": "Visualizing Censored Data: Patients 1 & 3",
    "text": "Visualizing Censored Data: Patients 1 & 3\n\n\n\nVisualizing Censored Data\n\n\n\nPatients 1 & 3: The event (e.g., death) was observed. We know their exact survival times.\n\n\n\n\n\n\n\nThese are uncensored observations. The event happened during the observation period. We have complete information for these patients."
  },
  {
    "objectID": "qmd/islp11.html#visualizing-censored-data-patient-2",
    "href": "qmd/islp11.html#visualizing-censored-data-patient-2",
    "title": "Survival Analysis",
    "section": "Visualizing Censored Data: Patient 2",
    "text": "Visualizing Censored Data: Patient 2\n\n\n\nVisualizing Censored Data\n\n\n\nPatient 2: Was alive at the end of the study. Their survival time is censored.\n\n\n\n\n\n\n\nThis is a right-censored observation. We know the patient survived at least until the end of the study, but we don‚Äôt know their exact survival time."
  },
  {
    "objectID": "qmd/islp11.html#visualizing-censored-data-patient-4",
    "href": "qmd/islp11.html#visualizing-censored-data-patient-4",
    "title": "Survival Analysis",
    "section": "Visualizing Censored Data: Patient 4",
    "text": "Visualizing Censored Data: Patient 4\n\n\n\nVisualizing Censored Data\n\n\n\nPatient 4: Dropped out of the study. Also censored.\n\n\n\n\n\n\n\nThis is another example of right-censoring. We lost track of the patient before the event occurred, so we don‚Äôt know their exact survival time."
  },
  {
    "objectID": "qmd/islp11.html#visualizing-censored-data-importance-of-censored-data",
    "href": "qmd/islp11.html#visualizing-censored-data-importance-of-censored-data",
    "title": "Survival Analysis",
    "section": "Visualizing Censored Data: Importance of Censored Data",
    "text": "Visualizing Censored Data: Importance of Censored Data\nImportant: Censored observations still provide valuable information! They tell us the event didn‚Äôt happen before a certain time. This is crucial for accurate analysis!"
  },
  {
    "objectID": "qmd/islp11.html#a-closer-look-at-censoring",
    "href": "qmd/islp11.html#a-closer-look-at-censoring",
    "title": "Survival Analysis",
    "section": "A Closer Look at Censoring",
    "text": "A Closer Look at Censoring\nCensoring isn‚Äôt always straightforward. The reason for censoring matters."
  },
  {
    "objectID": "qmd/islp11.html#independent-censoring",
    "href": "qmd/islp11.html#independent-censoring",
    "title": "Survival Analysis",
    "section": "Independent Censoring",
    "text": "Independent Censoring\n\nIndependent Censoring: The censoring mechanism is unrelated to the survival time (conditional on features). This is a crucial assumption for many survival analysis methods.\n\n\n\n\n\n\n\nThis means that the reason someone is censored doesn‚Äôt provide extra information about their likely survival time, beyond what we already know from their features. Think of it like a coin flip ‚Äì the reason you stop flipping (censoring) doesn‚Äôt affect the probability of heads or tails (survival). ü™ô"
  },
  {
    "objectID": "qmd/islp11.html#independent-censoring-example-of-violation",
    "href": "qmd/islp11.html#independent-censoring-example-of-violation",
    "title": "Survival Analysis",
    "section": "Independent Censoring: Example of Violation",
    "text": "Independent Censoring: Example of Violation\n\nExample of violation: Patients dropping out because they are very sick. This biases the results, making survival times seem longer than they are.\n\n\n\n\n\n\n\nThis is informative censoring. The censoring is related to the survival time, which violates the independence assumption. It‚Äôs like stacking the deck in a card game ‚Äì the outcome is no longer random! üÉè‚ùå"
  },
  {
    "objectID": "qmd/islp11.html#types-of-censoring",
    "href": "qmd/islp11.html#types-of-censoring",
    "title": "Survival Analysis",
    "section": "Types of Censoring",
    "text": "Types of Censoring\nTypes of Censoring\n\nRight Censoring: Most common. We know the event time is greater than the observed time (T ‚â• Y).\nLeft Censoring: We know the event time is less than or equal to the observed time.\nInterval Censoring: We know the event time falls within a specific interval."
  },
  {
    "objectID": "qmd/islp11.html#types-of-censoring---illustration",
    "href": "qmd/islp11.html#types-of-censoring---illustration",
    "title": "Survival Analysis",
    "section": "Types of Censoring - Illustration",
    "text": "Types of Censoring - Illustration\n\n\n\nalt text\n\n\n\n\n\n\n\n\nThe image shows the example of Right Censoring"
  },
  {
    "objectID": "qmd/islp11.html#focus-on-right-censoring",
    "href": "qmd/islp11.html#focus-on-right-censoring",
    "title": "Survival Analysis",
    "section": "Focus on Right Censoring",
    "text": "Focus on Right Censoring\n\n\n\n\n\n\nWe will focus mainly on right censoring, the most prevalent type in practice. It‚Äôs the most common scenario we encounter."
  },
  {
    "objectID": "qmd/islp11.html#the-kaplan-meier-survival-curve",
    "href": "qmd/islp11.html#the-kaplan-meier-survival-curve",
    "title": "Survival Analysis",
    "section": "The Kaplan-Meier Survival Curve",
    "text": "The Kaplan-Meier Survival Curve\nThe survival curve, denoted by S(t), is a fundamental concept. It gives the probability of surviving past time t:\n\\[S(t) = Pr(T &gt; t)\\]\n\n\n\n\n\n\nThis is the cornerstone of survival analysis! It tells us the probability of ‚Äúsurviving‚Äù (not experiencing the event) beyond a certain time."
  },
  {
    "objectID": "qmd/islp11.html#survival-curve-interpretation",
    "href": "qmd/islp11.html#survival-curve-interpretation",
    "title": "Survival Analysis",
    "section": "Survival Curve Interpretation",
    "text": "Survival Curve Interpretation\nThe larger the value of S(t), the more likely the event will occur at time greater than t.\n\n\n\n\n\n\nA higher S(t) means a higher chance of not experiencing the event before time t. It‚Äôs like the probability of staying dry in the rain ‚Äì the longer you stay inside, the higher the probability of staying dry! ‚òî"
  },
  {
    "objectID": "qmd/islp11.html#kaplan-meier-estimator",
    "href": "qmd/islp11.html#kaplan-meier-estimator",
    "title": "Survival Analysis",
    "section": "Kaplan-Meier Estimator",
    "text": "Kaplan-Meier Estimator\nHow do we estimate S(t) from data with censoring? The Kaplan-Meier estimator is a powerful tool. It‚Äôs like a statistical superhero for survival data! ü¶∏‚Äç‚ôÄÔ∏è"
  },
  {
    "objectID": "qmd/islp11.html#estimating-the-survival-curve-challenges",
    "href": "qmd/islp11.html#estimating-the-survival-curve-challenges",
    "title": "Survival Analysis",
    "section": "Estimating the Survival Curve: Challenges",
    "text": "Estimating the Survival Curve: Challenges\nLet‚Äôs consider the BrainCancer dataset. We want to estimate S(20): the probability of surviving at least 20 months. Naive approaches fail:"
  },
  {
    "objectID": "qmd/islp11.html#naive-approach-1-using-y-20",
    "href": "qmd/islp11.html#naive-approach-1-using-y-20",
    "title": "Survival Analysis",
    "section": "Naive Approach 1: Using Y > 20",
    "text": "Naive Approach 1: Using Y &gt; 20\n\nSimply using Y &gt; 20: This ignores that Y is not always the true survival time (due to censoring). It underestimates survival.\n\n\n\n\n\n\n\nUsing Y directly treats censored observations as if the event occurred at the censoring time, leading to an underestimate. It‚Äôs like saying everyone who left the party early went home, even if they might have gone somewhere else! üéâüè†‚ùì"
  },
  {
    "objectID": "qmd/islp11.html#naive-approach-2-ignoring-censored-observations",
    "href": "qmd/islp11.html#naive-approach-2-ignoring-censored-observations",
    "title": "Survival Analysis",
    "section": "Naive Approach 2: Ignoring Censored Observations",
    "text": "Naive Approach 2: Ignoring Censored Observations\n\nIgnoring censored observations: This throws away valuable information. A patient censored at 19.9 months almost certainly would have survived past 20.\n\n\n\n\n\n\n\nDiscarding censored data reduces the sample size and biases the results. It‚Äôs like throwing away puzzle pieces ‚Äì you can‚Äôt see the full picture! üß©üóëÔ∏è"
  },
  {
    "objectID": "qmd/islp11.html#kaplan-meier-the-solution",
    "href": "qmd/islp11.html#kaplan-meier-the-solution",
    "title": "Survival Analysis",
    "section": "Kaplan-Meier: The Solution",
    "text": "Kaplan-Meier: The Solution\n\n\n\n\n\n\nThe Kaplan-Meier estimator elegantly handles censoring to provide a more accurate estimate. It‚Äôs the clever way to deal with incomplete data!üí°"
  },
  {
    "objectID": "qmd/islp11.html#the-kaplan-meier-estimator-intuition",
    "href": "qmd/islp11.html#the-kaplan-meier-estimator-intuition",
    "title": "Survival Analysis",
    "section": "The Kaplan-Meier Estimator: Intuition",
    "text": "The Kaplan-Meier Estimator: Intuition\nThe Kaplan-Meier estimator works sequentially, considering events as they unfold in time. It‚Äôs like watching a movie frame by frame! üéûÔ∏è"
  },
  {
    "objectID": "qmd/islp11.html#kaplan-meier-key-idea",
    "href": "qmd/islp11.html#kaplan-meier-key-idea",
    "title": "Survival Analysis",
    "section": "Kaplan-Meier: Key Idea",
    "text": "Kaplan-Meier: Key Idea\n\nKey Idea: At each death time, we calculate the conditional probability of surviving that time point, given survival up to that point.\n\n\n\nWe then multiply these conditional probabilities together to get the overall survival probability. It‚Äôs like calculating the probability of a chain of events! üîó"
  },
  {
    "objectID": "qmd/islp11.html#kaplan-meier-notation",
    "href": "qmd/islp11.html#kaplan-meier-notation",
    "title": "Survival Analysis",
    "section": "Kaplan-Meier: Notation",
    "text": "Kaplan-Meier: Notation\n\nLet \\(d_1 &lt; d_2 &lt; ... &lt; d_K\\) be the distinct death times.\n\\(q_k\\): Number of deaths at time \\(d_k\\).\n\\(r_k\\): Number of individuals at risk (alive and in the study) just before \\(d_k\\). This is the risk set.\n\n\n\n\n\n\n\nThese are the ingredients we need for our Kaplan-Meier recipe! üç≤"
  },
  {
    "objectID": "qmd/islp11.html#kaplan-meier-estimator-formula",
    "href": "qmd/islp11.html#kaplan-meier-estimator-formula",
    "title": "Survival Analysis",
    "section": "Kaplan-Meier Estimator Formula",
    "text": "Kaplan-Meier Estimator Formula\nThe Kaplan-Meier estimator formula is:\n\\[\n\\hat{S}(d_k) = \\prod_{j=1}^{k} \\left( \\frac{r_j - q_j}{r_j} \\right)\n\\]\n\n\n\n\n\n\nThis looks complicated, but we‚Äôll break it down step-by-step!"
  },
  {
    "objectID": "qmd/islp11.html#kaplan-meier-estimator-step-function",
    "href": "qmd/islp11.html#kaplan-meier-estimator-step-function",
    "title": "Survival Analysis",
    "section": "Kaplan-Meier Estimator: Step Function",
    "text": "Kaplan-Meier Estimator: Step Function\nFor times between death times, \\(\\hat{S}(t)\\) remains constant, creating a step-like curve. It‚Äôs like a staircase, not a smooth slope! ü™ú"
  },
  {
    "objectID": "qmd/islp11.html#the-kaplan-meier-estimator-explanation",
    "href": "qmd/islp11.html#the-kaplan-meier-estimator-explanation",
    "title": "Survival Analysis",
    "section": "The Kaplan-Meier Estimator: Explanation",
    "text": "The Kaplan-Meier Estimator: Explanation\nThe formula is derived from the law of total probability:\n\\[\nPr(T &gt; d_k) = Pr(T &gt; d_k | T &gt; d_{k-1})Pr(T &gt; d_{k-1}) + Pr(T&gt;d_k|T \\leq d_{k-1})Pr(T\\leq d_{k-1})\n\\]\n\n\n\n\n\n\nThis is a fundamental probability rule, breaking down a complex probability into simpler parts."
  },
  {
    "objectID": "qmd/islp11.html#kaplan-meier-simplification",
    "href": "qmd/islp11.html#kaplan-meier-simplification",
    "title": "Survival Analysis",
    "section": "Kaplan-Meier: Simplification",
    "text": "Kaplan-Meier: Simplification\nSince \\(d_{k-1} &lt; d_k\\), \\(Pr(T&gt;d_k|T \\leq d_{k-1}) = 0\\), then the above formula is:\n\\[\nS(d_k) = Pr(T &gt; d_k) = Pr(T &gt; d_k | T &gt; d_{k-1})Pr(T &gt; d_{k-1})\n\\]\n\n\n\n\n\n\nBecause if you‚Äôve already survived past \\(d_{k-1}\\), you can‚Äôt possibly experience the event before \\(d_{k-1}\\)!"
  },
  {
    "objectID": "qmd/islp11.html#kaplan-meier-recursive-formula",
    "href": "qmd/islp11.html#kaplan-meier-recursive-formula",
    "title": "Survival Analysis",
    "section": "Kaplan-Meier: Recursive Formula",
    "text": "Kaplan-Meier: Recursive Formula\nPlug in \\(S(t)\\) and rearrange the above formula: \\[\nS(d_k) = Pr(T&gt;d_k|T&gt;d_{k-1}) \\times ... \\times Pr(T&gt;d_2|T&gt;d_1)Pr(T&gt;d_1)\n\\]\n\n\n\n\n\n\nWe‚Äôre expressing the overall survival probability as a product of conditional probabilities."
  },
  {
    "objectID": "qmd/islp11.html#kaplan-meier-conditional-probability-estimation",
    "href": "qmd/islp11.html#kaplan-meier-conditional-probability-estimation",
    "title": "Survival Analysis",
    "section": "Kaplan-Meier: Conditional Probability Estimation",
    "text": "Kaplan-Meier: Conditional Probability Estimation\nWe estimate each term on the right-hand side using the fraction of the risk set at time \\(d_j\\) who survived past time \\(d_j\\):\n\\[\\widehat{Pr}(T &gt; d_j | T &gt; d_{j-1}) = (r_j - q_j) / r_j\\]\n\n\n\n\n\n\nThis is the heart of the Kaplan-Meier estimator! We‚Äôre using the observed data to estimate these conditional probabilities."
  },
  {
    "objectID": "qmd/islp11.html#kaplan-meier-final-estimator",
    "href": "qmd/islp11.html#kaplan-meier-final-estimator",
    "title": "Survival Analysis",
    "section": "Kaplan-Meier: Final Estimator",
    "text": "Kaplan-Meier: Final Estimator\nFinally, we arrive at the Kaplan-Meier estimator:\n\\[\n\\hat{S}(d_k) = \\prod_{j=1}^{k} \\left( \\frac{r_j - q_j}{r_j} \\right)\n\\]\n\n\n\n\n\n\nThis is the formula we use to calculate the estimated survival probabilities!"
  },
  {
    "objectID": "qmd/islp11.html#kaplan-meier-curve-example",
    "href": "qmd/islp11.html#kaplan-meier-curve-example",
    "title": "Survival Analysis",
    "section": "Kaplan-Meier Curve: Example",
    "text": "Kaplan-Meier Curve: Example\nHere‚Äôs the Kaplan-Meier curve for the BrainCancer data:\n\n\n\nKaplan-Meier Curve for BrainCancer Data"
  },
  {
    "objectID": "qmd/islp11.html#kaplan-meier-curve-interpretation",
    "href": "qmd/islp11.html#kaplan-meier-curve-interpretation",
    "title": "Survival Analysis",
    "section": "Kaplan-Meier Curve: Interpretation",
    "text": "Kaplan-Meier Curve: Interpretation\n\n\n\nKaplan-Meier Curve for BrainCancer Data\n\n\n\nThe curve steps down at each observed death time.\nThe height of the curve at any time point represents the estimated survival probability.\nThe estimated probability of survival past 20 months is 71%.\n\n\n\n\n\n\n\nWe can read the estimated survival probabilities directly from the curve!"
  },
  {
    "objectID": "qmd/islp11.html#comparing-survival-curves-the-log-rank-test",
    "href": "qmd/islp11.html#comparing-survival-curves-the-log-rank-test",
    "title": "Survival Analysis",
    "section": "Comparing Survival Curves: The Log-Rank Test",
    "text": "Comparing Survival Curves: The Log-Rank Test\nOften, we want to compare survival curves between groups (e.g., males vs.¬†females). Is there a significant difference in survival between groups? ü§î"
  },
  {
    "objectID": "qmd/islp11.html#log-rank-test",
    "href": "qmd/islp11.html#log-rank-test",
    "title": "Survival Analysis",
    "section": "Log-Rank Test",
    "text": "Log-Rank Test\n\n\n\nLog-Rank Test Example\n\n\nThe log-rank test is a statistical test for comparing survival curves. It accounts for censoring. It‚Äôs like a statistical judge deciding if there‚Äôs a real difference! ‚öñÔ∏è"
  },
  {
    "objectID": "qmd/islp11.html#log-rank-test-details",
    "href": "qmd/islp11.html#log-rank-test-details",
    "title": "Survival Analysis",
    "section": "Log-Rank Test: Details",
    "text": "Log-Rank Test: Details\nThe log-rank test examines events sequentially, like the Kaplan-Meier estimator. It‚Äôs a step-by-step comparison!"
  },
  {
    "objectID": "qmd/islp11.html#log-rank-test-2x2-table",
    "href": "qmd/islp11.html#log-rank-test-2x2-table",
    "title": "Survival Analysis",
    "section": "Log-Rank Test: 2x2 Table",
    "text": "Log-Rank Test: 2x2 Table\n\nAt each death time \\(d_k\\), we construct a 2x2 table:\n\n\n\n\n\nGroup 1\nGroup 2\nTotal\n\n\n\n\nDied\n\\(q_{1k}\\)\n\\(q_{2k}\\)\n\\(q_k\\)\n\n\nSurvived\n\\(r_{1k}-q_{1k}\\)\n\\(r_{2k}-q_{2k}\\)\n\\(r_k-q_k\\)\n\n\nTotal\n\\(r_{1k}\\)\n\\(r_{2k}\\)\n\\(r_k\\)\n\n\n\n\n\n\n\n\n\n\n\\(r_{1k}\\), \\(r_{2k}\\): Number at risk in each group at time \\(d_k\\).\n\\(q_{1k}\\), \\(q_{2k}\\): Number of deaths in each group at time \\(d_k\\).\n\n\n\n\n\n\n\nThis table summarizes the observed events and the number of individuals at risk in each group at each death time."
  },
  {
    "objectID": "qmd/islp11.html#log-rank-test-key-idea",
    "href": "qmd/islp11.html#log-rank-test-key-idea",
    "title": "Survival Analysis",
    "section": "Log-Rank Test: Key Idea",
    "text": "Log-Rank Test: Key Idea\n\nKey Idea: If there‚Äôs no difference in survival, we‚Äôd expect the proportion of deaths in each group to be proportional to the number at risk. It‚Äôs like expecting a fair coin to land heads and tails equally often! ü™ô"
  },
  {
    "objectID": "qmd/islp11.html#log-rank-test-statistic-w",
    "href": "qmd/islp11.html#log-rank-test-statistic-w",
    "title": "Survival Analysis",
    "section": "Log-Rank Test Statistic (W)",
    "text": "Log-Rank Test Statistic (W)\nThe log-rank test statistic (W) is calculated based on the observed and expected number of deaths in group 1:\n\\[\nW = \\frac{\\sum_{k=1}^{K}(q_{1k} - \\mu_k)}{\\sqrt{\\sum_{k=1}^{K}Var(q_{1k})}}\n\\] where $ _k = q_k $ and \\(Var(q_{1k}) = \\frac{q_k(r_{1k}/r_k)(1-r_{1k}/r_k)(r_k - q_k)}{r_k - 1}\\)\n\n\n\n\n\n\nThis formula measures the difference between what we observed and what we would expect if there were no difference between the groups."
  },
  {
    "objectID": "qmd/islp11.html#log-rank-test-null-distribution",
    "href": "qmd/islp11.html#log-rank-test-null-distribution",
    "title": "Survival Analysis",
    "section": "Log-Rank Test: Null Distribution",
    "text": "Log-Rank Test: Null Distribution\nUnder the null hypothesis (no difference in survival), W approximately follows a standard normal distribution. This allows us to calculate a p-value!"
  },
  {
    "objectID": "qmd/islp11.html#log-rank-test-brain-cancer-example",
    "href": "qmd/islp11.html#log-rank-test-brain-cancer-example",
    "title": "Survival Analysis",
    "section": "Log-Rank Test: Brain Cancer Example",
    "text": "Log-Rank Test: Brain Cancer Example\nComparing survival times of males and females in the BrainCancer data:"
  },
  {
    "objectID": "qmd/islp11.html#log-rank-test-results",
    "href": "qmd/islp11.html#log-rank-test-results",
    "title": "Survival Analysis",
    "section": "Log-Rank Test: Results",
    "text": "Log-Rank Test: Results\n\nLog-rank test statistic W = 1.2.\nTwo-sided p-value = 0.2 (using the theoretical null distribution).\nWe cannot reject the null hypothesis of no difference in survival curves between males and females.\n\n\n\n\n\n\n\nIn this case, the data doesn‚Äôt provide strong evidence of a difference in survival between males and females."
  },
  {
    "objectID": "qmd/islp11.html#regression-models-with-a-survival-response",
    "href": "qmd/islp11.html#regression-models-with-a-survival-response",
    "title": "Survival Analysis",
    "section": "Regression Models with a Survival Response",
    "text": "Regression Models with a Survival Response\nSo far, we‚Äôve looked at describing survival curves and comparing them between groups. Now, we want to predict survival time based on covariates (features). It‚Äôs like predicting the future, but with data! üîÆüìä"
  },
  {
    "objectID": "qmd/islp11.html#regression-setup",
    "href": "qmd/islp11.html#regression-setup",
    "title": "Survival Analysis",
    "section": "Regression Setup",
    "text": "Regression Setup\n\nWe have observations of the form \\((Y_i, \\delta_i, X_i)\\), where:\n\n\\(Y_i\\) is the observed time (min(T, C)).\n\\(\\delta_i\\) is the status indicator.\n\\(X_i\\) is a vector of features.\n\nA simple linear regression of log(Y) on X is problematic due to censoring."
  },
  {
    "objectID": "qmd/islp11.html#why-not-regress-on-y-directly",
    "href": "qmd/islp11.html#why-not-regress-on-y-directly",
    "title": "Survival Analysis",
    "section": "Why Not Regress on Y Directly?",
    "text": "Why Not Regress on Y Directly?\n\nWhy not regress on Y directly?: We are interested in T not Y. Y is just what we see, and the difference from T exists due to censoring."
  },
  {
    "objectID": "qmd/islp11.html#the-solution-hazard-function",
    "href": "qmd/islp11.html#the-solution-hazard-function",
    "title": "Survival Analysis",
    "section": "The Solution: Hazard Function",
    "text": "The Solution: Hazard Function\n\nSolution: Use a sequential approach, similar to Kaplan-Meier and the log-rank test. We introduce the hazard function. It‚Äôs like focusing on the instantaneous risk, rather than the overall survival time."
  },
  {
    "objectID": "qmd/islp11.html#the-hazard-function",
    "href": "qmd/islp11.html#the-hazard-function",
    "title": "Survival Analysis",
    "section": "The Hazard Function",
    "text": "The Hazard Function\nThe hazard function, h(t), is also known as the hazard rate or force of mortality. It represents the instantaneous risk of the event occurring at time t, given survival up to time t:\n\\[h(t) = \\lim_{\\Delta t \\to 0} \\frac{Pr(t &lt; T \\leq t + \\Delta t | T &gt; t)}{\\Delta t}\\]\n\n\n\n\n\n\nThis is a key concept for modeling survival data!"
  },
  {
    "objectID": "qmd/islp11.html#hazard-function-intuition",
    "href": "qmd/islp11.html#hazard-function-intuition",
    "title": "Survival Analysis",
    "section": "Hazard Function: Intuition",
    "text": "Hazard Function: Intuition\n\nThink of it as the ‚Äúdeath rate‚Äù in a tiny interval after time t, given survival up to t. It‚Äôs like the risk of slipping on a banana peel right now, given you haven‚Äôt slipped yet! üçå\nIt‚Äôs closely related to the survival curve, S(t).\nIt‚Äôs crucial for modeling survival data as a function of covariates."
  },
  {
    "objectID": "qmd/islp11.html#hazard-function-more-details",
    "href": "qmd/islp11.html#hazard-function-more-details",
    "title": "Survival Analysis",
    "section": "Hazard Function: More Details",
    "text": "Hazard Function: More Details\n\\[\n\\begin{aligned}\nh(t) &= \\lim_{\\Delta t \\to 0} Pr((t&lt;T\\le t+\\Delta t)\\cap (T&gt;t))/\\Delta t \\over Pr(T&gt;t) \\\\\n&= \\lim_{\\Delta t \\to 0} {Pr(t&lt;T\\le t+\\Delta t) / \\Delta t \\over Pr(T&gt;t)} \\\\\n&= {f(t) \\over S(t)}\n\\end{aligned}\n\\]\nwhere\n\\[\nf(t) = \\lim_{\\Delta t\\to 0} {Pr(t&lt;T\\le t+\\Delta t)\\over \\Delta t}\n\\]\n\n\n\n\n\n\nWe can derive the relationship between the hazard function, probability density function and survival function."
  },
  {
    "objectID": "qmd/islp11.html#hazard-function-probability-density-function",
    "href": "qmd/islp11.html#hazard-function-probability-density-function",
    "title": "Survival Analysis",
    "section": "Hazard Function: Probability Density Function",
    "text": "Hazard Function: Probability Density Function\n\\(f(t)\\) is probability density function.\n\n\n\n\n\n\n\\(f(t)\\) is different from \\(S(t)\\), it measures the instantaneous probability."
  },
  {
    "objectID": "qmd/islp11.html#hazard-function-likelihood",
    "href": "qmd/islp11.html#hazard-function-likelihood",
    "title": "Survival Analysis",
    "section": "Hazard Function: Likelihood",
    "text": "Hazard Function: Likelihood\nThe likelihood associated with the i-th observation is:\n\\[\nL_i = \\begin{cases}\nf(y_i) \\quad \\text{if the i-th observation is not censored} \\\\\nS(y_i) \\quad \\text{if the i-th observation is censored}\n\\end{cases} \\\\\n= f(y_i)^{\\delta_i}S(y_i)^{1-\\delta_i}\n\\]\n\n\n\n\n\n\nThis is how we quantify the contribution of each observation to the overall likelihood."
  },
  {
    "objectID": "qmd/islp11.html#likelihood-explanation",
    "href": "qmd/islp11.html#likelihood-explanation",
    "title": "Survival Analysis",
    "section": "Likelihood: Explanation",
    "text": "Likelihood: Explanation\nIf \\(Y=y_i\\) and the i-th observation is not censored, then the likelihood is the probability of dying in a tiny interval around time \\(y_i\\).\n\nIf the i-th observation is censored, then the likelihood is the probability of surviving at least until time \\(y_i\\)"
  },
  {
    "objectID": "qmd/islp11.html#cox-proportional-hazards-model",
    "href": "qmd/islp11.html#cox-proportional-hazards-model",
    "title": "Survival Analysis",
    "section": "Cox Proportional Hazards Model",
    "text": "Cox Proportional Hazards Model\nThe Cox proportional hazards model is a powerful and flexible approach to model the relationship between covariates and the hazard function. It‚Äôs the workhorse of survival regression! üêé"
  },
  {
    "objectID": "qmd/islp11.html#proportional-hazards-assumption",
    "href": "qmd/islp11.html#proportional-hazards-assumption",
    "title": "Survival Analysis",
    "section": "Proportional Hazards Assumption",
    "text": "Proportional Hazards Assumption\nThe Proportional Hazards Assumption:\n\\[h(t|x_i) = h_0(t) \\exp(\\sum_{j=1}^{p} x_{ij}\\beta_j)\\]\n\n\n\n\n\n\nThis is the core assumption of the Cox model!"
  },
  {
    "objectID": "qmd/islp11.html#cox-model-components",
    "href": "qmd/islp11.html#cox-model-components",
    "title": "Survival Analysis",
    "section": "Cox Model: Components",
    "text": "Cox Model: Components\n\n\\(h(t|x_i)\\): Hazard function for an individual with features \\(x_i\\).\n\\(h_0(t)\\): Baseline hazard function. This is the hazard for an individual with all features equal to zero. It‚Äôs left unspecified.\n\\(\\exp(\\sum_{j=1}^{p} x_{ij}\\beta_j)\\): Relative risk. It‚Äôs a multiplicative factor that scales the baseline hazard based on the features.\n\n\n\n\n\n\n\nThe model breaks down the hazard into a baseline part and a part that depends on the features."
  },
  {
    "objectID": "qmd/islp11.html#cox-model-key-features",
    "href": "qmd/islp11.html#cox-model-key-features",
    "title": "Survival Analysis",
    "section": "Cox Model: Key Features",
    "text": "Cox Model: Key Features\n\nThe key is that we don‚Äôt assume a specific form for \\(h_0(t)\\). This makes the model very flexible. It can handle many different underlying hazard patterns!\nA one-unit increase in \\(x_{ij}\\) multiplies the hazard by a factor of \\(\\exp(\\beta_j)\\). This is the proportional hazards assumption ‚Äì the effect of a covariate is constant over time."
  },
  {
    "objectID": "qmd/islp11.html#proportional-hazards-illustration",
    "href": "qmd/islp11.html#proportional-hazards-illustration",
    "title": "Survival Analysis",
    "section": "Proportional Hazards: Illustration",
    "text": "Proportional Hazards: Illustration\n\n\n\nProportional Hazards Illustration"
  },
  {
    "objectID": "qmd/islp11.html#proportional-hazards-interpretation",
    "href": "qmd/islp11.html#proportional-hazards-interpretation",
    "title": "Survival Analysis",
    "section": "Proportional Hazards: Interpretation",
    "text": "Proportional Hazards: Interpretation\n\n\n\nProportional Hazards Illustration\n\n\n\nTop Row: Proportional hazards holds. Log hazard functions are parallel; survival curves don‚Äôt cross. The ratio of hazards between groups is constant over time."
  },
  {
    "objectID": "qmd/islp11.html#proportional-hazards-interpretation-1",
    "href": "qmd/islp11.html#proportional-hazards-interpretation-1",
    "title": "Survival Analysis",
    "section": "Proportional Hazards: Interpretation",
    "text": "Proportional Hazards: Interpretation\n\n\n\nProportional Hazards Illustration\n\n\n\nBottom Row: Proportional hazards doesn‚Äôt hold. Log hazard and survival curves cross. The ratio of hazards changes over time.\n\n\n\n\n\n\n\nThis visual check helps us assess whether the proportional hazards assumption is reasonable."
  },
  {
    "objectID": "qmd/islp11.html#cox-proportional-hazards-model-estimation",
    "href": "qmd/islp11.html#cox-proportional-hazards-model-estimation",
    "title": "Survival Analysis",
    "section": "Cox Proportional Hazards Model: Estimation",
    "text": "Cox Proportional Hazards Model: Estimation\nHow do we estimate the coefficients, \\(\\beta\\), in the Cox model without knowing \\(h_0(t)\\)? We use the partial likelihood. It‚Äôs a clever trick to get around the unknown baseline hazard!"
  },
  {
    "objectID": "qmd/islp11.html#partial-likelihood-intuition",
    "href": "qmd/islp11.html#partial-likelihood-intuition",
    "title": "Survival Analysis",
    "section": "Partial Likelihood: Intuition",
    "text": "Partial Likelihood: Intuition\n\nAssume no ties in failure times.\nConsider the ith observation, which fails at time \\(y_i\\) (\\(\\delta_i = 1\\)).\nWhat‚Äôs the probability that this observation fails at \\(y_i\\), given the set of individuals at risk at that time?\n\n\n\n\n\n\n\nWe‚Äôre focusing on the relative risk of failure, compared to others who are still at risk."
  },
  {
    "objectID": "qmd/islp11.html#partial-likelihood-probability-calculation",
    "href": "qmd/islp11.html#partial-likelihood-probability-calculation",
    "title": "Survival Analysis",
    "section": "Partial Likelihood: Probability Calculation",
    "text": "Partial Likelihood: Probability Calculation\n\nThe probability that i-th observation is the one to fail at time \\(y_i\\) is:\n\n\\[\n\\frac{h_0(y_i) \\exp(\\sum_{j=1}^p x_{ij}\\beta_j)}{\\sum_{i':y_{i'}\\ge y_i}h_0(y_i)\\exp(\\sum_{j=1}^{p}x_{i'j}\\beta_j)} = \\frac{\\exp(\\sum_{j=1}^p x_{ij}\\beta_j)}{\\sum_{i':y_{i'}\\ge y_i}\\exp(\\sum_{j=1}^{p}x_{i'j}\\beta_j)}\n\\]"
  },
  {
    "objectID": "qmd/islp11.html#partial-likelihood-baseline-hazard-cancellation",
    "href": "qmd/islp11.html#partial-likelihood-baseline-hazard-cancellation",
    "title": "Survival Analysis",
    "section": "Partial Likelihood: Baseline Hazard Cancellation",
    "text": "Partial Likelihood: Baseline Hazard Cancellation\n\nCrucially, \\(h_0(y_i)\\) cancels out! This is the magic of the partial likelihood! ‚ú®"
  },
  {
    "objectID": "qmd/islp11.html#the-partial-likelihood",
    "href": "qmd/islp11.html#the-partial-likelihood",
    "title": "Survival Analysis",
    "section": "The Partial Likelihood",
    "text": "The Partial Likelihood\n\nThe partial likelihood is the product of these probabilities over all uncensored observations:\n\n\\[PL(\\beta) = \\prod_{i:\\delta_i = 1} \\frac{\\exp(\\sum_{j=1}^p x_{ij}\\beta_j)}{\\sum_{i':y_{i'}\\ge y_i}\\exp(\\sum_{j=1}^{p}x_{i'j}\\beta_j)}\\]\n\n\n\n\n\n\nWe multiply the probabilities together, assuming independence between observations."
  },
  {
    "objectID": "qmd/islp11.html#partial-likelihood-maximization",
    "href": "qmd/islp11.html#partial-likelihood-maximization",
    "title": "Survival Analysis",
    "section": "Partial Likelihood: Maximization",
    "text": "Partial Likelihood: Maximization\n\nWe estimate \\(\\beta\\) by maximizing the partial likelihood.\nThis is done numerically (no closed-form solution). Computers do the heavy lifting! üíª"
  },
  {
    "objectID": "qmd/islp11.html#cox-model-example-brain-cancer-data",
    "href": "qmd/islp11.html#cox-model-example-brain-cancer-data",
    "title": "Survival Analysis",
    "section": "Cox Model: Example (Brain Cancer Data)",
    "text": "Cox Model: Example (Brain Cancer Data)\nLet‚Äôs apply the Cox model to the BrainCancer data:\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. error\nz-statistic\np-value\n\n\n\n\nsex[Male]\n0.18\n0.36\n0.51\n0.61\n\n\ndiagnosis[LG Glioma]\n0.92\n0.64\n1.43\n0.15\n\n\ndiagnosis[HG Glioma]\n2.15\n0.45\n4.78\n0.00\n\n\ndiagnosis[Other]\n0.89\n0.66\n1.35\n0.18\n\n\nloc[Supratentorial]\n0.44\n0.70\n0.63\n0.53\n\n\nki\n-0.05\n0.02\n-3.00\n&lt;0.01\n\n\ngtv\n0.03\n0.02\n1.54\n0.12\n\n\nstereo[SRT]\n0.18\n0.60\n0.30\n0.77"
  },
  {
    "objectID": "qmd/islp11.html#cox-model-interpretation---sex",
    "href": "qmd/islp11.html#cox-model-interpretation---sex",
    "title": "Survival Analysis",
    "section": "Cox Model: Interpretation - Sex",
    "text": "Cox Model: Interpretation - Sex\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. error\nz-statistic\np-value\n\n\n\n\nsex[Male]\n0.18\n0.36\n0.51\n0.61\n\n\ndiagnosis[LG Glioma]\n0.92\n0.64\n1.43\n0.15\n\n\ndiagnosis[HG Glioma]\n2.15\n0.45\n4.78\n0.00\n\n\ndiagnosis[Other]\n0.89\n0.66\n1.35\n0.18\n\n\nloc[Supratentorial]\n0.44\n0.70\n0.63\n0.53\n\n\nki\n-0.05\n0.02\n-3.00\n&lt;0.01\n\n\ngtv\n0.03\n0.02\n1.54\n0.12\n\n\nstereo[SRT]\n0.18\n0.60\n0.30\n0.77\n\n\n\n\nInterpretation:\n\nMales have an estimated hazard 1.2 times greater than females (e0.18), but this is not statistically significant."
  },
  {
    "objectID": "qmd/islp11.html#cox-model-interpretation---karnofsky-index",
    "href": "qmd/islp11.html#cox-model-interpretation---karnofsky-index",
    "title": "Survival Analysis",
    "section": "Cox Model: Interpretation - Karnofsky Index",
    "text": "Cox Model: Interpretation - Karnofsky Index\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. error\nz-statistic\np-value\n\n\n\n\nsex[Male]\n0.18\n0.36\n0.51\n0.61\n\n\ndiagnosis[LG Glioma]\n0.92\n0.64\n1.43\n0.15\n\n\ndiagnosis[HG Glioma]\n2.15\n0.45\n4.78\n0.00\n\n\ndiagnosis[Other]\n0.89\n0.66\n1.35\n0.18\n\n\nloc[Supratentorial]\n0.44\n0.70\n0.63\n0.53\n\n\nki\n-0.05\n0.02\n-3.00\n&lt;0.01\n\n\ngtv\n0.03\n0.02\n1.54\n0.12\n\n\nstereo[SRT]\n0.18\n0.60\n0.30\n0.77\n\n\n\n\nInterpretation:\n\nHigher Karnofsky index (ki) is associated with a lower hazard (e-0.05 = 0.95), and this effect is significant.\n\n\n\n\n\n\n\n\nEach coefficient tells us how the hazard changes with a one-unit increase in the corresponding feature, holding all other features constant."
  },
  {
    "objectID": "qmd/islp11.html#cox-model-hypothesis-testing",
    "href": "qmd/islp11.html#cox-model-hypothesis-testing",
    "title": "Survival Analysis",
    "section": "Cox Model: Hypothesis Testing",
    "text": "Cox Model: Hypothesis Testing\n\nThe p-value associated with a coefficient tests the null hypothesis that the coefficient is zero. This tells us if the feature has a statistically significant effect on the hazard."
  },
  {
    "objectID": "qmd/islp11.html#cox-model-example-publication-data",
    "href": "qmd/islp11.html#cox-model-example-publication-data",
    "title": "Survival Analysis",
    "section": "Cox Model: Example (Publication Data)",
    "text": "Cox Model: Example (Publication Data)\nNext, we will introduce the dataset Publication, involving the time to publication of journal papers reporting the results of clinical trials funded by the National Heart, Lung, and Blood Institute.\n\nFor 244 trials, the time in months until publication is recorded."
  },
  {
    "objectID": "qmd/islp11.html#cox-model-example-publication-data---log-rank-test",
    "href": "qmd/islp11.html#cox-model-example-publication-data---log-rank-test",
    "title": "Survival Analysis",
    "section": "Cox Model: Example (Publication Data) - Log-Rank Test",
    "text": "Cox Model: Example (Publication Data) - Log-Rank Test\n\n\n\nalt text\n\n\n\nUsing the log-rank test, we can test whether the studies with positive results have significant difference in publication time."
  },
  {
    "objectID": "qmd/islp11.html#cox-model-example-publication-data---log-rank-test-result",
    "href": "qmd/islp11.html#cox-model-example-publication-data---log-rank-test-result",
    "title": "Survival Analysis",
    "section": "Cox Model: Example (Publication Data) - Log-Rank Test Result",
    "text": "Cox Model: Example (Publication Data) - Log-Rank Test Result\n\n\n\nalt text\n\n\n\nThe figure shows slight evidence that time until publication is lower for studies with a positive result.\nHowever, the log-rank test yields a very unimpressive p-value of 0.36.\n\n\n\n\n\n\n\nThe log-rank test suggests a weak, non-significant difference."
  },
  {
    "objectID": "qmd/islp11.html#cox-model-example-publication-data---cox-model",
    "href": "qmd/islp11.html#cox-model-example-publication-data---cox-model",
    "title": "Survival Analysis",
    "section": "Cox Model: Example (Publication Data) - Cox Model",
    "text": "Cox Model: Example (Publication Data) - Cox Model\nNow, let‚Äôs fit Cox‚Äôs proportional hazards model using all available features:\n\n\n\nVariable\nCoefficient\nStd. error\nz-statistic\np-value\n\n\n\n\nposres[Yes]\n0.55\n0.18\n3.02\n0.00\n\n\nmulti[Yes]\n0.15\n0.31\n0.47\n0.64\n\n\nclinend[Yes]\n0.51\n0.27\n1.89\n0.06\n\n\nmech[K01]\n1.05\n1.06\n1.00\n0.32\n\n\nmech[K23]\n-0.48\n1.05\n-0.45\n0.65\n\n\nmech[P01]\n-0.31\n0.78\n-0.40\n0.69\n\n\nmech[P50]\n0.60\n1.06\n0.57\n0.57\n\n\nmech[R01]\n0.10\n0.32\n0.30\n0.76\n\n\nmech[R18]\n1.05\n1.05\n0.99\n0.32\n\n\nmech[R21]\n-0.05\n1.06\n-0.04\n0.97\n\n\nmech[R24, K24]\n0.81\n1.05\n0.77\n0.44\n\n\nmech[R42]\n-14.78\n3414.38\n-0.00\n1.00\n\n\nmech[R44]\n-0.57\n0.77\n-0.73\n0.46\n\n\nmech[RC2]\n-14.92\n2243.60\n-0.01\n0.99\n\n\nmech[U01]\n-0.22\n0.32\n-0.70\n0.48\n\n\nmech[U54]\n0.47\n1.07\n0.44\n0.66\n\n\nsampsize\n0.00\n0.00\n0.19\n0.85\n\n\nbudget\n0.00\n0.00\n1.67\n0.09\n\n\nimpact\n0.06\n0.01\n8.23\n0.00"
  },
  {
    "objectID": "qmd/islp11.html#cox-model-example-publication-data---interpretation---positive-result",
    "href": "qmd/islp11.html#cox-model-example-publication-data---interpretation---positive-result",
    "title": "Survival Analysis",
    "section": "Cox Model: Example (Publication Data) - Interpretation - Positive Result",
    "text": "Cox Model: Example (Publication Data) - Interpretation - Positive Result\n\n\n\nVariable\nCoefficient\nStd. error\nz-statistic\np-value\n\n\n\n\nposres[Yes]\n0.55\n0.18\n3.02\n0.00\n\n\nmulti[Yes]\n0.15\n0.31\n0.47\n0.64\n\n\nclinend[Yes]\n0.51\n0.27\n1.89\n0.06\n\n\nmech[K01]\n1.05\n1.06\n1.00\n0.32\n\n\nmech[K23]\n-0.48\n1.05\n-0.45\n0.65\n\n\nmech[P01]\n-0.31\n0.78\n-0.40\n0.69\n\n\nmech[P50]\n0.60\n1.06\n0.57\n0.57\n\n\nmech[R01]\n0.10\n0.32\n0.30\n0.76\n\n\nmech[R18]\n1.05\n1.05\n0.99\n0.32\n\n\nmech[R21]\n-0.05\n1.06\n-0.04\n0.97\n\n\nmech[R24, K24]\n0.81\n1.05\n0.77\n0.44\n\n\nmech[R42]\n-14.78\n3414.38\n-0.00\n1.00\n\n\nmech[R44]\n-0.57\n0.77\n-0.73\n0.46\n\n\nmech[RC2]\n-14.92\n2243.60\n-0.01\n0.99\n\n\nmech[U01]\n-0.22\n0.32\n-0.70\n0.48\n\n\nmech[U54]\n0.47\n1.07\n0.44\n0.66\n\n\nsampsize\n0.00\n0.00\n0.19\n0.85\n\n\nbudget\n0.00\n0.00\n1.67\n0.09\n\n\nimpact\n0.06\n0.01\n8.23\n0.00\n\n\n\n\nWe find that the chance of publication of a study with a positive result is \\(e^{0.55} = 1.74\\) time higher than the chance of publication of a study with a negative result at any point in time, holding all other covariates fixed."
  },
  {
    "objectID": "qmd/islp11.html#cox-model-example-publication-data---interpretation---impact",
    "href": "qmd/islp11.html#cox-model-example-publication-data---interpretation---impact",
    "title": "Survival Analysis",
    "section": "Cox Model: Example (Publication Data) - Interpretation - Impact",
    "text": "Cox Model: Example (Publication Data) - Interpretation - Impact\n\n\n\nVariable\nCoefficient\nStd. error\nz-statistic\np-value\n\n\n\n\nposres[Yes]\n0.55\n0.18\n3.02\n0.00\n\n\nmulti[Yes]\n0.15\n0.31\n0.47\n0.64\n\n\nclinend[Yes]\n0.51\n0.27\n1.89\n0.06\n\n\nmech[K01]\n1.05\n1.06\n1.00\n0.32\n\n\nmech[K23]\n-0.48\n1.05\n-0.45\n0.65\n\n\nmech[P01]\n-0.31\n0.78\n-0.40\n0.69\n\n\nmech[P50]\n0.60\n1.06\n0.57\n0.57\n\n\nmech[R01]\n0.10\n0.32\n0.30\n0.76\n\n\nmech[R18]\n1.05\n1.05\n0.99\n0.32\n\n\nmech[R21]\n-0.05\n1.06\n-0.04\n0.97\n\n\nmech[R24, K24]\n0.81\n1.05\n0.77\n0.44\n\n\nmech[R42]\n-14.78\n3414.38\n-0.00\n1.00\n\n\nmech[R44]\n-0.57\n0.77\n-0.73\n0.46\n\n\nmech[RC2]\n-14.92\n2243.60\n-0.01\n0.99\n\n\nmech[U01]\n-0.22\n0.32\n-0.70\n0.48\n\n\nmech[U54]\n0.47\n1.07\n0.44\n0.66\n\n\nsampsize\n0.00\n0.00\n0.19\n0.85\n\n\nbudget\n0.00\n0.00\n1.67\n0.09\n\n\nimpact\n0.06\n0.01\n8.23\n0.00\n\n\n\n\nWe also find the impact of a study has a very significant positive relationship to the chance of publication."
  },
  {
    "objectID": "qmd/islp11.html#cox-model-example-publication-data---adjusted-curves",
    "href": "qmd/islp11.html#cox-model-example-publication-data---adjusted-curves",
    "title": "Survival Analysis",
    "section": "Cox Model: Example (Publication Data) - Adjusted Curves",
    "text": "Cox Model: Example (Publication Data) - Adjusted Curves\n\n\n\nAdjusted Survival Curves for Publication Data"
  },
  {
    "objectID": "qmd/islp11.html#adjusted-curves-interpretation",
    "href": "qmd/islp11.html#adjusted-curves-interpretation",
    "title": "Survival Analysis",
    "section": "Adjusted Curves: Interpretation",
    "text": "Adjusted Curves: Interpretation\n\n\n\nAdjusted Survival Curves for Publication Data\n\n\n\nAfter adjusting for other covariates (using representative values), we see a much clearer difference in survival curves between positive and negative results.\nThis highlights the importance of considering multiple predictors. Controlling for other factors reveals a stronger effect of the study‚Äôs results on publication time."
  },
  {
    "objectID": "qmd/islp11.html#shrinkage-for-the-cox-model",
    "href": "qmd/islp11.html#shrinkage-for-the-cox-model",
    "title": "Survival Analysis",
    "section": "Shrinkage for the Cox Model",
    "text": "Shrinkage for the Cox Model\nWe can apply shrinkage methods (like ridge and lasso) to the Cox model. This is useful for high-dimensional data (many features) and can improve prediction accuracy."
  },
  {
    "objectID": "qmd/islp11.html#shrinkage-idea",
    "href": "qmd/islp11.html#shrinkage-idea",
    "title": "Survival Analysis",
    "section": "Shrinkage: Idea",
    "text": "Shrinkage: Idea\n\nIdea: Minimize a penalized version of the negative log partial likelihood:\n\\[-\\log\\left(\\prod_{i:\\delta_i=1} \\frac{\\exp(\\sum_{j=1}^p x_{ij}\\beta_j)}{\\sum_{i':y_{i'}\\ge y_i}\\exp(\\sum_{j=1}^{p}x_{i'j}\\beta_j)}\\right) + \\lambda P(\\beta)\\]\n\n\\(\\lambda\\): Tuning parameter.\n\\(P(\\beta)\\): Penalty term (e.g., lasso: \\(\\sum_{j=1}^p |\\beta_j|\\)).\n\n\n\n\n\n\n\n\nThis adds a penalty to the model‚Äôs complexity, encouraging simpler models with fewer features."
  },
  {
    "objectID": "qmd/islp11.html#shrinkage-publication-data-example",
    "href": "qmd/islp11.html#shrinkage-publication-data-example",
    "title": "Survival Analysis",
    "section": "Shrinkage: Publication Data Example",
    "text": "Shrinkage: Publication Data Example\n\nWe now apply lasso-penalized Cox model to the Publication data. This helps us select the most important features for predicting publication time."
  },
  {
    "objectID": "qmd/islp11.html#shrinkage-cross-validation",
    "href": "qmd/islp11.html#shrinkage-cross-validation",
    "title": "Survival Analysis",
    "section": "Shrinkage: Cross-Validation",
    "text": "Shrinkage: Cross-Validation\n\nThe figure on the right hand displays the cross-validation results.\nNote the ‚ÄúU-shape‚Äù of the partial likelihood deviance.\nSpecifically, the cross-validation error is minimized when just two predictors, budget and impact, have non-zero estimated coefficients.\n\n\n\n\nPartial likelihood deviance\n\n\n\n\n\n\n\n\nCross-validation helps us choose the optimal level of shrinkage (the value of Œª)."
  },
  {
    "objectID": "qmd/islp11.html#assessing-model-fit-on-test-data",
    "href": "qmd/islp11.html#assessing-model-fit-on-test-data",
    "title": "Survival Analysis",
    "section": "Assessing Model Fit on Test Data",
    "text": "Assessing Model Fit on Test Data\nWe can use risk score to categorize the observations based on their ‚Äúrisk‚Äù.\n\nFor example, we use the risk score: \\[\nbudget_i \\cdot \\hat{\\beta}_{budget} + impact_i \\cdot \\hat{\\beta}_{impact}\n\\] where \\(\\hat{\\beta}_{budget}\\) and \\(\\hat{\\beta}_{impact}\\) are the coefficients estimates for these two features from the training set."
  },
  {
    "objectID": "qmd/islp11.html#assessing-model-fit---result",
    "href": "qmd/islp11.html#assessing-model-fit---result",
    "title": "Survival Analysis",
    "section": "Assessing Model Fit - Result",
    "text": "Assessing Model Fit - Result\n\n\n\nalt text\n\n\n\nThe figure shows that there is clear separation between the three strata, and that the strata are correctly ordered in terms of low, medium, and high risk of publication.\n\n\n\n\n\n\n\nWe can evaluate how well the model separates observations into different risk groups on a test set."
  },
  {
    "objectID": "qmd/islp11.html#additional-topics",
    "href": "qmd/islp11.html#additional-topics",
    "title": "Survival Analysis",
    "section": "Additional Topics",
    "text": "Additional Topics\n\nArea Under the Curve (AUC) for Survival Analysis: Harrell‚Äôs concordance index (C-index) generalizes AUC to survival data, accounting for censoring. This measures the model‚Äôs ability to discriminate between individuals with different event times.\nChoice of Time Scale: The definition of ‚Äútime zero‚Äù can be crucial and depends on the context. (e.g., time since diagnosis, time since start of treatment, age).\nTime-Dependent Covariates: The Cox model can handle predictors that change over time. (e.g., a patient‚Äôs treatment status might change during the study).\nChecking the Proportional Hazards Assumption: Visual checks (log hazard plots) and stratification can help assess the assumption. If the assumption is violated, we might need to use more complex models.\nSurvival Trees: Tree-based methods can be adapted for survival analysis. These are useful for finding non-linear relationships and interactions between features."
  },
  {
    "objectID": "qmd/islp11.html#summary",
    "href": "qmd/islp11.html#summary",
    "title": "Survival Analysis",
    "section": "Summary",
    "text": "Summary\n\nSurvival analysis deals with time-to-event data, where censoring is a key challenge. We‚Äôre not just predicting if something happens, but when.\nThe Kaplan-Meier estimator provides a non-parametric estimate of the survival curve. It‚Äôs our first line of defense for understanding survival probabilities.\nThe log-rank test compares survival curves between groups. It helps us determine if there are significant differences in survival.\nThe Cox proportional hazards model allows us to model the relationship between covariates and the hazard function, making it a powerful tool for prediction. It‚Äôs the workhorse for understanding how features influence survival.\nShrinkage methods can be applied to the Cox model. This helps us deal with high-dimensional data and improve prediction.\nVarious extensions and considerations (AUC, time scale, time-dependent covariates, proportional hazards assumption, survival trees) broaden the applicability of survival analysis."
  },
  {
    "objectID": "qmd/islp11.html#thoughts-and-discussion",
    "href": "qmd/islp11.html#thoughts-and-discussion",
    "title": "Survival Analysis",
    "section": "Thoughts and Discussion",
    "text": "Thoughts and Discussion\n\nHow might survival analysis be applied in your field of interest? Think beyond the examples we‚Äôve discussed!\nWhat are the ethical considerations when dealing with survival data, especially in medical contexts? Think about privacy, informed consent, and the potential impact of predictions on individuals.\nHow could you explain the concept of censoring to someone without a statistical background? Use a simple analogy or real-world example.\nCan you think of situations where the proportional hazards assumption might be violated? How could you address this?\nWhat are the limitations of survival analysis? What types of questions can it not answer?"
  },
  {
    "objectID": "qmd/islp3.html",
    "href": "qmd/islp3.html",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Data mining is the process of discovering patterns, insights, and knowledge from large datasets. It involves using various techniques from statistics, computer science, and database management to extract valuable information. It‚Äôs like being a detective, but instead of solving crimes, you‚Äôre uncovering hidden clues within data.\n\n\n\n\n\n\n\nThink of it like sifting through a mountain of data to find hidden gems of information! üíé It‚Äôs like panning for gold, but instead of gold, you‚Äôre finding valuable insights!"
  },
  {
    "objectID": "qmd/islp3.html#what-is-data-mining",
    "href": "qmd/islp3.html#what-is-data-mining",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Data mining is the process of discovering patterns, insights, and knowledge from large datasets. It involves using various techniques from statistics, computer science, and database management to extract valuable information. It‚Äôs like being a detective, but instead of solving crimes, you‚Äôre uncovering hidden clues within data.\n\n\n\n\n\n\n\nThink of it like sifting through a mountain of data to find hidden gems of information! üíé It‚Äôs like panning for gold, but instead of gold, you‚Äôre finding valuable insights!"
  },
  {
    "objectID": "qmd/islp3.html#what-is-machine-learning",
    "href": "qmd/islp3.html#what-is-machine-learning",
    "title": "Introduction to Linear Regression",
    "section": "What is Machine Learning? ü§ñ",
    "text": "What is Machine Learning? ü§ñ\n\nMachine learning is a field of artificial intelligence (AI) that focuses on enabling computers to learn from data without being explicitly programmed (i.e., without being given specific rules for every situation). It involves developing algorithms that can identify patterns, make predictions, and improve their performance over time as they are exposed to more data.\n\n\n\n\n\n\n\nEssentially, machine learning allows computers to learn and adapt, much like humans do! The computer learns from the data, rather than having a programmer tell it exactly what to do."
  },
  {
    "objectID": "qmd/islp3.html#what-is-statistical-learning",
    "href": "qmd/islp3.html#what-is-statistical-learning",
    "title": "Introduction to Linear Regression",
    "section": "What is Statistical Learning? üìà",
    "text": "What is Statistical Learning? üìà\n\nStatistical learning is a framework for understanding data using statistical methods. It encompasses a vast set of tools for modeling and understanding complex datasets. It‚Äôs a recently developed area in statistics and blends with parallel developments in computer science, such as machine learning. It provides the theoretical underpinnings for many machine learning techniques.\n\n\n\n\n\n\n\nIt is the theoretical foundation that underpins many machine learning techniques. Think of it as the ‚Äúscience‚Äù behind the ‚Äúmagic‚Äù of machine learning. It provides the mathematical and statistical rigor."
  },
  {
    "objectID": "qmd/islp3.html#relationship-between-data-mining-machine-learning-and-statistical-learning",
    "href": "qmd/islp3.html#relationship-between-data-mining-machine-learning-and-statistical-learning",
    "title": "Introduction to Linear Regression",
    "section": "Relationship Between Data Mining, Machine Learning, and Statistical Learning",
    "text": "Relationship Between Data Mining, Machine Learning, and Statistical Learning\n\n\n\n\n\ngraph LR\n    A[Data Mining] --&gt; C(Common Ground)\n    B[Machine Learning] --&gt; C\n    D[Statistical Learning] --&gt; C\n    C --&gt; E[Insights & Predictions]\n\n\n\n\n\n\n\n\n\n\n\n\nData mining, machine learning, and statistical learning overlap significantly. They all involve analyzing data to extract insights and make predictions. Statistical learning provides a theoretical foundation, while machine learning focuses on algorithms and prediction. Data mining applies these techniques to large, often unstructured datasets, to uncover hidden patterns."
  },
  {
    "objectID": "qmd/islp3.html#introduction-to-linear-regression",
    "href": "qmd/islp3.html#introduction-to-linear-regression",
    "title": "Introduction to Linear Regression",
    "section": "Introduction to Linear Regression",
    "text": "Introduction to Linear Regression\n\nLinear regression is a fundamental approach in supervised learning, a type of machine learning where we have labeled data (input features and corresponding output values). It‚Äôs used primarily for predicting a quantitative response ‚Äì a numerical value (e.g., sales, price, height).\n\n\n\n\n\n\n\nIt‚Äôs like trying to find the best-fitting straight line through a set of data points. üìè Think of plotting points on a graph and then drawing the line that best captures the overall trend."
  },
  {
    "objectID": "qmd/islp3.html#introduction-to-linear-regression-continued",
    "href": "qmd/islp3.html#introduction-to-linear-regression-continued",
    "title": "Introduction to Linear Regression",
    "section": "Introduction to Linear Regression (Continued)",
    "text": "Introduction to Linear Regression (Continued)\n\nLinear regression is a cornerstone in statistical learning. It‚Äôs widely used, extensively studied, and forms the basis for many other, more advanced techniques.\nMany advanced statistical learning methods can be seen as extensions or generalizations of linear regression. It‚Äôs often the first technique you learn, and it‚Äôs a building block for more complex methods."
  },
  {
    "objectID": "qmd/islp3.html#visualizing-linear-regression",
    "href": "qmd/islp3.html#visualizing-linear-regression",
    "title": "Introduction to Linear Regression",
    "section": "Visualizing Linear Regression",
    "text": "Visualizing Linear Regression\n\n\n\nRelationship between advertising spending and sales\n\n\n\n\n\n\n\n\nThis plot shows the relationship between advertising spending (on TV, radio, and newspaper) and sales. The x-axis represents the advertising budget (in thousands of dollars) for each medium, and the y-axis represents sales (in thousands of units). Linear regression helps us understand and quantify these relationships ‚Äì how much does sales increase for each dollar spent on advertising?"
  },
  {
    "objectID": "qmd/islp3.html#key-questions-in-linear-regression-13",
    "href": "qmd/islp3.html#key-questions-in-linear-regression-13",
    "title": "Introduction to Linear Regression",
    "section": "Key Questions in Linear Regression (1/3)",
    "text": "Key Questions in Linear Regression (1/3)\nWe‚Äôll explore the Advertising data to address several key questions:\n\nRelationship Existence: Is there a statistically significant connection between advertising budget and sales? üìä In other words, is there any evidence that advertising spending affects sales at all?"
  },
  {
    "objectID": "qmd/islp3.html#key-questions-in-linear-regression-23",
    "href": "qmd/islp3.html#key-questions-in-linear-regression-23",
    "title": "Introduction to Linear Regression",
    "section": "Key Questions in Linear Regression (2/3)",
    "text": "Key Questions in Linear Regression (2/3)\n\nRelationship Strength: If there is a relationship, how strong is the link between budget and sales? üí™ Is it a weak, moderate, or strong connection?\nMedia Contribution: Which advertising media (TV, radio, newspaper) contribute to sales? üì∫üìªüì∞ Are all media equally effective, or do some have a greater impact?\nAssociation Size: How much does sales increase for each dollar spent on each medium? üíµ Quantifying the impact"
  },
  {
    "objectID": "qmd/islp3.html#key-questions-in-linear-regression-33",
    "href": "qmd/islp3.html#key-questions-in-linear-regression-33",
    "title": "Introduction to Linear Regression",
    "section": "Key Questions in Linear Regression (3/3)",
    "text": "Key Questions in Linear Regression (3/3)\n\nPrediction Accuracy: Can we accurately predict future sales based on advertising spending? üîÆ How reliable are our predictions?\nLinearity Check: Is the relationship between advertising and sales linear? üìè Does a straight line adequately capture the relationship, or is a curve a better fit?\nMedia Synergy: Do the advertising media work together synergistically (interaction effect)? ü§ù Does spending on one medium enhance the effectiveness of another?\n\n\n\n\n\n\n\nAddressing these questions helps determine if a relationship exists, its strength, individual media contributions, prediction accuracy, the linearity of the connection, and potential synergy effects. By answering these, we can make informed decisions about advertising strategies."
  },
  {
    "objectID": "qmd/islp3.html#simple-linear-regression-the-basics",
    "href": "qmd/islp3.html#simple-linear-regression-the-basics",
    "title": "Introduction to Linear Regression",
    "section": "Simple Linear Regression: The Basics",
    "text": "Simple Linear Regression: The Basics\nSimple linear regression predicts a quantitative response, \\(Y\\), using a single predictor variable, \\(X\\), assuming a linear relationship:\n\\[\nY \\approx \\beta_0 + \\beta_1X\n\\]\n\n\\(\\beta_0\\): Intercept (value of \\(Y\\) when \\(X = 0\\)). This is where the line crosses the Y-axis.\n\\(\\beta_1\\): Slope (change in \\(Y\\) for a one-unit increase in \\(X\\)). This determines how steep the line is.\n\\(\\beta_0\\) and \\(\\beta_1\\) are the model coefficients or parameters. These are the values we need to estimate from the data."
  },
  {
    "objectID": "qmd/islp3.html#simple-linear-regression-equation-visualization",
    "href": "qmd/islp3.html#simple-linear-regression-equation-visualization",
    "title": "Introduction to Linear Regression",
    "section": "Simple Linear Regression: Equation Visualization",
    "text": "Simple Linear Regression: Equation Visualization\nThe equation \\(Y \\approx \\beta_0 + \\beta_1X\\) represents a straight line:\n\n\n\n\n\ngraph LR\n    subgraph Linear Equation\n    A[Y] --&gt; B(Œ≤‚ÇÄ + Œ≤‚ÇÅX)\n    end\n    B --&gt; C[Œ≤‚ÇÄ: Intercept]\n    B --&gt; D[Œ≤‚ÇÅ: Slope]\n    C --&gt; E[Value of Y when X = 0]\n    D --&gt; F[Change in Y for a one-unit increase in X]\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis equation defines a straight line where the intercept (\\(\\beta_0\\)) is the point where the line crosses the Y-axis (when X is zero), and the slope (\\(\\beta_1\\)) determines the steepness and direction of the line. If Œ≤‚ÇÅ is positive, Y increases as X increases. If Œ≤‚ÇÅ is negative, Y decreases as X increases."
  },
  {
    "objectID": "qmd/islp3.html#simple-linear-regression-example",
    "href": "qmd/islp3.html#simple-linear-regression-example",
    "title": "Introduction to Linear Regression",
    "section": "Simple Linear Regression: Example",
    "text": "Simple Linear Regression: Example\nFor instance, let‚Äôs regress sales onto TV advertising:\n\\[\n\\text{sales} \\approx \\beta_0 + \\beta_1 \\times \\text{TV}\n\\]\n\n\\(Y\\): Sales (in thousands of units). This is the response variable ‚Äì what we‚Äôre trying to predict.\n\\(X\\): TV advertising budget (in thousands of dollars). This is the predictor variable ‚Äì what we‚Äôre using to make the prediction."
  },
  {
    "objectID": "qmd/islp3.html#simple-linear-regression-prediction",
    "href": "qmd/islp3.html#simple-linear-regression-prediction",
    "title": "Introduction to Linear Regression",
    "section": "Simple Linear Regression: Prediction",
    "text": "Simple Linear Regression: Prediction\nOnce we estimate the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) (denoted as \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\)), we can predict sales for a given TV advertising budget (x):\n\\[\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x\n\\]\n\n\n\n\n\n\nThe ‚Äúhat‚Äù symbol (^) indicates an estimated value. \\(\\hat{y}\\) is the predicted value of sales, based on our estimated linear model. It‚Äôs our best guess for the sales, given the TV advertising budget."
  },
  {
    "objectID": "qmd/islp3.html#estimating-the-coefficients-least-squares",
    "href": "qmd/islp3.html#estimating-the-coefficients-least-squares",
    "title": "Introduction to Linear Regression",
    "section": "Estimating the Coefficients: Least Squares",
    "text": "Estimating the Coefficients: Least Squares\nOur goal is to find \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) that best fit the data. ‚ÄúBest fit‚Äù means the line should be as close as possible to the data points \\((x_i, y_i)\\). We use the least squares method. Intuitively, we want to find the line that minimizes the overall ‚Äúerror‚Äù between the observed data and the predicted values."
  },
  {
    "objectID": "qmd/islp3.html#estimating-the-coefficients-residual-sum-of-squares-rss",
    "href": "qmd/islp3.html#estimating-the-coefficients-residual-sum-of-squares-rss",
    "title": "Introduction to Linear Regression",
    "section": "Estimating the Coefficients: Residual Sum of Squares (RSS)",
    "text": "Estimating the Coefficients: Residual Sum of Squares (RSS)\nThe least squares method minimizes the residual sum of squares (RSS):\n\\[\n\\text{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 = \\sum_{i=1}^{n} (y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i)^2\n\\]\n\n\\(y_i\\): Actual sales for the \\(i\\)-th observation. The observed value.\n\\(\\hat{y_i}\\): Predicted sales for the \\(i\\)-th observation. The value predicted by our linear model.\n\\(e_i = y_i - \\hat{y_i}\\): The residual for the \\(i\\)-th observation (the difference between the actual and predicted values). This is the ‚Äúerror‚Äù for a single data point."
  },
  {
    "objectID": "qmd/islp3.html#visualizing-residuals",
    "href": "qmd/islp3.html#visualizing-residuals",
    "title": "Introduction to Linear Regression",
    "section": "Visualizing Residuals",
    "text": "Visualizing Residuals\n\n\n\n\n\ngraph LR\n    subgraph Residual Calculation\n    A[Observed Data (y·µ¢)] --&gt; C{Residual (e·µ¢)}\n    B[Predicted Data (≈∑·µ¢)] --&gt; C\n    C --&gt; D[e·µ¢ = y·µ¢ - ≈∑·µ¢]\n    end\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe residual (\\(e_i\\)) represents the error in our prediction for each data point. It‚Äôs the vertical distance between the actual data point and the point on the regression line. Least squares aims to minimize the sum of the squared residuals. We square the residuals to ensure they are all positive and to penalize larger errors more heavily."
  },
  {
    "objectID": "qmd/islp3.html#estimating-the-coefficients-least-squares-explained",
    "href": "qmd/islp3.html#estimating-the-coefficients-least-squares-explained",
    "title": "Introduction to Linear Regression",
    "section": "Estimating the Coefficients: Least Squares Explained",
    "text": "Estimating the Coefficients: Least Squares Explained\n\n\n\n\n\n\nLeast squares finds the line that minimizes the sum of the squared vertical distances between the data points and the line. We‚Äôre essentially finding the line that makes the overall prediction error as small as possible. By squaring the distances, we ensure that positive and negative errors don‚Äôt cancel each other out, and we give more weight to larger errors."
  },
  {
    "objectID": "qmd/islp3.html#estimating-the-coefficients-formulas",
    "href": "qmd/islp3.html#estimating-the-coefficients-formulas",
    "title": "Introduction to Linear Regression",
    "section": "Estimating the Coefficients: Formulas",
    "text": "Estimating the Coefficients: Formulas\nUsing calculus (specifically, taking partial derivatives and setting them to zero), we find the values of \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) that minimize RSS:\n\\[\n\\hat{\\beta_1} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n\\]\n\\[\n\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\n\\]\n\n\\(\\bar{x}\\): Sample mean of \\(X\\). The average value of the predictor.\n\\(\\bar{y}\\): Sample mean of \\(Y\\). The average value of the response."
  },
  {
    "objectID": "qmd/islp3.html#interpreting-the-coefficient-formulas-12",
    "href": "qmd/islp3.html#interpreting-the-coefficient-formulas-12",
    "title": "Introduction to Linear Regression",
    "section": "Interpreting the Coefficient Formulas (1/2)",
    "text": "Interpreting the Coefficient Formulas (1/2)\nThe formula for the slope, \\(\\hat{\\beta_1}\\), can be interpreted as:\n\\[\n\\hat{\\beta_1} = \\frac{\\text{Covariance}(X, Y)}{\\text{Variance}(X)}\n\\]\n\n\n\n\n\n\nThe numerator measures the covariance between X and Y, which indicates how much X and Y vary together. The denominator measures the variance of X, which indicates how much X varies on its own. The slope is essentially the covariance scaled by the variance of X."
  },
  {
    "objectID": "qmd/islp3.html#interpreting-the-coefficient-formulas-22",
    "href": "qmd/islp3.html#interpreting-the-coefficient-formulas-22",
    "title": "Introduction to Linear Regression",
    "section": "Interpreting the Coefficient Formulas (2/2)",
    "text": "Interpreting the Coefficient Formulas (2/2)\nThe formula for the intercept, \\(\\hat{\\beta_0}\\), ensures that the regression line passes through the point (\\(\\bar{x}\\), \\(\\bar{y}\\)).\n\\[\n\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\n\\]\n\n\n\n\n\n\nThis means that when X is equal to its average value (\\(\\bar{x}\\)), the predicted value of Y (\\(\\hat{y}\\)) will be equal to the average value of Y (\\(\\bar{y}\\))."
  },
  {
    "objectID": "qmd/islp3.html#visualizing-the-least-squares-fit",
    "href": "qmd/islp3.html#visualizing-the-least-squares-fit",
    "title": "Introduction to Linear Regression",
    "section": "Visualizing the Least Squares Fit",
    "text": "Visualizing the Least Squares Fit\n\n\n\nLeast Squares Fit"
  },
  {
    "objectID": "qmd/islp3.html#visualizing-the-least-squares-fit-explanation",
    "href": "qmd/islp3.html#visualizing-the-least-squares-fit-explanation",
    "title": "Introduction to Linear Regression",
    "section": "Visualizing the Least Squares Fit: Explanation",
    "text": "Visualizing the Least Squares Fit: Explanation\n\n\n\n\n\n\nThis figure shows the least squares regression line for sales versus TV advertising. Each grey line segment represents a residual ‚Äì the difference between the actual sales value and the sales value predicted by the line. The least squares method finds the line that minimizes the sum of the squares of these residuals, resulting in the ‚Äúbest-fitting‚Äù line."
  },
  {
    "objectID": "qmd/islp3.html#assessing-coefficient-accuracy-population-vs.-sample",
    "href": "qmd/islp3.html#assessing-coefficient-accuracy-population-vs.-sample",
    "title": "Introduction to Linear Regression",
    "section": "Assessing Coefficient Accuracy: Population vs.¬†Sample",
    "text": "Assessing Coefficient Accuracy: Population vs.¬†Sample\n\nPopulation Regression Line: The ‚Äútrue‚Äù (but usually unknown) relationship: \\(Y = \\beta_0 + \\beta_1X + \\epsilon\\). This represents the ideal, underlying relationship in the entire population. We almost never know this.\nLeast Squares Line: The estimated relationship based on our sample: \\(\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x\\). This is our best guess at the true relationship, based on the data we have. It‚Äôs an estimate of the population regression line.\n\\(\\epsilon\\): random error term, which captures all the factors that influence Y but are not included in the model. It represents the inherent randomness in the relationship."
  },
  {
    "objectID": "qmd/islp3.html#population-vs.-sample-regression-lines-visualization",
    "href": "qmd/islp3.html#population-vs.-sample-regression-lines-visualization",
    "title": "Introduction to Linear Regression",
    "section": "Population vs.¬†Sample Regression Lines: Visualization",
    "text": "Population vs.¬†Sample Regression Lines: Visualization\n\n\n\nPopulation vs.¬†Sample Regression Lines"
  },
  {
    "objectID": "qmd/islp3.html#population-vs-sample-regression-lines-left-panel-explanation",
    "href": "qmd/islp3.html#population-vs-sample-regression-lines-left-panel-explanation",
    "title": "Introduction to Linear Regression",
    "section": "Population vs Sample Regression Lines: Left Panel Explanation",
    "text": "Population vs Sample Regression Lines: Left Panel Explanation\n\n\n\n\n\n\nThe left panel shows the true population regression line (red) and the estimated least squares line (blue) from a single sample. The blue line is our best estimate of the red line, based on one particular set of data."
  },
  {
    "objectID": "qmd/islp3.html#population-vs-sample-regression-lines-right-panel-explanation",
    "href": "qmd/islp3.html#population-vs-sample-regression-lines-right-panel-explanation",
    "title": "Introduction to Linear Regression",
    "section": "Population vs Sample Regression Lines: Right Panel Explanation",
    "text": "Population vs Sample Regression Lines: Right Panel Explanation\n\n\n\nPopulation vs.¬†Sample Regression Lines\n\n\n\n\n\n\n\n\nThe right panel shows ten different least squares lines, each estimated from a different sample drawn from the same population. The least squares lines vary, but they cluster around the true population line (red). This illustrates the concept of sampling variability ‚Äì our estimates will vary depending on the specific sample we draw."
  },
  {
    "objectID": "qmd/islp3.html#assessing-coefficient-accuracy-unbiasedness",
    "href": "qmd/islp3.html#assessing-coefficient-accuracy-unbiasedness",
    "title": "Introduction to Linear Regression",
    "section": "Assessing Coefficient Accuracy: Unbiasedness",
    "text": "Assessing Coefficient Accuracy: Unbiasedness\n\n\\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) are estimates of the true (unknown) parameters \\(\\beta_0\\) and \\(\\beta_1\\).\nThese estimates are unbiased: on average, they will equal the true values."
  },
  {
    "objectID": "qmd/islp3.html#unbiasedness-explained",
    "href": "qmd/islp3.html#unbiasedness-explained",
    "title": "Introduction to Linear Regression",
    "section": "Unbiasedness Explained",
    "text": "Unbiasedness Explained\n\n\n\n\n\n\nUnbiasedness means that our estimation method doesn‚Äôt systematically over- or underestimate the true values. If we took many samples and calculated the estimates each time, the average of the estimates would converge to the true values. This is a desirable property of an estimator ‚Äì it means that, in the long run, our estimation method will give us the right answer."
  },
  {
    "objectID": "qmd/islp3.html#assessing-coefficient-accuracy-standard-error",
    "href": "qmd/islp3.html#assessing-coefficient-accuracy-standard-error",
    "title": "Introduction to Linear Regression",
    "section": "Assessing Coefficient Accuracy: Standard Error",
    "text": "Assessing Coefficient Accuracy: Standard Error\n\nStandard Error: Measures the average amount that an estimate (\\(\\hat{\\beta_0}\\) or \\(\\hat{\\beta_1}\\)) differs from the true value (\\(\\beta_0\\) or \\(\\beta_1\\)). It quantifies the typical uncertainty in our estimates. It‚Äôs like a measure of the ‚Äúspread‚Äù of the estimates we would get if we took many samples."
  },
  {
    "objectID": "qmd/islp3.html#standard-error-formulas",
    "href": "qmd/islp3.html#standard-error-formulas",
    "title": "Introduction to Linear Regression",
    "section": "Standard Error: Formulas",
    "text": "Standard Error: Formulas\nFormulas for standard errors:\n\\[\n\\text{SE}(\\hat{\\beta_0})^2 = \\sigma^2 \\left[ \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} \\right]\n\\]\n\\[\n\\text{SE}(\\hat{\\beta_1})^2 = \\frac{\\sigma^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n\\] - \\(\\sigma^2\\): Variance of the error term \\(\\epsilon\\) (usually unknown, estimated by the residual standard error, RSE). This represents the variability of the data around the true regression line."
  },
  {
    "objectID": "qmd/islp3.html#standard-error-interpretation-12",
    "href": "qmd/islp3.html#standard-error-interpretation-12",
    "title": "Introduction to Linear Regression",
    "section": "Standard Error: Interpretation (1/2)",
    "text": "Standard Error: Interpretation (1/2)\n\\[\n\\text{SE}(\\hat{\\beta_0})^2 = \\sigma^2 \\left[ \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} \\right]\n\\]\n\n\n\n\n\n\nThe standard error of the intercept depends on the sample size (n), the variance of the error term (œÉ¬≤), the average value of X (xÃÑ), and the spread of the X values. A larger sample size (n) leads to a smaller standard error."
  },
  {
    "objectID": "qmd/islp3.html#standard-error-interpretation-22",
    "href": "qmd/islp3.html#standard-error-interpretation-22",
    "title": "Introduction to Linear Regression",
    "section": "Standard Error: Interpretation (2/2)",
    "text": "Standard Error: Interpretation (2/2)\n\\[\n\\text{SE}(\\hat{\\beta_1})^2 = \\frac{\\sigma^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n\\]\n\n\n\n\n\n\nSmaller standard errors indicate more precise estimates. Notice that SE(\\(\\hat{\\beta_1}\\)) is smaller when the \\(x_i\\) values are more spread out ‚Äì having a wider range of X values (larger denominator) gives us more information about the slope, leading to a more precise estimate. Also, a smaller error variance (œÉ¬≤) leads to a smaller standard error."
  },
  {
    "objectID": "qmd/islp3.html#assessing-coefficient-accuracy-confidence-intervals",
    "href": "qmd/islp3.html#assessing-coefficient-accuracy-confidence-intervals",
    "title": "Introduction to Linear Regression",
    "section": "Assessing Coefficient Accuracy: Confidence Intervals",
    "text": "Assessing Coefficient Accuracy: Confidence Intervals\n\nConfidence Interval: A range of values that is likely to contain the true unknown value of a parameter, with a certain level of confidence (e.g., 95%). It gives us a sense of how much our estimate might vary if we took different samples."
  },
  {
    "objectID": "qmd/islp3.html#confidence-interval-formula-and-interpretation",
    "href": "qmd/islp3.html#confidence-interval-formula-and-interpretation",
    "title": "Introduction to Linear Regression",
    "section": "Confidence Interval: Formula and Interpretation",
    "text": "Confidence Interval: Formula and Interpretation\n\nApproximate 95% confidence interval for \\(\\beta_1\\):\n\n\\[\n\\hat{\\beta_1} \\pm 2 \\cdot \\text{SE}(\\hat{\\beta_1})\n\\]\n\n\n\n\n\n\nThis means that if we were to repeatedly sample from the population and construct 95% confidence intervals, approximately 95% of those intervals would contain the true value of \\(\\beta_1\\). It gives us a range within which we are reasonably confident the true parameter lies. The interval is centered around our estimate (\\(\\hat{\\beta_1}\\)) and its width depends on the standard error."
  },
  {
    "objectID": "qmd/islp3.html#hypothesis-testing",
    "href": "qmd/islp3.html#hypothesis-testing",
    "title": "Introduction to Linear Regression",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nNull Hypothesis (H‚ÇÄ): There is no relationship between \\(X\\) and \\(Y\\) (\\(\\beta_1 = 0\\)). This is the ‚Äúskeptical‚Äù viewpoint ‚Äì we assume there‚Äôs no relationship unless the data provide strong evidence otherwise.\nAlternative Hypothesis (H‚Çê): There is some relationship between \\(X\\) and \\(Y\\) (\\(\\beta_1 \\neq 0\\)). This is what we‚Äôre trying to find evidence for."
  },
  {
    "objectID": "qmd/islp3.html#hypothesis-testing-t-statistic-and-p-value",
    "href": "qmd/islp3.html#hypothesis-testing-t-statistic-and-p-value",
    "title": "Introduction to Linear Regression",
    "section": "Hypothesis Testing: t-statistic and p-value",
    "text": "Hypothesis Testing: t-statistic and p-value\n\nt-statistic: Measures how many standard deviations \\(\\hat{\\beta_1}\\) is away from 0:\n\n\\[\nt = \\frac{\\hat{\\beta_1} - 0}{\\text{SE}(\\hat{\\beta_1})}\n\\]\n\np-value: The probability of observing a t-statistic as extreme as, or more extreme than, the one calculated, assuming \\(H_0\\) is true. It tells us how likely it is to see data like ours if there really is no relationship between X and Y."
  },
  {
    "objectID": "qmd/islp3.html#hypothesis-testing-interpretation",
    "href": "qmd/islp3.html#hypothesis-testing-interpretation",
    "title": "Introduction to Linear Regression",
    "section": "Hypothesis Testing: Interpretation",
    "text": "Hypothesis Testing: Interpretation\n\n\n\n\n\n\nA small p-value (typically &lt; 0.05) provides evidence against the null hypothesis, suggesting a statistically significant relationship between X and Y. A large t-statistic (in absolute value) corresponds to a small p-value. If the p-value is small, it means it‚Äôs unlikely to observe such a large t-statistic if there were truly no relationship between X and Y."
  },
  {
    "objectID": "qmd/islp3.html#hypothesis-testing-example-advertising-data",
    "href": "qmd/islp3.html#hypothesis-testing-example-advertising-data",
    "title": "Introduction to Linear Regression",
    "section": "Hypothesis Testing: Example (Advertising Data)",
    "text": "Hypothesis Testing: Example (Advertising Data)\n\n\n\nPredictor\nCoefficient\nStd. error\nt-statistic\np-value\n\n\n\n\nIntercept\n7.0325\n0.4578\n15.36\n&lt; 0.0001\n\n\nTV\n0.0475\n0.0027\n17.67\n&lt; 0.0001"
  },
  {
    "objectID": "qmd/islp3.html#hypothesis-testing-example-interpretation",
    "href": "qmd/islp3.html#hypothesis-testing-example-interpretation",
    "title": "Introduction to Linear Regression",
    "section": "Hypothesis Testing: Example Interpretation",
    "text": "Hypothesis Testing: Example Interpretation\n\n\n\n\n\n\nThe table shows the results of regressing sales on TV advertising. The very small p-value for TV provides strong evidence that \\(\\beta_1 \\neq 0\\), meaning there is a statistically significant relationship between TV advertising and sales. The t-statistic for TV (17.67) is very large, indicating that the estimated coefficient for TV is many standard errors away from zero."
  },
  {
    "objectID": "qmd/islp3.html#assessing-model-accuracy-rse",
    "href": "qmd/islp3.html#assessing-model-accuracy-rse",
    "title": "Introduction to Linear Regression",
    "section": "Assessing Model Accuracy: RSE",
    "text": "Assessing Model Accuracy: RSE\n\nResidual Standard Error (RSE): An estimate of the standard deviation of the error term \\(\\epsilon\\). It represents the average amount that the response will deviate from the true regression line. It‚Äôs a measure of the model‚Äôs lack of fit ‚Äì how much the data points tend to scatter around the regression line."
  },
  {
    "objectID": "qmd/islp3.html#rse-formula",
    "href": "qmd/islp3.html#rse-formula",
    "title": "Introduction to Linear Regression",
    "section": "RSE: Formula",
    "text": "RSE: Formula\n\\[\n\\text{RSE} = \\sqrt{\\frac{1}{n-2}\\text{RSS}} = \\sqrt{\\frac{1}{n-2}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}\n\\]\n\n\n\n\n\n\nLower RSE values indicate a better fit, meaning the model‚Äôs predictions are closer to the actual values. The RSE is measured in the units of Y. The (n-2) in the denominator represents the degrees of freedom, accounting for the fact that we‚Äôve estimated two parameters (Œ≤‚ÇÄ and Œ≤‚ÇÅ)."
  },
  {
    "objectID": "qmd/islp3.html#assessing-model-accuracy-r¬≤",
    "href": "qmd/islp3.html#assessing-model-accuracy-r¬≤",
    "title": "Introduction to Linear Regression",
    "section": "Assessing Model Accuracy: R¬≤",
    "text": "Assessing Model Accuracy: R¬≤\n\nR¬≤ Statistic: Measures the proportion of variance explained by the model. It‚Äôs a measure of how well the model captures the variability in the response. It always falls between 0 and 1."
  },
  {
    "objectID": "qmd/islp3.html#r¬≤-formula",
    "href": "qmd/islp3.html#r¬≤-formula",
    "title": "Introduction to Linear Regression",
    "section": "R¬≤: Formula",
    "text": "R¬≤: Formula\n\\[\nR^2 = \\frac{\\text{TSS} - \\text{RSS}}{\\text{TSS}} = 1 - \\frac{\\text{RSS}}{\\text{TSS}}\n\\]\n\nTotal Sum of Squares (TSS): \\(\\sum(y_i - \\bar{y})^2\\) - Measures the total variance in the response \\(Y\\) before considering the predictor. It represents the total variability in the response."
  },
  {
    "objectID": "qmd/islp3.html#r¬≤-interpretation",
    "href": "qmd/islp3.html#r¬≤-interpretation",
    "title": "Introduction to Linear Regression",
    "section": "R¬≤: Interpretation",
    "text": "R¬≤: Interpretation\n\n\n\n\n\n\nR¬≤ closer to 1 indicates that a large proportion of the variability in the response is explained by the regression. An R¬≤ of 0 means the model explains none of the variability (the model is no better than just predicting the average value of Y). In simple linear regression, R¬≤ is the square of the correlation between X and Y. It can be interpreted as the percentage of the variation in Y that can be attributed to X."
  },
  {
    "objectID": "qmd/islp3.html#assessing-model-accuracy-example-advertising-data",
    "href": "qmd/islp3.html#assessing-model-accuracy-example-advertising-data",
    "title": "Introduction to Linear Regression",
    "section": "Assessing Model Accuracy: Example (Advertising Data)",
    "text": "Assessing Model Accuracy: Example (Advertising Data)\n\n\n\nQuantity\nValue\n\n\n\n\nResidual standard error\n3.26\n\n\nR¬≤\n0.612\n\n\nF-statistic\n312.1"
  },
  {
    "objectID": "qmd/islp3.html#assessing-model-accuracy-example-interpretation",
    "href": "qmd/islp3.html#assessing-model-accuracy-example-interpretation",
    "title": "Introduction to Linear Regression",
    "section": "Assessing Model Accuracy: Example Interpretation",
    "text": "Assessing Model Accuracy: Example Interpretation\n\n\n\n\n\n\nFor the regression of sales on TV, the RSE is 3.26 (thousands of units). This means that, on average, the actual sales values deviate from the true regression line by about 3,260 units. The R¬≤ is 0.612, meaning that 61.2% of the variability in sales is explained by TV advertising. The F-statistic is a measure of overall model significance (relevant for multiple regression, discussed later)."
  },
  {
    "objectID": "qmd/islp3.html#multiple-linear-regression",
    "href": "qmd/islp3.html#multiple-linear-regression",
    "title": "Introduction to Linear Regression",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nExtends simple linear regression to handle multiple predictors:\n\\[\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p + \\epsilon\n\\]\n\n\\(\\beta_j\\): The average effect on \\(Y\\) of a one-unit increase in \\(X_j\\), holding all other predictors fixed. This is a crucial point ‚Äì the interpretation of each coefficient is conditional on the other predictors being in the model.\n\n\n\n\n\n\n\nEach predictor now has its own slope coefficient, representing its unique contribution to the response, while controlling for the other predictors. This allows us to isolate the effect of each predictor."
  },
  {
    "objectID": "qmd/islp3.html#multiple-linear-regression-example-advertising-data",
    "href": "qmd/islp3.html#multiple-linear-regression-example-advertising-data",
    "title": "Introduction to Linear Regression",
    "section": "Multiple Linear Regression: Example (Advertising Data)",
    "text": "Multiple Linear Regression: Example (Advertising Data)\n\\[\n\\text{sales} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\beta_3 \\times \\text{newspaper} + \\epsilon\n\\]\nHere, we‚Äôre trying to predict sales using TV, radio, and newspaper advertising budgets."
  },
  {
    "objectID": "qmd/islp3.html#multiple-linear-regression-example-results",
    "href": "qmd/islp3.html#multiple-linear-regression-example-results",
    "title": "Introduction to Linear Regression",
    "section": "Multiple Linear Regression: Example Results",
    "text": "Multiple Linear Regression: Example Results\n\n\n\nPredictor\nCoefficient\nStd. error\nt-statistic\np-value\n\n\n\n\nIntercept\n2.939\n0.3119\n9.42\n&lt; 0.0001\n\n\nTV\n0.046\n0.0014\n32.81\n&lt; 0.0001\n\n\nradio\n0.189\n0.0086\n21.89\n&lt; 0.0001\n\n\nnewspaper\n-0.001\n0.0059\n-0.18\n0.8599"
  },
  {
    "objectID": "qmd/islp3.html#multiple-linear-regression-example-interpretation",
    "href": "qmd/islp3.html#multiple-linear-regression-example-interpretation",
    "title": "Introduction to Linear Regression",
    "section": "Multiple Linear Regression: Example Interpretation",
    "text": "Multiple Linear Regression: Example Interpretation\n\n\n\n\n\n\nHolding TV and newspaper advertising fixed, spending an additional $1,000 on radio advertising is associated with an increase in sales of approximately 189 units. The newspaper coefficient is not statistically significant (p-value &gt; 0.05), suggesting that after accounting for TV and radio advertising, newspaper advertising does not have a significant impact on sales."
  },
  {
    "objectID": "qmd/islp3.html#correlation-between-predictors",
    "href": "qmd/islp3.html#correlation-between-predictors",
    "title": "Introduction to Linear Regression",
    "section": "Correlation Between Predictors",
    "text": "Correlation Between Predictors\n\n\n\n\nTV\nradio\nnewspaper\nsales\n\n\n\n\nTV\n1.0000\n0.0548\n0.0567\n0.7822\n\n\nradio\n0.0548\n1.0000\n0.3541\n0.5762\n\n\nnewspaper\n0.0567\n0.3541\n1.0000\n0.2283\n\n\nsales\n0.7822\n0.5762\n0.2283\n1.0000"
  },
  {
    "objectID": "qmd/islp3.html#correlation-matrix-explanation",
    "href": "qmd/islp3.html#correlation-matrix-explanation",
    "title": "Introduction to Linear Regression",
    "section": "Correlation Matrix: Explanation",
    "text": "Correlation Matrix: Explanation\n\n\n\n\n\n\nThis correlation matrix shows the pairwise correlations between the variables in the Advertising data. Notice the moderate correlation (0.35) between radio and newspaper. This correlation can affect the coefficients in the multiple regression model, explaining why the newspaper coefficient is not significant in the multiple regression, even though it might be significant in a simple linear regression with only newspaper. The correlation between radio and newspaper ‚Äúconfounds‚Äù the effect of newspaper on sales."
  },
  {
    "objectID": "qmd/islp3.html#important-questions-in-multiple-linear-regression",
    "href": "qmd/islp3.html#important-questions-in-multiple-linear-regression",
    "title": "Introduction to Linear Regression",
    "section": "Important Questions in Multiple Linear Regression",
    "text": "Important Questions in Multiple Linear Regression\n\nAny Useful Predictors? Is at least one predictor useful in predicting the response? (F-test, discussed next)\nAll or Subset? Do all predictors help explain \\(Y\\), or only a subset? (Variable selection)\nModel Fit: How well does the model fit the data? (RSE, R¬≤)\nPrediction: Given predictor values, what should we predict for the response, and how accurate is our prediction? (Prediction intervals, confidence intervals)"
  },
  {
    "objectID": "qmd/islp3.html#one-is-there-a-relationship-f-test",
    "href": "qmd/islp3.html#one-is-there-a-relationship-f-test",
    "title": "Introduction to Linear Regression",
    "section": "One: Is There a Relationship? (F-test)",
    "text": "One: Is There a Relationship? (F-test)\n\nNull Hypothesis (H‚ÇÄ): All coefficients are zero (\\(\\beta_1 = \\beta_2 = \\dots = \\beta_p = 0\\)). This means none of the predictors are related to the response.\nAlternative Hypothesis (H‚Çê): At least one coefficient is non-zero. This means at least one predictor is related to the response."
  },
  {
    "objectID": "qmd/islp3.html#f-statistic-formula",
    "href": "qmd/islp3.html#f-statistic-formula",
    "title": "Introduction to Linear Regression",
    "section": "F-statistic: Formula",
    "text": "F-statistic: Formula\n\nF-statistic:\n\n\\[\nF = \\frac{(\\text{TSS} - \\text{RSS})/p}{\\text{RSS}/(n - p - 1)}\n\\]\n\n\n\n\n\n\nIf there‚Äôs no relationship between the response and predictors, the F-statistic will be close to 1. If H‚Çê is true, F will be greater than 1. The larger the F-statistic, the stronger the evidence against the null hypothesis. The numerator represents the variance explained by the model, and the denominator represents the unexplained variance. The values p and (n - p - 1) are the degrees of freedom for the numerator and denominator, respectively."
  },
  {
    "objectID": "qmd/islp3.html#f-test-example-advertising-data",
    "href": "qmd/islp3.html#f-test-example-advertising-data",
    "title": "Introduction to Linear Regression",
    "section": "F-test: Example (Advertising Data)",
    "text": "F-test: Example (Advertising Data)\n\n\n\nQuantity\nValue\n\n\n\n\nResidual standard error\n1.69\n\n\nR¬≤\n0.897\n\n\nF-statistic\n570"
  },
  {
    "objectID": "qmd/islp3.html#f-test-example-interpretation",
    "href": "qmd/islp3.html#f-test-example-interpretation",
    "title": "Introduction to Linear Regression",
    "section": "F-test: Example Interpretation",
    "text": "F-test: Example Interpretation\n\n\n\n\n\n\nThe F-statistic for the multiple regression of sales on TV, radio, and newspaper is 570. This is much larger than 1, providing strong evidence against the null hypothesis. The associated p-value is essentially zero, indicating that at least one advertising medium is significantly related to sales. The high R¬≤ (0.897) also indicates a good model fit ‚Äì the predictors explain a large proportion of the variance in sales."
  },
  {
    "objectID": "qmd/islp3.html#two-deciding-on-important-variables-variable-selection",
    "href": "qmd/islp3.html#two-deciding-on-important-variables-variable-selection",
    "title": "Introduction to Linear Regression",
    "section": "Two: Deciding on Important Variables (Variable Selection)",
    "text": "Two: Deciding on Important Variables (Variable Selection)\n\nGoal: Identify the subset of predictors that are most strongly related to the response. We want to find the most important predictors and exclude those that don‚Äôt contribute meaningfully to the model. We aim for a parsimonious model ‚Äì one that is as simple as possible while still explaining the data well.\nMethods:\n\nForward Selection: Start with the null model (intercept only) and add predictors one by one, based on which improves the model fit the most (e.g., largest decrease in RSS or increase in R¬≤).\nBackward Selection: Start with all predictors and remove them one by one, based on which has the least impact on the model fit (e.g., smallest decrease in RSS or decrease in R¬≤).\nMixed Selection: Combination of forward and backward selection, allowing for both adding and removing predictors at each step."
  },
  {
    "objectID": "qmd/islp3.html#variable-selection-explanation",
    "href": "qmd/islp3.html#variable-selection-explanation",
    "title": "Introduction to Linear Regression",
    "section": "Variable Selection: Explanation",
    "text": "Variable Selection: Explanation\n\n\n\n\n\n\nWe typically can‚Äôt try all possible subsets of predictors (there are 2p of them!), so we use these more efficient, step-wise methods to find a good model. These methods provide a computationally feasible way to search for a good subset of predictors."
  },
  {
    "objectID": "qmd/islp3.html#three-model-fit-rse-and-r¬≤",
    "href": "qmd/islp3.html#three-model-fit-rse-and-r¬≤",
    "title": "Introduction to Linear Regression",
    "section": "Three: Model Fit (RSE and R¬≤)",
    "text": "Three: Model Fit (RSE and R¬≤)\n\nRSE and R¬≤: Same interpretations as in simple linear regression. RSE measures the average prediction error, and R¬≤ measures the proportion of variance explained.\nImportant Note: R¬≤ will always increase when more variables are added to the model, even if those variables are only weakly associated with the response. This is because adding variables always reduces the RSS on the training data."
  },
  {
    "objectID": "qmd/islp3.html#model-fit-caveat",
    "href": "qmd/islp3.html#model-fit-caveat",
    "title": "Introduction to Linear Regression",
    "section": "Model Fit: Caveat",
    "text": "Model Fit: Caveat\n\n\n\n\n\n\nAdding more variables may not improve predictions on new data (test data). We need to be careful not to overfit the training data by including too many predictors. Techniques like cross-validation can help assess model performance on unseen data and prevent overfitting. Adjusted R¬≤ is another metric that penalizes the addition of unnecessary variables."
  },
  {
    "objectID": "qmd/islp3.html#four-predictions",
    "href": "qmd/islp3.html#four-predictions",
    "title": "Introduction to Linear Regression",
    "section": "Four: Predictions",
    "text": "Four: Predictions\nThree sources of uncertainty in predictions:\n\nCoefficient Uncertainty: The least squares plane is only an estimate of the true population regression plane. (Reducible error ‚Äì we can reduce this by getting more data).\nModel Bias: The linear model is likely an approximation of the true relationship. (Reducible error ‚Äì we can reduce this by using a more flexible model).\nIrreducible Error: Even if we knew the true relationship, we couldn‚Äôt predict \\(Y\\) perfectly because of the random error \\(\\epsilon\\). (We can‚Äôt reduce this)."
  },
  {
    "objectID": "qmd/islp3.html#addressing-prediction-uncertainty",
    "href": "qmd/islp3.html#addressing-prediction-uncertainty",
    "title": "Introduction to Linear Regression",
    "section": "Addressing Prediction Uncertainty",
    "text": "Addressing Prediction Uncertainty\n\nCoefficient Uncertainty: Addressed with confidence intervals.\nModel Bias: Addressed by considering more complex models (e.g., non-linear models, interaction terms).\nIrreducible Error: Addressed with prediction intervals."
  },
  {
    "objectID": "qmd/islp3.html#confidence-vs.-prediction-intervals",
    "href": "qmd/islp3.html#confidence-vs.-prediction-intervals",
    "title": "Introduction to Linear Regression",
    "section": "Confidence vs.¬†Prediction Intervals",
    "text": "Confidence vs.¬†Prediction Intervals\n\nConfidence Interval: Quantifies uncertainty around the average response value for a given set of predictor values. It tells us where the average response is likely to fall, given the predictor values.\nPrediction Interval: Quantifies uncertainty around a single response value for a given set of predictor values. It tells us where an individual response is likely to fall, given the predictor values."
  },
  {
    "objectID": "qmd/islp3.html#confidence-vs.-prediction-intervals-comparison",
    "href": "qmd/islp3.html#confidence-vs.-prediction-intervals-comparison",
    "title": "Introduction to Linear Regression",
    "section": "Confidence vs.¬†Prediction Intervals: Comparison",
    "text": "Confidence vs.¬†Prediction Intervals: Comparison\n\n\n\n\n\n\nPrediction intervals are always wider than confidence intervals because they account for both the uncertainty in estimating the population regression plane and the inherent variability of individual data points around that plane (the irreducible error). Confidence intervals only account for the uncertainty in estimating the average response."
  },
  {
    "objectID": "qmd/islp3.html#qualitative-predictors",
    "href": "qmd/islp3.html#qualitative-predictors",
    "title": "Introduction to Linear Regression",
    "section": "Qualitative Predictors",
    "text": "Qualitative Predictors\n\nQualitative Predictor (Factor): A variable with categorical values (levels). Examples: gender (male/female), region (North/South/East/West), type of product (A/B/C).\nDummy Variable: A numerical variable used to represent a qualitative predictor in a regression model. We convert categorical values into numerical codes."
  },
  {
    "objectID": "qmd/islp3.html#dummy-variables-how-they-work",
    "href": "qmd/islp3.html#dummy-variables-how-they-work",
    "title": "Introduction to Linear Regression",
    "section": "Dummy Variables: How They Work",
    "text": "Dummy Variables: How They Work\n-   For a predictor with two levels: Create *one* dummy variable.\n-   For a predictor with more than two levels: Create *one fewer dummy variable than the number of levels.\n-   One level will serve as a *baseline* (reference) level.\n\n\n\n\n\n\nEach dummy variable is coded as 0 or 1, indicating the absence or presence of a particular level. The baseline level is implicitly represented when all dummy variables are 0."
  },
  {
    "objectID": "qmd/islp3.html#qualitative-predictors-example-credit-data",
    "href": "qmd/islp3.html#qualitative-predictors-example-credit-data",
    "title": "Introduction to Linear Regression",
    "section": "Qualitative Predictors: Example (Credit Data)",
    "text": "Qualitative Predictors: Example (Credit Data)\nWe want to predict balance using the own variable (whether someone owns a house).\n\nCreate a dummy variable:\n\n\\[\nx_i = \\begin{cases}\n1 & \\text{if person } i \\text{ owns a house} \\\\\n0 & \\text{if person } i \\text{ does not own a house}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "qmd/islp3.html#qualitative-predictors-regression-model",
    "href": "qmd/islp3.html#qualitative-predictors-regression-model",
    "title": "Introduction to Linear Regression",
    "section": "Qualitative Predictors: Regression Model",
    "text": "Qualitative Predictors: Regression Model\n\nRegression model:\n\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i = \\begin{cases}\n\\beta_0 + \\beta_1 + \\epsilon_i & \\text{if person } i \\text{ owns a house} \\\\\n\\beta_0 + \\epsilon_i & \\text{if person } i \\text{ does not own a house}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "qmd/islp3.html#qualitative-predictors-interpretation",
    "href": "qmd/islp3.html#qualitative-predictors-interpretation",
    "title": "Introduction to Linear Regression",
    "section": "Qualitative Predictors: Interpretation",
    "text": "Qualitative Predictors: Interpretation\n\n\n\n\n\n\n\\(\\beta_0\\) represents the average credit card balance for non-owners (the baseline group). \\(\\beta_0 + \\beta_1\\) represents the average balance for owners. \\(\\beta_1\\) is the average difference in balance between owners and non-owners. The coefficient of the dummy variable represents the difference in the mean response between the level represented by the dummy variable and the baseline level."
  },
  {
    "objectID": "qmd/islp3.html#qualitative-predictors-more-than-two-levels",
    "href": "qmd/islp3.html#qualitative-predictors-more-than-two-levels",
    "title": "Introduction to Linear Regression",
    "section": "Qualitative Predictors: More than Two Levels",
    "text": "Qualitative Predictors: More than Two Levels\nSuppose we have a qualitative predictor, region, with three levels: North, South, and West. We create two dummy variables:\n\\[\nx_{i1} = \\begin{cases}\n1 & \\text{if person } i \\text{ is from the North} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\n\\[\nx_{i2} = \\begin{cases}\n1 & \\text{if person } i \\text{ is from the South} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "qmd/islp3.html#qualitative-predictors-multi-level-regression",
    "href": "qmd/islp3.html#qualitative-predictors-multi-level-regression",
    "title": "Introduction to Linear Regression",
    "section": "Qualitative Predictors: Multi-Level Regression",
    "text": "Qualitative Predictors: Multi-Level Regression\nThe regression model would be:\n\\[\ny_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\epsilon_i = \\begin{cases}\n\\beta_0 + \\beta_1 + \\epsilon_i & \\text{if North} \\\\\n\\beta_0 + \\beta_2 + \\epsilon_i & \\text{if South} \\\\\n\\beta_0 + \\epsilon_i & \\text{if West (baseline)}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "qmd/islp3.html#qualitative-predictors-multi-level-interpretation",
    "href": "qmd/islp3.html#qualitative-predictors-multi-level-interpretation",
    "title": "Introduction to Linear Regression",
    "section": "Qualitative Predictors: Multi-Level Interpretation",
    "text": "Qualitative Predictors: Multi-Level Interpretation\n\n\n\n\n\n\n\\(\\beta_0\\) represents the average balance for people from the West (the baseline). \\(\\beta_1\\) represents the average difference in balance between people from the North and the West. \\(\\beta_2\\) represents the average difference in balance between people from the South and the West."
  },
  {
    "objectID": "qmd/islp3.html#interactions",
    "href": "qmd/islp3.html#interactions",
    "title": "Introduction to Linear Regression",
    "section": "Interactions",
    "text": "Interactions\n\nAdditive Assumption: The effect of one predictor on the response does not depend on the values of other predictors. The effect of each predictor is independent of the others. This is a simplifying assumption, but it may not always be true.\nInteraction Effect (Synergy): The effect of one predictor on the response does depend on the values of other predictors. The combined effect of two predictors is different from the sum of their individual effects.\nInteraction Term: Include the product of two predictors in the model to capture the interaction effect."
  },
  {
    "objectID": "qmd/islp3.html#interactions-explanation",
    "href": "qmd/islp3.html#interactions-explanation",
    "title": "Introduction to Linear Regression",
    "section": "Interactions: Explanation",
    "text": "Interactions: Explanation\n\n\n\n\n\n\nInteractions allow the relationship between a predictor and the response to vary depending on the values of other predictors. They allow for a more flexible and realistic model, capturing situations where the effect of one predictor is amplified or diminished by another."
  },
  {
    "objectID": "qmd/islp3.html#interactions-example-advertising-data",
    "href": "qmd/islp3.html#interactions-example-advertising-data",
    "title": "Introduction to Linear Regression",
    "section": "Interactions: Example (Advertising Data)",
    "text": "Interactions: Example (Advertising Data)\n\\[\n\\text{sales} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\beta_3 \\times (\\text{TV} \\times \\text{radio}) + \\epsilon\n\\]\nThis model includes an interaction term between TV and radio advertising."
  },
  {
    "objectID": "qmd/islp3.html#interactions-rewritten-equation",
    "href": "qmd/islp3.html#interactions-rewritten-equation",
    "title": "Introduction to Linear Regression",
    "section": "Interactions: Rewritten Equation",
    "text": "Interactions: Rewritten Equation\nThe model with the interaction term can be rewritten as:\n\\[\n\\text{sales} = \\beta_0 + (\\beta_1 + \\beta_3 \\times \\text{radio}) \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\epsilon\n\\]\n\n\n\n\n\n\nNow, the slope for TV (\\(\\beta_1 + \\beta_3 \\times \\text{radio}\\)) depends on the value of radio. The interaction term (\\(\\beta_3\\)) allows for synergy between the advertising media. If \\(\\beta_3\\) is positive, the effect of TV advertising increases as radio advertising increases. If \\(\\beta_3\\) is negative, the effect of TV advertising decreases as radio advertising increases."
  },
  {
    "objectID": "qmd/islp3.html#interactions-qualitative-and-quantitative-predictors",
    "href": "qmd/islp3.html#interactions-qualitative-and-quantitative-predictors",
    "title": "Introduction to Linear Regression",
    "section": "Interactions: Qualitative and Quantitative Predictors",
    "text": "Interactions: Qualitative and Quantitative Predictors\nWe can also include interactions between qualitative and quantitative predictors. For example, we could model the relationship between balance, income, and student (a qualitative variable indicating whether someone is a student) as:\n\\[\n\\text{balance} = \\beta_0 + \\beta_1 \\times \\text{income} + \\beta_2 \\times \\text{student} + \\beta_3 \\times (\\text{income} \\times \\text{student}) + \\epsilon\n\\]\nwhere student is a dummy variable (1 if student, 0 otherwise)."
  },
  {
    "objectID": "qmd/islp3.html#interaction-qualitative-and-quantitative-interpretation",
    "href": "qmd/islp3.html#interaction-qualitative-and-quantitative-interpretation",
    "title": "Introduction to Linear Regression",
    "section": "Interaction: Qualitative and Quantitative Interpretation",
    "text": "Interaction: Qualitative and Quantitative Interpretation\n\n\n\n\n\n\nThis model allows for different slopes for students and non-students. The coefficient Œ≤‚ÇÉ represents the difference in the slope of the income-balance relationship between students and non-students. This allows the effect of income on balance to be different for students compared to non-students."
  },
  {
    "objectID": "qmd/islp3.html#non-linear-relationships-polynomial-regression",
    "href": "qmd/islp3.html#non-linear-relationships-polynomial-regression",
    "title": "Introduction to Linear Regression",
    "section": "Non-linear Relationships: Polynomial Regression",
    "text": "Non-linear Relationships: Polynomial Regression\n\nLinearity Assumption: The relationship between the predictors and the response is linear. This is another simplifying assumption that may not always hold.\nPolynomial Regression: Include polynomial terms (e.g., \\(X^2\\), \\(X^3\\)) of the predictors in the model to capture non-linear relationships."
  },
  {
    "objectID": "qmd/islp3.html#polynomial-regression-example",
    "href": "qmd/islp3.html#polynomial-regression-example",
    "title": "Introduction to Linear Regression",
    "section": "Polynomial Regression: Example",
    "text": "Polynomial Regression: Example\n\\[\n\\text{mpg} = \\beta_0 + \\beta_1 \\times \\text{horsepower} + \\beta_2 \\times \\text{horsepower}^2 + \\epsilon\n\\]\n\n\n\n\n\n\nThis is still a linear regression model (linear in the coefficients), but it models a non-linear (quadratic) relationship between mpg and horsepower. We‚Äôre fitting a curve rather than a straight line to the data."
  },
  {
    "objectID": "qmd/islp3.html#visualizing-polynomial-regression",
    "href": "qmd/islp3.html#visualizing-polynomial-regression",
    "title": "Introduction to Linear Regression",
    "section": "Visualizing Polynomial Regression",
    "text": "Visualizing Polynomial Regression\n ## Polynomial Regression: degree = 1\n\n\n\nPolynomial Regression\n\n\n\n\n\n\n\n\nHere is the linear fit (dashed), and It does not fit the data well"
  },
  {
    "objectID": "qmd/islp3.html#polynomial-regression-degree-2",
    "href": "qmd/islp3.html#polynomial-regression-degree-2",
    "title": "Introduction to Linear Regression",
    "section": "Polynomial Regression: degree = 2",
    "text": "Polynomial Regression: degree = 2\n\n\n\nPolynomial Regression\n\n\n\n\n\n\n\n\nThe plot shows a linear fit (dashed) and a quadratic fit (degree = 2, solid) to the relationship between horsepower and mpg. The quadratic fit captures the non-linear relationship better. We can see that as horsepower increases, mpg initially decreases, but then the rate of decrease slows down, and eventually, mpg might even start to increase slightly at very high horsepower levels."
  },
  {
    "objectID": "qmd/islp3.html#potential-problems-in-linear-regression",
    "href": "qmd/islp3.html#potential-problems-in-linear-regression",
    "title": "Introduction to Linear Regression",
    "section": "Potential Problems in Linear Regression",
    "text": "Potential Problems in Linear Regression\n\nNon-linearity: The relationship between response and predictors is not linear.\nCorrelation of Error Terms: Errors are not independent (e.g., time series data).\nNon-constant Variance of Error Terms (Heteroscedasticity): Variance of errors changes with the response or predictors.\nOutliers: Observations with unusual response values.\nHigh Leverage Points: Observations with unusual predictor values.\nCollinearity: Predictors are highly correlated."
  },
  {
    "objectID": "qmd/islp3.html#potential-problems-impact",
    "href": "qmd/islp3.html#potential-problems-impact",
    "title": "Introduction to Linear Regression",
    "section": "Potential Problems: Impact",
    "text": "Potential Problems: Impact\n\n\n\n\n\n\nThese problems can affect the accuracy and interpretability of the regression model. They can lead to biased coefficient estimates, incorrect standard errors, and misleading conclusions. It‚Äôs important to diagnose and address these issues to ensure the model is reliable and valid."
  },
  {
    "objectID": "qmd/islp3.html#problem-1-non-linearity",
    "href": "qmd/islp3.html#problem-1-non-linearity",
    "title": "Introduction to Linear Regression",
    "section": "Problem 1: Non-linearity",
    "text": "Problem 1: Non-linearity\n\nDetection: Residual plots (plot of residuals vs.¬†predicted values or predictors). A non-linear pattern in the residual plot suggests non-linearity.\nSolution: Non-linear transformations of the predictors (log, square root, polynomial terms), or use non-linear regression models."
  },
  {
    "objectID": "qmd/islp3.html#problem-2-correlation-of-error-terms",
    "href": "qmd/islp3.html#problem-2-correlation-of-error-terms",
    "title": "Introduction to Linear Regression",
    "section": "Problem 2: Correlation of Error Terms",
    "text": "Problem 2: Correlation of Error Terms\n\nDetection: Examine the context of the data (e.g., time series data, clustered data). Autocorrelation plots for time series data.\nSolution: Use time series methods (e.g., autoregressive models) or generalized least squares."
  },
  {
    "objectID": "qmd/islp3.html#problem-3-heteroscedasticity",
    "href": "qmd/islp3.html#problem-3-heteroscedasticity",
    "title": "Introduction to Linear Regression",
    "section": "Problem 3: Heteroscedasticity",
    "text": "Problem 3: Heteroscedasticity\n\nDetection: Residual plots. A ‚Äúfunnel‚Äù shape (increasing or decreasing spread of residuals) suggests heteroscedasticity.\nSolution: Transformations of the response variable (e.g., log(Y)), weighted least squares."
  },
  {
    "objectID": "qmd/islp3.html#problem-4-outliers",
    "href": "qmd/islp3.html#problem-4-outliers",
    "title": "Introduction to Linear Regression",
    "section": "Problem 4: Outliers",
    "text": "Problem 4: Outliers\n\nDetection: Residual plots, studentized residuals. Observations with very large residuals (e.g., |studentized residual| &gt; 3) are potential outliers.\nSolution: Investigate the outliers. If they are due to data entry errors, correct them. If not, consider robust regression methods or removing the outliers (with caution)."
  },
  {
    "objectID": "qmd/islp3.html#problem-5-high-leverage-points",
    "href": "qmd/islp3.html#problem-5-high-leverage-points",
    "title": "Introduction to Linear Regression",
    "section": "Problem 5: High Leverage Points",
    "text": "Problem 5: High Leverage Points\n\nDetection: Leverage statistics. Observations with high leverage have a large influence on the regression line.\nSolution: Investigate the points. If they are errors, correct them. If not, consider their impact on the model."
  },
  {
    "objectID": "qmd/islp3.html#problem-6-collinearity",
    "href": "qmd/islp3.html#problem-6-collinearity",
    "title": "Introduction to Linear Regression",
    "section": "Problem 6: Collinearity",
    "text": "Problem 6: Collinearity\n\nDetection: Correlation matrix of predictors. Variance inflation factor (VIF). High correlation (&gt; 0.7 or 0.8) or VIF values (&gt; 5 or 10) suggest collinearity.\nSolution: Remove one or more of the collinear predictors, combine collinear predictors, or use regularization techniques (ridge regression, lasso)."
  },
  {
    "objectID": "qmd/islp3.html#summary",
    "href": "qmd/islp3.html#summary",
    "title": "Introduction to Linear Regression",
    "section": "Summary",
    "text": "Summary\n\nLinear regression is a fundamental and versatile tool for predicting a quantitative response.\nIt relies on assumptions about linearity, additivity, and the error terms. It‚Äôs important to check these assumptions.\nWe can assess model fit (RSE, R¬≤), coefficient significance (t-statistics, p-values), and overall significance (F-test).\nMultiple linear regression handles multiple predictors, allowing us to isolate the effect of each predictor while controlling for others.\nExtensions (qualitative predictors, interactions, polynomial regression) enhance flexibility, allowing us to model more complex relationships.\nDiagnosing and addressing potential problems is crucial for ensuring the reliability and validity of the model."
  },
  {
    "objectID": "qmd/islp3.html#thoughts-and-discussion",
    "href": "qmd/islp3.html#thoughts-and-discussion",
    "title": "Introduction to Linear Regression",
    "section": "Thoughts and Discussion",
    "text": "Thoughts and Discussion\n\nHow do we choose the ‚Äúbest‚Äù model among a set of possible models? (Model selection criteria, cross-validation, adjusted R¬≤, AIC, BIC)\nWhat are the limitations of linear regression, and when might other methods be more appropriate? (Non-linear relationships, complex interactions, non-normal errors, high dimensionality)\nHow can we effectively diagnose and address the potential problems in linear regression? (Residual plots, transformations, robust regression, generalized linear models)\nHow can the insights from a linear regression model be used to inform real-world decisions? (Marketing: optimizing advertising spend, finance: predicting stock prices, healthcare: identifying risk factors for diseases)\nHow does the size and quality of the data affect the model outcome? (More data is generally better; garbage in, garbage out; consider potential biases in the data)\nWhat are some ethical considerations when using linear regression (or any statistical model) for decision-making? (Fairness, transparency, accountability)"
  },
  {
    "objectID": "qmd/islp5.html",
    "href": "qmd/islp5.html",
    "title": "Resampling Methods",
    "section": "",
    "text": "Welcome to Chapter 5: Resampling Methods! üéâ\nThis chapter introduces powerful statistical techniques that help us better understand and utilize our models. We will learn two key resampling methods: Cross-Validation and Bootstrap. These methods are essential tools in the data scientist‚Äôs toolkit!"
  },
  {
    "objectID": "qmd/islp5.html#introduction",
    "href": "qmd/islp5.html#introduction",
    "title": "Resampling Methods",
    "section": "",
    "text": "Welcome to Chapter 5: Resampling Methods! üéâ\nThis chapter introduces powerful statistical techniques that help us better understand and utilize our models. We will learn two key resampling methods: Cross-Validation and Bootstrap. These methods are essential tools in the data scientist‚Äôs toolkit!"
  },
  {
    "objectID": "qmd/islp5.html#introduction-what-will-be-covered",
    "href": "qmd/islp5.html#introduction-what-will-be-covered",
    "title": "Resampling Methods",
    "section": "Introduction: What Will be Covered?",
    "text": "Introduction: What Will be Covered?"
  },
  {
    "objectID": "qmd/islp5.html#introduction-roadmap",
    "href": "qmd/islp5.html#introduction-roadmap",
    "title": "Resampling Methods",
    "section": "Introduction: Roadmap",
    "text": "Introduction: Roadmap\nThis image provides an overview of what will be covered in this chapter about resampling methods. It shows the conceptual flow and various techniques:\n\nCross-Validation: Used for model assessment and selection. Techniques include the validation set approach, leave-one-out cross-validation (LOOCV), and k-fold cross-validation.\nBootstrap: A method for quantifying uncertainty in our estimates."
  },
  {
    "objectID": "qmd/islp5.html#what-are-resampling-methods",
    "href": "qmd/islp5.html#what-are-resampling-methods",
    "title": "Resampling Methods",
    "section": "What are Resampling Methods?",
    "text": "What are Resampling Methods?\n\n\nResampling methods are indispensable tools in modern statistics. üíØ\nThey involve repeatedly drawing samples from a training set and refitting a model of interest on each sample.\nImagine you are baking a cake; you can modify the amount of each ingredient to get many versions of the cake. Resampling methods is like that, you are making many ‚Äúcakes‚Äù (model) by ingredients(data)."
  },
  {
    "objectID": "qmd/islp5.html#resampling-methods-detailed-explanation",
    "href": "qmd/islp5.html#resampling-methods-detailed-explanation",
    "title": "Resampling Methods",
    "section": "Resampling Methods: Detailed Explanation",
    "text": "Resampling Methods: Detailed Explanation\n\nThis allows us to obtain additional information about the fitted model.\n\nExample: Consider linear regression. Resampling lets us examine the variability of the coefficient estimates. How much do the slope and intercept change with different samples?\nThis information is usually difficult to get with a single model fit on the original training sample alone. We only have one set of estimates!\n\nResampling can be computationally expensive.\n\nWe refit the same statistical method multiple times with different data subsets.\nHowever, with modern computing power, this is usually not a major obstacle. üí™"
  },
  {
    "objectID": "qmd/islp5.html#key-concepts",
    "href": "qmd/islp5.html#key-concepts",
    "title": "Resampling Methods",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nModel Assessment: Evaluating a model‚Äôs performance.\n\nCrucially, we want to estimate the test error. This tells us how well the model will generalize to unseen data.\n\nModel Selection: Choosing the best level of flexibility for a model.\n\nExample: For polynomial regression, what degree of polynomial should we use? For k-NN, how many neighbors?\n\nBootstrap: A method to measure the accuracy (or uncertainty) of a parameter estimate or a statistical learning method.\n\nThe Bootstrap helps us quantify how much our estimates might vary if we had different samples."
  },
  {
    "objectID": "qmd/islp5.html#why-resampling",
    "href": "qmd/islp5.html#why-resampling",
    "title": "Resampling Methods",
    "section": "Why Resampling?",
    "text": "Why Resampling?\n\nSuppose you fit a linear regression model. How confident are you in the coefficient estimates (slope and intercept)?\nAre the estimates reliable, or would they change significantly with a slightly different sample?\nResampling helps us answer this:\n\nRepeatedly draw different samples from the training data.\nFit the model to each new sample.\nExamine how much the fitted models differ from each other.\n\nThis gives us a sense of the variability (or uncertainty) of our estimates, which goes beyond a single model fit."
  },
  {
    "objectID": "qmd/islp5.html#two-main-resampling-methods",
    "href": "qmd/islp5.html#two-main-resampling-methods",
    "title": "Resampling Methods",
    "section": "Two Main Resampling Methods",
    "text": "Two Main Resampling Methods\nWe‚Äôll cover two commonly used resampling methods:\n\nCross-Validation:\n\nPrimarily for estimating test error. Essential for both model assessment (how good is a model?) and model selection (which model is best?).\nIt gives us an idea of how well our model will perform on new, unseen data.\n\nBootstrap:\n\nPrimarily for quantifying uncertainty. Example: calculating standard errors of coefficient estimates.\nUseful when standard software doesn‚Äôt give us the uncertainty, or when assumptions of standard methods aren‚Äôt valid."
  },
  {
    "objectID": "qmd/islp5.html#cross-validation-introduction",
    "href": "qmd/islp5.html#cross-validation-introduction",
    "title": "Resampling Methods",
    "section": "Cross-Validation: Introduction",
    "text": "Cross-Validation: Introduction\n\nRemember the critical distinction between test error and training error (from Chapter 2)?\n\nTraining Error: Calculated on the data used to train the model. It often underestimates the test error. (The model is optimized for this specific data.)\nTest Error: The average error on new, unseen data. This is what we truly care about! It shows how well the model generalizes.\nOur goal: statistical learning methods with low test error.\n\nIdeally, we‚Äôd have a large, separate test set. But, we often don‚Äôt!\nCross-validation cleverly estimates test error using only the available training data. üòâ"
  },
  {
    "objectID": "qmd/islp5.html#the-core-idea-of-cross-validation",
    "href": "qmd/islp5.html#the-core-idea-of-cross-validation",
    "title": "Resampling Methods",
    "section": "The Core Idea of Cross-Validation",
    "text": "The Core Idea of Cross-Validation\n\nHold Out Data: We hold out a portion of the training data. This held-out subset acts as a miniature ‚Äútest set‚Äù.\nTrain and Predict:\n\nTrain the model on the remaining data (the data not held out).\nPredict the responses for the held-out observations.\n\nEstimate Test Error:\n\nThe held-out data wasn‚Äôt used for training.\nSo, the prediction error on this subset provides a more realistic estimate of the test error."
  },
  {
    "objectID": "qmd/islp5.html#different-cross-validation-techniques",
    "href": "qmd/islp5.html#different-cross-validation-techniques",
    "title": "Resampling Methods",
    "section": "Different Cross-Validation Techniques",
    "text": "Different Cross-Validation Techniques\n\nSeveral different cross-validation techniques exist, based on how we hold out the data. We‚Äôll discuss these next."
  },
  {
    "objectID": "qmd/islp5.html#the-core-idea-of-cross-validation-visualized",
    "href": "qmd/islp5.html#the-core-idea-of-cross-validation-visualized",
    "title": "Resampling Methods",
    "section": "The Core Idea of Cross-Validation: Visualized",
    "text": "The Core Idea of Cross-Validation: Visualized"
  },
  {
    "objectID": "qmd/islp5.html#the-core-idea-of-cross-validation-visualized-1",
    "href": "qmd/islp5.html#the-core-idea-of-cross-validation-visualized-1",
    "title": "Resampling Methods",
    "section": "The Core Idea of Cross-Validation: Visualized",
    "text": "The Core Idea of Cross-Validation: Visualized\n\nThis figure illustrates the validation set approach, a basic form of cross-validation.\nA set of n observations is randomly split into two parts:\n\nTraining set (shown in blue).\nValidation set (shown in beige)."
  },
  {
    "objectID": "qmd/islp5.html#the-validation-set-approach",
    "href": "qmd/islp5.html#the-validation-set-approach",
    "title": "Resampling Methods",
    "section": "5.1.1 The Validation Set Approach",
    "text": "5.1.1 The Validation Set Approach\n\nSimplest form of cross-validation.\nProcedure:\n\nRandomly split the data into two parts:\n\nTraining set: Used to fit the model.\nValidation set (or hold-out set): Used to estimate the test error.\n\nFit the model to the training set."
  },
  {
    "objectID": "qmd/islp5.html#validation-set-approach-procedure-continued",
    "href": "qmd/islp5.html#validation-set-approach-procedure-continued",
    "title": "Resampling Methods",
    "section": "Validation Set Approach: Procedure (Continued)",
    "text": "Validation Set Approach: Procedure (Continued)\n3. Use the fitted model to *predict* the responses for the observations in the *validation set*.\n4. Calculate the validation set error.\n    -   **Example:** For regression, use Mean Squared Error (MSE).\n    -   This error is our estimate of the *test error*."
  },
  {
    "objectID": "qmd/islp5.html#validation-set-approach-example-auto-data",
    "href": "qmd/islp5.html#validation-set-approach-example-auto-data",
    "title": "Resampling Methods",
    "section": "Validation Set Approach: Example (Auto Data)",
    "text": "Validation Set Approach: Example (Auto Data)\n\nRecall the Auto dataset (Chapter 3). We saw a non-linear relationship between mpg and horsepower.\nLet‚Äôs use the validation set approach to compare models:\n\nLinear model: mpg ~ horsepower\nQuadratic model: mpg ~ horsepower + horsepower¬≤\nCubic model: mpg ~ horsepower + horsepower¬≤ + horsepower¬≥\n\nWhich model predicts mpg best? Cross-validation helps us decide!"
  },
  {
    "objectID": "qmd/islp5.html#validation-set-approach-example-auto-data---left-panel",
    "href": "qmd/islp5.html#validation-set-approach-example-auto-data---left-panel",
    "title": "Resampling Methods",
    "section": "Validation Set Approach: Example (Auto Data) - Left Panel",
    "text": "Validation Set Approach: Example (Auto Data) - Left Panel"
  },
  {
    "objectID": "qmd/islp5.html#validation-set-approach-example-auto-data---left-panel---explanation",
    "href": "qmd/islp5.html#validation-set-approach-example-auto-data---left-panel---explanation",
    "title": "Resampling Methods",
    "section": "Validation Set Approach: Example (Auto Data) - Left Panel - Explanation",
    "text": "Validation Set Approach: Example (Auto Data) - Left Panel - Explanation\n\nLeft Panel: Shows the validation set MSE for a single random split of the data.\nBlue: Linear model.\nOrange: Quadratic model.\nGreen: Cubic model.\nThe quadratic model (orange) has lower MSE than the linear model (blue), suggesting a better fit.\nThe cubic model (green) has slightly higher MSE than the quadratic, suggesting overfitting."
  },
  {
    "objectID": "qmd/islp5.html#validation-set-approach-example-auto-data---right-panel",
    "href": "qmd/islp5.html#validation-set-approach-example-auto-data---right-panel",
    "title": "Resampling Methods",
    "section": "Validation Set Approach: Example (Auto Data) - Right Panel",
    "text": "Validation Set Approach: Example (Auto Data) - Right Panel"
  },
  {
    "objectID": "qmd/islp5.html#validation-set-approach-example-auto-data---right-panel---explanation",
    "href": "qmd/islp5.html#validation-set-approach-example-auto-data---right-panel---explanation",
    "title": "Resampling Methods",
    "section": "Validation Set Approach: Example (Auto Data) - Right Panel - Explanation",
    "text": "Validation Set Approach: Example (Auto Data) - Right Panel - Explanation\n\nRight Panel: Shows validation set MSE for ten different random splits.\nNotice the variability! The MSE changes depending on which observations are in the training and validation sets.\nThis illustrates a key drawback: The validation set approach can be highly variable."
  },
  {
    "objectID": "qmd/islp5.html#validation-set-approach-drawbacks---variability",
    "href": "qmd/islp5.html#validation-set-approach-drawbacks---variability",
    "title": "Resampling Methods",
    "section": "Validation Set Approach: Drawbacks - Variability",
    "text": "Validation Set Approach: Drawbacks - Variability\n\n\nHigh Variability: The test error estimate can change significantly depending on which observations are assigned to the training and validation sets (as seen in the right panel). This makes the estimate less reliable."
  },
  {
    "objectID": "qmd/islp5.html#validation-set-approach-drawbacks---overestimation",
    "href": "qmd/islp5.html#validation-set-approach-drawbacks---overestimation",
    "title": "Resampling Methods",
    "section": "Validation Set Approach: Drawbacks - Overestimation",
    "text": "Validation Set Approach: Drawbacks - Overestimation\n\nOverestimation of Test Error: Only a subset of the data is used to train the model. Models typically perform worse with less data.\n\nTherefore, the validation set error tends to overestimate the test error of a model trained on the entire dataset.\nWe‚Äôre evaluating a model trained on less data than we actually have."
  },
  {
    "objectID": "qmd/islp5.html#leave-one-out-cross-validation-loocv",
    "href": "qmd/islp5.html#leave-one-out-cross-validation-loocv",
    "title": "Resampling Methods",
    "section": "5.1.2 Leave-One-Out Cross-Validation (LOOCV)",
    "text": "5.1.2 Leave-One-Out Cross-Validation (LOOCV)\n\nAddresses the drawbacks of the validation set approach. A more refined approach.\nProcedure:\n\nFor each observation i (from 1 to n):\n\nHold out observation i as the validation set. The validation set has only one observation!"
  },
  {
    "objectID": "qmd/islp5.html#loocv-procedure-continued",
    "href": "qmd/islp5.html#loocv-procedure-continued",
    "title": "Resampling Methods",
    "section": "LOOCV: Procedure (Continued)",
    "text": "LOOCV: Procedure (Continued)\n    -   Train the model on the remaining *n-1* observations.\n    -   *Predict* the response for the held-out observation *i*.\n    -   Calculate the error for observation *i*.\n        -  **Example (Regression):**  `(yi - ≈∑i)¬≤`  (squared error).\n        -  $y_i$: Actual value for observation *i*.\n        -  $\\hat{y}_i$: Predicted value for observation *i*."
  },
  {
    "objectID": "qmd/islp5.html#loocv-procedure-continued-1",
    "href": "qmd/islp5.html#loocv-procedure-continued-1",
    "title": "Resampling Methods",
    "section": "LOOCV: Procedure (Continued)",
    "text": "LOOCV: Procedure (Continued)\n2.  Calculate the LOOCV estimate of the test MSE by averaging the *n* individual errors:\n\n$$\nCV_{(n)} = \\frac{1}{n}\\sum_{i=1}^{n}MSE_i\n$$\nWhere $MSE_i = (y_i - \\hat{y}_i)^2$."
  },
  {
    "objectID": "qmd/islp5.html#loocv-visualized",
    "href": "qmd/islp5.html#loocv-visualized",
    "title": "Resampling Methods",
    "section": "LOOCV: Visualized",
    "text": "LOOCV: Visualized"
  },
  {
    "objectID": "qmd/islp5.html#loocv-visualized-1",
    "href": "qmd/islp5.html#loocv-visualized-1",
    "title": "Resampling Methods",
    "section": "LOOCV: Visualized",
    "text": "LOOCV: Visualized\n\nSchematic of LOOCV.\nn data points are repeatedly split:\n\nTraining set (blue): Contains all but one observation (n-1 observations).\nValidation set (beige): Contains only that one observation.\n\nThe test error is estimated by averaging the n resulting MSEs."
  },
  {
    "objectID": "qmd/islp5.html#loocv-advantages---less-bias",
    "href": "qmd/islp5.html#loocv-advantages---less-bias",
    "title": "Resampling Methods",
    "section": "LOOCV: Advantages - Less Bias",
    "text": "LOOCV: Advantages - Less Bias\n\nLess Bias: LOOCV uses almost all the data (n-1 observations) for training in each iteration.\n\nThis results in a less biased estimate of the test error compared to the validation set approach.\nThe model trained in each iteration is very similar to the model we‚Äôd train on the full dataset."
  },
  {
    "objectID": "qmd/islp5.html#loocv-advantages---no-randomness",
    "href": "qmd/islp5.html#loocv-advantages---no-randomness",
    "title": "Resampling Methods",
    "section": "LOOCV: Advantages - No Randomness",
    "text": "LOOCV: Advantages - No Randomness\n\nNo Randomness: Unlike the validation set approach, LOOCV always produces the same result. There‚Äôs no random splitting.\n\nEvery observation gets to be the validation data exactly once."
  },
  {
    "objectID": "qmd/islp5.html#loocv-auto-data-example---left-panel",
    "href": "qmd/islp5.html#loocv-auto-data-example---left-panel",
    "title": "Resampling Methods",
    "section": "LOOCV: Auto Data Example - Left Panel",
    "text": "LOOCV: Auto Data Example - Left Panel"
  },
  {
    "objectID": "qmd/islp5.html#loocv-auto-data-example---left-panel---explanation",
    "href": "qmd/islp5.html#loocv-auto-data-example---left-panel---explanation",
    "title": "Resampling Methods",
    "section": "LOOCV: Auto Data Example - Left Panel - Explanation",
    "text": "LOOCV: Auto Data Example - Left Panel - Explanation\n\nLeft Panel: The LOOCV error curve for different polynomial models predicting mpg from horsepower.\nThe curve shows how the LOOCV error changes as the model complexity (degree of the polynomial) increases."
  },
  {
    "objectID": "qmd/islp5.html#loocv-auto-data-example---right-panel",
    "href": "qmd/islp5.html#loocv-auto-data-example---right-panel",
    "title": "Resampling Methods",
    "section": "LOOCV: Auto Data Example - Right Panel",
    "text": "LOOCV: Auto Data Example - Right Panel"
  },
  {
    "objectID": "qmd/islp5.html#loocv-auto-data-example---right-panel---explanation",
    "href": "qmd/islp5.html#loocv-auto-data-example---right-panel---explanation",
    "title": "Resampling Methods",
    "section": "LOOCV: Auto Data Example - Right Panel - Explanation",
    "text": "LOOCV: Auto Data Example - Right Panel - Explanation\n\nRight Panel: Shows multiple 10-fold CV curves (we‚Äôll discuss k-fold CV shortly).\nThis demonstrates that 10-fold CV can produce different results depending on the random splits, while LOOCV is deterministic (always gives the same result)."
  },
  {
    "objectID": "qmd/islp5.html#loocv-a-computational-shortcut",
    "href": "qmd/islp5.html#loocv-a-computational-shortcut",
    "title": "Resampling Methods",
    "section": "LOOCV: A Computational Shortcut",
    "text": "LOOCV: A Computational Shortcut\n\nLOOCV can be computationally expensive: It requires n model fits. Imagine n = 1 million!\nShortcut for Least Squares Linear/Polynomial Regression: A remarkable formula exists! ‚ú®\n\n\\[\nCV_{(n)} = \\frac{1}{n}\\sum_{i=1}^{n}\\left( \\frac{y_i - \\hat{y}_i}{1 - h_i} \\right)^2\n\\]"
  },
  {
    "objectID": "qmd/islp5.html#loocv-shortcut-explanation",
    "href": "qmd/islp5.html#loocv-shortcut-explanation",
    "title": "Resampling Methods",
    "section": "LOOCV Shortcut: Explanation",
    "text": "LOOCV Shortcut: Explanation\n\n\\(\\hat{y}_i\\) is the ith fitted value from the original least squares fit (using the entire dataset).\n\\(h_i\\) is the leverage statistic.\n\nLeverage measures how much an observation influences its own fitted value. Observations with high leverage have a larger influence on the fitted model.\n\nKey Point: This formula allows us to calculate LOOCV with the cost of just one model fit! This is a huge computational saving."
  },
  {
    "objectID": "qmd/islp5.html#loocv-shortcut-important-note",
    "href": "qmd/islp5.html#loocv-shortcut-important-note",
    "title": "Resampling Methods",
    "section": "LOOCV Shortcut: Important Note",
    "text": "LOOCV Shortcut: Important Note\n\nThis shortcut does not generally apply to other models (like logistic regression or more complex models).\nIt‚Äôs specific to least squares linear and polynomial regression.\nFor other models, you typically need to perform the full LOOCV procedure (fitting the model n times)."
  },
  {
    "objectID": "qmd/islp5.html#k-fold-cross-validation",
    "href": "qmd/islp5.html#k-fold-cross-validation",
    "title": "Resampling Methods",
    "section": "5.1.3 k-Fold Cross-Validation",
    "text": "5.1.3 k-Fold Cross-Validation\n\nA compromise between the validation set approach and LOOCV. Balances computational cost and accuracy.\nProcedure:\n\nRandomly divide the data into k groups (or ‚Äúfolds‚Äù) of approximately equal size."
  },
  {
    "objectID": "qmd/islp5.html#k-fold-cv-procedure-continued",
    "href": "qmd/islp5.html#k-fold-cv-procedure-continued",
    "title": "Resampling Methods",
    "section": "k-Fold CV: Procedure (Continued)",
    "text": "k-Fold CV: Procedure (Continued)\n2.  For each fold *j* (from 1 to *k*):\n    -   Treat fold *j* as the validation set.\n    -   Train the model on the remaining *k-1* folds.\n    -   Compute the error (e.g., MSE) on the held-out fold *j*."
  },
  {
    "objectID": "qmd/islp5.html#k-fold-cv-procedure-continued-1",
    "href": "qmd/islp5.html#k-fold-cv-procedure-continued-1",
    "title": "Resampling Methods",
    "section": "k-Fold CV: Procedure (Continued)",
    "text": "k-Fold CV: Procedure (Continued)\n3. Calculate the k-fold CV estimate by averaging the *k* errors:\n\n$$\nCV_{(k)} = \\frac{1}{k}\\sum_{i=1}^{k}MSE_i\n$$\nWhere $MSE_i$ is the mean squared error calculated on fold *i*."
  },
  {
    "objectID": "qmd/islp5.html#k-fold-cv-visualized",
    "href": "qmd/islp5.html#k-fold-cv-visualized",
    "title": "Resampling Methods",
    "section": "k-Fold CV: Visualized",
    "text": "k-Fold CV: Visualized"
  },
  {
    "objectID": "qmd/islp5.html#k-fold-cv-visualized---explanation",
    "href": "qmd/islp5.html#k-fold-cv-visualized---explanation",
    "title": "Resampling Methods",
    "section": "k-Fold CV: Visualized - Explanation",
    "text": "k-Fold CV: Visualized - Explanation\n\nSchematic of 5-fold CV.\nn observations are randomly split into five non-overlapping groups.\nEach ‚Äúfifth‚Äù acts as a validation set (beige), and the remainder as the training set (blue).\nThe test error is estimated by averaging the five resulting MSE estimates."
  },
  {
    "objectID": "qmd/islp5.html#k-fold-cv-choosing-k",
    "href": "qmd/islp5.html#k-fold-cv-choosing-k",
    "title": "Resampling Methods",
    "section": "k-Fold CV: Choosing k",
    "text": "k-Fold CV: Choosing k\n\nCommon choices for k are 5 or 10. These values have been empirically shown to provide good estimates of the test error.\nComputational Advantage: k-fold CV (with k &lt; n) is less computationally expensive than LOOCV. It requires only k model fits, not n. This is significant for large datasets."
  },
  {
    "objectID": "qmd/islp5.html#bias-variance-trade-off-for-k-fold-cv",
    "href": "qmd/islp5.html#bias-variance-trade-off-for-k-fold-cv",
    "title": "Resampling Methods",
    "section": "5.1.4 Bias-Variance Trade-Off for k-Fold CV",
    "text": "5.1.4 Bias-Variance Trade-Off for k-Fold CV\n\nBias:\n\nLOOCV: Nearly unbiased (uses almost all data for training).\nk-fold CV: Slightly more bias (uses (k-1)n/k observations for training).\nValidation set approach: Most bias (uses roughly half the data, leading to overestimation of test error)."
  },
  {
    "objectID": "qmd/islp5.html#bias-variance-trade-off-for-k-fold-cv-continued",
    "href": "qmd/islp5.html#bias-variance-trade-off-for-k-fold-cv-continued",
    "title": "Resampling Methods",
    "section": "Bias-Variance Trade-Off for k-Fold CV (Continued)",
    "text": "Bias-Variance Trade-Off for k-Fold CV (Continued)\n\nVariance:\n\nLOOCV: Higher variance than k-fold CV. The n fitted models in LOOCV are highly correlated (they share almost all their training data). Averaging highly correlated quantities has higher variance.\nk-fold CV: Averages k models with less overlap in training data, leading to lower variance.\n\nConclusion: 5-fold or 10-fold CV often achieve a good balance between bias and variance. They offer a reasonable compromise between accuracy and computational cost."
  },
  {
    "objectID": "qmd/islp5.html#cross-validation-on-simulated-data",
    "href": "qmd/islp5.html#cross-validation-on-simulated-data",
    "title": "Resampling Methods",
    "section": "Cross-Validation on Simulated Data",
    "text": "Cross-Validation on Simulated Data"
  },
  {
    "objectID": "qmd/islp5.html#cross-validation-on-simulated-data-1",
    "href": "qmd/islp5.html#cross-validation-on-simulated-data-1",
    "title": "Resampling Methods",
    "section": "Cross-Validation on Simulated Data",
    "text": "Cross-Validation on Simulated Data\n\nBlue: True test MSE (known because it‚Äôs simulated data).\nBlack Dashed: LOOCV estimate.\nOrange Solid: 10-fold CV estimate.\nCrosses: Minimum points of each curve.\nThe plots show that CV curves can sometimes underestimate the true test MSE.\nHowever, they generally identify the correct level of flexibility (the model with the lowest test error)."
  },
  {
    "objectID": "qmd/islp5.html#cross-validation-for-classification",
    "href": "qmd/islp5.html#cross-validation-for-classification",
    "title": "Resampling Methods",
    "section": "5.1.5 Cross-Validation for Classification",
    "text": "5.1.5 Cross-Validation for Classification\n\nSo far, we‚Äôve focused on regression (quantitative response).\nCross-validation works similarly for classification (qualitative response).\nInstead of MSE, we use the number of misclassified observations to quantify error."
  },
  {
    "objectID": "qmd/islp5.html#cross-validation-for-classification-continued",
    "href": "qmd/islp5.html#cross-validation-for-classification-continued",
    "title": "Resampling Methods",
    "section": "Cross-Validation for Classification (Continued)",
    "text": "Cross-Validation for Classification (Continued)\n\nExample: LOOCV error rate:\n\n\\[\nCV_{(n)} = \\frac{1}{n}\\sum_{i=1}^{n}Err_i\n\\]\nwhere \\(Err_i = I(y_i \\neq \\hat{y}_i)\\). - \\(I(y_i \\neq \\hat{y}_i)\\) is an indicator function: - 1 if misclassified (predicted class \\(\\hat{y}_i\\) is different from the true class \\(y_i\\)). - 0 otherwise.\n\nThe k-fold CV error rate is calculated similarly, averaging the misclassification rates across the k folds."
  },
  {
    "objectID": "qmd/islp5.html#cv-for-classification-example---decision-boundaries",
    "href": "qmd/islp5.html#cv-for-classification-example---decision-boundaries",
    "title": "Resampling Methods",
    "section": "CV for Classification: Example - Decision Boundaries",
    "text": "CV for Classification: Example - Decision Boundaries"
  },
  {
    "objectID": "qmd/islp5.html#cv-for-classification-example---decision-boundaries-1",
    "href": "qmd/islp5.html#cv-for-classification-example---decision-boundaries-1",
    "title": "Resampling Methods",
    "section": "CV for Classification: Example - Decision Boundaries",
    "text": "CV for Classification: Example - Decision Boundaries\n\nPurple Dashed Line: Bayes decision boundary (the optimal decision boundary, usually unknown in practice).\nBlack Lines: Decision boundaries from logistic regression with different polynomial degrees. Observe how the decision boundary changes with model complexity."
  },
  {
    "objectID": "qmd/islp5.html#cv-for-classification-example---error-curves",
    "href": "qmd/islp5.html#cv-for-classification-example---error-curves",
    "title": "Resampling Methods",
    "section": "CV for Classification: Example - Error Curves",
    "text": "CV for Classification: Example - Error Curves"
  },
  {
    "objectID": "qmd/islp5.html#cv-for-classification-example---error-curves-1",
    "href": "qmd/islp5.html#cv-for-classification-example---error-curves-1",
    "title": "Resampling Methods",
    "section": "CV for Classification: Example - Error Curves",
    "text": "CV for Classification: Example - Error Curves\n\nLeft: Logistic regression with polynomial terms.\nRight: KNN classifier with different values of K (number of neighbors).\nBrown: True test error.\nBlue: Training error (typically overly optimistic).\nBlack: 10-fold CV error.\nCV curves often underestimate the true test error.\nCrucially, they tend to identify the minimum, corresponding to the best model complexity."
  },
  {
    "objectID": "qmd/islp5.html#the-bootstrap",
    "href": "qmd/islp5.html#the-bootstrap",
    "title": "Resampling Methods",
    "section": "5.2 The Bootstrap",
    "text": "5.2 The Bootstrap\n\nA powerful and widely applicable tool for quantifying the uncertainty associated with an estimator or statistical learning method.\nIt helps us understand how much our estimates might vary if we had different samples from the population.\nExample: Estimating standard errors of regression coefficients. (Standard software does this for linear regression, but the bootstrap is useful in more general cases.)"
  },
  {
    "objectID": "qmd/islp5.html#bootstrap-the-core-idea",
    "href": "qmd/islp5.html#bootstrap-the-core-idea",
    "title": "Resampling Methods",
    "section": "Bootstrap: The Core Idea",
    "text": "Bootstrap: The Core Idea\n\nProblem: We usually cannot generate new samples from the population. We only have our one observed dataset.\nBootstrap Solution: Repeatedly sample observations with replacement from the original dataset to create bootstrap datasets.\n\nWith Replacement: The same observation can appear multiple times in a bootstrap dataset. This is the key to the bootstrap!"
  },
  {
    "objectID": "qmd/islp5.html#bootstrap-the-core-idea-continued",
    "href": "qmd/islp5.html#bootstrap-the-core-idea-continued",
    "title": "Resampling Methods",
    "section": "Bootstrap: The Core Idea (Continued)",
    "text": "Bootstrap: The Core Idea (Continued)\n\nWe treat the original dataset as if it were the population. This allows us to simulate the process of drawing multiple samples."
  },
  {
    "objectID": "qmd/islp5.html#bootstrap-visualized",
    "href": "qmd/islp5.html#bootstrap-visualized",
    "title": "Resampling Methods",
    "section": "Bootstrap: Visualized",
    "text": "Bootstrap: Visualized"
  },
  {
    "objectID": "qmd/islp5.html#bootstrap-visualized-1",
    "href": "qmd/islp5.html#bootstrap-visualized-1",
    "title": "Resampling Methods",
    "section": "Bootstrap: Visualized",
    "text": "Bootstrap: Visualized\n\nGraphical illustration of the bootstrap with a small sample (n = 3).\nEach bootstrap dataset contains n observations, sampled with replacement from the original data.\nNote that the same observation can appear multiple times in a single bootstrap sample."
  },
  {
    "objectID": "qmd/islp5.html#bootstrap-procedure",
    "href": "qmd/islp5.html#bootstrap-procedure",
    "title": "Resampling Methods",
    "section": "Bootstrap: Procedure",
    "text": "Bootstrap: Procedure\n\nCreate B bootstrap datasets (each of size n).\n\nSample with replacement from the original data.\nB is typically a large number (e.g., 100 or 1000)."
  },
  {
    "objectID": "qmd/islp5.html#bootstrap-procedure-continued",
    "href": "qmd/islp5.html#bootstrap-procedure-continued",
    "title": "Resampling Methods",
    "section": "Bootstrap: Procedure (Continued)",
    "text": "Bootstrap: Procedure (Continued)\n\nFor each bootstrap dataset:\n\nCalculate the statistic of interest (e.g., regression coefficient, mean, median, etc.).\nThis yields B bootstrap estimates."
  },
  {
    "objectID": "qmd/islp5.html#bootstrap-procedure-continued-1",
    "href": "qmd/islp5.html#bootstrap-procedure-continued-1",
    "title": "Resampling Methods",
    "section": "Bootstrap: Procedure (Continued)",
    "text": "Bootstrap: Procedure (Continued)\n\nEstimate the standard error of the statistic using the standard deviation of the B bootstrap estimates:\n\n\\[SE_B(\\hat{\\alpha}) = \\sqrt{\\frac{1}{B-1} \\sum_{r=1}^B \\left( \\hat{\\alpha}^{*r} - \\frac{1}{B}\\sum_{r'=1}^B \\hat{\\alpha}^{*r'} \\right)^2}\\] - \\(\\hat{\\alpha}^{*r}\\): Estimate of the statistic from the r-th bootstrap dataset."
  },
  {
    "objectID": "qmd/islp5.html#bootstrap-investment-example",
    "href": "qmd/islp5.html#bootstrap-investment-example",
    "title": "Resampling Methods",
    "section": "Bootstrap: Investment Example",
    "text": "Bootstrap: Investment Example\n\nWe want to invest in two assets, X and Y.\nOur goal: Minimize the risk (variance) of our investment.\nThe optimal fraction (Œ±) to invest in X is:\n\n\\[\n\\alpha = \\frac{\\sigma_Y^2 - \\sigma_{XY}}{\\sigma_X^2 + \\sigma_Y^2 - 2\\sigma_{XY}}\n\\]"
  },
  {
    "objectID": "qmd/islp5.html#investment-example-explanation",
    "href": "qmd/islp5.html#investment-example-explanation",
    "title": "Resampling Methods",
    "section": "Investment Example: Explanation",
    "text": "Investment Example: Explanation\n\n\\(\\sigma_X^2\\): Variance of returns for asset X.\n\\(\\sigma_Y^2\\): Variance of returns for asset Y.\n\\(\\sigma_{XY}\\): Covariance between returns of assets X and Y.\nIn practice, these population variances and covariance are unknown. We only have estimates from our observed data."
  },
  {
    "objectID": "qmd/islp5.html#bootstrap-example---simulated-data",
    "href": "qmd/islp5.html#bootstrap-example---simulated-data",
    "title": "Resampling Methods",
    "section": "Bootstrap Example - Simulated Data",
    "text": "Bootstrap Example - Simulated Data"
  },
  {
    "objectID": "qmd/islp5.html#bootstrap-example---simulated-data-1",
    "href": "qmd/islp5.html#bootstrap-example---simulated-data-1",
    "title": "Resampling Methods",
    "section": "Bootstrap Example - Simulated Data",
    "text": "Bootstrap Example - Simulated Data\n\nEach panel shows 100 simulated returns for investments X and Y.\nThis simulates different possible scenarios for asset returns."
  },
  {
    "objectID": "qmd/islp5.html#bootstrap-example---histograms",
    "href": "qmd/islp5.html#bootstrap-example---histograms",
    "title": "Resampling Methods",
    "section": "Bootstrap: Example - Histograms",
    "text": "Bootstrap: Example - Histograms"
  },
  {
    "objectID": "qmd/islp5.html#bootstrap-example---histograms---explanation",
    "href": "qmd/islp5.html#bootstrap-example---histograms---explanation",
    "title": "Resampling Methods",
    "section": "Bootstrap: Example - Histograms - Explanation",
    "text": "Bootstrap: Example - Histograms - Explanation\n\nLeft: Histogram of Œ± estimates from 1,000 simulated datasets (if we could repeatedly sample from the true population).\nCenter: Histogram of Œ± estimates from 1,000 bootstrap samples from a single dataset (what we can do in practice).\nRight: Boxplots comparing the two distributions."
  },
  {
    "objectID": "qmd/islp5.html#bootstrap-example-conclusion",
    "href": "qmd/islp5.html#bootstrap-example-conclusion",
    "title": "Resampling Methods",
    "section": "Bootstrap Example: Conclusion",
    "text": "Bootstrap Example: Conclusion\n\nThe bootstrap accurately estimates the variability of Œ±!\nThe distribution of bootstrap estimates (center) closely resembles the distribution of estimates from repeated sampling (left).\nThis demonstrates how the bootstrap can provide a good estimate of uncertainty even with only one dataset."
  },
  {
    "objectID": "qmd/islp5.html#summary",
    "href": "qmd/islp5.html#summary",
    "title": "Resampling Methods",
    "section": "Summary",
    "text": "Summary\n\nResampling methods are essential for:\n\nAssessing model performance (cross-validation).\nQuantifying uncertainty (bootstrap)."
  },
  {
    "objectID": "qmd/islp5.html#summary-cross-validation",
    "href": "qmd/islp5.html#summary-cross-validation",
    "title": "Resampling Methods",
    "section": "Summary: Cross-Validation",
    "text": "Summary: Cross-Validation\n\nCross-validation:\n\nEstimates test error by holding out data (creating a ‚Äúfake‚Äù test set).\nCommon techniques: Validation set, LOOCV, and k-fold CV.\nk-fold CV (k=5 or k=10) often provides a good bias-variance trade-off, balancing accuracy and computation."
  },
  {
    "objectID": "qmd/islp5.html#summary-bootstrap",
    "href": "qmd/islp5.html#summary-bootstrap",
    "title": "Resampling Methods",
    "section": "Summary: Bootstrap",
    "text": "Summary: Bootstrap\n\nBootstrap:\n\nEstimates uncertainty by resampling with replacement from the original data.\nWidely applicable, especially when standard error formulas are unavailable or assumptions are violated."
  },
  {
    "objectID": "qmd/islp5.html#thoughts-and-discussion",
    "href": "qmd/islp5.html#thoughts-and-discussion",
    "title": "Resampling Methods",
    "section": "Thoughts and Discussion ü§î",
    "text": "Thoughts and Discussion ü§î\n\nWhen might you prefer LOOCV over k-fold CV, despite the higher computational cost?\n\nAnswer: With very small datasets, the bias reduction of LOOCV might be crucial. Also, when computational cost isn‚Äôt a major concern (simple models).\n\nCan you think of situations where the bootstrap would be particularly useful?\n\nAnswer: When working with a statistic without a simple standard error formula (e.g., median, custom metric), or when assumptions of standard methods (like normality) are violated."
  },
  {
    "objectID": "qmd/islp5.html#thoughts-and-discussion-continued",
    "href": "qmd/islp5.html#thoughts-and-discussion-continued",
    "title": "Resampling Methods",
    "section": "Thoughts and Discussion ü§î (Continued)",
    "text": "Thoughts and Discussion ü§î (Continued)\n\nHow do these resampling methods relate to the concepts of bias and variance we discussed in earlier chapters?\n\nAnswer: Cross-validation helps us choose models with a good balance of bias and variance (by estimating test error). The bootstrap helps us estimate the variance of parameter estimates.\n\nWhat are the limitations of these methods? When might they not be appropriate?\n\nAnswer: Resampling can be computationally intensive. Cross-validation assumes independent and identically distributed (i.i.d.) data. The bootstrap relies on the original sample being representative of the population. Severe violations of these assumptions can lead to misleading results."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "ÊàëÊòØÈÇ±È£ûÔºåËøôÊòØÊàëÁöÑ‰∏™‰∫∫ÁΩëÁ´ôÔºå\nÁî®‰∫éÂàÜ‰∫´‰∏Ä‰∫õÊï∞ÊçÆÂàÜÊûêÊñáÁ´†ÔºåËøòÊúâ‰∏™‰∫∫ÁöÑËÆ∞ÂΩïÂíåËµÑÊñô„ÄÇ"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "big data mining and analysis",
    "section": "",
    "text": "Èí±Êπñ62-235\n1-16Âë®Âë®‰∏â‰∏äÂçà3-4ËäÇ"
  },
  {
    "objectID": "index.html#location",
    "href": "index.html#location",
    "title": "big data mining and analysis",
    "section": "",
    "text": "Èí±Êπñ62-235\n1-16Âë®Âë®‰∏â‰∏äÂçà3-4ËäÇ"
  },
  {
    "objectID": "index.html#textbook",
    "href": "index.html#textbook",
    "title": "big data mining and analysis",
    "section": "textbook",
    "text": "textbook\nThe textbook used is the renowned ISL : Introduction to Statistical Learning.\n\nhttps://www.statlearning.com/"
  },
  {
    "objectID": "index.html#ppt",
    "href": "index.html#ppt",
    "title": "big data mining and analysis",
    "section": "ppt",
    "text": "ppt\n chapter1 \n chapter2 \n chapter3 \n chapter4 \n chapter5 \n chapter6 \n chapter7 \n chapter8 \n chapter9 \n chapter10 \n chapter11 \n chapter12 \n chapter13"
  },
  {
    "objectID": "qmd/islp6.html",
    "href": "qmd/islp6.html",
    "title": "Linear Model Selection and Regularization",
    "section": "",
    "text": "Last time, we explored the fundamentals of linear regression, a powerful tool for modeling relationships between variables. It allows us to understand how a dependent variable changes with one or more independent variables.\nHowever, the simple linear model, while interpretable and often effective, has limitations. In this chapter, we extend the linear model, enhancing its capabilities. We will discuss:\n\nWays in which the simple linear model can be improved.\nAlternative fitting procedures instead of least squares."
  },
  {
    "objectID": "qmd/islp6.html#introduction-beyond-simple-linear-regression",
    "href": "qmd/islp6.html#introduction-beyond-simple-linear-regression",
    "title": "Linear Model Selection and Regularization",
    "section": "",
    "text": "Last time, we explored the fundamentals of linear regression, a powerful tool for modeling relationships between variables. It allows us to understand how a dependent variable changes with one or more independent variables.\nHowever, the simple linear model, while interpretable and often effective, has limitations. In this chapter, we extend the linear model, enhancing its capabilities. We will discuss:\n\nWays in which the simple linear model can be improved.\nAlternative fitting procedures instead of least squares."
  },
  {
    "objectID": "qmd/islp6.html#introduction-goals",
    "href": "qmd/islp6.html#introduction-goals",
    "title": "Linear Model Selection and Regularization",
    "section": "Introduction: Goals",
    "text": "Introduction: Goals\nThe primary goals of these extensions are twofold:\n\nBetter Prediction Accuracy üí™: We aim to create models that make more accurate predictions on new, unseen data.\nImproved Model Interpretability üßê: We want to develop models that are easier to understand, highlighting the most important factors influencing the outcome.\n\n\n\n\n\n\n\nData mining and machine learning, in essence, are to find the most suitable model from a collection of potential models to best fit the data at hand (which includes training data and test data)."
  },
  {
    "objectID": "qmd/islp6.html#data-mining-machine-learning-and-statistical-learning",
    "href": "qmd/islp6.html#data-mining-machine-learning-and-statistical-learning",
    "title": "Linear Model Selection and Regularization",
    "section": "Data Mining, Machine Learning and Statistical Learning",
    "text": "Data Mining, Machine Learning and Statistical Learning\n\n\n\n\n\ngraph LR\n    A[Data Mining] --&gt; C(Common Ground)\n    B[Machine Learning] --&gt; C\n    D[Statistical Learning] --&gt; C\n    C --&gt; E[Insights & Predictions]\n\n\n\n\n\n\n\nData Mining: Discovering patterns and insights from large datasets. It often employs machine learning techniques.\nMachine Learning: Algorithms that allow computers to learn from data without explicit programming.\nStatistical Learning: A set of tools for modeling and understanding complex datasets. It‚Äôs a blend of statistics and machine learning, focusing on both inference and prediction.\nAll three aim to extract meaningful information and make predictions from data."
  },
  {
    "objectID": "qmd/islp6.html#why-go-beyond-least-squares",
    "href": "qmd/islp6.html#why-go-beyond-least-squares",
    "title": "Linear Model Selection and Regularization",
    "section": "Why Go Beyond Least Squares?",
    "text": "Why Go Beyond Least Squares?\nLet‚Äôs recall our standard linear model:\n\\[\nY = \\beta_0 + \\beta_1X_1 + \\dots + \\beta_pX_p + \\epsilon\n\\]\nWhere:\n\n\\(Y\\) is the response variable.\n\\(X_1, \\dots, X_p\\) are the predictor variables.\n\\(\\beta_0, \\dots, \\beta_p\\) are the coefficients (parameters) to be estimated.\n\\(\\epsilon\\) is the error term.\nWe usually use Least Squares to fit this model, finding the coefficients that minimize the sum of squared differences between the observed and predicted values.\nLinear model has distinct advantages in terms of inference (understanding the relationship between variables) and competitive in relation to non-linear methods.\nBut, plain Least Squares has some limitations."
  },
  {
    "objectID": "qmd/islp6.html#why-go-beyond-least-squares-cont.",
    "href": "qmd/islp6.html#why-go-beyond-least-squares-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "Why Go Beyond Least Squares? (Cont.)",
    "text": "Why Go Beyond Least Squares? (Cont.)\n\n\n\n\n\n\nWhen do we need to use another fitting procedure instead of least squares?\n\n\n\nWe will discuss the limitations from two aspects, prediction accuracy and model interpretability."
  },
  {
    "objectID": "qmd/islp6.html#limitations-of-least-squares-prediction-accuracy",
    "href": "qmd/islp6.html#limitations-of-least-squares-prediction-accuracy",
    "title": "Linear Model Selection and Regularization",
    "section": "Limitations of Least Squares: Prediction Accuracy",
    "text": "Limitations of Least Squares: Prediction Accuracy\n\nLow Bias, Low Variance (Ideal): When the true relationship between the predictors and the response is approximately linear and you have many more observations (n) than predictors (p) (\\(n \\gg p\\)), least squares works great! The estimates have low bias and low variance.\n\nLow Bias: The model captures the true underlying relationship well.\nLow Variance: The model‚Äôs predictions are stable and don‚Äôt fluctuate much if you were to train it on different datasets."
  },
  {
    "objectID": "qmd/islp6.html#limitations-of-least-squares-prediction-accuracy-cont.",
    "href": "qmd/islp6.html#limitations-of-least-squares-prediction-accuracy-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "Limitations of Least Squares: Prediction Accuracy (Cont.)",
    "text": "Limitations of Least Squares: Prediction Accuracy (Cont.)\n\nHigh Variance (Problem): If n is not much larger than p, the least squares fit can have high variability. The model becomes too sensitive to the specific training data. This leads to overfitting ü§Ø - the model fits the training data too closely and performs poorly on new data."
  },
  {
    "objectID": "qmd/islp6.html#limitations-of-least-squares-prediction-accuracy-cont.-1",
    "href": "qmd/islp6.html#limitations-of-least-squares-prediction-accuracy-cont.-1",
    "title": "Linear Model Selection and Regularization",
    "section": "Limitations of Least Squares: Prediction Accuracy (Cont.)",
    "text": "Limitations of Least Squares: Prediction Accuracy (Cont.)\n\nNo Unique Solution (Big Problem): If p &gt; n (more predictors than observations), there‚Äôs no longer a unique least squares solution! This is often called the ‚Äúhigh-dimensional‚Äù case. Many possible coefficient values will fit the training data perfectly, leading to huge variance and terrible predictions on new data."
  },
  {
    "objectID": "qmd/islp6.html#limitations-of-least-squares-prediction-accuracy-cont.-2",
    "href": "qmd/islp6.html#limitations-of-least-squares-prediction-accuracy-cont.-2",
    "title": "Linear Model Selection and Regularization",
    "section": "Limitations of Least Squares: Prediction Accuracy (Cont.)",
    "text": "Limitations of Least Squares: Prediction Accuracy (Cont.)\n\n\n\n\n\n\nOverfitting: A model that fits the training data too well, capturing noise and random fluctuations rather than the true underlying relationship. It won‚Äôt generalize well to new data.\n\n\n\nA good model should not only fit the training data well but also have good predictive performance on new data (test data). Therefore, when we say a model is good, we generally consider two aspects: its fit to the training data and its fit to the test data."
  },
  {
    "objectID": "qmd/islp6.html#limitations-of-least-squares-model-interpretability",
    "href": "qmd/islp6.html#limitations-of-least-squares-model-interpretability",
    "title": "Linear Model Selection and Regularization",
    "section": "Limitations of Least Squares: Model Interpretability",
    "text": "Limitations of Least Squares: Model Interpretability\n\nIrrelevant Variables: Often, some predictors in your model aren‚Äôt actually related to the response. Including these irrelevant variables adds unnecessary complexity to the model, making it harder to understand the key drivers. We‚Äôd like to remove these irrelevant variables."
  },
  {
    "objectID": "qmd/islp6.html#limitations-of-least-squares-model-interpretability-cont.",
    "href": "qmd/islp6.html#limitations-of-least-squares-model-interpretability-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "Limitations of Least Squares: Model Interpretability (Cont.)",
    "text": "Limitations of Least Squares: Model Interpretability (Cont.)\n\nLeast Squares Doesn‚Äôt Zero Out: Least squares rarely sets coefficients exactly to zero. Even if a variable is irrelevant, its coefficient will usually be a small, non-zero value. This makes it hard to identify the truly important variables."
  },
  {
    "objectID": "qmd/islp6.html#limitations-of-least-squares-model-interpretability-cont.-1",
    "href": "qmd/islp6.html#limitations-of-least-squares-model-interpretability-cont.-1",
    "title": "Linear Model Selection and Regularization",
    "section": "Limitations of Least Squares: Model Interpretability (Cont.)",
    "text": "Limitations of Least Squares: Model Interpretability (Cont.)\n\nFeature/Variable Selection: We want methods that automatically perform feature selection (or variable selection) ‚Äì excluding irrelevant variables to create a simpler, more interpretable model. This helps us focus on the most important factors."
  },
  {
    "objectID": "qmd/islp6.html#limitations-of-least-squares-model-interpretability-cont.-2",
    "href": "qmd/islp6.html#limitations-of-least-squares-model-interpretability-cont.-2",
    "title": "Linear Model Selection and Regularization",
    "section": "Limitations of Least Squares: Model Interpretability (Cont.)",
    "text": "Limitations of Least Squares: Model Interpretability (Cont.)\n\n\n\n\n\n\nA model with fewer, carefully selected variables is often easier to understand and explain. It highlights the key drivers of the response."
  },
  {
    "objectID": "qmd/islp6.html#three-classes-of-methods",
    "href": "qmd/islp6.html#three-classes-of-methods",
    "title": "Linear Model Selection and Regularization",
    "section": "Three Classes of Methods",
    "text": "Three Classes of Methods\nTo address these limitations, we explore three main classes of methods that offer alternatives to least squares:\n\nSubset Selection: Identify a subset of the p predictors that are most related to the response. Fit a model using least squares on this reduced set of variables. This simplifies the model and improves interpretability.\nShrinkage (Regularization): Fit a model with all p predictors, but shrink the estimated coefficients towards zero. This reduces variance and can improve prediction accuracy. Some methods (like the lasso) can even set coefficients exactly to zero, performing variable selection.\nDimension Reduction: Project the p predictors into an M-dimensional subspace (M &lt; p). This means creating M linear combinations (projections) of the original variables. Use these projections as predictors in a least squares model. This reduces the complexity of the problem."
  },
  {
    "objectID": "qmd/islp6.html#subset-selection",
    "href": "qmd/islp6.html#subset-selection",
    "title": "Linear Model Selection and Regularization",
    "section": "1. Subset Selection",
    "text": "1. Subset Selection\nWe will introduce several methods to select subsets of predictors. Here we consider best subset and stepwise model selection procedures. The goal is to find a smaller group of predictors that still explain the response well.\n\n1.1 Best Subset Selection\n\nThe Idea: Fit a separate least squares regression for every possible combination of the p predictors. This is an exhaustive search through all possible models. Then, choose the ‚Äúbest‚Äù model from this set based on some criterion.\nExhaustive Search: If you have p predictors, you have 2p possible models!\n\n(e.g., 10 predictors = 1,024 models; 20 predictors = over 1 million models!)\nFor each predictor, it can either be in the model or not, leading to 2 choices for each."
  },
  {
    "objectID": "qmd/islp6.html#best-subset-selection-algorithm",
    "href": "qmd/islp6.html#best-subset-selection-algorithm",
    "title": "Linear Model Selection and Regularization",
    "section": "Best Subset Selection Algorithm",
    "text": "Best Subset Selection Algorithm\n\n\n\n\n\n\nAlgorithm 6.1 Best subset selection\n\n\n\n\nNull Model (M0): A model with no predictors. It simply predicts the sample mean (\\(\\bar{y}\\)) of the response for all observations. This serves as a baseline.\nFor k = 1, 2, ‚Ä¶, p: (where k is the number of predictors)\n\nFit all \\(\\binom{p}{k}\\) models that contain exactly k predictors. This is the number of ways to choose k predictors out of p.\nPick the ‚Äúbest‚Äù model among these \\(\\binom{p}{k}\\) models, and call it Mk. ‚ÄúBest‚Äù is defined as having the smallest Residual Sum of Squares (RSS) or, equivalently, the largest R2. RSS measures the error between the model‚Äôs predictions and the actual values.\n\nSelect the ultimate best model: From the models M0, M1, ‚Ä¶, Mp (one best model for each size), choose the single best model using a method that estimates the test error, such as:\n\nValidation set error\nCross-validation error\nCp (AIC)\nBIC\nAdjusted R2"
  },
  {
    "objectID": "qmd/islp6.html#best-subset-selection-illustration",
    "href": "qmd/islp6.html#best-subset-selection-illustration",
    "title": "Linear Model Selection and Regularization",
    "section": "Best Subset Selection: Illustration",
    "text": "Best Subset Selection: Illustration\n\n\n\nCredit data: RSS and \\(R^2\\) for all possible models. The red frontier tracks the best model for each number of predictors."
  },
  {
    "objectID": "qmd/islp6.html#best-subset-selection-illustration-cont.",
    "href": "qmd/islp6.html#best-subset-selection-illustration-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "Best Subset Selection: Illustration (Cont.)",
    "text": "Best Subset Selection: Illustration (Cont.)\n\nFigure 6.1: Shows RSS and R2 for all possible models on the Credit dataset. This dataset contains information about credit card holders, and the goal is to predict their credit card balance.\nThe data contains ten predictors, but the x-axis ranges to 11. The reason is that one of the predictors is categorical, taking three values. It is split up into two dummy variables.\nA categorical variable is also called a qualitative variable. Examples of categorical variables include gender (male, female), region (North, South, East, West), education level (high school, bachelor‚Äôs, master‚Äôs, doctorate), etc."
  },
  {
    "objectID": "qmd/islp6.html#best-subset-selection-illustration-cont.-1",
    "href": "qmd/islp6.html#best-subset-selection-illustration-cont.-1",
    "title": "Linear Model Selection and Regularization",
    "section": "Best Subset Selection: Illustration (Cont.)",
    "text": "Best Subset Selection: Illustration (Cont.)\n\n\n\nCredit data: RSS and \\(R^2\\) for all possible models. The red frontier tracks the best model for each number of predictors.\n\n\n\nThe red line connects the best models for each size (lowest RSS or highest R2). For each number of predictors, the red line indicates the model that performs best on the training data.\nAs expected, RSS decreases and R2 increases as more variables are added. However, the improvements become very small after just a few variables. This suggests that adding more variables beyond a certain point doesn‚Äôt significantly improve the model‚Äôs fit to the training data."
  },
  {
    "objectID": "qmd/islp6.html#best-subset-selection-choosing-the-best-model",
    "href": "qmd/islp6.html#best-subset-selection-choosing-the-best-model",
    "title": "Linear Model Selection and Regularization",
    "section": "Best Subset Selection: Choosing the Best Model",
    "text": "Best Subset Selection: Choosing the Best Model\n\n\n\n\n\n\nThe RSS of these p + 1 models decreases monotonically, and the R¬≤ increases monotonically, as the number of features included in the models increases. So we can‚Äôt use them to select the best model!\n\n\n\n\nTraining Error vs.¬†Test Error: Low RSS and high R2 indicate a good fit to the training data. But we want a model that performs well on new, unseen data (low test error). Training error is often much smaller than test error! This is because the model is specifically optimized to fit the training data.\nNeed a Different Criterion: We can‚Äôt use RSS or R2 directly to select the best model (from among M0, M1, ‚Ä¶, Mp) because they only reflect the fit on the training data. We need to estimate the test error."
  },
  {
    "objectID": "qmd/islp6.html#best-subset-selection-computational-limitations",
    "href": "qmd/islp6.html#best-subset-selection-computational-limitations",
    "title": "Linear Model Selection and Regularization",
    "section": "Best Subset Selection: Computational Limitations",
    "text": "Best Subset Selection: Computational Limitations\n\nExponential Growth: The number of possible models (2p) grows very quickly as p increases. This makes the search computationally expensive.\nInfeasible for Large p: Best subset selection becomes computationally infeasible for even moderately large values of p (e.g., p &gt; 40). It simply takes too long to fit all possible models.\nStatistical Problems (Large p): With a huge search space, there‚Äôs a higher chance of finding models that fit the training data well by chance, even if they have no real predictive power. This leads to overfitting and high variance in the coefficient estimates. The model becomes too ‚Äútuned‚Äù to the training data and doesn‚Äôt generalize well.\n\n\n1.2 Stepwise Selection\nBest subset selection is often computationally infeasible for large p. Thus, stepwise methods are attractive alternatives. They offer a more efficient way to search for a good model.\n\nStepwise methods explore a far more restricted set of models. Instead of considering all possible models, they make sequential decisions to add or remove variables.\n\n\n1.2.1 Forward Stepwise Selection\n\nThe Idea: Start with the null model (no predictors). Add predictors one-at-a-time, always choosing the variable that gives the greatest additional improvement to the fit. It‚Äôs a ‚Äúgreedy‚Äù approach, making the best local decision at each step.\n\n\n\n\n\n\n\nAlgorithm 6.2 Forward stepwise selection\n\n\n\n\nNull Model (M0): Start with the model containing no predictors.\nFor k = 0, 1, ‚Ä¶, p-1:\n\nConsider all p - k models that add one additional predictor to the current model (Mk). This means adding each of the remaining variables, one at a time.\nChoose the ‚Äúbest‚Äù of these p - k models (smallest RSS or highest R2), and call it Mk+1. ‚ÄúBest‚Äù is again based on the training data fit.\n\nSelect the ultimate best model: Choose the single best model from M0, M1, ‚Ä¶, Mp using validation set error, cross-validation, Cp, BIC, or adjusted R2. This step uses a method that estimates the test error."
  },
  {
    "objectID": "qmd/islp6.html#forward-stepwise-selection-computational-advantage",
    "href": "qmd/islp6.html#forward-stepwise-selection-computational-advantage",
    "title": "Linear Model Selection and Regularization",
    "section": "Forward Stepwise Selection: Computational Advantage",
    "text": "Forward Stepwise Selection: Computational Advantage\n\nMuch Fewer Models: Forward stepwise selection considers many fewer models than best subset selection.\n\nBest subset: 2p models.\nForward stepwise: 1 + p(p+1)/2 models. (This is the sum of the integers from 1 to p, plus 1 for the null model.)\nExample: If p = 20, best subset considers over 1 million models, while forward stepwise considers only 211.\n\nComputational Efficiency: This makes forward stepwise selection computationally feasible for much larger values of p."
  },
  {
    "objectID": "qmd/islp6.html#forward-stepwise-selection-limitations",
    "href": "qmd/islp6.html#forward-stepwise-selection-limitations",
    "title": "Linear Model Selection and Regularization",
    "section": "Forward Stepwise Selection: Limitations",
    "text": "Forward Stepwise Selection: Limitations\n\nNot Guaranteed Optimal: Forward stepwise selection is not guaranteed to find the best possible model out of all 2p possibilities. It‚Äôs a greedy algorithm ‚Äì it makes the locally optimal choice at each step, which may not lead to the globally optimal solution. It might miss the true best model.\nExample:\n\nSuppose the best 1-variable model contains X1.\nThe best 2-variable model might contain X2 and X3.\nForward stepwise won‚Äôt find this, because it must keep X1 in the 2-variable model, having chosen it in the first step."
  },
  {
    "objectID": "qmd/islp6.html#forward-stepwise-selection-vs.-best-subset-selection-an-example",
    "href": "qmd/islp6.html#forward-stepwise-selection-vs.-best-subset-selection-an-example",
    "title": "Linear Model Selection and Regularization",
    "section": "Forward Stepwise Selection vs.¬†Best Subset Selection: An Example",
    "text": "Forward Stepwise Selection vs.¬†Best Subset Selection: An Example\n\nComparison on the Credit dataset.\n\n\n\n\n\n\n\n# Variables\nBest Subset\nForward Stepwise\n\n\n\n\nOne\nrating\nrating\n\n\nTwo\nrating, income\nrating, income\n\n\nThree\nrating, income, student\nrating, income, student\n\n\nFour\ncards, income, student, limit\nrating, income, student, limit\n\n\n\n\nThe table compares the models selected by best subset selection and forward stepwise selection on the Credit dataset."
  },
  {
    "objectID": "qmd/islp6.html#forward-stepwise-selection-vs.-best-subset-selection-an-example-cont.",
    "href": "qmd/islp6.html#forward-stepwise-selection-vs.-best-subset-selection-an-example-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "Forward Stepwise Selection vs.¬†Best Subset Selection: An Example (Cont.)",
    "text": "Forward Stepwise Selection vs.¬†Best Subset Selection: An Example (Cont.)\n\nThe first three models selected are identical. Both methods choose the same variables in the same order for the first three steps.\nThe fourth models differ. Best subset selection chooses ‚Äúcards,‚Äù while forward stepwise keeps ‚Äúrating.‚Äù\nBut in this example, the four-variable models perform very similarly (see Figure 6.1), so the difference isn‚Äôt crucial. The performance difference between these two 4-variable models is likely small."
  },
  {
    "objectID": "qmd/islp6.html#forward-stepwise-in-high-dimensions",
    "href": "qmd/islp6.html#forward-stepwise-in-high-dimensions",
    "title": "Linear Model Selection and Regularization",
    "section": "Forward Stepwise in High Dimensions",
    "text": "Forward Stepwise in High Dimensions\n\nn &lt; p Case: Forward stepwise selection can be used even when n &lt; p (more predictors than observations). This is a major advantage.\nLimitation: In this case, you can only build models up to size Mn-1, because least squares can‚Äôt fit a unique solution when p ‚â• n. You can‚Äôt add more variables than you have observations.\n\n\n1.2.2 Backward Stepwise Selection\n\nThe Idea: Start with the full model (all p predictors). Remove the least useful predictor one-at-a-time. It‚Äôs the opposite of forward stepwise.\n\n\n\n\n\n\n\nAlgorithm 6.3 Backward stepwise selection\n\n\n\n\nFull Model (Mp): Begin with the model containing all p predictors.\nFor k = p, p-1, ‚Ä¶, 1:\n\nConsider all k models that remove one predictor from the current model (Mk).\nChoose the ‚Äúbest‚Äù of these k models (smallest RSS or highest R2), and call it Mk-1. Again, ‚Äúbest‚Äù is based on training data fit.\n\nSelect the ultimate best model: Select the single best model from M0, ‚Ä¶, Mp using validation set error, cross-validation, Cp, BIC, or adjusted R2. This uses a method that estimates test error."
  },
  {
    "objectID": "qmd/islp6.html#backward-stepwise-selection-properties",
    "href": "qmd/islp6.html#backward-stepwise-selection-properties",
    "title": "Linear Model Selection and Regularization",
    "section": "Backward Stepwise Selection: Properties",
    "text": "Backward Stepwise Selection: Properties\n\nComputational Advantage: Like forward stepwise, backward stepwise considers only 1 + p(p+1)/2 models, making it computationally efficient.\nNot Guaranteed Optimal: Like forward stepwise, it‚Äôs not guaranteed to find the best possible model.\nRequirement: n &gt; p: Backward stepwise selection requires that n &gt; p (more observations than predictors) so that the full model (with all p predictors) can be fit. This is a significant limitation compared to forward stepwise."
  },
  {
    "objectID": "qmd/islp6.html#backward-stepwise-selection-when-to-use",
    "href": "qmd/islp6.html#backward-stepwise-selection-when-to-use",
    "title": "Linear Model Selection and Regularization",
    "section": "Backward Stepwise Selection: When to use",
    "text": "Backward Stepwise Selection: When to use\n\n\n\n\n\n\nForward stepwise can be used even when n &lt; p, and so is the only viable subset method when p is very large.\n\n\n\n\nBackward stepwise is not useful when p &gt; n.\n\n\n1.2.3 Hybrid Approaches\n\nCombine Forward and Backward: Hybrid methods combine aspects of forward and backward stepwise selection. They try to get the benefits of both.\nAdd and Remove: Variables are added sequentially (like forward). But, after adding each new variable, the method may also remove any variables that no longer contribute significantly to the model fit (based on some criterion). This allows the algorithm to ‚Äúcorrect‚Äù earlier decisions.\nGoal: Try to mimic best subset selection while retaining the computational advantages of stepwise methods. They aim for a better solution than pure forward or backward stepwise, while still being computationally efficient.\n\n\n\n1.3 Choosing the Optimal Model\n\n\n\n\n\n\nBest subset selection, forward selection, and backward selection result in the creation of a set of models, each of which contains a subset of the p predictors.\n\n\n\n\nThe Challenge: How do we choose the best model from among the set of models (M0, ‚Ä¶, Mp) generated by subset selection or stepwise selection? We cannot simply use the model that has the smallest RSS and the largest R2! Those metrics are based on the training data and are likely to be overly optimistic.\nNeed to Estimate Test Error: We need to estimate the test error of each model ‚Äì how well it will perform on new data.\nTwo Main Approaches:\n\nIndirectly Estimate Test Error: Adjust the training error (e.g., RSS) to account for the bias due to overfitting. These adjustments penalize model complexity.\nDirectly Estimate Test Error: Use a validation set or cross-validation. These methods directly assess performance on data not used for training."
  },
  {
    "objectID": "qmd/islp6.html#indirectly-estimating-test-error-cp-aic-bic-adjusted-r2",
    "href": "qmd/islp6.html#indirectly-estimating-test-error-cp-aic-bic-adjusted-r2",
    "title": "Linear Model Selection and Regularization",
    "section": "Indirectly Estimating Test Error: Cp, AIC, BIC, Adjusted R2",
    "text": "Indirectly Estimating Test Error: Cp, AIC, BIC, Adjusted R2\n\nTraining Error is Deceptive: The training set MSE (RSS/n) generally underestimates the test MSE. This is because least squares specifically minimizes the training RSS. The model is optimized for the training data, so it will naturally perform better on that data than on unseen data.\nAdjusting for Model Size: We need to adjust the training error to account for the fact that it tends to be too optimistic. Several techniques do this:\n\nCp\nAkaike Information Criterion (AIC)\nBayesian Information Criterion (BIC)\nAdjusted R2\nAll of these add a penalty to the training error that increases with the number of predictors."
  },
  {
    "objectID": "qmd/islp6.html#cp-aic-bic-adjusted-r2-formulas",
    "href": "qmd/islp6.html#cp-aic-bic-adjusted-r2-formulas",
    "title": "Linear Model Selection and Regularization",
    "section": "Cp, AIC, BIC, Adjusted R2: Formulas",
    "text": "Cp, AIC, BIC, Adjusted R2: Formulas\nFor a least squares model with d predictors, these statistics are computed as:\n\nCp: \\[\nC_p = \\frac{1}{n}(RSS + 2d\\hat{\\sigma}^2)\n\\]\n\n\\(\\hat{\\sigma}^2\\) is an estimate of the error variance (the variance of the noise term, \\(\\epsilon\\)). This is usually estimated from the full model (using all p predictors).\nAdds a penalty of \\(2d\\hat{\\sigma}^2\\) to the RSS. This penalty increases linearly with the number of predictors (d)."
  },
  {
    "objectID": "qmd/islp6.html#cp-aic-bic-adjusted-r2-formulas-cont.",
    "href": "qmd/islp6.html#cp-aic-bic-adjusted-r2-formulas-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "Cp, AIC, BIC, Adjusted R2: Formulas (Cont.)",
    "text": "Cp, AIC, BIC, Adjusted R2: Formulas (Cont.)\n\nAIC: \\[\nAIC = \\frac{1}{n}(RSS + 2d\\hat{\\sigma}^2)\n\\]\n\nFor linear model with Gaussian (normally distributed) errors, AIC is proportional to Cp. They are essentially equivalent in this context. AIC is more broadly applicable to other types of models."
  },
  {
    "objectID": "qmd/islp6.html#cp-aic-bic-adjusted-r2-formulas-cont.-1",
    "href": "qmd/islp6.html#cp-aic-bic-adjusted-r2-formulas-cont.-1",
    "title": "Linear Model Selection and Regularization",
    "section": "Cp, AIC, BIC, Adjusted R2: Formulas (Cont.)",
    "text": "Cp, AIC, BIC, Adjusted R2: Formulas (Cont.)\n\nBIC: \\[\nBIC = \\frac{1}{n}(RSS + log(n)d\\hat{\\sigma}^2)\n\\]\n\nSimilar to Cp, but the penalty for the number of predictors is multiplied by log(n).\nSince log(n) &gt; 2 for n &gt; 7, BIC generally penalizes models with more variables more heavily than Cp, leading to the selection of smaller models. The penalty for adding a predictor is larger with BIC than with Cp (assuming n &gt; 7)."
  },
  {
    "objectID": "qmd/islp6.html#cp-aic-bic-adjusted-r2-formulas-cont.-2",
    "href": "qmd/islp6.html#cp-aic-bic-adjusted-r2-formulas-cont.-2",
    "title": "Linear Model Selection and Regularization",
    "section": "Cp, AIC, BIC, Adjusted R2: Formulas (Cont.)",
    "text": "Cp, AIC, BIC, Adjusted R2: Formulas (Cont.)\n\nAdjusted R2: \\[\nAdjusted \\ R^2 = 1 - \\frac{RSS/(n - d - 1)}{TSS/(n-1)}\n\\]\nWhere TSS (Total Sum of Squares) = \\(\\sum(y_i - \\bar{y})^2\\), which represents total variance in the response.\nMaximizing the adjusted R¬≤ is equivalent to minimizing \\(\\frac{RSS}{n-d-1}\\). Unlike RSS, adjusted R¬≤ accounts for the number of predictors."
  },
  {
    "objectID": "qmd/islp6.html#cp-aic-bic-adjusted-r2-interpretation",
    "href": "qmd/islp6.html#cp-aic-bic-adjusted-r2-interpretation",
    "title": "Linear Model Selection and Regularization",
    "section": "Cp, AIC, BIC, Adjusted R2: Interpretation",
    "text": "Cp, AIC, BIC, Adjusted R2: Interpretation\n\nLow Values are Good (Cp, AIC, BIC): For Cp, AIC, and BIC, we choose the model with the lowest value. Lower values indicate a better trade-off between model fit and complexity.\nHigh Values are Good (Adjusted R2): For adjusted R2, we choose the model with the highest value. Higher values indicate a better fit, adjusted for the number of predictors.\nTheoretical Justification: Cp, AIC, and BIC have theoretical justifications (though they rely on assumptions that may not always hold). They are derived from principles of statistical information theory. Adjusted R2 is more intuitive but less theoretically grounded."
  },
  {
    "objectID": "qmd/islp6.html#cp-aic-bic-adjusted-r2-example",
    "href": "qmd/islp6.html#cp-aic-bic-adjusted-r2-example",
    "title": "Linear Model Selection and Regularization",
    "section": "Cp, AIC, BIC, Adjusted R2: Example",
    "text": "Cp, AIC, BIC, Adjusted R2: Example\n\n\n\nCp, BIC, and adjusted R2 for the best models of each size on the Credit data."
  },
  {
    "objectID": "qmd/islp6.html#cp-aic-bic-adjusted-r2-example-cont.",
    "href": "qmd/islp6.html#cp-aic-bic-adjusted-r2-example-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "Cp, AIC, BIC, Adjusted R2: Example (Cont.)",
    "text": "Cp, AIC, BIC, Adjusted R2: Example (Cont.)\n\nFigure 6.2: Shows these statistics for the Credit dataset.\nCp and BIC are estimates of test MSE. They aim to approximate how well the model would perform on new data.\nBIC selects a model with 4 variables (income, limit, cards, student).\nCp selects a 6-variable model.\nAdjusted R2 selects a 7-variable model."
  },
  {
    "objectID": "qmd/islp6.html#directly-estimating-test-error-validation-and-cross-validation",
    "href": "qmd/islp6.html#directly-estimating-test-error-validation-and-cross-validation",
    "title": "Linear Model Selection and Regularization",
    "section": "Directly Estimating Test Error: Validation and Cross-Validation",
    "text": "Directly Estimating Test Error: Validation and Cross-Validation\n\nDirect Estimation: Instead of adjusting the training error, we can directly estimate the test error using:\n\nValidation set approach\nCross-validation approach\n\nAdvantages:\n\nProvide a direct estimate of the test error. This is more reliable than adjusting the training error.\nMake fewer assumptions about the true underlying model. They are more generally applicable.\nCan be used in a wider range of model selection tasks (not just linear models).\n\nComputational Cost: Historically, cross-validation was computationally expensive. Now, with fast computers, this is less of a concern."
  },
  {
    "objectID": "qmd/islp6.html#validation-and-cross-validation-example",
    "href": "qmd/islp6.html#validation-and-cross-validation-example",
    "title": "Linear Model Selection and Regularization",
    "section": "Validation and Cross-Validation: Example",
    "text": "Validation and Cross-Validation: Example\n\n\n\nBIC, validation set error, and cross-validation error for the best models of each size on the Credit data."
  },
  {
    "objectID": "qmd/islp6.html#validation-and-cross-validation-example-cont.",
    "href": "qmd/islp6.html#validation-and-cross-validation-example-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "Validation and Cross-Validation: Example (Cont.)",
    "text": "Validation and Cross-Validation: Example (Cont.)\n\nFigure 6.3: Shows BIC, validation set error, and cross-validation error for the Credit data.\nValidation and cross-validation both select a 6-variable model.\nAll three approaches suggest that models with 4, 5, or 6 variables are quite similar. The estimated test error is relatively flat for these model sizes.\nOne-Standard-Error Rule: A practical rule for choosing among models with similar estimated test error.\n\nCalculate the standard error of the estimated test MSE for each model size. This represents the uncertainty in the estimate.\nSelect the smallest model for which the estimated test error is within one standard error of the lowest point on the curve.\nRationale: Choose the simplest model among those that perform comparably. We prefer simpler models if they perform almost as well as more complex models.\n\nApply to this example, we may choose the three-variable model."
  },
  {
    "objectID": "qmd/islp6.html#shrinkage-methods",
    "href": "qmd/islp6.html#shrinkage-methods",
    "title": "Linear Model Selection and Regularization",
    "section": "2. Shrinkage Methods",
    "text": "2. Shrinkage Methods\n\nAlternative to Subset Selection: Instead of selecting a subset of variables, shrinkage methods (also called regularization methods) fit a model with all p predictors, but constrain or regularize the coefficient estimates. They ‚Äúshrink‚Äù the coefficients towards zero.\nHow it Works: Shrinkage methods shrink the coefficient estimates towards zero.\nWhy Shrink?: Shrinking the coefficients can significantly reduce their variance. This can improve prediction accuracy, especially when the least squares estimates have high variance.\nTwo Main Techniques:\n\nRidge regression\nLasso\n\n\n\n2.1 Ridge Regression\n\nRecall Least Squares: Least squares minimizes the Residual Sum of Squares (RSS):\n\n\\[\nRSS = \\sum_{i=1}^{n}(y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij})^2\n\\]\n\nThis finds the coefficients that minimize the sum of squared differences between the observed responses (\\(y_i\\)) and the predicted responses (\\(\\hat{y}_i = \\beta_0 + \\sum_{j=1}^{p}\\beta_jx_{ij}\\))."
  },
  {
    "objectID": "qmd/islp6.html#ridge-regression-cont.",
    "href": "qmd/islp6.html#ridge-regression-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "Ridge Regression (Cont.)",
    "text": "Ridge Regression (Cont.)\n\nRidge Regression: Ridge regression minimizes a slightly different quantity:\n\n\\[\n\\sum_{i=1}^{n}(y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij})^2 + \\lambda\\sum_{j=1}^{p}\\beta_j^2 = RSS + \\lambda\\sum_{j=1}^{p}\\beta_j^2\n\\]\n\nŒª (Tuning Parameter): Œª ‚â• 0 is a tuning parameter that controls the amount of shrinkage. It determines the strength of the penalty."
  },
  {
    "objectID": "qmd/islp6.html#ridge-regression-the-shrinkage-penalty",
    "href": "qmd/islp6.html#ridge-regression-the-shrinkage-penalty",
    "title": "Linear Model Selection and Regularization",
    "section": "Ridge Regression: The Shrinkage Penalty",
    "text": "Ridge Regression: The Shrinkage Penalty\n\\[\n\\sum_{i=1}^{n}(y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij})^2 + \\lambda\\sum_{j=1}^{p}\\beta_j^2 = RSS + \\lambda\\sum_{j=1}^{p}\\beta_j^2\n\\]\n\nTwo Parts:\n\nRSS: Measures how well the model fits the data. Smaller RSS means a better fit.\nShrinkage Penalty (ŒªŒ£Œ≤j2): Penalizes large coefficients. This term is small when Œ≤1, ‚Ä¶, Œ≤p are close to zero. The penalty is the sum of the squared coefficients (excluding the intercept)."
  },
  {
    "objectID": "qmd/islp6.html#ridge-regression-the-shrinkage-penalty-cont.",
    "href": "qmd/islp6.html#ridge-regression-the-shrinkage-penalty-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "Ridge Regression: The Shrinkage Penalty (Cont.)",
    "text": "Ridge Regression: The Shrinkage Penalty (Cont.)\n\nTuning Parameter (Œª):\n\nŒª = 0: No penalty. Ridge regression is the same as least squares.\nŒª ‚Üí ‚àû: Coefficients are shrunk all the way to zero (this would result in the null model, predicting only the mean of the response).\n0 &lt; Œª &lt; ‚àû: Controls the trade-off between fitting the data well (low RSS) and shrinking the coefficients (small penalty)."
  },
  {
    "objectID": "qmd/islp6.html#ridge-regression-the-intercept",
    "href": "qmd/islp6.html#ridge-regression-the-intercept",
    "title": "Linear Model Selection and Regularization",
    "section": "Ridge Regression: The Intercept",
    "text": "Ridge Regression: The Intercept\n\nNo Shrinkage on Intercept: Notice that the shrinkage penalty is not applied to the intercept (Œ≤0).\nWhy?: We want to shrink the coefficients of the predictors, but not the intercept, which represents the average value of the response when all predictors are zero (or at their mean values, if centered). Shrinking the intercept would bias the predictions.\nCentering Predictors: If the predictors are centered (mean of zero) before performing ridge regression, then the estimated intercept will be the sample mean of the response: \\(\\hat{\\beta}_0 = \\bar{y}\\). Centering simplifies the calculations and ensures the intercept has a meaningful interpretation."
  },
  {
    "objectID": "qmd/islp6.html#ridge-regression-example-on-credit-data",
    "href": "qmd/islp6.html#ridge-regression-example-on-credit-data",
    "title": "Linear Model Selection and Regularization",
    "section": "Ridge Regression: Example on Credit Data",
    "text": "Ridge Regression: Example on Credit Data\n\n\n\nStandardized ridge regression coefficients for the Credit data, as a function of Œª and ||Œ≤ŒªR||2 / ||Œ≤||2."
  },
  {
    "objectID": "qmd/islp6.html#ridge-regression-example-on-credit-data-cont.",
    "href": "qmd/islp6.html#ridge-regression-example-on-credit-data-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "Ridge Regression: Example on Credit Data (Cont.)",
    "text": "Ridge Regression: Example on Credit Data (Cont.)\n\nFigure 6.4: Shows ridge regression coefficient estimates for the Credit data.\nLeft Panel: Coefficients plotted against Œª.\n\nŒª = 0: Coefficients are the same as least squares.\nAs Œª increases, coefficients shrink towards zero.\n\nRight Panel: Coefficients plotted against ||Œ≤ŒªR||2 / ||Œ≤||2.\n||Œ≤ŒªR||2 represents the l2 norm of ridge regression coefficients.\n||Œ≤||2 represents the l2 norm of least squares coefficients.\nThe x-axis can be seen as how much the ridge regression coefficient estimates have been shrunken towards zero."
  },
  {
    "objectID": "qmd/islp6.html#ridge-regression-standardization",
    "href": "qmd/islp6.html#ridge-regression-standardization",
    "title": "Linear Model Selection and Regularization",
    "section": "Ridge Regression: Standardization",
    "text": "Ridge Regression: Standardization\n\nScale Equivariance (Least Squares): Least squares coefficient estimates are scale equivariant. Multiplying a predictor by a constant c simply scales the corresponding coefficient by 1/c. The overall prediction remains unchanged.\nScale Dependence (Ridge Regression): Ridge regression coefficients can change substantially when multiplying a predictor by a constant. This is because of the Œ£Œ≤j2 term in the penalty. If a coefficient is large, squaring it makes it even larger, increasing the penalty. Scaling a predictor changes the scale of its coefficient, thus changing the penalty."
  },
  {
    "objectID": "qmd/islp6.html#ridge-regression-standardization-cont.",
    "href": "qmd/islp6.html#ridge-regression-standardization-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "Ridge Regression: Standardization (Cont.)",
    "text": "Ridge Regression: Standardization (Cont.)\n\nStandardization: It‚Äôs best to apply ridge regression after standardizing the predictors:\n\n\\[\n\\tilde{x}_{ij} = \\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(x_{ij}-\\bar{x}_j)^2}}\n\\]\n\nThis formula standardizes each predictor by subtracting its mean and dividing by its standard deviation. This ensures that all predictors are on the same scale (mean of 0, standard deviation of 1).\nThis ensures that all predictors are on the same scale. Standardization makes the penalty apply equally to all predictors, regardless of their original units."
  },
  {
    "objectID": "qmd/islp6.html#why-does-ridge-regression-improve-over-least-squares",
    "href": "qmd/islp6.html#why-does-ridge-regression-improve-over-least-squares",
    "title": "Linear Model Selection and Regularization",
    "section": "Why Does Ridge Regression Improve Over Least Squares?",
    "text": "Why Does Ridge Regression Improve Over Least Squares?\n\nBias-Variance Trade-Off: Ridge regression‚Äôs advantage comes from the bias-variance trade-off. This is a fundamental concept in statistical learning.\n\nAs Œª increases:\n\nFlexibility of the model decreases. The model becomes less able to fit the training data perfectly.\nVariance decreases. The model‚Äôs predictions become more stable.\nBias increases. The model‚Äôs predictions, on average, deviate more from the true values.\n\n\nFinding the Sweet Spot: The goal is to find a value of Œª that reduces variance more than it increases bias, leading to a lower test MSE (Mean Squared Error)."
  },
  {
    "objectID": "qmd/islp6.html#ridge-regression-bias-variance-trade-off-illustrated",
    "href": "qmd/islp6.html#ridge-regression-bias-variance-trade-off-illustrated",
    "title": "Linear Model Selection and Regularization",
    "section": "Ridge Regression: Bias-Variance Trade-Off Illustrated",
    "text": "Ridge Regression: Bias-Variance Trade-Off Illustrated\n\n\n\nSquared bias, variance, and test MSE for ridge regression on a simulated dataset."
  },
  {
    "objectID": "qmd/islp6.html#ridge-regression-bias-variance-trade-off-illustrated-cont.",
    "href": "qmd/islp6.html#ridge-regression-bias-variance-trade-off-illustrated-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "Ridge Regression: Bias-Variance Trade-Off Illustrated (Cont.)",
    "text": "Ridge Regression: Bias-Variance Trade-Off Illustrated (Cont.)\n\nFigure 6.5: Shows bias, variance, and test MSE for ridge regression on a simulated dataset.\nAs Œª increases, variance decreases rapidly at first, with only a small increase in bias. This leads to a decrease in MSE.\nEventually, the decrease in variance slows, and the increase in bias accelerates, causing the MSE to increase. The penalty becomes too strong, and the model underfits.\nThe minimum MSE is achieved at a moderate value of Œª. This is the optimal level of shrinkage."
  },
  {
    "objectID": "qmd/islp6.html#when-does-ridge-regression-work-well",
    "href": "qmd/islp6.html#when-does-ridge-regression-work-well",
    "title": "Linear Model Selection and Regularization",
    "section": "When Does Ridge Regression Work Well?",
    "text": "When Does Ridge Regression Work Well?\n\nHigh Variance in Least Squares: Ridge regression works best in situations where the least squares estimates have high variance. This often happens when:\n\nn is not much larger than p. The number of observations is not significantly greater than the number of predictors.\np is close to n.\np &gt; n (though least squares doesn‚Äôt have a unique solution in this case). Ridge regression can still provide a solution, even when p &gt; n.\n\nComputational Advantage: Ridge regression is also computationally efficient, even for large p. There‚Äôs a closed-form solution, making it relatively fast to compute.\n\n\n2.2 The Lasso\n\nDisadvantage of Ridge Regression: Ridge regression includes all p predictors in the final model. The penalty shrinks coefficients towards zero, but it doesn‚Äôt set any of them exactly to zero (unless Œª = ‚àû). This can make interpretation difficult when p is large. It doesn‚Äôt perform variable selection.\nThe Lasso: An Alternative: The lasso is a more recent alternative to ridge regression that overcomes this disadvantage. It can perform variable selection."
  },
  {
    "objectID": "qmd/islp6.html#the-lasso-penalty",
    "href": "qmd/islp6.html#the-lasso-penalty",
    "title": "Linear Model Selection and Regularization",
    "section": "The Lasso: Penalty",
    "text": "The Lasso: Penalty\n\nLasso Penalty: The lasso uses a different penalty term:\n\n\\[\n\\sum_{i=1}^{n}(y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij})^2 + \\lambda\\sum_{j=1}^{p}|\\beta_j| = RSS + \\lambda\\sum_{j=1}^{p}|\\beta_j|\n\\]\n\nAbsolute Value Penalty: The lasso uses an l1 penalty (absolute value of coefficients) instead of an l2 penalty (squared coefficients, used in ridge regression)."
  },
  {
    "objectID": "qmd/islp6.html#the-lasso-variable-selection",
    "href": "qmd/islp6.html#the-lasso-variable-selection",
    "title": "Linear Model Selection and Regularization",
    "section": "The Lasso: Variable Selection",
    "text": "The Lasso: Variable Selection\n\nShrinkage and Selection: Like ridge regression, the lasso shrinks coefficients towards zero.\nKey Difference: The l1 penalty has the effect of forcing some coefficients to be exactly zero when Œª is sufficiently large. This is the crucial difference from ridge regression.\nVariable Selection: This means the lasso performs variable selection! It automatically excludes some variables from the model.\nSparse Models: The lasso yields sparse models ‚Äì models that involve only a subset of the variables. ‚ÄúSparse‚Äù means that many of the coefficients are zero."
  },
  {
    "objectID": "qmd/islp6.html#the-lasso-example-on-credit-data",
    "href": "qmd/islp6.html#the-lasso-example-on-credit-data",
    "title": "Linear Model Selection and Regularization",
    "section": "The Lasso: Example on Credit Data",
    "text": "The Lasso: Example on Credit Data\n\n\n\nStandardized lasso coefficients for the Credit data, as a function of Œª and ||Œ≤ŒªL||1 / ||Œ≤||1."
  },
  {
    "objectID": "qmd/islp6.html#the-lasso-example-on-credit-data-cont.",
    "href": "qmd/islp6.html#the-lasso-example-on-credit-data-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "The Lasso: Example on Credit Data (Cont.)",
    "text": "The Lasso: Example on Credit Data (Cont.)\n\nFigure 6.6: Shows lasso coefficient estimates for the Credit data.\nAs Œª increases, coefficients shrink towards zero. But unlike ridge regression, some coefficients are set exactly to zero.\nThis leads to a simpler, more interpretable model. The lasso identifies the most important predictors and excludes the rest."
  },
  {
    "objectID": "qmd/islp6.html#another-formulation-for-ridge-regression-and-the-lasso",
    "href": "qmd/islp6.html#another-formulation-for-ridge-regression-and-the-lasso",
    "title": "Linear Model Selection and Regularization",
    "section": "Another Formulation for Ridge Regression and the Lasso",
    "text": "Another Formulation for Ridge Regression and the Lasso\n\nBoth ridge regression and the lasso can be formulated as constrained optimization problems. This provides an alternative way to understand them.\n\nRidge Regression: \\[\n  \\underset{\\beta}{minimize} \\left\\{ \\sum_{i=1}^{n}(y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij})^2 \\right\\} \\quad subject \\ to \\ \\sum_{j=1}^{p}\\beta_j^2 \\le s\n  \\]\nMinimize the RSS, subject to a constraint on the sum of the squared coefficients. s is a tuning parameter that controls the amount of shrinkage.\nLasso: \\[\n  \\underset{\\beta}{minimize} \\left\\{ \\sum_{i=1}^{n}(y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_jx_{ij})^2 \\right\\} \\quad subject \\ to \\ \\sum_{j=1}^{p}|\\beta_j| \\le s\n  \\]\nMinimize the RSS, subject to a constraint on the sum of the absolute values of the coefficients.\n\nThe above formulations reveal a close connection between the lasso, ridge regression, and best subset selection."
  },
  {
    "objectID": "qmd/islp6.html#the-variable-selection-property-of-the-lasso",
    "href": "qmd/islp6.html#the-variable-selection-property-of-the-lasso",
    "title": "Linear Model Selection and Regularization",
    "section": "The Variable Selection Property of the Lasso",
    "text": "The Variable Selection Property of the Lasso\n\n\n\nContours of the error and constraint functions for the lasso (left) and ridge regression (right).\n\n\n\nFigure 6.7: Contours of the error and constraint functions for the lasso (left) and ridge regression (right).\nThe solid blue areas are the constraint regions.\nThe red ellipses are the contours of the RSS."
  },
  {
    "objectID": "qmd/islp6.html#the-variable-selection-property-of-the-lasso-cont.",
    "href": "qmd/islp6.html#the-variable-selection-property-of-the-lasso-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "The Variable Selection Property of the Lasso (Cont.)",
    "text": "The Variable Selection Property of the Lasso (Cont.)\n\n\n\nContours of the error and constraint functions for the lasso (left) and ridge regression (right).\n\n\n\nWhy does the lasso set coefficients to zero, while ridge regression doesn‚Äôt?\n\nConsider the constraint regions (where the solution must lie):\n\nRidge regression: A circle (l2 constraint: \\(\\sum_{j=1}^{p}\\beta_j^2 \\le s\\)).\nLasso: A diamond (l1 constraint: \\(\\sum_{j=1}^{p}|\\beta_j| \\le s\\)).\n\n\nThe solution is the first point where the ‚Äúellipse‚Äù (contour of constant RSS) touches the constraint region. The ellipses represent combinations of coefficients that have the same RSS.\nBecause the lasso constraint has corners, the ellipse often intersects at an axis, setting one coefficient to zero. When the ellipse hits a corner of the diamond, one of the coefficients is zero.\nRidge regression‚Äôs circular constraint doesn‚Äôt have corners, so this rarely happens. The ellipse is unlikely to intersect the circle exactly on an axis."
  },
  {
    "objectID": "qmd/islp6.html#comparing-the-lasso-and-ridge-regression",
    "href": "qmd/islp6.html#comparing-the-lasso-and-ridge-regression",
    "title": "Linear Model Selection and Regularization",
    "section": "Comparing the Lasso and Ridge Regression",
    "text": "Comparing the Lasso and Ridge Regression\n\nInterpretability: The lasso has a major advantage in terms of interpretability, producing simpler models with fewer variables. This makes it easier to understand the key factors influencing the response.\nPrediction Accuracy: Which method is better for prediction depends on the true underlying relationship between the predictors and the response. It‚Äôs data-dependent.\n\nFew Important Predictors: If only a few predictors are truly important (with large coefficients), the lasso tends to perform better. It will identify these key predictors and exclude the irrelevant ones.\nMany Important Predictors: If many predictors have small or moderate-sized coefficients, ridge regression tends to perform better. Shrinking all coefficients towards zero, without setting any to zero, is more appropriate in this case.\n\nUnknown Truth: In practice, we don‚Äôt know which scenario is true. Cross-validation can help us choose the best approach for a particular dataset."
  },
  {
    "objectID": "qmd/islp6.html#comparing-the-lasso-and-ridge-regression-simulated-examples",
    "href": "qmd/islp6.html#comparing-the-lasso-and-ridge-regression-simulated-examples",
    "title": "Linear Model Selection and Regularization",
    "section": "Comparing the Lasso and Ridge Regression: Simulated Examples",
    "text": "Comparing the Lasso and Ridge Regression: Simulated Examples\n\n\n\nLasso and ridge regression on a simulated dataset where all predictors are related to the response."
  },
  {
    "objectID": "qmd/islp6.html#comparing-the-lasso-and-ridge-regression-simulated-examples-cont.",
    "href": "qmd/islp6.html#comparing-the-lasso-and-ridge-regression-simulated-examples-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "Comparing the Lasso and Ridge Regression: Simulated Examples (Cont.)",
    "text": "Comparing the Lasso and Ridge Regression: Simulated Examples (Cont.)\n\nFigure 6.8: All 45 predictors are related to the response.\nRidge regression slightly outperforms the lasso. Since all predictors contribute to the response, shrinking all coefficients (ridge regression) is better than setting some to zero (lasso).\nThe minimum MSE of ridge regression is slightly smaller than that of the lasso."
  },
  {
    "objectID": "qmd/islp6.html#comparing-the-lasso-and-ridge-regression-simulated-examples-1",
    "href": "qmd/islp6.html#comparing-the-lasso-and-ridge-regression-simulated-examples-1",
    "title": "Linear Model Selection and Regularization",
    "section": "Comparing the Lasso and Ridge Regression: Simulated Examples",
    "text": "Comparing the Lasso and Ridge Regression: Simulated Examples\n\n\n\nLasso and ridge regression on a simulated dataset where only two predictors are related to the response."
  },
  {
    "objectID": "qmd/islp6.html#comparing-the-lasso-and-ridge-regression-simulated-examples-cont.-1",
    "href": "qmd/islp6.html#comparing-the-lasso-and-ridge-regression-simulated-examples-cont.-1",
    "title": "Linear Model Selection and Regularization",
    "section": "Comparing the Lasso and Ridge Regression: Simulated Examples (Cont.)",
    "text": "Comparing the Lasso and Ridge Regression: Simulated Examples (Cont.)\n\nFigure 6.9: Only 2 of 45 predictors are related to the response. This is a sparse setting.\nThe lasso tends to outperform ridge regression. The lasso correctly identifies the two important predictors and sets the coefficients of the irrelevant predictors to zero."
  },
  {
    "objectID": "qmd/islp6.html#a-simple-special-case-for-ridge-regression-and-the-lasso",
    "href": "qmd/islp6.html#a-simple-special-case-for-ridge-regression-and-the-lasso",
    "title": "Linear Model Selection and Regularization",
    "section": "A Simple Special Case for Ridge Regression and the Lasso",
    "text": "A Simple Special Case for Ridge Regression and the Lasso\nWe consider a simple situation:\n\n\\(n=p\\). The number of observations equals the number of predictors.\n\\(\\mathbf{X}\\) is a diagonal matrix with 1‚Äôs on the diagonal. This means the predictors are uncorrelated and have unit variance.\nNo intercept.\n\nThen, it can be shown:\n\nRidge regression shrinks each least squares coefficient estimate by the same proportion. \\[\n\\hat{\\beta}_j^R = \\frac{y_j}{1 + \\lambda}\n\\]"
  },
  {
    "objectID": "qmd/islp6.html#a-simple-special-case-for-ridge-regression-and-the-lasso-cont.",
    "href": "qmd/islp6.html#a-simple-special-case-for-ridge-regression-and-the-lasso-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "A Simple Special Case for Ridge Regression and the Lasso (Cont.)",
    "text": "A Simple Special Case for Ridge Regression and the Lasso (Cont.)\n\nLasso soft-threshold the least squares coefficient estimates. \\[\n\\hat{\\beta}_j^L =\n\\begin{cases}\ny_j - \\lambda/2 & \\text{if } y_j &gt; \\lambda/2 \\\\\ny_j + \\lambda/2,  & \\text{if } y_j &lt; -\\lambda/2 \\\\\n0 & \\text{if } |y_j| \\le \\lambda/2\n\\end{cases}\n\\]\nIf the least squares estimate (\\(y_j\\)) is larger than \\(\\lambda/2\\) or smaller than \\(-\\lambda/2\\), the lasso shrinks it towards zero.\nIf it is between \\(-\\lambda/2\\) and \\(\\lambda/2\\), the lasso sets it to zero. This is called ‚Äúsoft-thresholding.‚Äù"
  },
  {
    "objectID": "qmd/islp6.html#a-simple-special-case-for-ridge-regression-and-the-lasso-1",
    "href": "qmd/islp6.html#a-simple-special-case-for-ridge-regression-and-the-lasso-1",
    "title": "Linear Model Selection and Regularization",
    "section": "A Simple Special Case for Ridge Regression and the Lasso",
    "text": "A Simple Special Case for Ridge Regression and the Lasso\n\n\n\nRidge and lasso coefficient estimates in the simple case."
  },
  {
    "objectID": "qmd/islp6.html#a-simple-special-case-for-ridge-regression-and-the-lasso-cont.-1",
    "href": "qmd/islp6.html#a-simple-special-case-for-ridge-regression-and-the-lasso-cont.-1",
    "title": "Linear Model Selection and Regularization",
    "section": "A Simple Special Case for Ridge Regression and the Lasso (Cont.)",
    "text": "A Simple Special Case for Ridge Regression and the Lasso (Cont.)\n\nFigure 6.10: It shows that:\nRidge regression shrinks each coefficient by same proportion.\nLasso shrinks all coefficients toward zero by a similar amount, and sufficiently small coefficients are shrunken all the way to zero.\n\n\n2.3 Selecting the Tuning Parameter\n\nCrucial Choice: Just like with subset selection, we need to choose the tuning parameter (Œª) for ridge regression and the lasso. The value of Œª determines the amount of shrinkage.\nCross-Validation: Cross-validation is a powerful method for selecting Œª.\n\nChoose a grid of Œª values. Try a range of values to see which works best.\nCompute the cross-validation error for each value of Œª. This estimates the test error for each Œª.\nSelect the Œª that gives the smallest cross-validation error. This is the value that is expected to perform best on new data.\nRe-fit the model using all of the data with the chosen Œª."
  },
  {
    "objectID": "qmd/islp6.html#selecting-Œª-example-for-ridge-regression",
    "href": "qmd/islp6.html#selecting-Œª-example-for-ridge-regression",
    "title": "Linear Model Selection and Regularization",
    "section": "Selecting Œª: Example for Ridge Regression",
    "text": "Selecting Œª: Example for Ridge Regression\n\n\n\nCross-validation error and coefficient estimates for ridge regression on the Credit data.\n\n\n\nFigure 6.12: Shows cross-validation for ridge regression on the Credit data.\nThe optimal Œª is relatively small, indicating a small amount of shrinkage.\nThe cross-validation error curve is quite flat, suggesting that a range of Œª values would work similarly well."
  },
  {
    "objectID": "qmd/islp6.html#selecting-Œª-example-for-lasso",
    "href": "qmd/islp6.html#selecting-Œª-example-for-lasso",
    "title": "Linear Model Selection and Regularization",
    "section": "Selecting Œª: Example for Lasso",
    "text": "Selecting Œª: Example for Lasso\n\n\n\nCross-validation error and coefficient estimates for the lasso on the simulated data from Figure 6.9."
  },
  {
    "objectID": "qmd/islp6.html#selecting-Œª-example-for-lasso-cont.",
    "href": "qmd/islp6.html#selecting-Œª-example-for-lasso-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "Selecting Œª: Example for Lasso (Cont.)",
    "text": "Selecting Œª: Example for Lasso (Cont.)\n\nFigure 6.13: Shows cross-validation for the lasso on the simulated data from Figure 6.9 (where only two predictors are truly related to the response).\nThe lasso correctly identifies the two signal variables (colored lines) and sets the coefficients of the noise variables (gray lines) to near zero.\nThe minimum cross-validation error occurs when only the signal variables have non-zero coefficients."
  },
  {
    "objectID": "qmd/islp6.html#dimension-reduction-methods",
    "href": "qmd/islp6.html#dimension-reduction-methods",
    "title": "Linear Model Selection and Regularization",
    "section": "3. Dimension Reduction Methods",
    "text": "3. Dimension Reduction Methods\n\nDifferent Approach: Instead of working directly with the original predictors (X1, ‚Ä¶, Xp), dimension reduction methods transform the predictors and then fit a least squares model using the transformed variables. They create new, fewer variables that are combinations of the original ones.\nLinear Combinations: Create M linear combinations (Z1, ‚Ä¶, ZM) of the original p predictors, where M &lt; p.\n\n\\[\nZ_m = \\sum_{j=1}^{p}\\phi_{jm}X_j\n\\]\n-   œÜ&lt;sub&gt;jm&lt;/sub&gt; are constants (weights) that define the linear combinations.  Each $Z_m$ is a weighted sum of all the original predictors."
  },
  {
    "objectID": "qmd/islp6.html#dimension-reduction-methods-cont.",
    "href": "qmd/islp6.html#dimension-reduction-methods-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "Dimension Reduction Methods (Cont.)",
    "text": "Dimension Reduction Methods (Cont.)\n\nReduced Dimension: Fit a linear regression model using Z1, ‚Ä¶, ZM as predictors:\n\n\\[\ny_i = \\theta_0 + \\sum_{m=1}^{M}\\theta_mz_{im} + \\epsilon_i\n\\]\n-   This reduces the problem from estimating *p*+1 coefficients (in the original model) to estimating *M*+1 coefficients.  This can significantly simplify the model."
  },
  {
    "objectID": "qmd/islp6.html#dimension-reduction-why-it-works",
    "href": "qmd/islp6.html#dimension-reduction-why-it-works",
    "title": "Linear Model Selection and Regularization",
    "section": "Dimension Reduction: Why it Works",
    "text": "Dimension Reduction: Why it Works\n\nConstraint: The coefficients in the dimension-reduced model are constrained by the linear combinations:\n\n\\[\n\\beta_j = \\sum_{m=1}^{M}\\theta_m\\phi_{jm}\n\\] - The original coefficients (\\(\\beta_j\\)) are now expressed in terms of the new coefficients (\\(\\theta_m\\)) and the weights (\\(\\phi_{jm}\\)).\n\nBias-Variance Trade-Off: This constraint can introduce bias, but if p is large relative to n, choosing M &lt;&lt; p can significantly reduce the variance of the fitted coefficients. By reducing the number of parameters to estimate, we reduce the model‚Äôs flexibility and its tendency to overfit."
  },
  {
    "objectID": "qmd/islp6.html#dimension-reduction-two-steps",
    "href": "qmd/islp6.html#dimension-reduction-two-steps",
    "title": "Linear Model Selection and Regularization",
    "section": "Dimension Reduction: Two Steps",
    "text": "Dimension Reduction: Two Steps\n\nTwo Steps:\n\nObtain the transformed predictors (Z1, ‚Ä¶, ZM). This is where the different dimension reduction methods differ.\nFit a least squares model using these M predictors. This is a standard linear regression.\n\nDifferent Methods: Different dimension reduction methods differ in how they choose the Zm (or, equivalently, the œÜjm). They have different ways of creating the linear combinations.\n\nPrincipal components regression (PCR)\nPartial least squares (PLS)\n\n\n\n3.1 Principal Components Regression (PCR)\n\nPrincipal Components Analysis (PCA): PCA is a technique for deriving a low-dimensional set of features from a larger set of variables. (More detail in Chapter 12.) It finds new variables (principal components) that capture the most variation in the original data.\nUnsupervised: PCA is an unsupervised method ‚Äì it identifies linear combinations that best represent the predictors (X), without considering the response (Y). It only looks at the relationships among the predictors.\n\n\nAn Overview of Principal Components Analysis\n\nPCA seeks to find the directions in the data along with which the observations vary the most. These directions are the principal components.\nFirst Principal Component: The first principal component is the direction in the data with the greatest variance. It‚Äôs the line that best captures the spread of the data."
  },
  {
    "objectID": "qmd/islp6.html#pca-example-on-advertising-data",
    "href": "qmd/islp6.html#pca-example-on-advertising-data",
    "title": "Linear Model Selection and Regularization",
    "section": "PCA: Example on Advertising Data",
    "text": "PCA: Example on Advertising Data\n\n\n\nPopulation size and ad spending for 100 cities. The first principal component is shown in green, and the second in blue.\n\n\n\nFigure 6.14: Shows population size (pop) and advertising spending (ad) for 100 cities.\nThe green line is the first principal component direction."
  },
  {
    "objectID": "qmd/islp6.html#pca-example-on-advertising-data-cont.",
    "href": "qmd/islp6.html#pca-example-on-advertising-data-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "PCA: Example on Advertising Data (Cont.)",
    "text": "PCA: Example on Advertising Data (Cont.)\n\n\n\nPopulation size and ad spending for 100 cities. The first principal component is shown in green, and the second in blue.\n\n\n\nProjecting the observations (data points) onto this line would maximize the variance of the projected points. The first principal component captures the direction of greatest variability in the data."
  },
  {
    "objectID": "qmd/islp6.html#pca-finding-the-first-principal-component",
    "href": "qmd/islp6.html#pca-finding-the-first-principal-component",
    "title": "Linear Model Selection and Regularization",
    "section": "PCA: Finding the First Principal Component",
    "text": "PCA: Finding the First Principal Component\n\nMathematical Representation: The first principal component can be written as:\n\n\\[\nZ_1 = 0.839 \\times (pop - \\overline{pop}) + 0.544 \\times (ad - \\overline{ad})\n\\]\n-   0.839 and 0.544 are the *principal component loadings* (the œÜ&lt;sub&gt;jm&lt;/sub&gt; values).  They define the direction of the first principal component.\n-   $\\overline{pop}$ and $\\overline{ad}$ are the means of pop and ad, respectively.  The variables are centered (mean-subtracted).\n\nInterpretation: Z1 is almost an average of the two variables (since the loadings are positive and similar in size). It represents a direction that captures a combination of population size and ad spending."
  },
  {
    "objectID": "qmd/islp6.html#pca-principal-component-scores",
    "href": "qmd/islp6.html#pca-principal-component-scores",
    "title": "Linear Model Selection and Regularization",
    "section": "PCA: Principal Component Scores",
    "text": "PCA: Principal Component Scores\n\\[\nZ_{i1} = 0.839 \\times (pop_i - \\overline{pop}) + 0.544 \\times (ad_i - \\overline{ad})\n\\]\n\nScores: The values zi1, ‚Ä¶, zn1 are called the principal component scores. They represent the ‚Äúcoordinates‚Äù of the data points along the first principal component direction. They are the values of the new variable, Z1, for each observation."
  },
  {
    "objectID": "qmd/islp6.html#pca-another-interpretation",
    "href": "qmd/islp6.html#pca-another-interpretation",
    "title": "Linear Model Selection and Regularization",
    "section": "PCA: Another Interpretation",
    "text": "PCA: Another Interpretation\n\nClosest Line: The first principal component vector defines the line that is as close as possible to the data (minimizing the sum of squared perpendicular distances). It‚Äôs the line that best fits the data in a least-squares sense.\n\n\n\n\nThe first principal component direction, with distances to the observations shown as dashed lines.\n\n\n\nFigure 6.15: The first principal component direction, with distances to the observations shown as dashed lines.\nLeft: Shows the perpendicular distances from each point to the first principal component line.\nRight: Rotates the plot so that the first principal component is horizontal."
  },
  {
    "objectID": "qmd/islp6.html#pca-another-interpretation-cont.",
    "href": "qmd/islp6.html#pca-another-interpretation-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "PCA: Another Interpretation (Cont.)",
    "text": "PCA: Another Interpretation (Cont.)\n\n\n\nThe first principal component direction, with distances to the observations shown as dashed lines.\n\n\n\nThe x-coordinate of each point in this rotated plot is its principal component score."
  },
  {
    "objectID": "qmd/islp6.html#pca-capturing-information",
    "href": "qmd/islp6.html#pca-capturing-information",
    "title": "Linear Model Selection and Regularization",
    "section": "PCA: Capturing Information",
    "text": "PCA: Capturing Information\n\n\n\nPlots of the first principal component scores versus pop and ad.\n\n\n\nFigure 6.16: Shows the first principal component scores (zi1) plotted against pop and ad."
  },
  {
    "objectID": "qmd/islp6.html#pca-capturing-information-cont.",
    "href": "qmd/islp6.html#pca-capturing-information-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "PCA: Capturing Information (Cont.)",
    "text": "PCA: Capturing Information (Cont.)\n\n\n\nPlots of the first principal component scores versus pop and ad.\n\n\n\nStrong Relationship: There‚Äôs a strong relationship, indicating that the first principal component captures much of the information in the original two variables. The first principal component score is highly correlated with both population size and ad spending."
  },
  {
    "objectID": "qmd/islp6.html#pca-multiple-principal-components",
    "href": "qmd/islp6.html#pca-multiple-principal-components",
    "title": "Linear Model Selection and Regularization",
    "section": "PCA: Multiple Principal Components",
    "text": "PCA: Multiple Principal Components\n\nMore than One: You can construct up to p distinct principal components.\nSecond Principal Component: The second principal component (Z2) is:\n\nA linear combination of the variables.\nUncorrelated with Z1. It captures variation in a direction independent of Z1.\nHas the largest variance among all linear combinations uncorrelated with Z1.\nOrthogonal (perpendicular) to the first principal component.\n\nSuccessive Components: Each subsequent principal component captures the maximum remaining variance, subject to being uncorrelated with the previous components. Each component captures a different ‚Äúdirection‚Äù of variation in the data."
  },
  {
    "objectID": "qmd/islp6.html#pca-second-principal-component",
    "href": "qmd/islp6.html#pca-second-principal-component",
    "title": "Linear Model Selection and Regularization",
    "section": "PCA: Second Principal Component",
    "text": "PCA: Second Principal Component\n\n\n\nPlots of the second principal component scores versus pop and ad.\n\n\n\nFigure 6.17: Shows the second principal component scores (zi2) plotted against pop and ad."
  },
  {
    "objectID": "qmd/islp6.html#pca-second-principal-component-cont.",
    "href": "qmd/islp6.html#pca-second-principal-component-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "PCA: Second Principal Component (Cont.)",
    "text": "PCA: Second Principal Component (Cont.)\n\n\n\nPlots of the second principal component scores versus pop and ad.\n\n\n\nWeak Relationship: There‚Äôs very little relationship, indicating that the second principal component captures much less information than the first.\n\n\nThe Principal Components Regression Approach\n\nThe Idea: Use the first M principal components (Z1, ‚Ä¶, ZM) as predictors in a linear regression model. This is PCR.\nAssumption: We assume that the directions in which X1, ‚Ä¶, Xp show the most variation are the directions that are associated with Y. This is the key assumption of PCR. If it holds, PCR can be effective.\nPotential for Improvement: If this assumption holds, PCR can outperform least squares, especially when M &lt;&lt; p, by reducing variance."
  },
  {
    "objectID": "qmd/islp6.html#pcr-example",
    "href": "qmd/islp6.html#pcr-example",
    "title": "Linear Model Selection and Regularization",
    "section": "PCR: Example",
    "text": "PCR: Example\n\n\n\nPCR applied to two simulated datasets."
  },
  {
    "objectID": "qmd/islp6.html#pcr-example-cont.",
    "href": "qmd/islp6.html#pcr-example-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "PCR: Example (Cont.)",
    "text": "PCR: Example (Cont.)\n\nFigure 6.18: Shows PCR applied to the simulated datasets from Figures 6.8 and 6.9.\nPCR with an appropriate choice of M can improve substantially over least squares.\nHowever, in this example, PCR does not perform as well as ridge regression or the lasso. This is because the data were generated in a way that required many principal components to model the response well."
  },
  {
    "objectID": "qmd/islp6.html#pcr-when-it-works-well",
    "href": "qmd/islp6.html#pcr-when-it-works-well",
    "title": "Linear Model Selection and Regularization",
    "section": "PCR: When it Works Well",
    "text": "PCR: When it Works Well\n\nFirst Few Components are Key: PCR tends to work well when the first few principal components capture most of the variation in the predictors and that variation is related to the response."
  },
  {
    "objectID": "qmd/islp6.html#pcr-when-it-works-well-cont.",
    "href": "qmd/islp6.html#pcr-when-it-works-well-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "PCR: When it Works Well (Cont.)",
    "text": "PCR: When it Works Well (Cont.)\n\nFigure 6.19: Shows an example where the response depends only on the first five principal components.\nPCR performs very well, achieving a low MSE with M = 5.\nPCR and ridge regression slightly outperform the lasso in this case."
  },
  {
    "objectID": "qmd/islp6.html#pcr-not-feature-selection",
    "href": "qmd/islp6.html#pcr-not-feature-selection",
    "title": "Linear Model Selection and Regularization",
    "section": "PCR: Not Feature Selection",
    "text": "PCR: Not Feature Selection\n\nLinear Combinations: PCR is not a feature selection method. Each principal component is a linear combination of all p original features. It doesn‚Äôt exclude any of the original variables.\nExample: In the advertising data, Z1 was a combination of both pop and ad.\nRelationship to Ridge Regression: PCR is more closely related to ridge regression than to the lasso. Both involve using all the original predictors, albeit in a transformed way."
  },
  {
    "objectID": "qmd/islp6.html#pcr-choosing-m-and-standardization",
    "href": "qmd/islp6.html#pcr-choosing-m-and-standardization",
    "title": "Linear Model Selection and Regularization",
    "section": "PCR: Choosing M and Standardization",
    "text": "PCR: Choosing M and Standardization\n\nChoosing M: The number of principal components (M) is typically chosen by cross-validation.\n\n\n\n\nPCR standardized coefficient estimates and cross-validation MSE on the Credit data."
  },
  {
    "objectID": "qmd/islp6.html#pcr-choosing-m-and-standardization-cont.",
    "href": "qmd/islp6.html#pcr-choosing-m-and-standardization-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "PCR: Choosing M and Standardization (Cont.)",
    "text": "PCR: Choosing M and Standardization (Cont.)\n\nFigure 6.20: Shows cross-validation for PCR on the Credit data.\nThe lowest cross-validation error occurs with M = 10, which is almost no dimension reduction.\nStandardization: It‚Äôs generally recommended to standardize each predictor before performing PCA (and thus PCR). This ensures that all variables are on the same scale. Otherwise, variables with larger variances will dominate the principal components.\n\n\n3.2 Partial Least Squares (PLS)\n\nSupervised Dimension Reduction: PLS is a supervised dimension reduction technique. Unlike PCR (which is unsupervised), PLS uses the response (Y) to help identify the new features (Z1, ‚Ä¶, ZM). It takes the response into account when creating the linear combinations.\nGoal: Find directions that explain both the response and the predictors. It tries to find components that are relevant to predicting the response."
  },
  {
    "objectID": "qmd/islp6.html#pls-computing-the-first-direction",
    "href": "qmd/islp6.html#pls-computing-the-first-direction",
    "title": "Linear Model Selection and Regularization",
    "section": "PLS: Computing the First Direction",
    "text": "PLS: Computing the First Direction\n\nStandardize Predictors: Standardize the p predictors (subtract the mean and divide by the standard deviation).\nSimple Linear Regressions: Compute the coefficient from the simple linear regression of Y onto each Xj (separately for each predictor). This measures the individual relationship between each predictor and the response.\nFirst PLS Direction: Set each œÜj1 in the equation for Z1 (Equation 6.16) equal to this coefficient. This means PLS places the highest weight on variables that are most strongly related to the response (based on the simple linear regressions)."
  },
  {
    "objectID": "qmd/islp6.html#pls-example",
    "href": "qmd/islp6.html#pls-example",
    "title": "Linear Model Selection and Regularization",
    "section": "PLS: Example",
    "text": "PLS: Example\n\n\n\nFirst PLS direction (solid line) and first PCR direction (dotted line) for the advertising data.\n\n\n\nFigure 6.21: Shows the first PLS direction (solid line) and first PCR direction (dotted line) for the advertising data, with Sales as the response and Population Size and Advertising Spending as predictors."
  },
  {
    "objectID": "qmd/islp6.html#pls-example-cont.",
    "href": "qmd/islp6.html#pls-example-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "PLS: Example (Cont.)",
    "text": "PLS: Example (Cont.)\n\n\n\nFirst PLS direction (solid line) and first PCR direction (dotted line) for the advertising data.\n\n\n\nPLS chooses a direction that emphasizes Population Size more than Advertising Spending, because Population Size is more correlated with Sales (based on the simple linear regressions)."
  },
  {
    "objectID": "qmd/islp6.html#pls-subsequent-directions",
    "href": "qmd/islp6.html#pls-subsequent-directions",
    "title": "Linear Model Selection and Regularization",
    "section": "PLS: Subsequent Directions",
    "text": "PLS: Subsequent Directions\n\nIterative Process:\n\nAdjust for Z1: Regress each variable (both the predictors and the response) on Z1 and take the residuals. This removes the information already explained by Z1. We ‚Äúorthogonalize‚Äù the data with respect to Z1.\nCompute Z2: Compute Z2 using these orthogonalized data, in the same way as Z1 was computed (using simple linear regressions of the residual response on the residual predictors).\nRepeat: Repeat this process M times to identify multiple PLS components (Z1, ‚Ä¶, ZM).\n\nFinal Model: Fit a linear model using Z1, ‚Ä¶, ZM as predictors, just like in PCR."
  },
  {
    "objectID": "qmd/islp6.html#pls-tuning-parameter-and-standardization",
    "href": "qmd/islp6.html#pls-tuning-parameter-and-standardization",
    "title": "Linear Model Selection and Regularization",
    "section": "PLS: Tuning Parameter and Standardization",
    "text": "PLS: Tuning Parameter and Standardization\n\nChoosing M: The number of PLS directions (M) is a tuning parameter, typically chosen by cross-validation.\nStandardization: It‚Äôs generally recommended to standardize both the predictors and the response before performing PLS.\nPerformance: In practice, PLS often performs no better than ridge regression or PCR. The supervised dimension reduction of PLS can reduce bias, but it also has the potential to increase variance."
  },
  {
    "objectID": "qmd/islp6.html#considerations-in-high-dimensions",
    "href": "qmd/islp6.html#considerations-in-high-dimensions",
    "title": "Linear Model Selection and Regularization",
    "section": "4. Considerations in High Dimensions",
    "text": "4. Considerations in High Dimensions\n\n4.1 High-Dimensional Data\n\nLow-Dimensional Setting: Most traditional statistical techniques are designed for the low-dimensional setting, where n (number of observations) is much greater than p (number of features).\nHigh-Dimensional Setting: In recent years, new technologies have led to a dramatic increase in the number of features that can be measured. We often encounter datasets where p is large, possibly even larger than n. This is the ‚Äúhigh-dimensional‚Äù setting."
  },
  {
    "objectID": "qmd/islp6.html#high-dimensional-data-examples",
    "href": "qmd/islp6.html#high-dimensional-data-examples",
    "title": "Linear Model Selection and Regularization",
    "section": "High-Dimensional Data: Examples",
    "text": "High-Dimensional Data: Examples\n\nExamples:\n\nGenomics: Measuring hundreds of thousands of single nucleotide polymorphisms (SNPs) to predict a trait (e.g., disease risk).\nMarketing: Using all search terms entered by users of a search engine to understand online shopping patterns.\nImage Analysis: Analyzing thousands of pixels in an image to classify objects.\n\n\n\n4.2 What Goes Wrong in High Dimensions?\n\nLeast Squares Fails: When p is as large as or larger than n, least squares cannot be used (or should not be used). It breaks down.\nPerfect Fit, But Useless: Least squares will always find a set of coefficients that perfectly fit the training data (zero residuals), regardless of whether there‚Äôs a true relationship between the features and the response. This is because there are more parameters than observations, allowing the model to perfectly interpolate the data.\nOverfitting: This perfect fit is a result of overfitting. The model is too flexible and captures noise in the data, leading to terrible performance on new data. It doesn‚Äôt generalize.\nCurse of Dimensionality: Adding more features, even if unrelated to response, can easily lead to the model overfitting the training data."
  },
  {
    "objectID": "qmd/islp6.html#regression-in-high-dimensions",
    "href": "qmd/islp6.html#regression-in-high-dimensions",
    "title": "Linear Model Selection and Regularization",
    "section": "4.3 Regression in High Dimensions",
    "text": "4.3 Regression in High Dimensions\n\nLess Flexible Methods: Many of the methods we‚Äôve discussed in this chapter ‚Äì forward stepwise selection, ridge regression, the lasso, and PCR ‚Äì are particularly useful in the high-dimensional setting.\nAvoiding Overfitting: These methods avoid overfitting by being less flexible than least squares. They constrain the model in some way, preventing it from capturing noise."
  },
  {
    "objectID": "qmd/islp6.html#regression-in-high-dimensions-example-with-the-lasso",
    "href": "qmd/islp6.html#regression-in-high-dimensions-example-with-the-lasso",
    "title": "Linear Model Selection and Regularization",
    "section": "Regression in High Dimensions: Example with the Lasso",
    "text": "Regression in High Dimensions: Example with the Lasso\n\n\n\nThe lasso performed with varying numbers of features (p) and a fixed sample size (n)."
  },
  {
    "objectID": "qmd/islp6.html#regression-in-high-dimensions-example-with-the-lasso-cont.",
    "href": "qmd/islp6.html#regression-in-high-dimensions-example-with-the-lasso-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "Regression in High Dimensions: Example with the Lasso (Cont.)",
    "text": "Regression in High Dimensions: Example with the Lasso (Cont.)\n\nFigure 6.24: Lasso with n=100, p can be 20, 50 and 2000.\nAs the number of features increases, the test set error increases, highlights the curse of dimensionality.\nThree Key Points:\n\nRegularization is Crucial: Regularization (or shrinkage) is essential in high-dimensional problems. It‚Äôs necessary to constrain the model.\nTuning Parameter Selection: Choosing the right tuning parameter (e.g., Œª for the lasso) is critical for good performance. Cross-validation is essential for this.\nCurse of Dimensionality: The test error tends to increase as the dimensionality of the problem increases, unless the additional features are truly associated with the response. Adding irrelevant variables makes the problem harder.\n\n\n\n4.4 Interpreting Results in High Dimensions\n\nMulticollinearity is Extreme: In the high-dimensional setting, multicollinearity is extreme. Any variable can be written as a linear combination of all the other variables. This makes it impossible to isolate the effect of individual predictors.\nCannot Identify True Predictors: This means we can never know exactly which variables (if any) are truly predictive of the outcome. We can only identify variables that are correlated with the true predictors."
  },
  {
    "objectID": "qmd/islp6.html#interpreting-results-in-high-dimensions-cont.",
    "href": "qmd/islp6.html#interpreting-results-in-high-dimensions-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "Interpreting Results in High Dimensions (Cont.)",
    "text": "Interpreting Results in High Dimensions (Cont.)\n\nCaution in Reporting: Be very cautious when reporting results. Don‚Äôt overstate conclusions. Avoid claiming to have found the ‚Äútrue‚Äù predictors.\nNever Use Training Data for Evaluation: Never use training data measures (sum of squared errors, p-values, R2) as evidence of a good model fit in the high-dimensional setting. These measures will be misleadingly optimistic. They will always look good, even if the model is terrible.\nUse Test Data or Cross-Validation: Always report results on an independent test set or using cross-validation. These are the only reliable ways to assess model performance in high dimensions."
  },
  {
    "objectID": "qmd/islp6.html#summary",
    "href": "qmd/islp6.html#summary",
    "title": "Linear Model Selection and Regularization",
    "section": "Summary",
    "text": "Summary\n\nBeyond Least Squares: We‚Äôve explored several alternatives to least squares for linear regression:\n\nSubset selection (best subset, forward stepwise, backward stepwise) - Choose a subset of the predictors.\nShrinkage methods (ridge regression, the lasso) - Shrink the coefficients towards zero.\nDimension reduction methods (PCR, PLS) - Transform the predictors into a smaller set of linear combinations."
  },
  {
    "objectID": "qmd/islp6.html#summary-cont.",
    "href": "qmd/islp6.html#summary-cont.",
    "title": "Linear Model Selection and Regularization",
    "section": "Summary (Cont.)",
    "text": "Summary (Cont.)\n\nGoals: These methods aim to improve:\n\nPrediction accuracy (by reducing variance, often at the cost of a small increase in bias).\nModel interpretability (by selecting a subset of variables or shrinking coefficients).\n\nHigh-Dimensional Data: These methods are particularly important in the high-dimensional setting (p ‚â• n), where least squares fails. They provide ways to fit models even when the number of predictors is large.\nCross-Validation: Cross-validation is a powerful tool for selecting tuning parameters and estimating the test error of different models. It‚Äôs essential for reliable model selection.\nThe choice of modeling method, the choice of tuning parameter, and the choice of assessment metrics all become especially important in high dimensions."
  },
  {
    "objectID": "qmd/islp6.html#thoughts-and-discussion",
    "href": "qmd/islp6.html#thoughts-and-discussion",
    "title": "Linear Model Selection and Regularization",
    "section": "Thoughts and Discussion",
    "text": "Thoughts and Discussion\n\nThink about situations where you might encounter high-dimensional data. What are the challenges and opportunities?\n\nChallenges: Overfitting, difficulty in interpretation, computational cost.\nOpportunities: Ability to model complex relationships, potential for improved prediction accuracy if the true signal is strong.\nExamples: Genomics, text analysis, image processing, financial modeling.\n\nHow would you choose between the different methods we‚Äôve discussed (subset selection, ridge regression, the lasso, PCR, PLS)? What factors would you consider?\n\nInterpretability: If interpretability is crucial, the lasso (or subset selection) is preferred.\nPrediction Accuracy: If prediction accuracy is the primary goal, try all methods and use cross-validation to compare.\nComputational Cost: For very large p, stepwise selection, ridge regression, or the lasso may be more computationally feasible than best subset selection.\nUnderlying Relationship: Consider whether you expect only a few predictors to be important (lasso) or many (ridge regression).\n\nWhat are the ethical implications of using high-dimensional data for prediction, especially in sensitive areas like healthcare or finance? How can we mitigate potential biases and ensure fairness?\n\nHigh dimensional data may contain sensitive attributes or proxies for them.\nModels trained on biased data can perpetuate or amplify existing biases.\nMitigation Strategies: Careful data collection, bias detection and mitigation techniques, transparency in modeling, and ongoing monitoring of model performance."
  },
  {
    "objectID": "qmd/islp7.html",
    "href": "qmd/islp7.html",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "",
    "text": "Let‚Äôs start with the basics! What are data mining, machine learning, and statistical learning?\nData Mining: The process of discovering patterns, anomalies, and knowledge from large datasets. Think of it as ‚Äúmining‚Äù for valuable insights in a mountain of data. ‚õèÔ∏è We‚Äôre looking for hidden treasures!\nMachine Learning: A subset of artificial intelligence (AI). It‚Äôs about enabling systems to learn from data without being explicitly programmed. Algorithms learn patterns and make predictions, like teaching a computer to learn by example. ü§ñ\nStatistical Learning: A framework of tools for understanding data. It‚Äôs closely related to both data mining and machine learning, but with a stronger emphasis on statistical models and inference. ü§î\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese fields are highly interdisciplinary!"
  },
  {
    "objectID": "qmd/islp7.html#introduction-to-data-mining-machine-learning-and-statistical-learning",
    "href": "qmd/islp7.html#introduction-to-data-mining-machine-learning-and-statistical-learning",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "",
    "text": "Let‚Äôs start with the basics! What are data mining, machine learning, and statistical learning?\nData Mining: The process of discovering patterns, anomalies, and knowledge from large datasets. Think of it as ‚Äúmining‚Äù for valuable insights in a mountain of data. ‚õèÔ∏è We‚Äôre looking for hidden treasures!\nMachine Learning: A subset of artificial intelligence (AI). It‚Äôs about enabling systems to learn from data without being explicitly programmed. Algorithms learn patterns and make predictions, like teaching a computer to learn by example. ü§ñ\nStatistical Learning: A framework of tools for understanding data. It‚Äôs closely related to both data mining and machine learning, but with a stronger emphasis on statistical models and inference. ü§î\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese fields are highly interdisciplinary!"
  },
  {
    "objectID": "qmd/islp7.html#relationship-data-mining-machine-learning-and-statistical-learning",
    "href": "qmd/islp7.html#relationship-data-mining-machine-learning-and-statistical-learning",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Relationship: Data Mining, Machine Learning, and Statistical Learning",
    "text": "Relationship: Data Mining, Machine Learning, and Statistical Learning\n\n\n\n\n\ngraph LR\n    A[Data Mining] --&gt; C(Common Ground)\n    B[Machine Learning] --&gt; C\n    D[Statistical Learning] --&gt; C\n    C --&gt; E[Insights & Predictions]\n\n\n\n\n\n\n\nCommon Goal: All three aim to extract insights and make predictions from data. They‚Äôre different paths to the same destination! üó∫Ô∏è\nData Mining: Often emphasizes discovering previously unknown patterns. It‚Äôs like exploratory detective work. üïµÔ∏è‚Äç‚ôÄÔ∏è\nMachine Learning: Focuses on prediction accuracy. It‚Äôs like building a super-powered prediction machine. ‚öôÔ∏è\nStatistical Learning: Emphasizes model interpretability and quantifying uncertainty. It‚Äôs like building a model and understanding how confident we are in its predictions. üìä"
  },
  {
    "objectID": "qmd/islp7.html#why-go-beyond-linearity",
    "href": "qmd/islp7.html#why-go-beyond-linearity",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Why Go Beyond Linearity?",
    "text": "Why Go Beyond Linearity?\nLinear models (like linear regression) are great! They‚Äôre simple, interpretable, and a good starting point. But‚Ä¶ they have limitations.\n\n\n\n\n\n\nLimitations of Linear Models:\n\nLinearity Assumption: They assume a straight-line relationship between predictors and the response. This is often too simplistic for the real world.\nLimited Predictive Power: If the true relationship is not linear, linear models will give poor predictions.\nIndependence Assumption: The effect of the change on one predictor is irrelavant to other predictors.\n\n\n\n\n\n\n\n\n\nAnalogy\n\n\n\nImagine trying to fit a straight line through a curved set of points. You‚Äôll miss the real pattern!"
  },
  {
    "objectID": "qmd/islp7.html#linear-models-vs.-reality-example",
    "href": "qmd/islp7.html#linear-models-vs.-reality-example",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Linear Models vs.¬†Reality (Example)",
    "text": "Linear Models vs.¬†Reality (Example)\n\n\n\n\n\n\nThe figure shows simulated data. - The red line represents the true (but unknown) relationship between the predictor and the response. - The blue line is the linear model we fitted to the data.\nKey Takeaway: Linear models might not capture the full complexity of the relationship."
  },
  {
    "objectID": "qmd/islp7.html#examples-of-linear-models",
    "href": "qmd/islp7.html#examples-of-linear-models",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Examples of Linear Models",
    "text": "Examples of Linear Models\nHere are some familiar examples of linear models:\n\nLinear Regression: The foundation! Predicts a continuous response.\nRidge Regression: Adds a penalty to reduce model complexity and prevent overfitting. It shrinks the coefficients towards zero.\nLasso: Similar to ridge, but uses a different penalty that can perform feature selection (setting some coefficients to zero).\nPCR (Principal Components Regression): Reduces dimensionality using PCA before applying linear regression."
  },
  {
    "objectID": "qmd/islp7.html#introduction-to-non-linear-approaches",
    "href": "qmd/islp7.html#introduction-to-non-linear-approaches",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Introduction to Non-Linear Approaches",
    "text": "Introduction to Non-Linear Approaches\nThis chapter is all about relaxing the linearity assumption! We‚Äôll explore techniques that can capture curved relationships.\nGoal: Find models that are both flexible (can fit complex patterns) and interpretable (we can understand how they work)."
  },
  {
    "objectID": "qmd/islp7.html#non-linear-approaches-overview",
    "href": "qmd/islp7.html#non-linear-approaches-overview",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Non-Linear Approaches: Overview",
    "text": "Non-Linear Approaches: Overview\nHere‚Äôs a roadmap of what we‚Äôll cover:\n\nPolynomial Regression: Add polynomial terms (e.g., \\(x^2\\), \\(x^3\\)) to capture curves.\nStep Functions: Divide the predictor‚Äôs range into bins and fit a constant in each. Like creating categories.\nRegression Splines: More flexible! Piecewise polynomials with smoothness constraints.\nSmoothing Splines: Minimize RSS plus a penalty for roughness. Balance fit and smoothness.\nLocal Regression: Fit a model locally using only nearby data.\nGeneralized Additive Models (GAMs): Extend these ideas to multiple predictors."
  },
  {
    "objectID": "qmd/islp7.html#polynomial-regression",
    "href": "qmd/islp7.html#polynomial-regression",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\nInstead of a straight line:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\]\nWe use a polynomial:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + ... + \\beta_d x_i^d + \\epsilon_i\n\\]\nThis allows us to model curves! üìà"
  },
  {
    "objectID": "qmd/islp7.html#polynomial-regression-important-points",
    "href": "qmd/islp7.html#polynomial-regression-important-points",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Polynomial Regression: Important Points",
    "text": "Polynomial Regression: Important Points\n\nLinear in Coefficients: The equation is linear in the coefficients (\\(\\beta_0, \\beta_1, ...\\)). This means we can still use least squares! üëç\nFocus on the Fitted Function: We usually don‚Äôt care about the individual coefficients. We look at the overall shape of the fitted function.\nDegree (d): The highest power (\\(d\\)) is the degree. We rarely use \\(d &gt; 3\\) or \\(4\\) because high degrees can lead to overly flexible and strange curves."
  },
  {
    "objectID": "qmd/islp7.html#polynomial-regression-example-wage-data",
    "href": "qmd/islp7.html#polynomial-regression-example-wage-data",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Polynomial Regression: Example (Wage Data)",
    "text": "Polynomial Regression: Example (Wage Data)\nWe‚Äôll use the Wage dataset (from the ISLR book) to predict wage based on age. Let‚Äôs see how a degree-4 polynomial fits the data."
  },
  {
    "objectID": "qmd/islp7.html#visualizing-the-polynomial-fit-wage-data",
    "href": "qmd/islp7.html#visualizing-the-polynomial-fit-wage-data",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Visualizing the Polynomial Fit (Wage Data)",
    "text": "Visualizing the Polynomial Fit (Wage Data)\n\n\nBlue Curve: The degree-4 polynomial fit. It captures the non-linear trend!\nDashed Curves: 95% confidence interval. This shows the uncertainty in our fit."
  },
  {
    "objectID": "qmd/islp7.html#understanding-the-confidence-interval",
    "href": "qmd/islp7.html#understanding-the-confidence-interval",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Understanding the Confidence Interval",
    "text": "Understanding the Confidence Interval\nThe confidence interval (dashed lines) tells us how much our fitted curve might vary. It‚Äôs calculated like this:\n\nFitted Value: For a specific age (\\(x_0\\)), we get the fitted value: \\(\\hat{f}(x_0)\\).\nVariance: We estimate the variance of the fit at that point: \\(\\text{Var}[\\hat{f}(x_0)]\\).\nStandard Error: The pointwise standard error is: \\(\\sqrt{\\text{Var}[\\hat{f}(x_0)]}\\).\nConfidence Interval: The 95% confidence interval is: \\(\\hat{f}(x_0) \\pm 2 \\cdot \\text{SE}[\\hat{f}(x_0)]\\).\n\n\n\n\n\n\n\nInterpretation\n\n\n\nWe‚Äôre 95% confident that the true relationship lies within the dashed curves."
  },
  {
    "objectID": "qmd/islp7.html#polynomial-logistic-regression",
    "href": "qmd/islp7.html#polynomial-logistic-regression",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Polynomial Logistic Regression",
    "text": "Polynomial Logistic Regression\nWe can also use polynomial terms in logistic regression to model a binary outcome (e.g., yes/no, 0/1).\nExample: Model the probability that wage &gt; 250, given age."
  },
  {
    "objectID": "qmd/islp7.html#polynomial-logistic-regression-equation",
    "href": "qmd/islp7.html#polynomial-logistic-regression-equation",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Polynomial Logistic Regression: Equation",
    "text": "Polynomial Logistic Regression: Equation\nThe equation looks like this:\n\\[\n\\text{Pr}(y_i &gt; 250 | x_i) = \\frac{\\exp(\\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + ... + \\beta_d x_i^d)}{1 + \\exp(\\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + ... + \\beta_d x_i^d)}\n\\]\nThis models the probability as a non-linear function of the predictor."
  },
  {
    "objectID": "qmd/islp7.html#visualizing-polynomial-logistic-regression-wage-data",
    "href": "qmd/islp7.html#visualizing-polynomial-logistic-regression-wage-data",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Visualizing Polynomial Logistic Regression (Wage Data)",
    "text": "Visualizing Polynomial Logistic Regression (Wage Data)\n\n\n\nNotice the wider confidence intervals for older ages. This is because there are fewer high earners in the dataset.\n\nKey Point: The confidence intervals are wider for older ages. This means we‚Äôre less certain about our predictions in that range, because we have less data."
  },
  {
    "objectID": "qmd/islp7.html#step-functions",
    "href": "qmd/islp7.html#step-functions",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Step Functions",
    "text": "Step Functions\n\nGlobal vs.¬†Local: Polynomial regression imposes a global structure (the same polynomial applies everywhere). Step functions are local.\nHow They Work:\n\nBins: Divide the range of the predictor (\\(X\\)) into bins using cutpoints (\\(c_1, c_2, ..., c_K\\)).\nConstant Fit: Fit a constant within each bin. The predicted value is the same for all values of \\(X\\) within a bin."
  },
  {
    "objectID": "qmd/islp7.html#step-functions-creating-indicator-variables",
    "href": "qmd/islp7.html#step-functions-creating-indicator-variables",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Step Functions: Creating Indicator Variables",
    "text": "Step Functions: Creating Indicator Variables\nTo do this, we create indicator variables:\n\\[\\begin{aligned}\nC_0(X) &= I(X &lt; c_1), \\\\\nC_1(X) &= I(c_1 \\le X &lt; c_2), \\\\\nC_2(X) &= I(c_2 \\le X &lt; c_3), \\\\\n&\\vdots \\\\\nC_{K-1}(X) &= I(c_{K-1} \\le X &lt; c_K), \\\\\nC_K(X) &= I(c_K \\le X),\n\\end{aligned}\\]\nwhere \\(I(\\cdot)\\) is an indicator function. It‚Äôs 1 if the condition is true, and 0 otherwise."
  },
  {
    "objectID": "qmd/islp7.html#step-functions-regression-equation",
    "href": "qmd/islp7.html#step-functions-regression-equation",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Step Functions: Regression Equation",
    "text": "Step Functions: Regression Equation\nWe then use least squares with \\(C_1(X), C_2(X), ..., C_K(X)\\) as predictors:\n\\[y_i = \\beta_0 + \\beta_1 C_1(x_i) + \\beta_2 C_2(x_i) + \\dots + \\beta_K C_K(x_i) + \\epsilon_i\\]\n\nMulticollinearity: We exclude \\(C_0(X)\\) to avoid perfect multicollinearity (the indicator functions sum to 1).\nInterpretation:\n\n\\(\\beta_0\\): Mean value of \\(Y\\) for \\(X &lt; c_1\\) (the baseline group).\n\\(\\beta_j\\): Average increase in the response for \\(X\\) in bin \\(j\\), relative to the baseline."
  },
  {
    "objectID": "qmd/islp7.html#step-functions-converting-continuous-to-categorical",
    "href": "qmd/islp7.html#step-functions-converting-continuous-to-categorical",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Step Functions: Converting Continuous to Categorical",
    "text": "Step Functions: Converting Continuous to Categorical\n\nOrdered Categorical Variable: We‚Äôve turned a continuous variable into an ordered categorical variable. The order of the categories matters!"
  },
  {
    "objectID": "qmd/islp7.html#step-functions-example-wage-data",
    "href": "qmd/islp7.html#step-functions-example-wage-data",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Step Functions: Example (Wage Data)",
    "text": "Step Functions: Example (Wage Data)\nLet‚Äôs fit step functions to the Wage data.\n\nLeft Panel: Piecewise constant fit to wage.\nRight Panel: Fitted probabilities from logistic regression for wage &gt; 250 (also using step functions)."
  },
  {
    "objectID": "qmd/islp7.html#visualizing-step-functions-wage-data",
    "href": "qmd/islp7.html#visualizing-step-functions-wage-data",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Visualizing Step Functions (Wage Data)",
    "text": "Visualizing Step Functions (Wage Data)\n\n\n\nStep functions can miss trends. They‚Äôre popular in biostatistics (e.g., using 5-year age groups).\n\nLimitation: Step functions can miss overall trends (like the initial increase of wage with age).\nUse Cases: Common in biostatistics and epidemiology (e.g., grouping ages into 5-year intervals)."
  },
  {
    "objectID": "qmd/islp7.html#basis-functions-a-general-framework",
    "href": "qmd/islp7.html#basis-functions-a-general-framework",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Basis Functions: A General Framework",
    "text": "Basis Functions: A General Framework\n\nGeneralization: Polynomial and piecewise-constant regression are special cases of the basis function approach.\nHow it Works:\n\nChoose Basis Functions: Select transformations to apply to \\(X\\): \\(b_1(X), b_2(X), ..., b_K(X)\\).\nLinear Model: Fit a linear model using these basis functions:\n\\[y_i = \\beta_0 + \\beta_1 b_1(x_i) + \\beta_2 b_2(x_i) + ... + \\beta_K b_K(x_i) + \\epsilon_i\\]"
  },
  {
    "objectID": "qmd/islp7.html#basis-functions-important-points",
    "href": "qmd/islp7.html#basis-functions-important-points",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Basis Functions: Important Points",
    "text": "Basis Functions: Important Points\n\nFixed and Known: The basis functions are fixed (we choose them).\nStill a Linear Model!: This is still a linear model in the coefficients! We can use least squares.\nInference Tools: All the usual inference tools (hypothesis tests, confidence intervals) still apply! This is a huge advantage."
  },
  {
    "objectID": "qmd/islp7.html#regression-splines-combining-polynomials-and-piecewise-constants",
    "href": "qmd/islp7.html#regression-splines-combining-polynomials-and-piecewise-constants",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Regression Splines: Combining Polynomials and Piecewise-Constants",
    "text": "Regression Splines: Combining Polynomials and Piecewise-Constants\n\nMotivation: Regression splines combine the best of both worlds! They‚Äôre more flexible than either approach alone.\nPiecewise Polynomials: Fit separate low-degree polynomials in different regions of \\(X\\), instead of one high-degree polynomial everywhere."
  },
  {
    "objectID": "qmd/islp7.html#regression-splines-piecewise-polynomials-details",
    "href": "qmd/islp7.html#regression-splines-piecewise-polynomials-details",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Regression Splines: Piecewise Polynomials (Details)",
    "text": "Regression Splines: Piecewise Polynomials (Details)\n\nExample: Piecewise Cubic:\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\epsilon_i\\]\nbut the coefficients (\\(\\beta_0, \\beta_1, \\beta_2, \\beta_3\\)) change in different regions.\nKnots: The points where the coefficients change are called knots."
  },
  {
    "objectID": "qmd/islp7.html#piecewise-polynomials-example-one-knot",
    "href": "qmd/islp7.html#piecewise-polynomials-example-one-knot",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Piecewise Polynomials: Example (One Knot)",
    "text": "Piecewise Polynomials: Example (One Knot)\nConsider a piecewise cubic with one knot at \\(c\\):\n\\[y_i = \\begin{cases}\n\\beta_{01} + \\beta_{11}x_i + \\beta_{21}x_i^2 + \\beta_{31}x_i^3 + \\epsilon_i & \\text{if } x_i &lt; c \\\\\n\\beta_{02} + \\beta_{12}x_i + \\beta_{22}x_i^2 + \\beta_{32}x_i^3 + \\epsilon_i & \\text{if } x_i \\ge c\n\\end{cases}\\]\nTwo different cubic polynomials!"
  },
  {
    "objectID": "qmd/islp7.html#unconstrained-piecewise-cubic-polynomial",
    "href": "qmd/islp7.html#unconstrained-piecewise-cubic-polynomial",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Unconstrained Piecewise Cubic Polynomial",
    "text": "Unconstrained Piecewise Cubic Polynomial\n\n\n\nProblem: It‚Äôs discontinuous! It looks terrible! (8 degrees of freedom)\n\nProblem: This fit is discontinuous at the knot. This is usually undesirable.\nDegrees of Freedom: 8 (2 sets of 4 parameters)."
  },
  {
    "objectID": "qmd/islp7.html#constraints-and-splines-making-it-smooth",
    "href": "qmd/islp7.html#constraints-and-splines-making-it-smooth",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Constraints and Splines: Making it Smooth",
    "text": "Constraints and Splines: Making it Smooth\nTo make it better, we add constraints:\n\n\nContinuity: The function must be continuous at the knot (the pieces must meet).\nContinuous 1st Derivative: The first derivative must be continuous (no sharp turns).\nContinuous 2nd Derivative: The second derivative can also be continuous (even smoother)."
  },
  {
    "objectID": "qmd/islp7.html#visualizing-constraints-unconstrained",
    "href": "qmd/islp7.html#visualizing-constraints-unconstrained",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Visualizing Constraints: Unconstrained",
    "text": "Visualizing Constraints: Unconstrained\nLet‚Äôs see the effect of adding constraints. First, the unconstrained version:"
  },
  {
    "objectID": "qmd/islp7.html#visualizing-constraints-continuous",
    "href": "qmd/islp7.html#visualizing-constraints-continuous",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Visualizing Constraints: Continuous",
    "text": "Visualizing Constraints: Continuous\nNow, with the continuity constraint:"
  },
  {
    "objectID": "qmd/islp7.html#visualizing-constraints-cubic-spline",
    "href": "qmd/islp7.html#visualizing-constraints-cubic-spline",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Visualizing Constraints: Cubic Spline",
    "text": "Visualizing Constraints: Cubic Spline\nFinally, the cubic spline (continuity, continuous 1st and 2nd derivatives):"
  },
  {
    "objectID": "qmd/islp7.html#constraints-and-degrees-of-freedom",
    "href": "qmd/islp7.html#constraints-and-degrees-of-freedom",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Constraints and Degrees of Freedom",
    "text": "Constraints and Degrees of Freedom\n\nReducing Degrees of Freedom: Each constraint reduces the degrees of freedom. We have fewer parameters to estimate.\nCubic Spline: A cubic spline with K knots has 4 + K degrees of freedom.\n\n4: From the base cubic polynomial.\nK: One for each knot."
  },
  {
    "objectID": "qmd/islp7.html#linear-spline",
    "href": "qmd/islp7.html#linear-spline",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Linear Spline",
    "text": "Linear Spline\nA linear spline is continuous, fitting straight lines that meet at the knots:"
  },
  {
    "objectID": "qmd/islp7.html#the-spline-basis-representation",
    "href": "qmd/islp7.html#the-spline-basis-representation",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "The Spline Basis Representation",
    "text": "The Spline Basis Representation\n\nBasis Function Model: A cubic spline with K knots can be written as:\n\\[y_i = \\beta_0 + \\beta_1 b_1(x_i) + \\beta_2 b_2(x_i) + ... + \\beta_{K+3} b_{K+3}(x_i) + \\epsilon_i\\]\nSimple Basis: A common basis is: \\(x, x^2, x^3\\), and truncated power basis functions:\n\\[h(x, \\xi) = (x - \\xi)^3_+ = \\begin{cases} (x-\\xi)^3 & \\text{if } x &gt; \\xi \\\\ 0 & \\text{otherwise} \\end{cases}\\]\nwhere \\(\\xi\\) is a knot."
  },
  {
    "objectID": "qmd/islp7.html#the-spline-basis-representation-truncated-power-basis",
    "href": "qmd/islp7.html#the-spline-basis-representation-truncated-power-basis",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "The Spline Basis Representation: Truncated Power Basis",
    "text": "The Spline Basis Representation: Truncated Power Basis\n\nTruncated Power Basis Function: \\(h(x, \\xi)\\) is zero until it reaches the knot (\\(\\xi\\)), then it becomes a cubic. This adds flexibility locally.\nFitting: We fit this model using least squares."
  },
  {
    "objectID": "qmd/islp7.html#the-spline-basis-representation-summary",
    "href": "qmd/islp7.html#the-spline-basis-representation-summary",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "The Spline Basis Representation: Summary",
    "text": "The Spline Basis Representation: Summary\n\nCubic Spline with K Knots: Use least squares with an intercept and 3 + K predictors: \\(X, X^2, X^3, h(X, \\xi_1), ..., h(X, \\xi_K)\\).\nBoundary Variance: Splines can have high variance at the boundaries (extreme values of X)."
  },
  {
    "objectID": "qmd/islp7.html#natural-splines",
    "href": "qmd/islp7.html#natural-splines",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Natural Splines",
    "text": "Natural Splines\n\nBoundary Constraints: A natural spline adds boundary constraints: the function is linear outside the boundary knots. This stabilizes the fit.\n\n\nThe figure shows a cubic spline and a natural cubic spline. The natural spline is linear beyond the boundary knots."
  },
  {
    "objectID": "qmd/islp7.html#choosing-the-number-and-locations-of-the-knots",
    "href": "qmd/islp7.html#choosing-the-number-and-locations-of-the-knots",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Choosing the Number and Locations of the Knots",
    "text": "Choosing the Number and Locations of the Knots\n\nKnot Placement: Where do we put the knots?\n\nMore Knots Where Function Varies: More knots where the function changes rapidly, fewer where it‚Äôs stable.\nUniform Placement: Often, place knots uniformly (e.g., at quantiles of the data).\nSoftware Automation: Specify the degrees of freedom, and the software places the knots."
  },
  {
    "objectID": "qmd/islp7.html#choosing-the-number-and-locations-of-the-knots-example",
    "href": "qmd/islp7.html#choosing-the-number-and-locations-of-the-knots-example",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Choosing the Number and Locations of the Knots: Example",
    "text": "Choosing the Number and Locations of the Knots: Example\n\nThis natural cubic spline (4 degrees of freedom) likely has one knot near the middle, where the curve changes most."
  },
  {
    "objectID": "qmd/islp7.html#choosing-the-number-of-knots-cross-validation",
    "href": "qmd/islp7.html#choosing-the-number-of-knots-cross-validation",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Choosing the Number of Knots: Cross-Validation",
    "text": "Choosing the Number of Knots: Cross-Validation\n\nHow Many Knots? Use cross-validation!\n\nFit Splines: Fit splines with different numbers of knots.\nCross-Validated RSS: Compute the cross-validated RSS for each.\nMinimize: Choose the number of knots that minimizes the cross-validated RSS. This gives the best generalization."
  },
  {
    "objectID": "qmd/islp7.html#cross-validation-example",
    "href": "qmd/islp7.html#cross-validation-example",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Cross-Validation Example",
    "text": "Cross-Validation Example\n\nThe plot shows ten-fold cross-validated MSE. Lower is better! The optimal degrees of freedom are where the curve is lowest."
  },
  {
    "objectID": "qmd/islp7.html#comparison-to-polynomial-regression",
    "href": "qmd/islp7.html#comparison-to-polynomial-regression",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Comparison to Polynomial Regression",
    "text": "Comparison to Polynomial Regression\n\nSplines vs.¬†Polynomials: Splines often give better results (lower test error).\nFlexibility:\n\nSplines: Increase flexibility by adding knots, keeping the degree fixed (usually cubic).\nPolynomials: Need high degrees for flexibility, which can lead to problems."
  },
  {
    "objectID": "qmd/islp7.html#splines-vs.-polynomials-visual-comparison",
    "href": "qmd/islp7.html#splines-vs.-polynomials-visual-comparison",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Splines vs.¬†Polynomials: Visual Comparison",
    "text": "Splines vs.¬†Polynomials: Visual Comparison\n\n\n\nHigh-degree polynomials can be wild, especially near boundaries.\n\nKey Point: High-degree polynomials can oscillate wildly. Splines are more stable."
  },
  {
    "objectID": "qmd/islp7.html#smoothing-splines-a-different-approach",
    "href": "qmd/islp7.html#smoothing-splines-a-different-approach",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Smoothing Splines: A Different Approach",
    "text": "Smoothing Splines: A Different Approach\n\nAlternative: Smoothing splines are another way to get a spline, but they take a different path.\nGoal: Find a function \\(g(x)\\) that fits the data well (small RSS) and is smooth."
  },
  {
    "objectID": "qmd/islp7.html#smoothing-splines-the-optimization-problem",
    "href": "qmd/islp7.html#smoothing-splines-the-optimization-problem",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Smoothing Splines: The Optimization Problem",
    "text": "Smoothing Splines: The Optimization Problem\n\nMinimization: Find \\(g(x)\\) that minimizes:\n\\[\\sum_{i=1}^{n}(y_i - g(x_i))^2 + \\lambda \\int g''(t)^2 dt\\]\nLoss + Penalty: This is a loss + penalty formulation.\n\nLoss: \\(\\sum_{i=1}^{n}(y_i - g(x_i))^2\\) (RSS). Measures fit.\nPenalty: \\(\\lambda \\int g''(t)^2 dt\\). Penalizes roughness."
  },
  {
    "objectID": "qmd/islp7.html#smoothing-splines-understanding-the-penalty",
    "href": "qmd/islp7.html#smoothing-splines-understanding-the-penalty",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Smoothing Splines: Understanding the Penalty",
    "text": "Smoothing Splines: Understanding the Penalty\n\nRoughness Penalty: \\(\\lambda \\int g''(t)^2 dt\\) measures the total change in the function‚Äôs slope.\n\n\\(g''(t)\\): Second derivative. Measures the rate of change of the slope.\n\\(\\int g''(t)^2 dt\\): Integrates the squared second derivative. Large value = wiggly. Small value = smooth.\n\\(\\lambda\\): Tuning parameter. Controls the trade-off between fit and smoothness."
  },
  {
    "objectID": "qmd/islp7.html#smoothing-splines-the-role-of-Œª",
    "href": "qmd/islp7.html#smoothing-splines-the-role-of-Œª",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Smoothing Splines: The Role of Œª",
    "text": "Smoothing Splines: The Role of Œª\n\nEffect of Œª:\n\nLarger \\(\\lambda\\) \\(\\Rightarrow\\) smoother \\(g\\).\n\\(\\lambda = 0\\): \\(g\\) interpolates the data (perfect fit, but probably overfit).\n\\(\\lambda \\to \\infty\\): \\(g\\) becomes a straight line (least squares line).\n\nBias-Variance Trade-Off: \\(\\lambda\\) controls the bias-variance trade-off."
  },
  {
    "objectID": "qmd/islp7.html#smoothing-splines-the-solution",
    "href": "qmd/islp7.html#smoothing-splines-the-solution",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Smoothing Splines: The Solution",
    "text": "Smoothing Splines: The Solution\n\nNatural Cubic Spline: The solution is a natural cubic spline!\nKnots at Data Points: This spline has knots at every unique \\(x_i\\) (every data point)! The penalty (\\(\\lambda\\)) controls the complexity."
  },
  {
    "objectID": "qmd/islp7.html#choosing-the-smoothing-parameter-Œª",
    "href": "qmd/islp7.html#choosing-the-smoothing-parameter-Œª",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Choosing the Smoothing Parameter (Œª)",
    "text": "Choosing the Smoothing Parameter (Œª)\n\nEffective Degrees of Freedom: \\(\\lambda\\) controls the effective degrees of freedom (\\(df_{\\lambda}\\)).\n\nAs \\(\\lambda\\) increases, \\(df_{\\lambda}\\) decreases from \\(n\\) to 2.\n\nLOOCV: Use leave-one-out cross-validation (LOOCV) to choose \\(\\lambda\\)."
  },
  {
    "objectID": "qmd/islp7.html#loocv-for-smoothing-splines",
    "href": "qmd/islp7.html#loocv-for-smoothing-splines",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "LOOCV for Smoothing Splines",
    "text": "LOOCV for Smoothing Splines\nThe LOOCV formula is:\n\\[\\text{RSS}_{cv}(\\lambda) = \\sum_{i=1}^{n} \\left( \\frac{y_i - \\hat{g}_{\\lambda}^{(-i)}(x_i)}{1 - \\{\\mathbf{S}_{\\lambda}\\}_{ii}} \\right)^2\\]\n\n\\(\\hat{g}_{\\lambda}^{(-i)}(x_i)\\): Fitted value leaving out observation \\(i\\).\n\\(\\mathbf{S}_{\\lambda}\\): A matrix depending on \\(\\lambda\\).\nEfficiency: We don‚Äôt need to refit \\(n\\) times! LOOCV can be computed efficiently."
  },
  {
    "objectID": "qmd/islp7.html#smoothing-splines-example-wage-data",
    "href": "qmd/islp7.html#smoothing-splines-example-wage-data",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Smoothing Splines: Example (Wage Data)",
    "text": "Smoothing Splines: Example (Wage Data)\nLet‚Äôs see smoothing splines on the Wage data.\n\nRed Curve: 16 effective degrees of freedom.\nBlue Curve: \\(\\lambda\\) chosen by LOOCV (6.8 effective degrees of freedom)."
  },
  {
    "objectID": "qmd/islp7.html#visualizing-smoothing-splines-wage-data",
    "href": "qmd/islp7.html#visualizing-smoothing-splines-wage-data",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Visualizing Smoothing Splines (Wage Data)",
    "text": "Visualizing Smoothing Splines (Wage Data)\n\n\n\nSimpler models are usually preferred.\n\nKey Point: We often prefer the simpler model (blue curve) unless there‚Äôs strong evidence for the more complex one."
  },
  {
    "objectID": "qmd/islp7.html#local-regression-fitting-locally",
    "href": "qmd/islp7.html#local-regression-fitting-locally",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Local Regression: Fitting Locally",
    "text": "Local Regression: Fitting Locally\n\nLocal Fits: Fit a model locally, using only nearby training points.\nAlgorithm:\n\nGather Neighbors: For a target point \\(x_0\\), gather the fraction \\(s = k/n\\) of closest points (\\(k\\) neighbors, \\(n\\) total points).\nAssign Weights: Weight each neighbor: \\(K_{i0} = K(x_i, x_0)\\).\n\nFarthest points: Weight = 0.\nClosest points: Highest weight.\n\nWeighted Least Squares: Fit weighted least squares:\n\\[\\text{minimize } \\sum_{i=1}^{n} K_{i0} (y_i - \\beta_0 - \\beta_1 x_i)^2\\]\nFitted Value: \\(\\hat{f}(x_0) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0\\)."
  },
  {
    "objectID": "qmd/islp7.html#local-regression-illustration",
    "href": "qmd/islp7.html#local-regression-illustration",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Local Regression: Illustration",
    "text": "Local Regression: Illustration\n\nThe shaded region shows the s = 1/3 closest neighbors to x0."
  },
  {
    "objectID": "qmd/islp7.html#local-regression-the-span-s",
    "href": "qmd/islp7.html#local-regression-the-span-s",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Local Regression: The Span (s)",
    "text": "Local Regression: The Span (s)\n\nKey Choice: The Span (s): Determines the neighborhood size.\n\nSmaller \\(s\\): More local, wigglier fit.\nLarger \\(s\\): Smoother, more global fit.\n\nCross-Validation: Use cross-validation to choose \\(s\\)."
  },
  {
    "objectID": "qmd/islp7.html#local-regression-example-wage-data",
    "href": "qmd/islp7.html#local-regression-example-wage-data",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Local Regression: Example (Wage Data)",
    "text": "Local Regression: Example (Wage Data)\n\nThe figure shows two local linear fits. A smaller span (s) gives a wigglier curve, and a larger span gives a smoother curve."
  },
  {
    "objectID": "qmd/islp7.html#local-regression-generalizations",
    "href": "qmd/islp7.html#local-regression-generalizations",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Local Regression: Generalizations",
    "text": "Local Regression: Generalizations\n\nVarying Coefficient Model: Local regression is a type of varying coefficient model (coefficients vary locally).\nMulti-dimensional: Can be generalized to multiple predictors."
  },
  {
    "objectID": "qmd/islp7.html#generalized-additive-models-gams",
    "href": "qmd/islp7.html#generalized-additive-models-gams",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Generalized Additive Models (GAMs)",
    "text": "Generalized Additive Models (GAMs)\n\nExtending Linear Regression: GAMs allow non-linear functions of predictors, while maintaining additivity.\nThe GAM Equation:\n\\[y_i = \\beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + ... + f_p(x_{ip}) + \\epsilon_i\\]\n\n\\(f_j\\): A (potentially) non-linear function for predictor \\(j\\)."
  },
  {
    "objectID": "qmd/islp7.html#gams-key-features",
    "href": "qmd/islp7.html#gams-key-features",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "GAMs: Key Features",
    "text": "GAMs: Key Features\n\nNon-Linear Functions: Allow non-linear \\(f_j\\) for each predictor.\nAdditivity: The model is additive. Effects of predictors are added together. This helps with interpretability.\nBuilding Blocks: Use various functions for \\(f_j\\):\n\nSplines (natural, smoothing)\nLocal regression\nPolynomials\nStep functions\nOther basis functions\n\nFitting: Use backfitting."
  },
  {
    "objectID": "qmd/islp7.html#gams-example-wage-data",
    "href": "qmd/islp7.html#gams-example-wage-data",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "GAMs: Example (Wage Data)",
    "text": "GAMs: Example (Wage Data)\nwage = Œ≤0 + f1(year) + f2(age) + f3(education) + Œµ\n\nyear, age: Quantitative.\neducation: Qualitative (5 levels)."
  },
  {
    "objectID": "qmd/islp7.html#gams-example-wage-data---implementation",
    "href": "qmd/islp7.html#gams-example-wage-data---implementation",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "GAMs: Example (Wage Data) - Implementation",
    "text": "GAMs: Example (Wage Data) - Implementation\n\nNatural Splines: Use natural splines for year and age.\nDummy Variables: Use dummy variables for education.\nLarge Regression: The GAM is a large regression on spline basis variables and dummy variables."
  },
  {
    "objectID": "qmd/islp7.html#gams-example-wage-data---natural-splines",
    "href": "qmd/islp7.html#gams-example-wage-data---natural-splines",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "GAMs: Example (Wage Data) - Natural Splines",
    "text": "GAMs: Example (Wage Data) - Natural Splines\n\nEach panel shows the effect of a predictor, holding others constant. Year and age are non-linear. Education‚Äôs effect is shown by level shifts."
  },
  {
    "objectID": "qmd/islp7.html#gams-example-wage-data---smoothing-splines",
    "href": "qmd/islp7.html#gams-example-wage-data---smoothing-splines",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "GAMs: Example (Wage Data) - Smoothing Splines",
    "text": "GAMs: Example (Wage Data) - Smoothing Splines\n\n\nSmoothing Splines: Can also use smoothing splines.\nSimilar Results: Natural and smoothing splines usually give similar results."
  },
  {
    "objectID": "qmd/islp7.html#gams-for-classification-problems",
    "href": "qmd/islp7.html#gams-for-classification-problems",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "GAMs for Classification Problems",
    "text": "GAMs for Classification Problems\n\nQualitative Response: GAMs can be used when \\(Y\\) is qualitative.\nLogistic Regression GAM: For a binary response:\n\\[\\log\\left( \\frac{p(X)}{1 - p(X)} \\right) = \\beta_0 + f_1(X_1) + f_2(X_2) + ... + f_p(X_p)\\]\nwhere \\(p(X) = \\text{Pr}(Y = 1 | X)\\)."
  },
  {
    "objectID": "qmd/islp7.html#gams-for-classification-example",
    "href": "qmd/islp7.html#gams-for-classification-example",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "GAMs for Classification: Example",
    "text": "GAMs for Classification: Example\n\nExample: Predict wage &gt; 250 using a GAM.\n\n\nThis shows a logistic regression GAM. Year and age have non-linear effects. The plot for education shows the effect of each level on the log-odds of high earnings."
  },
  {
    "objectID": "qmd/islp7.html#pros-and-cons-of-gams",
    "href": "qmd/islp7.html#pros-and-cons-of-gams",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Pros and Cons of GAMs",
    "text": "Pros and Cons of GAMs\n\n\nPros:\n\nAutomatic Non-Linearity: Model non-linearity automatically.\nPotentially More Accurate: Better predictions when relationships are non-linear.\nInterpretability: Additivity helps with interpretation.\nSmoothness Summarization: Smoothness is summarized by effective degrees of freedom.\n\nCons:\n\nAdditivity Restriction: GAMs are additive. They can miss interactions (unless explicitly added).\nInteraction Handling\n\nThe effect of the change of one predictor on the response variable may also depend on other predictors. This is so-called interaction.\nBy default, GAM assumes no interaction between predictors.\nWe can manually add interaction terms: y ~ x1 + x2 + f(x3, x4)."
  },
  {
    "objectID": "qmd/islp7.html#summary",
    "href": "qmd/islp7.html#summary",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Summary",
    "text": "Summary\n\nBeyond Linearity: We explored techniques for going beyond linear models.\nNon-Linear Techniques: Learned about:\n\nPolynomial regression\nStep functions\nRegression splines\nSmoothing splines\nLocal regression\nGAMs\n\nFlexibility and Interpretability: These methods offer more flexibility while maintaining interpretability.\nCross-Validation: Crucial for choosing tuning parameters to avoid overfitting.\nGAMs: Extend non-linear ideas to multiple predictors."
  },
  {
    "objectID": "qmd/islp7.html#thoughts-and-discussion",
    "href": "qmd/islp7.html#thoughts-and-discussion",
    "title": "Statistical Learning - Beyond Linearity",
    "section": "Thoughts and Discussion",
    "text": "Thoughts and Discussion\n\nLinear vs.¬†Non-Linear: When choose linear vs.¬†non-linear? Consider simplicity vs.¬†accuracy.\nComparing Techniques: How do the techniques compare in flexibility and interpretability?\nBeyond GAMs: When might a GAM not be enough? (Complex interactions, highly non-linear relationships).\nInteractions in GAMs: How can you add interactions? Trade-offs?\nSmoothing Splines vs.¬†Regression Splines: Discuss the trade-offs between smoothing and regression splines. Consider computation, implementation, and knot placement."
  },
  {
    "objectID": "qmd/islp2.html",
    "href": "qmd/islp2.html",
    "title": "Introduction to Statistical Learning",
    "section": "",
    "text": "Statistical learning is a vast set of tools for understanding data. These tools can be classified as supervised or unsupervised. üìä It‚Äôs like having a toolbox filled with different instruments to analyze and interpret the information hidden within datasets."
  },
  {
    "objectID": "qmd/islp2.html#introduction-to-statistical-learning",
    "href": "qmd/islp2.html#introduction-to-statistical-learning",
    "title": "Introduction to Statistical Learning",
    "section": "",
    "text": "Statistical learning is a vast set of tools for understanding data. These tools can be classified as supervised or unsupervised. üìä It‚Äôs like having a toolbox filled with different instruments to analyze and interpret the information hidden within datasets."
  },
  {
    "objectID": "qmd/islp2.html#supervised-vs.-unsupervised-learning",
    "href": "qmd/islp2.html#supervised-vs.-unsupervised-learning",
    "title": "Introduction to Statistical Learning",
    "section": "Supervised vs.¬†Unsupervised Learning",
    "text": "Supervised vs.¬†Unsupervised Learning\n\nSupervised statistical learning involves building a statistical model for predicting, or estimating, an output based on one or more inputs. We have a target variable we want to predict, like a teacher guiding the learning process. üéØ Think of predicting house prices based on features like size, location, and number of bedrooms.\nWith unsupervised statistical learning, there are inputs but no supervising output; nevertheless, we can learn relationships and structure from such data. We‚Äôre exploring the data to find patterns, like a detective searching for clues without knowing exactly what they‚Äôre looking for. üîç An example is grouping customers into different segments based on their purchasing behavior."
  },
  {
    "objectID": "qmd/islp2.html#key-concepts",
    "href": "qmd/islp2.html#key-concepts",
    "title": "Introduction to Statistical Learning",
    "section": "Key Concepts",
    "text": "Key Concepts\nThis chapter introduces many of the key concepts of statistical learning, focusing on the fundamental ideas, which include:\n\nData mining, machine learning and statistical learning relationship. ‚õèÔ∏èü§ñüìà\nEstimating f: How do we find the best function to describe the relationship between our variables?\nThe trade-off between prediction accuracy and model interpretability: Can we have both a highly accurate and easily understandable model? ü§î\nSupervised versus unsupervised learning: The difference between having a target variable and exploring the data without one.\nAssessing model accuracy: How do we know if our model is good? We‚Äôll look at metrics like mean square error, the bias-variance, and Bayes error rate."
  },
  {
    "objectID": "qmd/islp2.html#data-mining-machine-learning-and-statistical-learning",
    "href": "qmd/islp2.html#data-mining-machine-learning-and-statistical-learning",
    "title": "Introduction to Statistical Learning",
    "section": "Data Mining, Machine Learning, and Statistical Learning",
    "text": "Data Mining, Machine Learning, and Statistical Learning\nLet‚Äôs clarify the relationship between these often-used terms. These are all related, but have slightly different focuses. üßê It‚Äôs like comparing different members of a family - they share common traits but also have unique characteristics."
  },
  {
    "objectID": "qmd/islp2.html#data-mining",
    "href": "qmd/islp2.html#data-mining",
    "title": "Introduction to Statistical Learning",
    "section": "Data Mining",
    "text": "Data Mining\n\n\n\n\n\n\n\nData Mining: The process of discovering patterns, anomalies, and insights from large datasets, often using computational techniques. It emphasizes finding any interesting pattern, even if it‚Äôs not directly related to a specific prediction task. Think of it as exploring a vast dataset to find hidden treasures. ‚õèÔ∏èüíé Imagine you‚Äôre sifting through a mountain of sand to find gold nuggets.\n\n\nExample: A supermarket analyzing purchase data to discover that people who buy diapers often also buy beer. This is a pattern, but not necessarily useful for prediction. This unexpected correlation could lead to strategic product placement in the store."
  },
  {
    "objectID": "qmd/islp2.html#machine-learning",
    "href": "qmd/islp2.html#machine-learning",
    "title": "Introduction to Statistical Learning",
    "section": "Machine Learning",
    "text": "Machine Learning\n\n\n\n\n\n\n\nMachine Learning: A field of computer science focused on algorithms that can learn from data without explicit programming. It‚Äôs heavily focused on prediction ‚Äì enabling computers to make accurate predictions on new, unseen data. Think of it as teaching a computer to learn from examples. ü§ñüìö Imagine a robot learning to play a game by observing human players and trying different strategies.\n\n\nExample: A spam filter learning to identify spam emails based on the words used in the email body and subject line. The filter improves its accuracy over time as it ‚Äúlearns‚Äù from more examples of spam and non-spam emails."
  },
  {
    "objectID": "qmd/islp2.html#statistical-learning",
    "href": "qmd/islp2.html#statistical-learning",
    "title": "Introduction to Statistical Learning",
    "section": "Statistical Learning",
    "text": "Statistical Learning\n\n\n\n\n\n\n\nStatistical Learning: A subfield of statistics that focuses on developing and applying statistical models and methods for prediction and inference. It emphasizes understanding the relationships between variables and making inferences about the underlying data-generating process. It combines the goals of understanding and prediction. üìàüîç Think of a scientist using data to build a model of a physical phenomenon, both to predict its behavior and to understand the underlying mechanisms.\n\n\nExample: Building a model to predict a patient‚Äôs risk of heart disease based on their age, blood pressure, cholesterol levels, and other risk factors, and understanding how each of those factors contributes to the risk. This understanding can help doctors develop better prevention and treatment strategies."
  },
  {
    "objectID": "qmd/islp2.html#common-ground",
    "href": "qmd/islp2.html#common-ground",
    "title": "Introduction to Statistical Learning",
    "section": "Common Ground",
    "text": "Common Ground\n\n\n\n\n\n\n\nAll the concepts are focusing on extracting information from data. They all aim to gain insights and/or make predictions based on data. It‚Äôs like different paths leading to the same destination ‚Äì understanding and utilizing data."
  },
  {
    "objectID": "qmd/islp2.html#relationships-visualized",
    "href": "qmd/islp2.html#relationships-visualized",
    "title": "Introduction to Statistical Learning",
    "section": "Relationships Visualized",
    "text": "Relationships Visualized\n\n\n\n\n\ngraph LR\n    A[Data Mining] --&gt; C(Common Ground: Extracting Information from Data)\n    B[Machine Learning] --&gt; C\n    D[Statistical Learning] --&gt; C\n    C --&gt; E[Insights & Predictions]\n\n\n\n\n\n\nThis diagram shows how data mining, machine learning, and statistical learning all share the common goal of extracting information from data, which leads to insights and predictions. It‚Äôs like a Venn diagram showing the overlap and distinct areas of each field."
  },
  {
    "objectID": "qmd/islp2.html#the-advertising-example",
    "href": "qmd/islp2.html#the-advertising-example",
    "title": "Introduction to Statistical Learning",
    "section": "The Advertising Example",
    "text": "The Advertising Example\nTo motivate our study, let‚Äôs consider a simple example. A company wants to understand how advertising spending affects product sales. üìàüõçÔ∏è Imagine you‚Äôre running a business and want to know how best to allocate your advertising budget."
  },
  {
    "objectID": "qmd/islp2.html#advertising-example-data-and-goal",
    "href": "qmd/islp2.html#advertising-example-data-and-goal",
    "title": "Introduction to Statistical Learning",
    "section": "Advertising Example: Data and Goal",
    "text": "Advertising Example: Data and Goal\n\nData: Sales of a product in 200 different markets, along with advertising budgets for TV, radio, and newspaper. This is like having a spreadsheet with sales figures and advertising spending for different regions.\nGoal: Build a model to predict sales based on the advertising budgets in the three media. The company wants to find a formula that connects advertising spending to sales."
  },
  {
    "objectID": "qmd/islp2.html#advertising-example-why",
    "href": "qmd/islp2.html#advertising-example-why",
    "title": "Introduction to Statistical Learning",
    "section": "Advertising Example: Why?",
    "text": "Advertising Example: Why?\n\nWhy? The company can‚Äôt directly control sales, but can control advertising spending. A good model helps them optimize their advertising budget to maximize sales. They want to know where to spend their money! üí∞‚û°Ô∏èüìà This is about getting the biggest bang for your buck ‚Äì maximizing return on investment."
  },
  {
    "objectID": "qmd/islp2.html#input-and-output-variables",
    "href": "qmd/islp2.html#input-and-output-variables",
    "title": "Introduction to Statistical Learning",
    "section": "Input and Output Variables",
    "text": "Input and Output Variables\nLet‚Äôs define the key components in our statistical learning framework."
  },
  {
    "objectID": "qmd/islp2.html#input-and-output-variables-definitions",
    "href": "qmd/islp2.html#input-and-output-variables-definitions",
    "title": "Introduction to Statistical Learning",
    "section": "Input and Output Variables: Definitions",
    "text": "Input and Output Variables: Definitions\nIn the advertising example:\n\nInput variables (X): Advertising budgets (TV, radio, newspaper). These are also called predictors, independent variables, or features. We often denote them as X‚ÇÅ, X‚ÇÇ, X‚ÇÉ, ‚Ä¶, X‚Çö. These are the things we can control or observe. üõ†Ô∏è These are like the ingredients in a recipe.\nOutput variable (Y): Sales. This is also called the response or dependent variable. We‚Äôre trying to predict or understand Y. This is the outcome we‚Äôre interested in. üéØ This is like the final dish in a recipe.\nStatistical learning will use all these terms interchangeably."
  },
  {
    "objectID": "qmd/islp2.html#input-and-output-variables-example",
    "href": "qmd/islp2.html#input-and-output-variables-example",
    "title": "Introduction to Statistical Learning",
    "section": "Input and Output Variables: Example",
    "text": "Input and Output Variables: Example\n\n\nExample:\n\nX‚ÇÅ = TV budget\nX‚ÇÇ = Radio budget\nX‚ÇÉ = Newspaper budget\nY = Sales\n\n\n\nHere, we have three input variables (p=3) representing the advertising budgets for different media, and one output variable (sales). It‚Äôs like having three dials (advertising budgets) that we can adjust to try to control the outcome (sales)."
  },
  {
    "objectID": "qmd/islp2.html#visualizing-the-advertising-data",
    "href": "qmd/islp2.html#visualizing-the-advertising-data",
    "title": "Introduction to Statistical Learning",
    "section": "Visualizing the Advertising Data",
    "text": "Visualizing the Advertising Data\nLet‚Äôs look at the relationship between advertising spending and sales."
  },
  {
    "objectID": "qmd/islp2.html#advertising-data-visualization",
    "href": "qmd/islp2.html#advertising-data-visualization",
    "title": "Introduction to Statistical Learning",
    "section": "Advertising Data: Visualization",
    "text": "Advertising Data: Visualization\n\n\n\n\n\n\n\n\n\nAdvertising Data: Sales vs.¬†Advertising Budgets"
  },
  {
    "objectID": "qmd/islp2.html#advertising-data-interpretation-tv",
    "href": "qmd/islp2.html#advertising-data-interpretation-tv",
    "title": "Introduction to Statistical Learning",
    "section": "Advertising Data: Interpretation (TV)",
    "text": "Advertising Data: Interpretation (TV)\n\n\n\n\n\n\n\n\n\nAdvertising Data: Sales vs.¬†Advertising Budgets\n\n\n\n\n\n\n\nThe leftmost plot shows sales versus TV advertising budget.\nThe blue line represents a simple linear model (least squares fit) to predict sales using TV advertising budget.\nObservation: There‚Äôs a clear positive relationship. As TV advertising spending increases, sales tend to increase as well. This suggests that TV advertising is effective."
  },
  {
    "objectID": "qmd/islp2.html#advertising-data-interpretation-radio",
    "href": "qmd/islp2.html#advertising-data-interpretation-radio",
    "title": "Introduction to Statistical Learning",
    "section": "Advertising Data: Interpretation (Radio)",
    "text": "Advertising Data: Interpretation (Radio)\n\n\n\n\n\n\n\n\n\nAdvertising Data: Sales vs.¬†Advertising Budgets\n\n\n\n\n\n\n\nThe center plot shows sales versus Radio advertising budget.\nThe blue line represents a simple linear model (least squares fit) to predict sales using Radio advertising budget.\nObservation: There‚Äôs also a positive relationship, although perhaps slightly less strong than with TV advertising. Radio advertising also seems to be effective."
  },
  {
    "objectID": "qmd/islp2.html#advertising-data-interpretation-newspaper",
    "href": "qmd/islp2.html#advertising-data-interpretation-newspaper",
    "title": "Introduction to Statistical Learning",
    "section": "Advertising Data: Interpretation (Newspaper)",
    "text": "Advertising Data: Interpretation (Newspaper)\n\n\n\n\n\n\n\n\n\nAdvertising Data: Sales vs.¬†Advertising Budgets\n\n\n\n\n\n\n\nThe rightmost plot shows sales versus Newspaper advertising budget.\nThe blue line represents a simple linear model (least squares fit) to predict sales using Newspaper advertising budget.\nObservation: The relationship is less clear. It‚Äôs not obvious whether newspaper advertising has a strong positive or negative effect on sales. This might suggest that newspaper advertising is less effective, or that the relationship is more complex."
  },
  {
    "objectID": "qmd/islp2.html#the-general-model",
    "href": "qmd/islp2.html#the-general-model",
    "title": "Introduction to Statistical Learning",
    "section": "The General Model",
    "text": "The General Model\nMore generally, we assume a relationship between the response Y and predictors X. This is the foundation of statistical learning."
  },
  {
    "objectID": "qmd/islp2.html#the-general-model-equation",
    "href": "qmd/islp2.html#the-general-model-equation",
    "title": "Introduction to Statistical Learning",
    "section": "The General Model: Equation",
    "text": "The General Model: Equation\n\\[\nY = f(X) + \\epsilon\n\\]\n\nY: The quantitative response variable we want to predict. This is the outcome we‚Äôre interested in.\nX: (X‚ÇÅ, X‚ÇÇ, ‚Ä¶, X‚Çö), a vector of p predictors. These are the factors we believe influence the outcome.\nf(X): An unknown function representing the systematic relationship between X and Y. This is what we want to estimate! This is the underlying pattern we‚Äôre trying to uncover. It‚Äôs like the secret formula that connects our inputs to the output.\nŒµ: A random error term, independent of X, with a mean of zero. It represents the variation in Y that cannot be explained by f(X). This acknowledges that our model won‚Äôt be perfect. It‚Äôs like the ‚Äúnoise‚Äù or randomness in the system."
  },
  {
    "objectID": "qmd/islp2.html#the-general-model-goal",
    "href": "qmd/islp2.html#the-general-model-goal",
    "title": "Introduction to Statistical Learning",
    "section": "The General Model: Goal",
    "text": "The General Model: Goal\n\nGoal of Statistical Learning: Estimate the unknown function f.\n\nOur primary goal is to find the best possible estimate of the function f, which describes the relationship between our predictors and the response. It‚Äôs like trying to find the best possible approximation of the secret formula."
  },
  {
    "objectID": "qmd/islp2.html#understanding-the-error-term-Œµ",
    "href": "qmd/islp2.html#understanding-the-error-term-Œµ",
    "title": "Introduction to Statistical Learning",
    "section": "Understanding the Error Term (Œµ)",
    "text": "Understanding the Error Term (Œµ)\nThe error term, Œµ, is crucial. It represents the ‚Äúnoise‚Äù in our data."
  },
  {
    "objectID": "qmd/islp2.html#the-error-term-explanation",
    "href": "qmd/islp2.html#the-error-term-explanation",
    "title": "Introduction to Statistical Learning",
    "section": "The Error Term: Explanation",
    "text": "The Error Term: Explanation\nThe error term, Œµ, captures all the factors that affect Y but are not included in our predictors X. This could include:\n\nUnmeasured variables: Factors influencing Y that we didn‚Äôt or couldn‚Äôt measure. (e.g., competitor activity, overall economic conditions, customer mood).\nMeasurement error: Inaccuracies in how we measured X or Y. (e.g., a survey respondent misremembering their income, a faulty sensor recording temperature).\nRandomness: Inherent variability in Y that can‚Äôt be perfectly predicted. (e.g., even with the same advertising spend, sales might fluctuate due to random chance, like a coin flip)."
  },
  {
    "objectID": "qmd/islp2.html#the-error-term-importance",
    "href": "qmd/islp2.html#the-error-term-importance",
    "title": "Introduction to Statistical Learning",
    "section": "The Error Term: Importance",
    "text": "The Error Term: Importance\n\n\n\n\n\n\nNote\n\n\n\nThe error term is crucial. It acknowledges that our models are approximations of reality. Even the ‚Äúbest‚Äù model won‚Äôt be perfect. It‚Äôs a reminder that there‚Äôs always some uncertainty. It‚Äôs like acknowledging that our map of the world is not the world itself."
  },
  {
    "objectID": "qmd/islp2.html#example-income-vs.-education",
    "href": "qmd/islp2.html#example-income-vs.-education",
    "title": "Introduction to Statistical Learning",
    "section": "Example: Income vs.¬†Education",
    "text": "Example: Income vs.¬†Education\nLet‚Äôs look at another example: predicting income based on education and seniority."
  },
  {
    "objectID": "qmd/islp2.html#income-vs.-education-visualization",
    "href": "qmd/islp2.html#income-vs.-education-visualization",
    "title": "Introduction to Statistical Learning",
    "section": "Income vs.¬†Education: Visualization",
    "text": "Income vs.¬†Education: Visualization\n\n\n\n\n\n\n\n\n\nIncome vs.¬†Years of Education and Seniority"
  },
  {
    "objectID": "qmd/islp2.html#income-vs.-education-interpretation-left",
    "href": "qmd/islp2.html#income-vs.-education-interpretation-left",
    "title": "Introduction to Statistical Learning",
    "section": "Income vs.¬†Education: Interpretation (Left)",
    "text": "Income vs.¬†Education: Interpretation (Left)\n\n\n\n\n\n\n\n\n\nIncome vs.¬†Years of Education and Seniority\n\n\n\n\n\n\n\nLeft: A 3D scatterplot of income (in thousands of dollars) versus years of education and years of seniority for 30 individuals. Each red point represents a person. This allows us to visualize the relationship between income and two predictors simultaneously."
  },
  {
    "objectID": "qmd/islp2.html#income-vs.-education-interpretation-right",
    "href": "qmd/islp2.html#income-vs.-education-interpretation-right",
    "title": "Introduction to Statistical Learning",
    "section": "Income vs.¬†Education: Interpretation (Right)",
    "text": "Income vs.¬†Education: Interpretation (Right)\n\n\n\n\n\n\n\n\n\nIncome vs.¬†Years of Education and Seniority\n\n\n\n\n\n\n\nRight: The true underlying relationship (blue surface), which is usually unknown (but known here because the data were simulated). The surface represents the average income for any given combination of education and seniority. This is like knowing the ‚Äútrue‚Äù formula for income."
  },
  {
    "objectID": "qmd/islp2.html#income-vs.-education-seniority-visualization-2d-projection",
    "href": "qmd/islp2.html#income-vs.-education-seniority-visualization-2d-projection",
    "title": "Introduction to Statistical Learning",
    "section": "Income vs.¬†Education, Seniority: Visualization (2D Projection)",
    "text": "Income vs.¬†Education, Seniority: Visualization (2D Projection)\n\n\n\n\n\n\n\n\n\nIncome vs.¬†Years of Education"
  },
  {
    "objectID": "qmd/islp2.html#income-vs.-education-interpretation-2d-left",
    "href": "qmd/islp2.html#income-vs.-education-interpretation-2d-left",
    "title": "Introduction to Statistical Learning",
    "section": "Income vs.¬†Education: Interpretation (2D, Left)",
    "text": "Income vs.¬†Education: Interpretation (2D, Left)\n\n\n\n\n\n\n\n\n\nIncome vs.¬†Years of Education\n\n\n\n\n\n\n\nLeft: Observed income (in thousands of dollars) versus years of education for 30 individuals. Each red point represents a person. This is a 2D projection of the 3D data, showing only the relationship between income and education."
  },
  {
    "objectID": "qmd/islp2.html#income-vs.-education-interpretation-2d-right",
    "href": "qmd/islp2.html#income-vs.-education-interpretation-2d-right",
    "title": "Introduction to Statistical Learning",
    "section": "Income vs.¬†Education: Interpretation (2D, Right)",
    "text": "Income vs.¬†Education: Interpretation (2D, Right)\n\n\n\n\n\n\n\n\n\nIncome vs.¬†Years of Education\n\n\n\n\n\n\n\nRight: The true underlying relationship (blue curve), which is usually unknown (but known here because the data were simulated). The black line segments represent errors associated with each data, showing the difference between the observed income and the true underlying relationship. This shows how individual incomes deviate from the average trend."
  },
  {
    "objectID": "qmd/islp2.html#income-vs.-education-overall-observation",
    "href": "qmd/islp2.html#income-vs.-education-overall-observation",
    "title": "Introduction to Statistical Learning",
    "section": "Income vs.¬†Education: Overall Observation",
    "text": "Income vs.¬†Education: Overall Observation\n\nObservation: More years of education and seniority generally lead to higher income, but there‚Äôs variation (the error). Not everyone with the same education and seniority level has the same income. This highlights the role of other factors and randomness in determining income."
  },
  {
    "objectID": "qmd/islp2.html#why-estimate-f",
    "href": "qmd/islp2.html#why-estimate-f",
    "title": "Introduction to Statistical Learning",
    "section": "Why Estimate f?",
    "text": "Why Estimate f?\nThere are two main reasons to estimate f: Prediction and Inference. It‚Äôs like having two different goals when exploring a new city ‚Äì you might want to find the fastest route to a specific destination (prediction), or you might want to understand the layout of the city and how different neighborhoods are connected (inference)."
  },
  {
    "objectID": "qmd/islp2.html#why-estimate-f-prediction",
    "href": "qmd/islp2.html#why-estimate-f-prediction",
    "title": "Introduction to Statistical Learning",
    "section": "Why Estimate f: Prediction",
    "text": "Why Estimate f: Prediction\n\nPrediction: We want to predict Y given a set of X values. We don‚Äôt necessarily care about the exact form of f, just that it gives accurate predictions (treat f as a ‚Äúblack box‚Äù). We want the best possible guess for Y. üîÆ This is like using a GPS to find the best route ‚Äì you don‚Äôt need to know how the GPS works internally, just that it gives you accurate directions.\n\\[\n\\hat{Y} = \\hat{f}(X)\n\\]\n\n≈∂: The prediction of Y. Our best guess for the value of Y.\nfÃÇ: Our estimate of f. The function we‚Äôve learned from the data."
  },
  {
    "objectID": "qmd/islp2.html#why-estimate-f-inference",
    "href": "qmd/islp2.html#why-estimate-f-inference",
    "title": "Introduction to Statistical Learning",
    "section": "Why Estimate f: Inference",
    "text": "Why Estimate f: Inference\n\nInference: We want to understand the relationship between Y and X. We do care about the form of f. We want to answer questions about how the predictors influence the response. üïµÔ∏è‚Äç‚ôÄÔ∏è This is like studying a map to understand how different roads are connected and how traffic flows in a city.\n\nWhich predictors are associated with the response? Which factors are most important?\nIs the relationship positive or negative? Does increasing a predictor increase or decrease the response?\nIs the relationship linear or more complex? Is the relationship a straight line or a curve?"
  },
  {
    "objectID": "qmd/islp2.html#prediction-reducible-and-irreducible-error",
    "href": "qmd/islp2.html#prediction-reducible-and-irreducible-error",
    "title": "Introduction to Statistical Learning",
    "section": "Prediction: Reducible and Irreducible Error",
    "text": "Prediction: Reducible and Irreducible Error\nThe accuracy of our prediction, ≈∂, depends on two types of error."
  },
  {
    "objectID": "qmd/islp2.html#prediction-error-decomposition",
    "href": "qmd/islp2.html#prediction-error-decomposition",
    "title": "Introduction to Statistical Learning",
    "section": "Prediction Error: Decomposition",
    "text": "Prediction Error: Decomposition\nThe accuracy of our prediction, ≈∂, depends on two types of error:\n\nReducible Error: Error due to our estimate of f (fÃÇ) not being perfect. We can reduce this error by choosing better statistical learning techniques, improving our model. üí™ This is like improving your driving skills to get to your destination faster.\nIrreducible Error: Error due to the random error term, Œµ. Even if we knew the true f, we cannot predict Œµ. This sets a limit on how accurate our predictions can be. This is the inherent randomness we can‚Äôt eliminate. ü§∑ This is like encountering unexpected traffic ‚Äì you can‚Äôt eliminate it, no matter how good a driver you are.\n\\[E(Y - \\hat{Y})^2 = \\underbrace{[f(X) - \\hat{f}(X)]^2}_{\\text{Reducible}} + \\underbrace{Var(\\epsilon)}_{\\text{Irreducible}}\\]\n\nE(Y - ≈∂)¬≤: The expected squared difference between the true value of Y and our prediction. This measures the average squared error.\n[f(X) - fÃÇ(X)]¬≤: The squared difference between the true function and our estimated function. This is the reducible error.\nVar(Œµ): The variance of the error term. This is the irreducible error."
  },
  {
    "objectID": "qmd/islp2.html#prediction-error-goal",
    "href": "qmd/islp2.html#prediction-error-goal",
    "title": "Introduction to Statistical Learning",
    "section": "Prediction Error: Goal",
    "text": "Prediction Error: Goal\n\n\n\n\n\n\nNote\n\n\n\nOur goal is to minimize the reducible error.\n\n\nWe focus on reducing the reducible error because that‚Äôs the part we can control through better modeling. It‚Äôs like focusing on improving our driving skills, rather than worrying about unpredictable traffic."
  },
  {
    "objectID": "qmd/islp2.html#inference-understanding-the-relationship",
    "href": "qmd/islp2.html#inference-understanding-the-relationship",
    "title": "Introduction to Statistical Learning",
    "section": "Inference: Understanding the Relationship",
    "text": "Inference: Understanding the Relationship\nWhen our goal is inference, we want to understand how Y changes as a function of X‚ÇÅ, ‚Ä¶, X‚Çö."
  },
  {
    "objectID": "qmd/islp2.html#inference-key-questions",
    "href": "qmd/islp2.html#inference-key-questions",
    "title": "Introduction to Statistical Learning",
    "section": "Inference: Key Questions",
    "text": "Inference: Key Questions\nWe‚Äôre interested in questions like:\n\nWhich predictors matter? Are all the X·µ¢ related to Y, or only a subset? (e.g., Does newspaper advertising actually impact sales?) This is like figuring out which ingredients are essential for a recipe.\nWhat‚Äôs the nature of the relationship? Is it positive, negative, linear, non-linear? (e.g., Does income increase linearly with education, or is there a diminishing return?) This is like understanding how the amount of each ingredient affects the taste of the dish.\nCan we simplify the model? Can we get a good understanding with a simpler model (e.g., a linear model)? (e.g., Can we ignore some predictors without losing much accuracy?) This is like simplifying a recipe without sacrificing the flavor.\n\nWe care about interpretability, the form of f, and statistical significance."
  },
  {
    "objectID": "qmd/islp2.html#example-modeling-for-prediction",
    "href": "qmd/islp2.html#example-modeling-for-prediction",
    "title": "Introduction to Statistical Learning",
    "section": "Example: Modeling for Prediction",
    "text": "Example: Modeling for Prediction\nLet‚Äôs see an example where prediction is the primary goal."
  },
  {
    "objectID": "qmd/islp2.html#prediction-example-direct-marketing",
    "href": "qmd/islp2.html#prediction-example-direct-marketing",
    "title": "Introduction to Statistical Learning",
    "section": "Prediction Example: Direct Marketing",
    "text": "Prediction Example: Direct Marketing\nScenario: A company wants to target a direct-marketing campaign to individuals likely to respond positively. They want to send their advertisements to the people most likely to buy their product.\n\nPredictors (X): Demographic variables (age, income, location, etc.). These are like characteristics of potential customers.\nResponse (Y): Response to the campaign (positive or negative). Did the customer buy the product or not?\nGoal: Accurately predict Y using X. The company doesn‚Äôt need to deeply understand why each predictor is related to the response, only that the prediction is accurate. They want to maximize the response rate to their campaign."
  },
  {
    "objectID": "qmd/islp2.html#prediction-example-black-box",
    "href": "qmd/islp2.html#prediction-example-black-box",
    "title": "Introduction to Statistical Learning",
    "section": "Prediction Example: Black Box",
    "text": "Prediction Example: Black Box\n\n\n\n\n\n\nNote\n\n\n\nThis is a classic prediction problem. The model is a ‚Äúblack box‚Äù.\n\n\nWe don‚Äôt necessarily care why certain people respond, just that they respond. It‚Äôs like knowing that a certain machine produces good results, without knowing exactly how it works internally."
  },
  {
    "objectID": "qmd/islp2.html#example-modeling-for-inference",
    "href": "qmd/islp2.html#example-modeling-for-inference",
    "title": "Introduction to Statistical Learning",
    "section": "Example: Modeling for Inference",
    "text": "Example: Modeling for Inference\nNow, let‚Äôs see an example where inference is the primary goal."
  },
  {
    "objectID": "qmd/islp2.html#inference-example-advertising-data",
    "href": "qmd/islp2.html#inference-example-advertising-data",
    "title": "Introduction to Statistical Learning",
    "section": "Inference Example: Advertising Data",
    "text": "Inference Example: Advertising Data\nScenario: Analyze the Advertising data (Figure 2.1). We want to understand how different types of advertising affect sales.\n\nPredictors (X): TV, radio, and newspaper advertising budgets. These are the different ways the company spends money on advertising.\nResponse (Y): Sales. The outcome the company wants to improve.\nGoal: Understand how each advertising medium affects sales."
  },
  {
    "objectID": "qmd/islp2.html#inference-example-questions",
    "href": "qmd/islp2.html#inference-example-questions",
    "title": "Introduction to Statistical Learning",
    "section": "Inference Example: Questions",
    "text": "Inference Example: Questions\nQuestions to answer:\n\nWhich media are associated with sales? Which types of advertising are most effective?\nWhich media generate the biggest boost in sales? Where should the company invest most of its advertising budget?\nHow large is the effect of TV advertising on sales? How much more can we expect to sell for every dollar spent on TV advertising?\n\nWe want to understand the causal relationships between advertising and sales."
  },
  {
    "objectID": "qmd/islp2.html#inference-example-understanding",
    "href": "qmd/islp2.html#inference-example-understanding",
    "title": "Introduction to Statistical Learning",
    "section": "Inference Example: Understanding",
    "text": "Inference Example: Understanding\n\n\n\n\n\n\nNote\n\n\n\nThis is an inference problem. We want to understand the relationships.\n\n\nWe care about the why, not just the prediction. It‚Äôs like trying to understand why a certain medicine works, not just that it works."
  },
  {
    "objectID": "qmd/islp2.html#how-do-we-estimate-f",
    "href": "qmd/islp2.html#how-do-we-estimate-f",
    "title": "Introduction to Statistical Learning",
    "section": "How Do We Estimate f?",
    "text": "How Do We Estimate f?\nWe use training data to ‚Äúteach‚Äù our statistical learning method how to estimate f. It‚Äôs like learning from examples."
  },
  {
    "objectID": "qmd/islp2.html#estimating-f-training-data",
    "href": "qmd/islp2.html#estimating-f-training-data",
    "title": "Introduction to Statistical Learning",
    "section": "Estimating f: Training Data",
    "text": "Estimating f: Training Data\n\nTraining data: A set of observed data points: {(x‚ÇÅ, y‚ÇÅ), (x‚ÇÇ, y‚ÇÇ), ‚Ä¶, (x‚Çô, y‚Çô)}, where:\n\nx·µ¢ = (x·µ¢‚ÇÅ, x·µ¢‚ÇÇ, ‚Ä¶, x·µ¢‚Çö)·µÄ is the vector of predictor values for the ith observation. This is like a set of measurements for each individual or item in our dataset.\ny·µ¢ is the response value for the ith observation. This is the outcome we observe for each individual or item.\n\nGoal: Find a function, fÃÇ, such that Y ‚âà fÃÇ(X) for any observation (X, Y). We want our estimated function to be close to the true function for all possible data points. We want our model to generalize well to new data.\nTwo broad approaches: Parametric and non-parametric methods. These are like two different strategies for learning ‚Äì one involves making assumptions, the other doesn‚Äôt."
  },
  {
    "objectID": "qmd/islp2.html#parametric-methods",
    "href": "qmd/islp2.html#parametric-methods",
    "title": "Introduction to Statistical Learning",
    "section": "Parametric Methods",
    "text": "Parametric Methods\nA two-step, model-based approach. We make an assumption about the shape of f. It‚Äôs like assuming a specific recipe for a dish."
  },
  {
    "objectID": "qmd/islp2.html#parametric-methods-steps",
    "href": "qmd/islp2.html#parametric-methods-steps",
    "title": "Introduction to Statistical Learning",
    "section": "Parametric Methods: Steps",
    "text": "Parametric Methods: Steps\n\nAssume a functional form for f. For example, assume f is linear:\n\\[\nf(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p\n\\]\nThis reduces the problem to estimating the p + 1 coefficients (Œ≤‚ÇÄ, Œ≤‚ÇÅ, ‚Ä¶, Œ≤‚Çö). We‚Äôve simplified the problem to estimating a fixed number of parameters. It‚Äôs like assuming the dish can be made with a specific set of ingredients and specific proportions.\n\nŒ≤‚ÇÄ is the intercept (the value of Y when all X‚Äôs are zero).\nŒ≤‚ÇÅ, Œ≤‚ÇÇ, ‚Ä¶, Œ≤‚Çö are the slopes (the change in Y for a one-unit increase in each X).\n\nUse training data to fit or train the model. Find the values of the parameters (Œ≤‚ÇÄ, Œ≤‚ÇÅ, ‚Ä¶, Œ≤‚Çö) that best fit the data. A common method is (ordinary) least squares. We use the data to find the best values for these parameters. It‚Äôs like adjusting the proportions of the ingredients to get the best taste, based on tasting (the training data)."
  },
  {
    "objectID": "qmd/islp2.html#parametric-methods-simplification",
    "href": "qmd/islp2.html#parametric-methods-simplification",
    "title": "Introduction to Statistical Learning",
    "section": "Parametric Methods: Simplification",
    "text": "Parametric Methods: Simplification\n\n\n\n\n\n\nNote\n\n\n\nParametric methods simplify the problem by assuming a specific form for f.\n\n\nThis simplification makes the problem much easier to solve. It‚Äôs like having a recipe ‚Äì it makes cooking easier, but it limits the possible variations of the dish."
  },
  {
    "objectID": "qmd/islp2.html#example-linear-model-fit-to-income-data",
    "href": "qmd/islp2.html#example-linear-model-fit-to-income-data",
    "title": "Introduction to Statistical Learning",
    "section": "Example: Linear Model Fit to Income Data",
    "text": "Example: Linear Model Fit to Income Data\nLet‚Äôs see how a linear model fits the income data."
  },
  {
    "objectID": "qmd/islp2.html#linear-model-fit-visualization",
    "href": "qmd/islp2.html#linear-model-fit-visualization",
    "title": "Introduction to Statistical Learning",
    "section": "Linear Model Fit: Visualization",
    "text": "Linear Model Fit: Visualization\n\n\n\n\n\n\n\n\n\nLinear Model Fit"
  },
  {
    "objectID": "qmd/islp2.html#linear-model-fit-explanation",
    "href": "qmd/islp2.html#linear-model-fit-explanation",
    "title": "Introduction to Statistical Learning",
    "section": "Linear Model Fit: Explanation",
    "text": "Linear Model Fit: Explanation\n\n\n\n\n\n\n\n\n\nLinear Model Fit\n\n\n\n\n\n\n\nA linear model (yellow plane) fit to the Income data (Figure 2.3).\nRed dots are the observed data points.\nThe model assumes: income ‚âà Œ≤‚ÇÄ + Œ≤‚ÇÅ √ó education + Œ≤‚ÇÇ √ó seniority.\nThe yellow plane represents the prediction from the linear model. It‚Äôs the best-fitting plane we can find within the constraint of a linear relationship."
  },
  {
    "objectID": "qmd/islp2.html#linear-model-fit-explanation-contd",
    "href": "qmd/islp2.html#linear-model-fit-explanation-contd",
    "title": "Introduction to Statistical Learning",
    "section": "Linear Model Fit: Explanation (Cont‚Äôd)",
    "text": "Linear Model Fit: Explanation (Cont‚Äôd)\n\n\n\n\n\n\n\n\n\nLinear Model Fit\n\n\n\n\n\n\n\nLinear model is relatively inflexible because it can generate linear functions only. It can only capture relationships that are approximately linear. It can‚Äôt capture curves or more complex patterns."
  },
  {
    "objectID": "qmd/islp2.html#parametric-methods-advantages-and-disadvantages",
    "href": "qmd/islp2.html#parametric-methods-advantages-and-disadvantages",
    "title": "Introduction to Statistical Learning",
    "section": "Parametric Methods: Advantages and Disadvantages",
    "text": "Parametric Methods: Advantages and Disadvantages\nLet‚Äôs weigh the pros and cons of parametric methods."
  },
  {
    "objectID": "qmd/islp2.html#parametric-methods-pros-and-cons",
    "href": "qmd/islp2.html#parametric-methods-pros-and-cons",
    "title": "Introduction to Statistical Learning",
    "section": "Parametric Methods: Pros and Cons",
    "text": "Parametric Methods: Pros and Cons\n\nAdvantage: Simplifies the problem of estimating f. It‚Äôs easier to estimate a few parameters than an entirely arbitrary function. Computationally efficient. It‚Äôs like having a recipe ‚Äì it‚Äôs easier to follow than to invent a dish from scratch.\nDisadvantage: The assumed form of f might be wrong. If the true f is very different from our assumed form, our estimate will be poor. The model might be too simple to capture the true relationship. It‚Äôs like trying to make a cake using a cookie recipe ‚Äì it won‚Äôt work very well.\nOverfitting: If we use a very complex (flexible) model, we might overfit the data. This means the model follows the noise (random error) too closely, resulting in poor predictions on new data. The model might be too complex and capture noise instead of the true signal. It‚Äôs like memorizing the training data instead of learning the underlying pattern."
  },
  {
    "objectID": "qmd/islp2.html#non-parametric-methods",
    "href": "qmd/islp2.html#non-parametric-methods",
    "title": "Introduction to Statistical Learning",
    "section": "Non-parametric Methods",
    "text": "Non-parametric Methods\nDon‚Äôt make assumptions about the shape of f. Let the data speak for itself! It‚Äôs like cooking without a recipe ‚Äì you rely on your senses and experience."
  },
  {
    "objectID": "qmd/islp2.html#non-parametric-methods-definition",
    "href": "qmd/islp2.html#non-parametric-methods-definition",
    "title": "Introduction to Statistical Learning",
    "section": "Non-parametric Methods: Definition",
    "text": "Non-parametric Methods: Definition\n\nDo not make explicit assumptions about the functional form of f. We don‚Äôt assume a specific equation for the relationship.\nSeek an estimate of f that gets as close to the data points as possible, without being too rough or wiggly. Try to find a smooth curve that fits the data well. It‚Äôs like trying to draw a smooth curve through a set of points.\nAdvantage: Can accurately fit a wider range of possible shapes for f. Avoids the risk of making a wrong assumption about the form of f. More flexible and can capture more complex relationships. It‚Äôs like being able to cook any dish, not just those with a recipe.\nDisadvantage: Requires a very large number of observations to get an accurate estimate of f. Can be computationally expensive. It‚Äôs like needing a lot of experience to cook without a recipe."
  },
  {
    "objectID": "qmd/islp2.html#example-thin-plate-spline-fit-to-income-data",
    "href": "qmd/islp2.html#example-thin-plate-spline-fit-to-income-data",
    "title": "Introduction to Statistical Learning",
    "section": "Example: Thin-Plate Spline Fit to Income Data",
    "text": "Example: Thin-Plate Spline Fit to Income Data\nLet‚Äôs see a non-parametric method in action."
  },
  {
    "objectID": "qmd/islp2.html#thin-plate-spline-visualization",
    "href": "qmd/islp2.html#thin-plate-spline-visualization",
    "title": "Introduction to Statistical Learning",
    "section": "Thin-Plate Spline: Visualization",
    "text": "Thin-Plate Spline: Visualization\n\n\n\n\n\n\n\n\n\nThin-Plate Spline Fit"
  },
  {
    "objectID": "qmd/islp2.html#thin-plate-spline-explanation",
    "href": "qmd/islp2.html#thin-plate-spline-explanation",
    "title": "Introduction to Statistical Learning",
    "section": "Thin-Plate Spline: Explanation",
    "text": "Thin-Plate Spline: Explanation\n\n\n\n\n\n\n\n\n\nThin-Plate Spline Fit\n\n\n\n\n\n\n\nA thin-plate spline (yellow surface) fit to the Income data. A thin-plate spline is a flexible method that can fit a wide variety of shapes.\nThis is a non-parametric method. No pre-specified model is assumed. The shape of the surface is determined entirely by the data."
  },
  {
    "objectID": "qmd/islp2.html#thin-plate-spline-explanationcontd",
    "href": "qmd/islp2.html#thin-plate-spline-explanationcontd",
    "title": "Introduction to Statistical Learning",
    "section": "Thin-Plate Spline: Explanation(Cont‚Äôd)",
    "text": "Thin-Plate Spline: Explanation(Cont‚Äôd)\n\n\n\n\n\n\n\n\n\nThin-Plate Spline Fit\n\n\n\n\n\n\n\nThe fit is much closer to the true f (Figure 2.3) than the linear fit. It captures the non-linear relationship between income, education, and seniority more accurately.\nThis is a smooth fit. It captures the general trend without being too wiggly. It‚Äôs not overly sensitive to individual data points."
  },
  {
    "objectID": "qmd/islp2.html#example-overfitting-with-a-rough-spline",
    "href": "qmd/islp2.html#example-overfitting-with-a-rough-spline",
    "title": "Introduction to Statistical Learning",
    "section": "Example: Overfitting with a Rough Spline",
    "text": "Example: Overfitting with a Rough Spline\nLet‚Äôs see what happens when we make the non-parametric model too flexible."
  },
  {
    "objectID": "qmd/islp2.html#rough-spline-visualization",
    "href": "qmd/islp2.html#rough-spline-visualization",
    "title": "Introduction to Statistical Learning",
    "section": "Rough Spline: Visualization",
    "text": "Rough Spline: Visualization\n\n\n\n\n\n\n\n\n\nRough Spline Fit"
  },
  {
    "objectID": "qmd/islp2.html#rough-spline-explanation",
    "href": "qmd/islp2.html#rough-spline-explanation",
    "title": "Introduction to Statistical Learning",
    "section": "Rough Spline: Explanation",
    "text": "Rough Spline: Explanation\n\n\n\n\n\n\n\n\n\nRough Spline Fit\n\n\n\n\n\n\n\nSame data, but a rougher thin-plate spline fit. This spline is more flexible than the previous one.\nThis fit perfectly matches the training data (zero error on training data!). It goes through every single data point."
  },
  {
    "objectID": "qmd/islp2.html#rough-spline-explanation-contd",
    "href": "qmd/islp2.html#rough-spline-explanation-contd",
    "title": "Introduction to Statistical Learning",
    "section": "Rough Spline: Explanation (Cont‚Äôd)",
    "text": "Rough Spline: Explanation (Cont‚Äôd)\n\n\n\n\n\n\n\n\n\nRough Spline Fit\n\n\n\n\n\n\n\nBUT: This is an example of overfitting. The fit is too wiggly and will likely perform poorly on new data. It has captured the noise, not just the underlying pattern. It‚Äôs learned the training data too well. It‚Äôs like memorizing the answers to a specific set of questions, rather than understanding the concepts."
  },
  {
    "objectID": "qmd/islp2.html#the-trade-off-between-prediction-accuracy-and-model-interpretability",
    "href": "qmd/islp2.html#the-trade-off-between-prediction-accuracy-and-model-interpretability",
    "title": "Introduction to Statistical Learning",
    "section": "The Trade-Off Between Prediction Accuracy and Model Interpretability",
    "text": "The Trade-Off Between Prediction Accuracy and Model Interpretability\nThere‚Äôs a fundamental trade-off in statistical learning: accuracy vs.¬†interpretability. It‚Äôs like choosing between a powerful but complex tool and a simple but easy-to-use tool."
  },
  {
    "objectID": "qmd/islp2.html#flexibility-and-interpretability",
    "href": "qmd/islp2.html#flexibility-and-interpretability",
    "title": "Introduction to Statistical Learning",
    "section": "Flexibility and Interpretability",
    "text": "Flexibility and Interpretability\n\nFlexibility: How many different shapes of functions can the method fit?\n\nLess flexible (restrictive): Linear regression (only linear functions). Simpler models. Like a simple tool that can only do one thing.\nMore flexible: Thin-plate splines, neural networks. More complex models. Like a multi-purpose tool that can do many things.\n\nInterpretability: How easy is it to understand the fitted model?\n\nMore interpretable: Linear regression (easy to understand coefficients). We can easily see how each predictor affects the response. Like a simple tool with clear instructions.\nLess interpretable: Complex, non-linear models (hard to see how each predictor affects the response). ‚ÄúBlack box‚Äù models. Like a complex machine with no explanation of how it works.\n\nGeneral rule: As flexibility increases, interpretability decreases."
  },
  {
    "objectID": "qmd/islp2.html#the-trade-off-visualization",
    "href": "qmd/islp2.html#the-trade-off-visualization",
    "title": "Introduction to Statistical Learning",
    "section": "The Trade-Off: Visualization",
    "text": "The Trade-Off: Visualization\n\n\n\n\n\n\n\n\n\nFlexibility\nInterpretability\n\n\n\n\nLow (e.g., Linear Regression)\nHigh\n\n\nHigh (e.g., Neural Networks)\nLow\n\n\n\n\n\n\n\nImportant Trade-off: We often have to choose between more accurate, but less interpretable models, and simpler, more interpretable models. We can‚Äôt always have both! It‚Äôs like choosing between a detailed but confusing map and a simplified but easy-to-read map."
  },
  {
    "objectID": "qmd/islp2.html#why-choose-a-more-restrictive-method",
    "href": "qmd/islp2.html#why-choose-a-more-restrictive-method",
    "title": "Introduction to Statistical Learning",
    "section": "Why Choose a More Restrictive Method?",
    "text": "Why Choose a More Restrictive Method?\nWhy might we choose a simpler model, even if it‚Äôs less flexible?"
  },
  {
    "objectID": "qmd/islp2.html#restrictive-methods-advantages",
    "href": "qmd/islp2.html#restrictive-methods-advantages",
    "title": "Introduction to Statistical Learning",
    "section": "Restrictive Methods: Advantages",
    "text": "Restrictive Methods: Advantages\nEven if we only care about prediction, a more restrictive model (like linear regression) can sometimes outperform a more flexible model!\nReasons:\n\nInference: If we‚Äôre interested in understanding the relationship, restrictive models are more interpretable. Easier to explain to stakeholders. It‚Äôs like using a simple model that everyone can understand.\nOverfitting: Flexible models can overfit the training data, leading to poor predictions on new data. A simpler model might generalize better. Less likely to be fooled by noise. It‚Äôs like using a more robust model that‚Äôs less sensitive to quirks in the data.\nCurse of Dimensionality: With many predictors, flexible models can be hard to fit well and require huge amounts of data. Simpler models are more robust when data is limited. It‚Äôs like using a simpler model when you don‚Äôt have a lot of information."
  },
  {
    "objectID": "qmd/islp2.html#supervised-vs.-unsupervised-learning-1",
    "href": "qmd/islp2.html#supervised-vs.-unsupervised-learning-1",
    "title": "Introduction to Statistical Learning",
    "section": "Supervised vs.¬†Unsupervised Learning",
    "text": "Supervised vs.¬†Unsupervised Learning\nLet‚Äôs revisit the distinction between supervised and unsupervised learning."
  },
  {
    "objectID": "qmd/islp2.html#supervised-learning-definition",
    "href": "qmd/islp2.html#supervised-learning-definition",
    "title": "Introduction to Statistical Learning",
    "section": "Supervised Learning: Definition",
    "text": "Supervised Learning: Definition\n\nSupervised Learning: We have both predictors (X) and a response (Y) for each observation. We want to learn the relationship between X and Y. We have a ‚Äúteacher‚Äù (the response variable) guiding the learning process. It‚Äôs like learning with a teacher who provides feedback.\n\nExamples: Regression, classification. Predicting a numerical value or a category.\nMost of the methods in this book are supervised."
  },
  {
    "objectID": "qmd/islp2.html#unsupervised-learning-definition",
    "href": "qmd/islp2.html#unsupervised-learning-definition",
    "title": "Introduction to Statistical Learning",
    "section": "Unsupervised Learning: Definition",
    "text": "Unsupervised Learning: Definition\n\nUnsupervised Learning: We only have predictors (X), no response (Y). We want to find patterns and structure in the data. We‚Äôre exploring the data without a specific target in mind. It‚Äôs like exploring a new city without a map or a destination.\n\nExample: Cluster analysis (grouping observations into clusters). Finding groups of similar observations."
  },
  {
    "objectID": "qmd/islp2.html#semi-supervised-learning",
    "href": "qmd/islp2.html#semi-supervised-learning",
    "title": "Introduction to Statistical Learning",
    "section": "Semi-supervised Learning",
    "text": "Semi-supervised Learning\n\nSemi-supervised Learning: A mix. We have (X, Y) for some observations, but only X for others. We have some labeled data and some unlabeled data. It‚Äôs like learning with a teacher who provides some feedback, but also encourages independent exploration."
  },
  {
    "objectID": "qmd/islp2.html#supervised-vs.-unsupervised-clear-distinction",
    "href": "qmd/islp2.html#supervised-vs.-unsupervised-clear-distinction",
    "title": "Introduction to Statistical Learning",
    "section": "Supervised vs.¬†Unsupervised: Clear Distinction?",
    "text": "Supervised vs.¬†Unsupervised: Clear Distinction?\n\n\n\n\n\n\nNote\n\n\n\nThe distinction between supervised and unsupervised learning isn‚Äôt always clear-cut.\n\n\nSome methods can be used in both supervised and unsupervised settings. It‚Äôs like having a tool that can be used for different purposes."
  },
  {
    "objectID": "qmd/islp2.html#example-cluster-analysis",
    "href": "qmd/islp2.html#example-cluster-analysis",
    "title": "Introduction to Statistical Learning",
    "section": "Example: Cluster Analysis",
    "text": "Example: Cluster Analysis\nLet‚Äôs look at an example of unsupervised learning: cluster analysis."
  },
  {
    "objectID": "qmd/islp2.html#cluster-analysis-visualization",
    "href": "qmd/islp2.html#cluster-analysis-visualization",
    "title": "Introduction to Statistical Learning",
    "section": "Cluster Analysis: Visualization",
    "text": "Cluster Analysis: Visualization\n\n\n\n\n\n\n\n\n\nCluster Analysis Example"
  },
  {
    "objectID": "qmd/islp2.html#cluster-analysis-explanation-left",
    "href": "qmd/islp2.html#cluster-analysis-explanation-left",
    "title": "Introduction to Statistical Learning",
    "section": "Cluster Analysis: Explanation (Left)",
    "text": "Cluster Analysis: Explanation (Left)\n\n\n\n\n\n\n\n\n\nCluster Analysis Example\n\n\n\n\n\n\n\nLeft: 150 observations, two variables (X‚ÇÅ, X‚ÇÇ).\nThree well-separated groups (clusters). Clustering should easily identify these. The groups are distinct and easy to separate. It‚Äôs like having three clearly separated piles of different objects."
  },
  {
    "objectID": "qmd/islp2.html#cluster-analysis-explanation-right",
    "href": "qmd/islp2.html#cluster-analysis-explanation-right",
    "title": "Introduction to Statistical Learning",
    "section": "Cluster Analysis: Explanation (Right)",
    "text": "Cluster Analysis: Explanation (Right)\n\n\n\n\n\n\n\n\n\nCluster Analysis Example\n\n\n\n\n\n\n\nRight: Overlapping groups. Clustering is much harder. The groups are mixed together, making it difficult to find clear boundaries. It‚Äôs like having piles of objects that are mixed together."
  },
  {
    "objectID": "qmd/islp2.html#cluster-analysis-explanation-goal",
    "href": "qmd/islp2.html#cluster-analysis-explanation-goal",
    "title": "Introduction to Statistical Learning",
    "section": "Cluster Analysis: Explanation (Goal)",
    "text": "Cluster Analysis: Explanation (Goal)\n\n\n\n\n\n\n\nGoal: Identify distinct groups without knowing the group labels beforehand. We‚Äôre trying to find hidden structure in the data. It‚Äôs like trying to sort objects into groups without knowing what the groups should be.\nIn the examples shown, there are only two variables, and we can check the scatterplots to identify clusters. But in practice, we often have many more variables, making visual inspection impossible. We need to use clustering and other unsupervised learning approaches."
  },
  {
    "objectID": "qmd/islp2.html#regression-vs.-classification-problems",
    "href": "qmd/islp2.html#regression-vs.-classification-problems",
    "title": "Introduction to Statistical Learning",
    "section": "Regression vs.¬†Classification Problems",
    "text": "Regression vs.¬†Classification Problems\nWithin supervised learning, we have two main types of problems: regression and classification. It‚Äôs like having two different types of questions ‚Äì one asking ‚Äúhow much?‚Äù and the other asking ‚Äúwhich one?‚Äù."
  },
  {
    "objectID": "qmd/islp2.html#regression-definition",
    "href": "qmd/islp2.html#regression-definition",
    "title": "Introduction to Statistical Learning",
    "section": "Regression: Definition",
    "text": "Regression: Definition\n\nRegression: The response variable (Y) is quantitative (numerical).\n\nExample: Predicting income, house price, stock return, temperature, age. The response can take on a continuous range of values. It‚Äôs like asking ‚Äúhow much?‚Äù or ‚Äúhow many?‚Äù."
  },
  {
    "objectID": "qmd/islp2.html#classification-definition",
    "href": "qmd/islp2.html#classification-definition",
    "title": "Introduction to Statistical Learning",
    "section": "Classification: Definition",
    "text": "Classification: Definition\n\nClassification: The response variable (Y) is qualitative (categorical).\n\nExample: Predicting whether someone will default on a loan (yes/no), which brand of product they‚Äôll buy (A/B/C), or a medical diagnosis (disease 1/disease 2/no disease), email spam or not spam. The response belongs to one of a set of categories. It‚Äôs like asking ‚Äúwhich one?‚Äù or ‚Äúwhat type?‚Äù."
  },
  {
    "objectID": "qmd/islp2.html#regression-vs.-classification-overlap",
    "href": "qmd/islp2.html#regression-vs.-classification-overlap",
    "title": "Introduction to Statistical Learning",
    "section": "Regression vs.¬†Classification: Overlap",
    "text": "Regression vs.¬†Classification: Overlap\n\n\n\n\n\n\nNote\n\n\n\n\nSome methods are better suited to regression, others to classification. But many methods can be used for both.\nWhether the predictors are quantitative or qualitative is usually less important than the type of response.\n\n\n\nThe type of response variable (quantitative or qualitative) is the key distinction. It‚Äôs like the type of answer you‚Äôre looking for determines the type of question you ask."
  },
  {
    "objectID": "qmd/islp2.html#assessing-model-accuracy-regression",
    "href": "qmd/islp2.html#assessing-model-accuracy-regression",
    "title": "Introduction to Statistical Learning",
    "section": "Assessing Model Accuracy: Regression",
    "text": "Assessing Model Accuracy: Regression\nHow do we measure how well our model performs in a regression setting? How do we know if our predictions are good?"
  },
  {
    "objectID": "qmd/islp2.html#model-accuracy-regression---mse",
    "href": "qmd/islp2.html#model-accuracy-regression---mse",
    "title": "Introduction to Statistical Learning",
    "section": "Model Accuracy: Regression - MSE",
    "text": "Model Accuracy: Regression - MSE\n\nGoal: Quantify how well our predictions match the observed data. We want our predictions to be close to the true values. It‚Äôs like measuring how close our darts are to the bullseye.\nMean Squared Error (MSE): A common measure in regression:\n\\[\n\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{f}(x_i))^2\n\\]\n\ny·µ¢: The true response value for the ith observation. The actual value.\nfÃÇ(x·µ¢): The predicted response value for the ith observation. Our model‚Äôs prediction.\nLower MSE is better (closer predictions). We want the average squared difference between our predictions and the true values to be small. It‚Äôs like wanting the average distance of our darts from the bullseye to be small."
  },
  {
    "objectID": "qmd/islp2.html#training-mse-vs.-test-mse",
    "href": "qmd/islp2.html#training-mse-vs.-test-mse",
    "title": "Introduction to Statistical Learning",
    "section": "Training MSE vs.¬†Test MSE",
    "text": "Training MSE vs.¬†Test MSE\nWe need to distinguish between how well our model fits the data it was trained on and how well it generalizes to new data."
  },
  {
    "objectID": "qmd/islp2.html#training-vs.-test-mse",
    "href": "qmd/islp2.html#training-vs.-test-mse",
    "title": "Introduction to Statistical Learning",
    "section": "Training vs.¬†Test MSE",
    "text": "Training vs.¬†Test MSE\n\nWe usually don‚Äôt care how well the model fits the training data. We care about how well it predicts new data (the test data). It‚Äôs like caring about how well a student performs on a real exam, not just on practice questions.\nA model with low training MSE might have high test MSE (overfitting!). It might be memorizing the training data instead of learning the underlying pattern. It‚Äôs like a student who memorizes the answers to practice questions but doesn‚Äôt understand the concepts.\nTraining MSE: Calculated using the training data. How well the model fits the data it was trained on.\nTest MSE: Calculated using new, unseen data (test data). This is what we really care about! This measures how well our model will perform in the real world. It‚Äôs like evaluating the model on data it has never seen before."
  },
  {
    "objectID": "qmd/islp2.html#training-vs.-test-mse-ideal-scenario",
    "href": "qmd/islp2.html#training-vs.-test-mse-ideal-scenario",
    "title": "Introduction to Statistical Learning",
    "section": "Training vs.¬†Test MSE: Ideal Scenario",
    "text": "Training vs.¬†Test MSE: Ideal Scenario\n\nIdeally: We‚Äôd choose the model with the lowest test MSE.\nProblem: We often don‚Äôt have test data when building the model.\nSolution: Techniques like cross-validation (Chapter 5) can help us estimate the test MSE using the training data. It‚Äôs like simulating a real exam using the practice questions."
  },
  {
    "objectID": "qmd/islp2.html#example-training-and-test-mse-vs.-flexibility",
    "href": "qmd/islp2.html#example-training-and-test-mse-vs.-flexibility",
    "title": "Introduction to Statistical Learning",
    "section": "Example: Training and Test MSE vs.¬†Flexibility",
    "text": "Example: Training and Test MSE vs.¬†Flexibility\nLet‚Äôs see how training and test MSE change as we vary model flexibility."
  },
  {
    "objectID": "qmd/islp2.html#mse-vs.-flexibility-visualization",
    "href": "qmd/islp2.html#mse-vs.-flexibility-visualization",
    "title": "Introduction to Statistical Learning",
    "section": "MSE vs.¬†Flexibility: Visualization",
    "text": "MSE vs.¬†Flexibility: Visualization\n\n\n\n\n\n\n\n\n\nMSE vs.¬†Flexibility"
  },
  {
    "objectID": "qmd/islp2.html#mse-vs.-flexibility-explanation-left",
    "href": "qmd/islp2.html#mse-vs.-flexibility-explanation-left",
    "title": "Introduction to Statistical Learning",
    "section": "MSE vs.¬†Flexibility: Explanation (Left)",
    "text": "MSE vs.¬†Flexibility: Explanation (Left)\n\n\n\n\n\n\n\n\n\nMSE vs.¬†Flexibility\n\n\n\n\n\n\n\nLeft: Data simulated from a non-linear f (black curve). The true relationship is a curve.\n\nThree fits:\n\nlinear (orange, a straight line, less flexible),\nsmooth spline (blue, a moderately flexible curve),\nwiggly spline (green, a very flexible curve)."
  },
  {
    "objectID": "qmd/islp2.html#mse-vs.-flexibility-explanation-right",
    "href": "qmd/islp2.html#mse-vs.-flexibility-explanation-right",
    "title": "Introduction to Statistical Learning",
    "section": "MSE vs.¬†Flexibility: Explanation (Right)",
    "text": "MSE vs.¬†Flexibility: Explanation (Right)\n\n\n\n\n\n\n\n\n\nMSE vs.¬†Flexibility\n\n\n\n\n\n\n\nRight:\n\nTraining MSE (grey curve): Decreases as flexibility increases. More flexible models fit the training data better. The green curve has the lowest training MSE.\nTest MSE (red curve): U-shaped. Decreases, then increases (overfitting). Too much flexibility leads to poor generalization. The blue curve has the lowest test MSE.\nDashed line: Minimum possible test MSE (irreducible error). Even the best possible model can‚Äôt achieve zero test MSE."
  },
  {
    "objectID": "qmd/islp2.html#mse-vs.-flexibility-explanation-observation",
    "href": "qmd/islp2.html#mse-vs.-flexibility-explanation-observation",
    "title": "Introduction to Statistical Learning",
    "section": "MSE vs.¬†Flexibility: Explanation (Observation)",
    "text": "MSE vs.¬†Flexibility: Explanation (Observation)\n\n\n\n\n\n\n\n\n\nMSE vs.¬†Flexibility\n\n\n\n\n\n\n\nObservation: The blue curve (moderate flexibility) has the lowest test MSE. This is the ‚Äúsweet spot‚Äù. It‚Äôs like Goldilocks finding the porridge that‚Äôs ‚Äújust right‚Äù."
  },
  {
    "objectID": "qmd/islp2.html#the-bias-variance-trade-off",
    "href": "qmd/islp2.html#the-bias-variance-trade-off",
    "title": "Introduction to Statistical Learning",
    "section": "The Bias-Variance Trade-Off",
    "text": "The Bias-Variance Trade-Off\nThe U-shape in the test MSE curve is due to two competing properties: bias and variance. It‚Äôs like a seesaw ‚Äì as one goes up, the other goes down."
  },
  {
    "objectID": "qmd/islp2.html#bias-and-variance-definitions",
    "href": "qmd/islp2.html#bias-and-variance-definitions",
    "title": "Introduction to Statistical Learning",
    "section": "Bias and Variance: Definitions",
    "text": "Bias and Variance: Definitions\nThe U-shape in the test MSE curve is due to two competing properties:\n\nVariance: How much would our estimate of f (fÃÇ) change if we used a different training set?\n\nHigh variance: fÃÇ changes a lot with different training sets (typical of flexible models). The model is sensitive to the specific training data. It‚Äôs like a dart player who is inconsistent ‚Äì their throws vary a lot.\nLow variance: fÃÇ is relatively stable (typical of less flexible models). The model is less sensitive to the specific training data. It‚Äôs like a dart player who is consistent ‚Äì their throws are always close together.\n\nBias: The error introduced by approximating a complex real-world problem with a simpler model.\n\nHigh bias: The model makes strong (and possibly wrong) assumptions about f (typical of less flexible models). The model is too simple to capture the true relationship. It‚Äôs like trying to fit a square peg in a round hole.\nLow bias: The model makes fewer assumptions (typical of flexible models). The model is flexible enough to capture the true relationship. It‚Äôs like using a moldable material that can fit any shape."
  },
  {
    "objectID": "qmd/islp2.html#bias-variance-tradeoff-decomposition-of-test-mse",
    "href": "qmd/islp2.html#bias-variance-tradeoff-decomposition-of-test-mse",
    "title": "Introduction to Statistical Learning",
    "section": "Bias-Variance Tradeoff: Decomposition of Test MSE",
    "text": "Bias-Variance Tradeoff: Decomposition of Test MSE\n\nExpected test MSE at x0 can be decomposed to: \\[E(y_0 - \\hat{f}(x_0))^2 = Var(\\hat{f}(x_0)) + [Bias(\\hat{f}(x_0))]^2 + Var(\\epsilon)\\]\nThis equation shows that the expected test MSE is the sum of the variance of our estimate, the squared bias of our estimate, and the variance of the error term. It‚Äôs like saying the total error is the sum of errors due to inconsistency, errors due to oversimplification, and errors due to randomness.\nTo minimize the expected test error, we need to select a statistical learning method that simultaneously achieves low variance and low bias."
  },
  {
    "objectID": "qmd/islp2.html#bias-variance-trade-off-illustration",
    "href": "qmd/islp2.html#bias-variance-trade-off-illustration",
    "title": "Introduction to Statistical Learning",
    "section": "Bias-Variance Trade-Off: Illustration",
    "text": "Bias-Variance Trade-Off: Illustration\nLet‚Äôs visualize the bias-variance trade-off."
  },
  {
    "objectID": "qmd/islp2.html#bias-variance-trade-off-visualization",
    "href": "qmd/islp2.html#bias-variance-trade-off-visualization",
    "title": "Introduction to Statistical Learning",
    "section": "Bias-Variance Trade-Off: Visualization",
    "text": "Bias-Variance Trade-Off: Visualization\n\n\n\n\n\n\n\n\n\nBias-Variance Trade-Off"
  },
  {
    "objectID": "qmd/islp2.html#bias-variance-trade-off-explanation-left",
    "href": "qmd/islp2.html#bias-variance-trade-off-explanation-left",
    "title": "Introduction to Statistical Learning",
    "section": "Bias-Variance Trade-Off: Explanation (Left)",
    "text": "Bias-Variance Trade-Off: Explanation (Left)\n\n\n\n\n\n\n\n\n\nBias-Variance Trade-Off\n\n\n\n\n\n\n\nSquared bias (blue), variance (orange), irreducible error (dashed), and test MSE (red) for the three examples in Figure 2.9.\nLeft (non-linear f): Bias decreases rapidly, variance increases slowly. As flexibility increases, bias decreases quickly, but variance increases gradually."
  },
  {
    "objectID": "qmd/islp2.html#bias-variance-trade-off-explanation-center",
    "href": "qmd/islp2.html#bias-variance-trade-off-explanation-center",
    "title": "Introduction to Statistical Learning",
    "section": "Bias-Variance Trade-Off: Explanation (Center)",
    "text": "Bias-Variance Trade-Off: Explanation (Center)\n\n\n\n\n\n\n\n\n\nBias-Variance Trade-Off\n\n\n\n\n\n\n\nCenter (nearly linear f): Bias is low, variance increases quickly. When the true relationship is close to linear, even simple models have low bias, but variance increases rapidly with flexibility."
  },
  {
    "objectID": "qmd/islp2.html#bias-variance-trade-off-explanation-right",
    "href": "qmd/islp2.html#bias-variance-trade-off-explanation-right",
    "title": "Introduction to Statistical Learning",
    "section": "Bias-Variance Trade-Off: Explanation (Right)",
    "text": "Bias-Variance Trade-Off: Explanation (Right)\n\n\n\n\n\n\n\n\n\nBias-Variance Trade-Off\n\n\n\n\n\n\n\nRight (very non-linear f): Bias decreases dramatically, variance is low. When the true relationship is highly complex, flexible models are needed to reduce bias, and the increase in variance is less of a concern."
  },
  {
    "objectID": "qmd/islp2.html#bias-variance-trade-off-key-takeaway",
    "href": "qmd/islp2.html#bias-variance-trade-off-key-takeaway",
    "title": "Introduction to Statistical Learning",
    "section": "Bias-Variance Trade-Off: Key Takeaway",
    "text": "Bias-Variance Trade-Off: Key Takeaway\n\n\n\n\n\n\nKey takeaway: Good models need both low variance and low bias. This is a trade-off! We need to find the right balance between bias and variance. It‚Äôs like finding the sweet spot on a seesaw."
  },
  {
    "objectID": "qmd/islp2.html#assessing-model-accuracy-classification",
    "href": "qmd/islp2.html#assessing-model-accuracy-classification",
    "title": "Introduction to Statistical Learning",
    "section": "Assessing Model Accuracy: Classification",
    "text": "Assessing Model Accuracy: Classification\nHow do we measure model performance in a classification setting? How do we know if our classifier is making good predictions?"
  },
  {
    "objectID": "qmd/islp2.html#model-accuracy-classification---error-rate",
    "href": "qmd/islp2.html#model-accuracy-classification---error-rate",
    "title": "Introduction to Statistical Learning",
    "section": "Model Accuracy: Classification - Error Rate",
    "text": "Model Accuracy: Classification - Error Rate\n\nError Rate: The proportion of mistakes made by the classifier. We count the number of times our model predicts the wrong class. It‚Äôs like counting the number of wrong answers on a test.\n\nTraining error rate:\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} I(y_i \\ne \\hat{y}_i)\n\\]\n\ny·µ¢: True class label. The correct category.\n≈∑·µ¢: Predicted class label. Our model‚Äôs prediction.\nI(y·µ¢ ‚â† ≈∑·µ¢): Indicator variable (1 if mistake, 0 if correct). A function that is 1 if the prediction is wrong and 0 if it‚Äôs correct.\n\nTest error rate: Ave(I(y_0 ‚â† ≈∑‚ÇÄ)). This is what we care about! The average error rate on new, unseen data. It‚Äôs like the average error rate on a real exam.\n\nGoal: Choose the classifier with the lowest test error rate."
  },
  {
    "objectID": "qmd/islp2.html#the-bayes-classifier",
    "href": "qmd/islp2.html#the-bayes-classifier",
    "title": "Introduction to Statistical Learning",
    "section": "The Bayes Classifier",
    "text": "The Bayes Classifier\nThe Bayes classifier is the theoretical ‚Äúbest‚Äù classifier. It‚Äôs like the ideal student who always gets the right answer."
  },
  {
    "objectID": "qmd/islp2.html#bayes-classifier-definition",
    "href": "qmd/islp2.html#bayes-classifier-definition",
    "title": "Introduction to Statistical Learning",
    "section": "Bayes Classifier: Definition",
    "text": "Bayes Classifier: Definition\n\nThe ‚Äúideal‚Äù classifier: Assigns each observation to the most likely class, given its predictor values. It makes the best possible prediction based on the true probabilities. It‚Äôs like knowing the exact probability of each answer being correct and always choosing the most probable one.\nConditional probability: Pr(Y = j | X = x‚ÇÄ) - the probability that Y = j (class j), given the predictor values x‚ÇÄ. The probability of belonging to a specific category, given the observed features.\nBayes Classifier: Assigns an observation to the class j for which Pr(Y = j | X = x‚ÇÄ) is largest. Choose the class with the highest probability.\nBayes Decision Boundary: The points where the conditional probabilities for different classes are equal. This is the boundary between where we would predict different classes. It‚Äôs like the dividing line between different territories.\nBayes Error Rate: The lowest possible test error rate achievable. Analogous to the irreducible error. This is the best we can possibly do, even with perfect knowledge. It‚Äôs like the minimum possible error rate, even for the ideal student."
  },
  {
    "objectID": "qmd/islp2.html#example-bayes-classifier",
    "href": "qmd/islp2.html#example-bayes-classifier",
    "title": "Introduction to Statistical Learning",
    "section": "Example: Bayes Classifier",
    "text": "Example: Bayes Classifier\nLet‚Äôs visualize the Bayes classifier."
  },
  {
    "objectID": "qmd/islp2.html#bayes-classifier-visualization",
    "href": "qmd/islp2.html#bayes-classifier-visualization",
    "title": "Introduction to Statistical Learning",
    "section": "Bayes Classifier: Visualization",
    "text": "Bayes Classifier: Visualization\n\n\n\n\n\n\n\n\n\nBayes Classifier Example"
  },
  {
    "objectID": "qmd/islp2.html#bayes-classifier-explanation",
    "href": "qmd/islp2.html#bayes-classifier-explanation",
    "title": "Introduction to Statistical Learning",
    "section": "Bayes Classifier: Explanation",
    "text": "Bayes Classifier: Explanation\n\n\n\n\n\n\n\n\n\nBayes Classifier Example\n\n\n\n\n\n\n\nSimulated data, two classes (orange, blue).\nPurple dashed line: Bayes decision boundary. This is the line where the probability of belonging to the orange class is equal to the probability of belonging to the blue class."
  },
  {
    "objectID": "qmd/islp2.html#bayes-classifier-explanation-contd",
    "href": "qmd/islp2.html#bayes-classifier-explanation-contd",
    "title": "Introduction to Statistical Learning",
    "section": "Bayes Classifier: Explanation (Cont‚Äôd)",
    "text": "Bayes Classifier: Explanation (Cont‚Äôd)\n\n\n\n\n\n\n\n\n\nBayes Classifier Example\n\n\n\n\n\n\n\nOrange/blue shaded regions: Regions where the Bayes classifier would predict orange/blue.\nThe Bayes error rate is greater than zero because the classes overlap. Even the best classifier will make mistakes because the classes are not perfectly separable. It‚Äôs like having some questions on a test that are ambiguous, even for the best student."
  },
  {
    "objectID": "qmd/islp2.html#k-nearest-neighbors-knn",
    "href": "qmd/islp2.html#k-nearest-neighbors-knn",
    "title": "Introduction to Statistical Learning",
    "section": "K-Nearest Neighbors (KNN)",
    "text": "K-Nearest Neighbors (KNN)\nKNN is a practical method that approximates the Bayes classifier. It‚Äôs like a practical student who tries to learn from their peers."
  },
  {
    "objectID": "qmd/islp2.html#knn-motivation",
    "href": "qmd/islp2.html#knn-motivation",
    "title": "Introduction to Statistical Learning",
    "section": "KNN: Motivation",
    "text": "KNN: Motivation\n\nProblem: In reality, we don‚Äôt know the conditional distribution of Y given X. So, we can‚Äôt directly use the Bayes classifier. We don‚Äôt know the true probabilities. It‚Äôs like not knowing the exact probabilities of each answer being correct.\nKNN: A non-parametric method that estimates the conditional distribution and then classifies based on the estimate. It uses the training data to approximate the probabilities. It‚Äôs like looking at similar past exam questions and their answers to guess the answer to a new question."
  },
  {
    "objectID": "qmd/islp2.html#knn-algorithm",
    "href": "qmd/islp2.html#knn-algorithm",
    "title": "Introduction to Statistical Learning",
    "section": "KNN: Algorithm",
    "text": "KNN: Algorithm\n\nHow it works:\n\nGiven a test observation, x‚ÇÄ, find the K closest training observations (the ‚Äúneighborhood‚Äù). Find the K most similar examples in the training data.\nEstimate the conditional probability for class j as the fraction of neighbors in the neighborhood whose response value is j. Calculate the proportion of neighbors belonging to each class.\nClassify x‚ÇÄ to the class with the highest estimated probability. Choose the class that is most common among the neighbors."
  },
  {
    "objectID": "qmd/islp2.html#example-knn",
    "href": "qmd/islp2.html#example-knn",
    "title": "Introduction to Statistical Learning",
    "section": "Example: KNN",
    "text": "Example: KNN\nLet‚Äôs visualize how KNN works.\n\n\n\n\n\n\n\n\n\nKNN Example"
  },
  {
    "objectID": "qmd/islp2.html#knn-explanation-left",
    "href": "qmd/islp2.html#knn-explanation-left",
    "title": "Introduction to Statistical Learning",
    "section": "KNN: Explanation (Left)",
    "text": "KNN: Explanation (Left)\n\n\n\n\n\n\n\n\n\nKNN Example\n\n\n\n\n\n\n\nLeft: Small training set (6 blue, 6 orange). Black cross is the test observation. We want to predict the class of the black cross.\nCircle shows the 3 nearest neighbors (K=3): 2 blue, 1 orange. We find the three closest training observations to the black cross.\nKNN predicts ‚Äúblue‚Äù. Because the majority of the neighbors are blue."
  },
  {
    "objectID": "qmd/islp2.html#knn-explanation-right",
    "href": "qmd/islp2.html#knn-explanation-right",
    "title": "Introduction to Statistical Learning",
    "section": "KNN: Explanation (Right)",
    "text": "KNN: Explanation (Right)\n\n\n\n\n\n\n\n\n\nKNN Example\n\n\n\n\n\n\n\nRight: KNN decision boundary (K=3) for all possible values of X‚ÇÅ and X‚ÇÇ. This shows how KNN would classify any point in the space. This is like drawing a map showing which class KNN would predict for any combination of X‚ÇÅ and X‚ÇÇ.\nKNN can produce a decision boundary and classifier that‚Äôs close to Bayes Classifier."
  },
  {
    "objectID": "qmd/islp2.html#knn-the-choice-of-k",
    "href": "qmd/islp2.html#knn-the-choice-of-k",
    "title": "Introduction to Statistical Learning",
    "section": "KNN: The Choice of K",
    "text": "KNN: The Choice of K\nThe choice of K (the number of neighbors) is crucial in KNN. It‚Äôs like choosing how many friends to ask for advice."
  },
  {
    "objectID": "qmd/islp2.html#knn-choosing-k",
    "href": "qmd/islp2.html#knn-choosing-k",
    "title": "Introduction to Statistical Learning",
    "section": "KNN: Choosing K",
    "text": "KNN: Choosing K\n\nThe choice of K (the number of neighbors) controls the flexibility of the KNN classifier.\n\nSmall K: More flexible, lower bias, higher variance (risk of overfitting). The decision boundary is more jagged. It‚Äôs like asking only a few close friends ‚Äì their opinions might be biased or vary a lot.\nLarge K: Less flexible, higher bias, lower variance. The decision boundary is smoother. It‚Äôs like asking many friends ‚Äì their opinions will be more stable but might not capture specific nuances.\n\nExample: Figure 2.15 and 2.16 shows KNN fits with different K.\nFinding the best K: We want to choose K to minimize the test error rate. Techniques like cross-validation can help. We need to find the value of K that gives the best generalization performance. It‚Äôs like finding the optimal number of friends to ask for advice to get the most reliable answer."
  },
  {
    "objectID": "qmd/islp2.html#knn-choosing-k-example-visualization-k1",
    "href": "qmd/islp2.html#knn-choosing-k-example-visualization-k1",
    "title": "Introduction to Statistical Learning",
    "section": "KNN: Choosing K, Example Visualization (K=1)",
    "text": "KNN: Choosing K, Example Visualization (K=1)\n\n\n\n\n\n\n\n\n\nKNN Example"
  },
  {
    "objectID": "qmd/islp2.html#knn-choosing-k-example-visualization-k100",
    "href": "qmd/islp2.html#knn-choosing-k-example-visualization-k100",
    "title": "Introduction to Statistical Learning",
    "section": "KNN: Choosing K, Example Visualization (K=100)",
    "text": "KNN: Choosing K, Example Visualization (K=100)\n\n\n\n\n\n\n\n\n\nKNN Example"
  },
  {
    "objectID": "qmd/islp2.html#knn-k1-vs-k100",
    "href": "qmd/islp2.html#knn-k1-vs-k100",
    "title": "Introduction to Statistical Learning",
    "section": "KNN: K=1 vs K=100",
    "text": "KNN: K=1 vs K=100\n\nFigure 2.15 (K=1): The KNN decision boundary is overly flexible, and follow the training data too closely.\nFigure 2.16 (K=100): The KNN decision boundary is almost linear and too inflexible."
  },
  {
    "objectID": "qmd/islp2.html#summary",
    "href": "qmd/islp2.html#summary",
    "title": "Introduction to Statistical Learning",
    "section": "Summary",
    "text": "Summary\nLet‚Äôs recap the key concepts we‚Äôve covered."
  },
  {
    "objectID": "qmd/islp2.html#summary-key-concepts",
    "href": "qmd/islp2.html#summary-key-concepts",
    "title": "Introduction to Statistical Learning",
    "section": "Summary: Key Concepts",
    "text": "Summary: Key Concepts\n\nStatistical learning is about estimating relationships between variables, for prediction and/or inference. It‚Äôs like using data to understand the world and make predictions.\nParametric methods assume a specific functional form; non-parametric methods don‚Äôt. It‚Äôs like choosing between cooking with a recipe and cooking without one.\nThere‚Äôs a trade-off between model flexibility and interpretability. It‚Äôs like choosing between a powerful but complex tool and a simple but easy-to-use tool.\nWe need to assess model accuracy using test data (or estimates of test error). It‚Äôs like evaluating a student on a real exam, not just practice questions.\nThe bias-variance trade-off is fundamental: Good models need both low bias and low variance. It‚Äôs like finding the right balance on a seesaw.\nIn classification, the Bayes classifier is optimal, but we often have to approximate it (e.g., with KNN). It‚Äôs like having an ideal student as a benchmark, but using practical methods to approximate their performance.\nChoosing the right level of flexibility is crucial. It‚Äôs like choosing the right tool for the job."
  },
  {
    "objectID": "qmd/islp2.html#thoughts-and-discussion",
    "href": "qmd/islp2.html#thoughts-and-discussion",
    "title": "Introduction to Statistical Learning",
    "section": "Thoughts and Discussion",
    "text": "Thoughts and Discussion\nLet‚Äôs think about some broader implications and questions."
  },
  {
    "objectID": "qmd/islp2.html#thoughts-and-discussion-questions",
    "href": "qmd/islp2.html#thoughts-and-discussion-questions",
    "title": "Introduction to Statistical Learning",
    "section": "Thoughts and Discussion: Questions",
    "text": "Thoughts and Discussion: Questions\n\nThink about real-world problems you‚Äôre interested in. Would you approach them with a focus on prediction, inference, or both? (e.g., predicting stock prices, understanding customer behavior, diagnosing diseases)\nCan you think of examples where a simple, interpretable model might be preferable to a more complex, ‚Äúblack box‚Äù model, even for prediction? (e.g., credit scoring, medical diagnosis where explainability is important)\nHow might the ‚Äúbest‚Äù model (in terms of test error) depend on the amount of data available? (e.g., with limited data, simpler models might be better; with lots of data, more complex models might be better)\nHow does the concept of ‚Äúoverfitting‚Äù relate to the bias-variance trade-off? (Overfitting is a result of high variance ‚Äì the model is too sensitive to the training data and doesn‚Äôt generalize well.)\nDiscuss the differences and similarities between supervised, unsupervised, and semi-supervised learning, and how they apply to real-world problems. (e.g.¬†Supervised: spam filtering; unsupervised: customer segmentation; semi-supervised: image classification with some labeled and some unlabeled images)"
  },
  {
    "objectID": "qmd/islp4.html",
    "href": "qmd/islp4.html",
    "title": "Statistical Learning: Classification",
    "section": "",
    "text": "So far, we‚Äôve journeyed through the world of regression, where we predicted numbers. Now, we‚Äôre switching gears to classification, where we predict categories. Think of it like sorting objects into different boxes! üì¶"
  },
  {
    "objectID": "qmd/islp4.html#welcome-to-classification",
    "href": "qmd/islp4.html#welcome-to-classification",
    "title": "Statistical Learning: Classification",
    "section": "",
    "text": "So far, we‚Äôve journeyed through the world of regression, where we predicted numbers. Now, we‚Äôre switching gears to classification, where we predict categories. Think of it like sorting objects into different boxes! üì¶"
  },
  {
    "objectID": "qmd/islp4.html#regression-vs.-classification-the-big-picture",
    "href": "qmd/islp4.html#regression-vs.-classification-the-big-picture",
    "title": "Statistical Learning: Classification",
    "section": "Regression vs.¬†Classification: The Big Picture",
    "text": "Regression vs.¬†Classification: The Big Picture\n\n\n\n\n\n\n\n\n\n\n\n\nRegression: Predicting a quantitative (numerical) response.\n\n\n\n\n\n\n\n\nExamples:\n\nPredicting house prices üè†\nEstimating sales revenue üí∞\nForecasting stock prices üìà"
  },
  {
    "objectID": "qmd/islp4.html#regression-vs.-classification-the-big-picture-continued",
    "href": "qmd/islp4.html#regression-vs.-classification-the-big-picture-continued",
    "title": "Statistical Learning: Classification",
    "section": "Regression vs.¬†Classification: The Big Picture (Continued)",
    "text": "Regression vs.¬†Classification: The Big Picture (Continued)\n\n\n\n\n\n\n\n\n\n\n\n\nClassification: Predicting a qualitative (categorical) response.\n\n\n\n\n\n\n\n\nExamples:\n\nDiagnosing a disease (present or absent) ‚öïÔ∏è\nFiltering spam emails (spam or not spam) üìß\nDetecting fraudulent transactions (fraud or not fraud) üí≥"
  },
  {
    "objectID": "qmd/islp4.html#what-is-classification",
    "href": "qmd/islp4.html#what-is-classification",
    "title": "Statistical Learning: Classification",
    "section": "What is Classification?",
    "text": "What is Classification?\n\n\n\n\n\n\n\nClassification is like sorting objects into distinct groups based on their features.\nWe assign each observation to a specific category or class.\n\n\n\n\n\n\n\n\n\ngraph LR\n    A[Observation 1] --&gt; C(Class A)\n    B[Observation 2] --&gt; D(Class B)\n    C --&gt; E[Classification]\n    D --&gt; E"
  },
  {
    "objectID": "qmd/islp4.html#classification-probabilities-first",
    "href": "qmd/islp4.html#classification-probabilities-first",
    "title": "Statistical Learning: Classification",
    "section": "Classification: Probabilities First",
    "text": "Classification: Probabilities First\n\nMany classification methods start by predicting the probability of an observation belonging to each category.\nThen, based on these probabilities (and a threshold), we make the final classification.\n\n\n\n\n\n\n\n\nExample:\n\nAn email: 70% chance of being spam, 30% chance of not being spam.\nWith a 50% threshold, we‚Äôd classify it as spam! üìß"
  },
  {
    "objectID": "qmd/islp4.html#classification-vs.-regression-a-clear-comparison",
    "href": "qmd/islp4.html#classification-vs.-regression-a-clear-comparison",
    "title": "Statistical Learning: Classification",
    "section": "Classification vs.¬†Regression: A Clear Comparison",
    "text": "Classification vs.¬†Regression: A Clear Comparison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nRegression\nClassification\n\n\n\n\nResponse Variable\nQuantitative (numerical)\nQualitative (categorical)\n\n\nGoal\nPredict a number\nPredict a category\n\n\nExample\nPredict house price\nPredict disease presence (yes/no)\n\n\nOutput\nContinuous value\nDiscrete category"
  },
  {
    "objectID": "qmd/islp4.html#why-not-linear-regression-for-categories",
    "href": "qmd/islp4.html#why-not-linear-regression-for-categories",
    "title": "Statistical Learning: Classification",
    "section": "Why NOT Linear Regression for Categories? ü§î",
    "text": "Why NOT Linear Regression for Categories? ü§î\n\nLinear regression is designed for numbers. What happens if we try to use it for categories? Let‚Äôs consider predicting a medical diagnosis with three possibilities:\n\nStroke\nDrug overdose\nEpileptic seizure"
  },
  {
    "objectID": "qmd/islp4.html#a-problematic-coding-scheme",
    "href": "qmd/islp4.html#a-problematic-coding-scheme",
    "title": "Statistical Learning: Classification",
    "section": "A Problematic Coding Scheme ü§ï",
    "text": "A Problematic Coding Scheme ü§ï\n\nWe could try to code these numerically:\n\n1 = Stroke\n2 = Drug overdose\n3 = Epileptic seizure\n\nBut this approach introduces serious problems!\n\n\n\n\nMedical conditions coding"
  },
  {
    "objectID": "qmd/islp4.html#the-problems-with-numerical-coding",
    "href": "qmd/islp4.html#the-problems-with-numerical-coding",
    "title": "Statistical Learning: Classification",
    "section": "The Problems with Numerical Coding ‚ùå",
    "text": "The Problems with Numerical Coding ‚ùå\n\n\n\n\n\n\n\nProblem 1: Arbitrary Order: The coding implies an order (stroke &lt; drug overdose &lt; epileptic seizure) that is medically meaningless.\nProblem 2: Equal Differences: It suggests equal differences between conditions (stroke to drug overdose = drug overdose to seizure), which is also not medically valid.\nProblem 3: Inconsistent Results: Different coding schemes (e.g., 1=drug overdose, 2=seizure, 3=stroke) would lead to completely different models and predictions!"
  },
  {
    "objectID": "qmd/islp4.html#binary-responses-a-special-case",
    "href": "qmd/islp4.html#binary-responses-a-special-case",
    "title": "Statistical Learning: Classification",
    "section": "Binary Responses: A Special Case?",
    "text": "Binary Responses: A Special Case?\n\nBinary Responses (Two Categories): If we have only two categories (yes/no, true/false), we can code them as 0 and 1. Linear regression can be used in this case.\nHowever:\n\nLinear regression might predict probabilities outside the [0, 1] range. This is nonsensical.\nBetter methods exist that are specifically designed for binary classification (like logistic regression!)."
  },
  {
    "objectID": "qmd/islp4.html#example-the-default-data",
    "href": "qmd/islp4.html#example-the-default-data",
    "title": "Statistical Learning: Classification",
    "section": "Example: The Default Data üí≥",
    "text": "Example: The Default Data üí≥\n\nWe‚Äôll use a simulated dataset called ‚ÄúDefault‚Äù to explore classification.\n\n\n\n\n\n\n\n\nGoal: Predict whether an individual will default on their credit card payment.\nResponse: default (Yes/No) ‚Äì a binary, qualitative variable.\nPredictors:\n\nbalance: Credit card balance (quantitative).\nincome: Annual income (quantitative)."
  },
  {
    "objectID": "qmd/islp4.html#visualizing-the-default-data",
    "href": "qmd/islp4.html#visualizing-the-default-data",
    "title": "Statistical Learning: Classification",
    "section": "Visualizing the Default Data",
    "text": "Visualizing the Default Data\n\n\n\nThe Default data set"
  },
  {
    "objectID": "qmd/islp4.html#default-data-left-panel---scatterplot",
    "href": "qmd/islp4.html#default-data-left-panel---scatterplot",
    "title": "Statistical Learning: Classification",
    "section": "Default Data: Left Panel - Scatterplot",
    "text": "Default Data: Left Panel - Scatterplot\n\n\n\n\n\n\n\n\n\nThe Default data set\n\n\n\n\n\n\nLeft Panel (Scatterplot):\n\nincome (x-axis) vs.¬†balance (y-axis).\nOrange: Individuals who defaulted.\nBlue: Individuals who did not default.\nWe see some separation: defaulters tend to have higher balances, but there‚Äôs overlap."
  },
  {
    "objectID": "qmd/islp4.html#default-data-center-panel---balance-boxplots",
    "href": "qmd/islp4.html#default-data-center-panel---balance-boxplots",
    "title": "Statistical Learning: Classification",
    "section": "Default Data: Center Panel - Balance Boxplots",
    "text": "Default Data: Center Panel - Balance Boxplots\n\n\n\n\n\n\n\n\n\nThe Default data set\n\n\n\n\n\n\nCenter Panel (Boxplots of Balance):\n\nCompares balance distribution for defaulters (orange) and non-defaulters (blue).\nThe orange boxplot is shifted higher, indicating higher balances for defaulters on average.\nBoxes show the interquartile range (IQR), and whiskers show the range (excluding outliers)."
  },
  {
    "objectID": "qmd/islp4.html#default-data-right-panel---income-boxplots",
    "href": "qmd/islp4.html#default-data-right-panel---income-boxplots",
    "title": "Statistical Learning: Classification",
    "section": "Default Data: Right Panel - Income Boxplots",
    "text": "Default Data: Right Panel - Income Boxplots\n\n\n\n\n\n\n\n\n\nThe Default data set\n\n\n\n\n\n\nRight Panel (Boxplots of Income):\n\nCompares income distribution for defaulters (orange) and non-defaulters (blue).\nThe relationship is much less clear than with balance. The boxplots largely overlap."
  },
  {
    "objectID": "qmd/islp4.html#default-data-key-takeaway",
    "href": "qmd/islp4.html#default-data-key-takeaway",
    "title": "Statistical Learning: Classification",
    "section": "Default Data: Key Takeaway",
    "text": "Default Data: Key Takeaway\n\nThe visualizations suggest that balance is a stronger predictor of default than income. Higher balances seem associated with a higher probability of default. This guides our model building."
  },
  {
    "objectID": "qmd/islp4.html#logistic-regression-modeling-the-probability",
    "href": "qmd/islp4.html#logistic-regression-modeling-the-probability",
    "title": "Statistical Learning: Classification",
    "section": "Logistic Regression: Modeling the Probability",
    "text": "Logistic Regression: Modeling the Probability\n\nLogistic regression doesn‚Äôt directly model the response (default). Instead, it models the probability that the response belongs to a specific category.\n\n\n\n\n\n\n\n\nSpecifically: We model Pr(default = Yes | balance, income) ‚Äì the probability of defaulting, given balance and income.\nThis probability will always be between 0 and 1, which makes perfect sense! üëç"
  },
  {
    "objectID": "qmd/islp4.html#logistic-regression-from-probability-to-classification",
    "href": "qmd/islp4.html#logistic-regression-from-probability-to-classification",
    "title": "Statistical Learning: Classification",
    "section": "Logistic Regression: From Probability to Classification",
    "text": "Logistic Regression: From Probability to Classification\n\nOnce we have the estimated probability of default, we can classify.\n\n\n\n\n\n\n\n\nExample:\n\nIf Pr(default = Yes | balance, income) &gt; 0.5, predict ‚Äúdefault = Yes‚Äù.\nIf Pr(default = Yes | balance, income) ‚â§ 0.5, predict ‚Äúdefault = No‚Äù.\n\nThe 0.5 threshold is common, but it‚Äôs adjustable."
  },
  {
    "objectID": "qmd/islp4.html#the-logistic-function-bending-between-0-and-1",
    "href": "qmd/islp4.html#the-logistic-function-bending-between-0-and-1",
    "title": "Statistical Learning: Classification",
    "section": "The Logistic Function: Bending Between 0 and 1",
    "text": "The Logistic Function: Bending Between 0 and 1\n\nWe need a function that always outputs a value between 0 and 1, no matter the input. The logistic function is perfect for this:\n\n\\[\np(X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}}\n\\]"
  },
  {
    "objectID": "qmd/islp4.html#logistic-function-breaking-it-down",
    "href": "qmd/islp4.html#logistic-function-breaking-it-down",
    "title": "Statistical Learning: Classification",
    "section": "Logistic Function: Breaking it Down",
    "text": "Logistic Function: Breaking it Down\n\np(X): Probability of the event (e.g., default) given predictor(s) X.\nŒ≤‚ÇÄ and Œ≤‚ÇÅ: Coefficients estimated from the data. They control the curve‚Äôs shape and position.\ne: The base of the natural logarithm (‚âà 2.718).\nCrucial Feature: No matter the value of Œ≤‚ÇÄ + Œ≤‚ÇÅX, the output p(X) is always between 0 and 1."
  },
  {
    "objectID": "qmd/islp4.html#the-logistic-function-an-s-shaped-curve",
    "href": "qmd/islp4.html#the-logistic-function-an-s-shaped-curve",
    "title": "Statistical Learning: Classification",
    "section": "The Logistic Function: An S-Shaped Curve",
    "text": "The Logistic Function: An S-Shaped Curve\n\nThe logistic function creates an S-shaped curve (a sigmoid).\n\n\n\n\n\n\ngraph LR\n    A[Low X] --&gt; B(Low p(X) ~ 0)\n    B --&gt; C{Increasing X}\n    C --&gt; D(Increasing p(X))\n    D --&gt; E[High X]\n    E --&gt; F(High p(X) ~ 1)\n    style C fill:#f9f,stroke:#333,stroke-width:2px\n\n\n\n\n\n\n\nAs X increases, p(X) smoothly transitions from near 0 to near 1."
  },
  {
    "objectID": "qmd/islp4.html#linear-vs.-logistic-a-visual-comparison",
    "href": "qmd/islp4.html#linear-vs.-logistic-a-visual-comparison",
    "title": "Statistical Learning: Classification",
    "section": "Linear vs.¬†Logistic: A Visual Comparison",
    "text": "Linear vs.¬†Logistic: A Visual Comparison\n\n\n\nClassification using the Default data"
  },
  {
    "objectID": "qmd/islp4.html#linear-regression-the-problem",
    "href": "qmd/islp4.html#linear-regression-the-problem",
    "title": "Statistical Learning: Classification",
    "section": "Linear Regression: The Problem",
    "text": "Linear Regression: The Problem\n\n\n\n\n\n\n\n\n\nClassification using the Default data\n\n\n\n\n\n\nLeft Panel (Linear Regression):\n\nBlue line: Fit of a linear regression to the default data (0/1 coded).\nMajor Issue: For low/high balance, predicted probabilities are outside [0, 1]! Probabilities can‚Äôt be negative or greater than 1. üôÅ"
  },
  {
    "objectID": "qmd/islp4.html#logistic-regression-the-solution",
    "href": "qmd/islp4.html#logistic-regression-the-solution",
    "title": "Statistical Learning: Classification",
    "section": "Logistic Regression: The Solution",
    "text": "Logistic Regression: The Solution\n\n\n\n\n\n\n\n\n\nClassification using the Default data\n\n\n\n\n\n\nRight Panel (Logistic Regression):\n\nOrange curve: Fit of a logistic regression.\nPredicted probabilities are always between 0 and 1, as they should be! üôÇ\nThe characteristic S-shape."
  },
  {
    "objectID": "qmd/islp4.html#linear-vs.-logistic-the-verdict",
    "href": "qmd/islp4.html#linear-vs.-logistic-the-verdict",
    "title": "Statistical Learning: Classification",
    "section": "Linear vs.¬†Logistic: The Verdict",
    "text": "Linear vs.¬†Logistic: The Verdict\n\nLogistic regression is far more appropriate for modeling probabilities and binary outcomes. It respects the fundamental constraint that probabilities must be between 0 and 1."
  },
  {
    "objectID": "qmd/islp4.html#odds-another-way-to-think-about-probability",
    "href": "qmd/islp4.html#odds-another-way-to-think-about-probability",
    "title": "Statistical Learning: Classification",
    "section": "Odds: Another Way to Think About Probability",
    "text": "Odds: Another Way to Think About Probability\n\nWe can rewrite the logistic model to highlight its connection to odds:\n\n\\[\n\\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1X}\n\\]\n\nLeft-hand side: The odds of the event (e.g., default).\n\nOdds range from 0 to ‚àû.\nOdds = 1: Event is equally likely to happen or not.\nOdds &gt; 1: Event is more likely to happen.\nOdds &lt; 1: Event is less likely to happen.\n\nRelationship: Odds = p / (1 - p)\nExample: If p(X) = 0.8, odds are 0.8 / (1 - 0.8) = 4. The event is four times more likely to happen than not."
  },
  {
    "objectID": "qmd/islp4.html#log-odds-logit-the-linear-connection",
    "href": "qmd/islp4.html#log-odds-logit-the-linear-connection",
    "title": "Statistical Learning: Classification",
    "section": "Log-Odds (Logit): The Linear Connection",
    "text": "Log-Odds (Logit): The Linear Connection\n\nTaking the logarithm of both sides of the odds equation:\n\n\\[\n\\log\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + \\beta_1X\n\\]\n\nLeft-hand side: The log-odds or logit.\nRight-hand side: A linear function of X.\nKey Insight: Logistic regression models the log-odds as a linear function of the predictors. This explains the ‚Äúlogistic‚Äù in its name, even though it‚Äôs for classification!"
  },
  {
    "objectID": "qmd/islp4.html#interpreting-logistic-regression-coefficients",
    "href": "qmd/islp4.html#interpreting-logistic-regression-coefficients",
    "title": "Statistical Learning: Classification",
    "section": "Interpreting Logistic Regression Coefficients",
    "text": "Interpreting Logistic Regression Coefficients\n\nLinear Regression: Œ≤‚ÇÅ is the average change in Y for a one-unit increase in X.\nLogistic Regression: The interpretation is different because we‚Äôre modeling the log-odds.\n\n\n\n\n\n\n\n\nŒ≤‚ÇÅ is the change in the log-odds for a one-unit increase in X.\nEquivalently, a one-unit increase in X multiplies the odds by eŒ≤‚ÇÅ.\nBecause of the non-linear S-curve, the amount p(X) changes for a one-unit increase in X depends on the current value of X. The change is not probability."
  },
  {
    "objectID": "qmd/islp4.html#estimating-coefficients-maximum-likelihood-mle",
    "href": "qmd/islp4.html#estimating-coefficients-maximum-likelihood-mle",
    "title": "Statistical Learning: Classification",
    "section": "Estimating Coefficients: Maximum Likelihood (MLE)",
    "text": "Estimating Coefficients: Maximum Likelihood (MLE)\n\nWe use maximum likelihood estimation (MLE) to find the best estimates for Œ≤‚ÇÄ, Œ≤‚ÇÅ, etc.\nGoal of MLE: Find the coefficient values that make the observed data most likely. We want to maximize the probability of observing the actual outcomes (defaults/non-defaults) in our training data, given the coefficients."
  },
  {
    "objectID": "qmd/islp4.html#the-likelihood-function-making-data-likely",
    "href": "qmd/islp4.html#the-likelihood-function-making-data-likely",
    "title": "Statistical Learning: Classification",
    "section": "The Likelihood Function: Making Data Likely",
    "text": "The Likelihood Function: Making Data Likely\n\nThe likelihood function for logistic regression is:\n\n\\[\nl(\\beta_0, \\beta_1) = \\prod_{i:y_i=1} p(x_i) \\prod_{i':y_{i'}=0} (1 - p(x_{i'}))\n\\]"
  },
  {
    "objectID": "qmd/islp4.html#likelihood-function-demystified",
    "href": "qmd/islp4.html#likelihood-function-demystified",
    "title": "Statistical Learning: Classification",
    "section": "Likelihood Function: Demystified",
    "text": "Likelihood Function: Demystified\n\nThis formula might look complex, but the core idea is simple.\nWe multiply the probabilities of observing each data point, given coefficients Œ≤‚ÇÄ and Œ≤‚ÇÅ.\nFirst Product: Œ†i:y·µ¢=1 p(x·µ¢)\n\nConsiders individuals who did default (y·µ¢ = 1).\nFor each, p(x·µ¢) is the probability of default, given their predictors (x·µ¢) and the current coefficients.\n\nSecond Product: Œ†i‚Äô:y·µ¢‚Äô=0 (1 - p(x·µ¢‚Äô))\n\nConsiders individuals who did not default (y·µ¢‚Äô = 0).\nFor each, 1 - p(x·µ¢‚Äô) is the probability of not defaulting.\n\nOverall: Multiplying these probabilities gives the likelihood of the entire dataset, given the coefficients. MLE finds the coefficients that maximize this product."
  },
  {
    "objectID": "qmd/islp4.html#optimization-letting-the-computer-do-the-work",
    "href": "qmd/islp4.html#optimization-letting-the-computer-do-the-work",
    "title": "Statistical Learning: Classification",
    "section": "Optimization: Letting the Computer Do the Work",
    "text": "Optimization: Letting the Computer Do the Work\n\nWe don‚Äôt maximize the likelihood function by hand! Statistical software (like R) has built-in optimization algorithms.\nThese algorithms find the values of Œ≤‚ÇÄ, Œ≤‚ÇÅ, etc., that maximize the likelihood, giving us the maximum likelihood estimates."
  },
  {
    "objectID": "qmd/islp4.html#making-predictions-from-coefficients-to-probabilities",
    "href": "qmd/islp4.html#making-predictions-from-coefficients-to-probabilities",
    "title": "Statistical Learning: Classification",
    "section": "Making Predictions: From Coefficients to Probabilities",
    "text": "Making Predictions: From Coefficients to Probabilities\n\nWith the estimated coefficients (Œ≤ÃÇ‚ÇÄ, Œ≤ÃÇ‚ÇÅ, etc.), we can predict the probability of default for any given predictor values.\nPlug the values into the logistic function:\n\np(X) = e(Œ≤ÃÇ‚ÇÄ + Œ≤ÃÇ‚ÇÅX) / (1 + e(Œ≤ÃÇ‚ÇÄ + Œ≤ÃÇ‚ÇÅX))"
  },
  {
    "objectID": "qmd/islp4.html#prediction-example-putting-it-into-practice",
    "href": "qmd/islp4.html#prediction-example-putting-it-into-practice",
    "title": "Statistical Learning: Classification",
    "section": "Prediction Example: Putting it into Practice",
    "text": "Prediction Example: Putting it into Practice\n\nSuppose we get these estimated coefficients (illustrative values):\n\nŒ≤ÃÇ‚ÇÄ = -2\nŒ≤ÃÇ‚ÇÅ = 0.005 (where X is balance)\n\nPredict default probability for two individuals:\n\nIndividual A: balance = $1,000\nIndividual B: balance = $2,000\n\nCalculations:\n\nIndividual A: p(X) = e(-2 + 0.005 * 1000) / (1 + e(-2 + 0.005 * 1000)) ‚âà 0.95\nIndividual B: p(X) = e(-2 + 0.005 * 2000) / (1 + e(-2 + 0.005 * 2000)) ‚âà 1\n\nPredictions:\n\nIndividual A: Estimated probability of default ‚âà 95%.\nIndividual B: Estimated probability of default ‚âà 100%."
  },
  {
    "objectID": "qmd/islp4.html#from-probabilities-to-classes-the-threshold",
    "href": "qmd/islp4.html#from-probabilities-to-classes-the-threshold",
    "title": "Statistical Learning: Classification",
    "section": "From Probabilities to Classes: The Threshold",
    "text": "From Probabilities to Classes: The Threshold\n\nAfter calculating probabilities, we classify using a threshold.\nCommon Threshold: 0.5\nClassification Rule:\n\nIf p(X) &gt; 0.5, predict ‚Äúdefault = Yes‚Äù.\nIf p(X) ‚â§ 0.5, predict ‚Äúdefault = No‚Äù.\n\nIn our example:\n\nIndividual A: Predict ‚Äúdefault = Yes‚Äù (0.95 &gt; 0.5).\nIndividual B: Predict ‚Äúdefault = Yes‚Äù (1 &gt; 0.5)."
  },
  {
    "objectID": "qmd/islp4.html#multiple-logistic-regression-more-predictors",
    "href": "qmd/islp4.html#multiple-logistic-regression-more-predictors",
    "title": "Statistical Learning: Classification",
    "section": "Multiple Logistic Regression: More Predictors!",
    "text": "Multiple Logistic Regression: More Predictors!\n\nLike linear regression, we can include multiple predictors.\nThe Model:\n\n\\[\n\\log\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p\n\\]\n\\[\np(X) = \\frac{e^{\\beta_0 + \\beta_1X_1 + \\dots + \\beta_pX_p}}{1 + e^{\\beta_0 + \\beta_1X_1 + \\dots + \\beta_pX_p}}\n\\]"
  },
  {
    "objectID": "qmd/islp4.html#multiple-logistic-regression-explained",
    "href": "qmd/islp4.html#multiple-logistic-regression-explained",
    "title": "Statistical Learning: Classification",
    "section": "Multiple Logistic Regression: Explained",
    "text": "Multiple Logistic Regression: Explained\n\nX‚ÇÅ, X‚ÇÇ, ‚Ä¶, Xp: The p predictors.\nŒ≤‚ÇÄ, Œ≤‚ÇÅ, Œ≤‚ÇÇ, ‚Ä¶, Œ≤p: Coefficients estimated from the data.\nThe log-odds are now a linear function of all predictors.\nWe still use MLE to estimate coefficients.\nInterpretation:\n\nŒ≤j: Change in log-odds for a one-unit increase in Xj, holding all other predictors constant.\nEquivalently, a one-unit increase in Xj multiplies the odds by eŒ≤j, holding others constant."
  },
  {
    "objectID": "qmd/islp4.html#example-multiple-logistic-regression-on-default",
    "href": "qmd/islp4.html#example-multiple-logistic-regression-on-default",
    "title": "Statistical Learning: Classification",
    "section": "Example: Multiple Logistic Regression on Default",
    "text": "Example: Multiple Logistic Regression on Default\n\nModel: default ~ balance + income + student\nstudent is a dummy variable:\n\nstudent = 1 if student, 0 if not.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. error\nz-statistic\np-value\n\n\n\n\nIntercept\n-10.8690\n0.4923\n-22.08\n&lt; 0.0001\n\n\nbalance\n0.0057\n0.0002\n24.74\n&lt; 0.0001\n\n\nincome\n0.0030\n0.0082\n0.37\n0.7115\n\n\nstudent[Yes]\n-0.6468\n0.2362\n-2.74\n0.0062"
  },
  {
    "objectID": "qmd/islp4.html#interpreting-the-coefficients-a-deep-dive",
    "href": "qmd/islp4.html#interpreting-the-coefficients-a-deep-dive",
    "title": "Statistical Learning: Classification",
    "section": "Interpreting the Coefficients: A Deep Dive",
    "text": "Interpreting the Coefficients: A Deep Dive\n\nIntercept (Œ≤‚ÇÄ = -10.8690): Log-odds of default for a non-student with balance = 0 and income = 0. Not directly interpretable.\nbalance (Œ≤‚ÇÅ = 0.0057): For each $1 increase in balance, the log-odds of default increase by 0.0057, holding income and student status constant. Equivalently, odds multiply by e0.0057 ‚âà 1.0057.\nincome (Œ≤‚ÇÇ = 0.0030): For each $1 increase in income, the log-odds of default increase by 0.003, holding other variables constant. It‚Äôs not significant.\nstudent[Yes] (Œ≤‚ÇÉ = -0.6468): Being a student decreases the log-odds of default by 0.6468, holding balance and income constant. Odds multiply by e-0.6468 ‚âà 0.52. Surprising! We might expect the opposite, but this is controlling for balance and income."
  },
  {
    "objectID": "qmd/islp4.html#confounding-unmasking-hidden-relationships",
    "href": "qmd/islp4.html#confounding-unmasking-hidden-relationships",
    "title": "Statistical Learning: Classification",
    "section": "Confounding: Unmasking Hidden Relationships",
    "text": "Confounding: Unmasking Hidden Relationships\n\nThe surprising student result is due to confounding.\n\n\n\n\nConfounding in the Default data."
  },
  {
    "objectID": "qmd/islp4.html#confounding-left-panel---default-rates",
    "href": "qmd/islp4.html#confounding-left-panel---default-rates",
    "title": "Statistical Learning: Classification",
    "section": "Confounding: Left Panel - Default Rates",
    "text": "Confounding: Left Panel - Default Rates\n\n\n\n\n\n\n\n\n\nConfounding in the Default data.\n\n\n\n\n\n\nLeft Panel (Default Rates vs.¬†Balance):\n\nOrange: Students.\nBlue: Non-students.\nSolid: Default rate at each balance level.\nDashed: Overall (average) default rate.\n\nKey: Students have higher overall default rates (dashed), but lower rates at each balance level (solid)."
  },
  {
    "objectID": "qmd/islp4.html#confounding-right-panel---balance-distribution",
    "href": "qmd/islp4.html#confounding-right-panel---balance-distribution",
    "title": "Statistical Learning: Classification",
    "section": "Confounding: Right Panel - Balance Distribution",
    "text": "Confounding: Right Panel - Balance Distribution\n\n\n\n\n\n\n\n\n\nConfounding in the Default data.\n\n\n\n\n\n\nRight Panel (Boxplots of Balance):\n\nCompares balance distribution for students (orange) and non-students (blue).\nStudents tend to have higher credit card balances."
  },
  {
    "objectID": "qmd/islp4.html#confounding-the-full-story",
    "href": "qmd/islp4.html#confounding-the-full-story",
    "title": "Statistical Learning: Classification",
    "section": "Confounding: The Full Story",
    "text": "Confounding: The Full Story\n\nStudents have higher balances. (Right panel boxplots)\nHigher balances lead to higher default rates. (Our initial finding)\nThus, students have higher overall default rates due to higher balances. (Dashed lines in left panel)\nBut, controlling for balance, students have lower default rates. (Solid lines in left panel, and the multiple logistic regression coefficient)\n\n\nConfounding: The effect of student status is mixed up with balance. Without controlling for balance, we get a misleading impression."
  },
  {
    "objectID": "qmd/islp4.html#multinomial-logistic-regression-beyond-two-categories",
    "href": "qmd/islp4.html#multinomial-logistic-regression-beyond-two-categories",
    "title": "Statistical Learning: Classification",
    "section": "Multinomial Logistic Regression: Beyond Two Categories",
    "text": "Multinomial Logistic Regression: Beyond Two Categories\n\nWhat if we have more than two response categories?\nExample: Medical diagnosis:\n\nStroke\nDrug overdose\nEpileptic seizure\n\nMultinomial logistic regression extends logistic regression to handle this."
  },
  {
    "objectID": "qmd/islp4.html#multinomial-logistic-regression-the-core-concept",
    "href": "qmd/islp4.html#multinomial-logistic-regression-the-core-concept",
    "title": "Statistical Learning: Classification",
    "section": "Multinomial Logistic Regression: The Core Concept",
    "text": "Multinomial Logistic Regression: The Core Concept\n\nWe choose one category as a baseline (e.g., ‚Äústroke‚Äù).\nWe model the log-odds of each other category relative to the baseline.\nModel:\n\n\\[\n\\log\\left(\\frac{\\Pr(Y = k|X = x)}{\\Pr(Y = K|X = x)}\\right) = \\beta_{k0} + \\beta_{k1}x_1 + \\dots + \\beta_{kp}x_p\n\\]"
  },
  {
    "objectID": "qmd/islp4.html#multinomial-logistic-regression-details",
    "href": "qmd/islp4.html#multinomial-logistic-regression-details",
    "title": "Statistical Learning: Classification",
    "section": "Multinomial Logistic Regression: Details",
    "text": "Multinomial Logistic Regression: Details\n\nk: Index for categories (k = 1, 2, ‚Ä¶, K-1). We have K-1 equations.\nK: Baseline category.\nŒ≤k0, Œ≤k1, ‚Ä¶, Œ≤kp: Coefficients for category k (relative to baseline). K-1 sets of these.\nX = (x‚ÇÅ, x‚ÇÇ, ‚Ä¶, xp): Predictors.\nThis models the log-odds of being in category k compared to the baseline K, as a linear function of the predictors.\nKey: The baseline choice is arbitrary. Predicted probabilities are the same regardless."
  },
  {
    "objectID": "qmd/islp4.html#multinomial-logistic-regression-softmax-coding-alternative",
    "href": "qmd/islp4.html#multinomial-logistic-regression-softmax-coding-alternative",
    "title": "Statistical Learning: Classification",
    "section": "Multinomial Logistic Regression: Softmax Coding (Alternative)",
    "text": "Multinomial Logistic Regression: Softmax Coding (Alternative)\n\nSoftmax coding is an equivalent formulation, treating all K categories symmetrically.\nModel:\n\n\\[\n\\Pr(Y = k|X = x) = \\frac{e^{\\beta_{k0} + \\beta_{k1}x_1 + \\dots + \\beta_{kp}x_p}}{\\sum_{l=1}^K e^{\\beta_{l0} + \\beta_{l1}x_1 + \\dots + \\beta_{lp}x_p}}\n\\]"
  },
  {
    "objectID": "qmd/islp4.html#softmax-coding-breakdown",
    "href": "qmd/islp4.html#softmax-coding-breakdown",
    "title": "Statistical Learning: Classification",
    "section": "Softmax Coding: Breakdown",
    "text": "Softmax Coding: Breakdown\n\nThis calculates Pr(Y = k | X).\nNumerator: Exponentiated linear combination for category k.\nDenominator: Sum of exponentiated linear combinations for all categories. Ensures probabilities sum to 1.\nSoftmax Function: Transforms linear combinations into probabilities. Generalizes the logistic function.\nUse in Machine Learning: Common in neural networks for multi-class classification."
  },
  {
    "objectID": "qmd/islp4.html#generative-models-a-different-approach",
    "href": "qmd/islp4.html#generative-models-a-different-approach",
    "title": "Statistical Learning: Classification",
    "section": "Generative Models: A Different Approach",
    "text": "Generative Models: A Different Approach\n\nSo far, we‚Äôve directly modeled Pr(Y = k | X) (e.g., logistic regression).\nGenerative models take an indirect approach:\n\nModel the distribution of predictors X separately for each response class. This is Pr(X | Y = k).\nUse Bayes‚Äô theorem to ‚Äúflip‚Äù this to get Pr(Y = k | X). This is what we want for classification."
  },
  {
    "objectID": "qmd/islp4.html#bayes-theorem-the-key-to-flipping",
    "href": "qmd/islp4.html#bayes-theorem-the-key-to-flipping",
    "title": "Statistical Learning: Classification",
    "section": "Bayes‚Äô Theorem: The Key to ‚ÄúFlipping‚Äù",
    "text": "Bayes‚Äô Theorem: The Key to ‚ÄúFlipping‚Äù\n\nDefinitions:\n\nœÄk: Prior probability of an observation coming from class k (overall probability).\nfk(X) = Pr(X | Y = k): Density function of predictors X for class k (how predictors are distributed within class k).\n\nBayes‚Äô Theorem:\n\n\\[\n\\Pr(Y = k|X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)}\n\\]"
  },
  {
    "objectID": "qmd/islp4.html#bayes-theorem-explained",
    "href": "qmd/islp4.html#bayes-theorem-explained",
    "title": "Statistical Learning: Classification",
    "section": "Bayes‚Äô Theorem: Explained",
    "text": "Bayes‚Äô Theorem: Explained\n\nLeft-hand side: Pr(Y = k | X = x) ‚Äì Posterior probability. Probability of class k, given predictors x. What we want.\nRight-hand side:\n\nœÄk: Prior probability of class k.\nfk(x): Density of predictors X for class k.\nŒ£l=1K œÄlfl(x): Normalizing constant (sum over all classes). Ensures probabilities sum to 1.\n\nIntuition: Bayes‚Äô theorem updates our prior belief (œÄk) based on evidence from predictors (fk(x)).\nTo use Bayes‚Äô theorem, we must estimate œÄk and fk(x). Estimating œÄk is easy, while estimating fk(x) is hard."
  },
  {
    "objectID": "qmd/islp4.html#why-generative-models-advantages",
    "href": "qmd/islp4.html#why-generative-models-advantages",
    "title": "Statistical Learning: Classification",
    "section": "Why Generative Models? Advantages",
    "text": "Why Generative Models? Advantages\n\nWhy this indirect approach when we have direct methods like logistic regression?\nStability: With substantial class separation (predictors clearly distinguish classes), logistic regression parameter estimates can be unstable. Generative models can be more stable.\nSmall Sample Size + Normality: If predictors are approximately normal within each class, and the sample size is small, generative models like LDA can be more accurate.\nMore Than Two Classes: Generative models easily handle multiple classes."
  },
  {
    "objectID": "qmd/islp4.html#linear-discriminant-analysis-lda-a-generative-model",
    "href": "qmd/islp4.html#linear-discriminant-analysis-lda-a-generative-model",
    "title": "Statistical Learning: Classification",
    "section": "Linear Discriminant Analysis (LDA): A Generative Model",
    "text": "Linear Discriminant Analysis (LDA): A Generative Model\n\nLDA is a generative model with specific assumptions about predictor distribution.\nLDA Assumptions:\n\nNormality: Density functions fk(x) are normal (Gaussian). Predictors follow a bell-shaped distribution within each class.\nCommon Variance: Common variance (œÉ¬≤) across all K classes. The spread is the same, though means may differ."
  },
  {
    "objectID": "qmd/islp4.html#lda-the-normal-density-single-predictor",
    "href": "qmd/islp4.html#lda-the-normal-density-single-predictor",
    "title": "Statistical Learning: Classification",
    "section": "LDA: The Normal Density (Single Predictor)",
    "text": "LDA: The Normal Density (Single Predictor)\n\nFor one predictor (p = 1):\n\n\\[\nf_k(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2\\right)\n\\]\n\nŒºk: Mean of predictor X for class k.\nœÉ¬≤: Common variance across all classes.\nThe familiar bell-shaped curve."
  },
  {
    "objectID": "qmd/islp4.html#lda-the-discriminant-function",
    "href": "qmd/islp4.html#lda-the-discriminant-function",
    "title": "Statistical Learning: Classification",
    "section": "LDA: The Discriminant Function",
    "text": "LDA: The Discriminant Function\n\nPlugging fk(x) into Bayes‚Äô theorem and simplifying gives a simple result.\nClassify to the class with the largest:\n\n\\[\n\\delta_k(x) = x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k)\n\\]\n\nThis is the discriminant function."
  },
  {
    "objectID": "qmd/islp4.html#lda-discriminant-function-breakdown",
    "href": "qmd/islp4.html#lda-discriminant-function-breakdown",
    "title": "Statistical Learning: Classification",
    "section": "LDA Discriminant Function: Breakdown",
    "text": "LDA Discriminant Function: Breakdown\n\nŒ¥k(x): Discriminant score for class k. Classify to the class with the highest score.\nKey: Linear in x. Hence, Linear Discriminant Analysis! Decision boundaries are straight lines (or hyperplanes).\nTerms:\n\nx ‚ãÖ (Œºk / œÉ¬≤): Influence of predictor x and class mean Œºk, scaled by variance.\n\n(Œºk¬≤ / 2œÉ¬≤): Constant for each class, related to mean.\n\nlog(œÄk): Incorporates prior probability of class k."
  },
  {
    "objectID": "qmd/islp4.html#lda-estimating-the-parameters-from-data",
    "href": "qmd/islp4.html#lda-estimating-the-parameters-from-data",
    "title": "Statistical Learning: Classification",
    "section": "LDA: Estimating the Parameters (From Data)",
    "text": "LDA: Estimating the Parameters (From Data)\n\nWe don‚Äôt know true parameters (œÄk, Œºk, œÉ¬≤). We estimate them from training data.\nEstimates:\n\nŒºÃÇk = (1/nk) Œ£i:y·µ¢=k x·µ¢: Sample mean of X for class k. Average predictor values for observations in class k.\nœÉÃÇ¬≤ = (1/(n-K)) Œ£k=1K Œ£i:y·µ¢=k (x·µ¢ - ŒºÃÇk)¬≤: Pooled variance estimate. Weighted average of within-class variances (reflects common variance assumption).\nœÄÃÇk = nk / n: Sample proportion of observations in class k.\n\nPlug estimates into the discriminant function for predictions."
  },
  {
    "objectID": "qmd/islp4.html#lda-a-visual-example-one-predictor",
    "href": "qmd/islp4.html#lda-a-visual-example-one-predictor",
    "title": "Statistical Learning: Classification",
    "section": "LDA: A Visual Example (One Predictor)",
    "text": "LDA: A Visual Example (One Predictor)\n\n\n\nOne-dimensional normal density functions and LDA decision boundary"
  },
  {
    "objectID": "qmd/islp4.html#lda-example-left-panel---density-functions",
    "href": "qmd/islp4.html#lda-example-left-panel---density-functions",
    "title": "Statistical Learning: Classification",
    "section": "LDA Example: Left Panel - Density Functions",
    "text": "LDA Example: Left Panel - Density Functions\n\n\n\n\n\n\n\n\n\nOne-dimensional normal density functions and LDA decision boundary\n\n\n\n\n\n\nLeft Panel:\n\nTwo normal density functions (p=1, K=2).\nOrange: Class 1 (f‚ÇÅ(x)).\nBlue: Class 2 (f‚ÇÇ(x)).\nDashed: Bayes decision boundary. Optimal boundary, given true distributions. Where densities intersect."
  },
  {
    "objectID": "qmd/islp4.html#lda-example-right-panel---histograms-and-boundaries",
    "href": "qmd/islp4.html#lda-example-right-panel---histograms-and-boundaries",
    "title": "Statistical Learning: Classification",
    "section": "LDA Example: Right Panel - Histograms and Boundaries",
    "text": "LDA Example: Right Panel - Histograms and Boundaries\n\n\n\n\n\n\n\n\n\nOne-dimensional normal density functions and LDA decision boundary\n\n\n\n\n\n\nRight Panel:\n\nHistograms of 20 observations from each class.\nOrange: Class 1.\nBlue: Class 2.\nDashed: Bayes decision boundary (same as left).\nSolid: LDA decision boundary. Found by LDA using estimated parameters.\n\nKey: LDA approximates the Bayes boundary. Here, with normal data and equal variances, LDA does well."
  },
  {
    "objectID": "qmd/islp4.html#lda-with-multiple-predictors-p-1-going-multivariate",
    "href": "qmd/islp4.html#lda-with-multiple-predictors-p-1-going-multivariate",
    "title": "Statistical Learning: Classification",
    "section": "LDA with Multiple Predictors (p > 1): Going Multivariate",
    "text": "LDA with Multiple Predictors (p &gt; 1): Going Multivariate\n\nWith multiple predictors, we extend LDA‚Äôs assumptions.\nAssumption: Predictors X = (X‚ÇÅ, X‚ÇÇ, ‚Ä¶, Xp) follow a multivariate Gaussian distribution, with a class-specific mean vector and a common covariance matrix.\nMultivariate Gaussian Density:\n\n\\[\nf(x) = \\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x - \\mu)^T\\Sigma^{-1}(x - \\mu)\\right)\n\\]"
  },
  {
    "objectID": "qmd/islp4.html#multivariate-gaussian-deconstructed",
    "href": "qmd/islp4.html#multivariate-gaussian-deconstructed",
    "title": "Statistical Learning: Classification",
    "section": "Multivariate Gaussian: Deconstructed",
    "text": "Multivariate Gaussian: Deconstructed\n\nx: Vector of predictor values (x‚ÇÅ, x‚ÇÇ, ‚Ä¶, xp).\nŒº: Mean vector (Œº‚ÇÅ, Œº‚ÇÇ, ‚Ä¶, Œºp). Mean of each predictor. In LDA, each class has its own Œºk.\nŒ£: Covariance matrix. p √ó p matrix describing relationships (covariances) between all predictor pairs. In LDA, common Œ£ for all classes.\n|Œ£|: Determinant of Œ£.\nGeneralizes the one-dimensional normal distribution to multiple dimensions."
  },
  {
    "objectID": "qmd/islp4.html#lda-with-multiple-predictors-the-discriminant-function",
    "href": "qmd/islp4.html#lda-with-multiple-predictors-the-discriminant-function",
    "title": "Statistical Learning: Classification",
    "section": "LDA with Multiple Predictors: The Discriminant Function",
    "text": "LDA with Multiple Predictors: The Discriminant Function\n\nPlug the multivariate Gaussian density into Bayes‚Äô theorem and simplify.\nClassify to the class with the largest:\n\n\\[\n\\delta_k(x) = x^T \\Sigma^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + \\log \\pi_k\n\\]\n\nKey: Still linear in x. LDA produces linear decision boundaries even with multiple predictors.\nWe estimate the parameters (Œºk, Œ£, œÄk) from the training data, similar to one-dimension case."
  },
  {
    "objectID": "qmd/islp4.html#lda-example-with-three-classes-visual",
    "href": "qmd/islp4.html#lda-example-with-three-classes-visual",
    "title": "Statistical Learning: Classification",
    "section": "LDA: Example with Three Classes (Visual)",
    "text": "LDA: Example with Three Classes (Visual)\n\n\n\nMultivariate Gaussian distribution with three classes, and LDA decision boundaries"
  },
  {
    "objectID": "qmd/islp4.html#lda-example-three-classes-left-panel---ellipses",
    "href": "qmd/islp4.html#lda-example-three-classes-left-panel---ellipses",
    "title": "Statistical Learning: Classification",
    "section": "LDA Example (Three Classes): Left Panel - Ellipses",
    "text": "LDA Example (Three Classes): Left Panel - Ellipses\n\n\n\n\n\n\n\n\n\nMultivariate Gaussian distribution with three classes, and LDA decision boundaries\n\n\n\n\n\n\nLeft Panel:\n\nTwo-dimensional example (p=2), three classes (K=3).\nEllipses: Contours of constant probability density for each class (95% probability, assuming multivariate Gaussian).\nDashed: Bayes decision boundaries. Optimal boundaries, given true distributions."
  },
  {
    "objectID": "qmd/islp4.html#lda-example-three-classes-right-panel---scatterplot-and-boundaries",
    "href": "qmd/islp4.html#lda-example-three-classes-right-panel---scatterplot-and-boundaries",
    "title": "Statistical Learning: Classification",
    "section": "LDA Example (Three Classes): Right Panel - Scatterplot and Boundaries",
    "text": "LDA Example (Three Classes): Right Panel - Scatterplot and Boundaries\n\n\n\n\n\n\n\n\n\nMultivariate Gaussian distribution with three classes, and LDA decision boundaries\n\n\n\n\n\n\nRight Panel:\n\n20 observations from each class.\nSolid: LDA decision boundaries. Found by LDA using estimated parameters.\n\nKey: LDA approximates Bayes boundaries. LDA boundaries are linear; Bayes boundaries are slightly curved."
  },
  {
    "objectID": "qmd/islp4.html#lda-on-the-default-data-assessing-performance",
    "href": "qmd/islp4.html#lda-on-the-default-data-assessing-performance",
    "title": "Statistical Learning: Classification",
    "section": "LDA on the Default Data: Assessing Performance",
    "text": "LDA on the Default Data: Assessing Performance\n\nApplying LDA to Default data (predicting default from balance and student).\nTraining Error Rate: 2.75%. Seems low, but be careful!\nTwo Problems with Training Error:\n\nOverfitting: Training error is usually lower than test error. The model might fit training data too closely, not generalizing well to new data.\nImbalanced Data: Only 3.33% of individuals defaulted in training data. Imbalanced. A classifier always predicting ‚Äúno default‚Äù (the null classifier) would have a 3.33% error rate. Our 2.75% isn‚Äôt much better!"
  },
  {
    "objectID": "qmd/islp4.html#confusion-matrix-a-detailed-performance-view",
    "href": "qmd/islp4.html#confusion-matrix-a-detailed-performance-view",
    "title": "Statistical Learning: Classification",
    "section": "Confusion Matrix: A Detailed Performance View",
    "text": "Confusion Matrix: A Detailed Performance View\n\nA confusion matrix summarizes classifier performance. It shows counts of:\n\nTrue Positives (TP)\nTrue Negatives (TN)\nFalse Positives (FP) - Type I error\nFalse Negatives (FN) - Type II error\n\n\n\n\n\n\n\n\n\n\n\n\nPredicted No Default\nPredicted Default\nTotal\n\n\n\n\nActual No Default\n9644\n23\n9667\n\n\nActual Default\n252\n81\n333\n\n\nTotal\n9896\n104\n10000"
  },
  {
    "objectID": "qmd/islp4.html#confusion-matrix-interpreted",
    "href": "qmd/islp4.html#confusion-matrix-interpreted",
    "title": "Statistical Learning: Classification",
    "section": "Confusion Matrix: Interpreted",
    "text": "Confusion Matrix: Interpreted\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicted No Default\nPredicted Default\nTotal\n\n\n\n\nActual No Default\n9644\n23\n9667\n\n\nActual Default\n252\n81\n333\n\n\nTotal\n9896\n104\n10000\n\n\n\n\n\n\n\n\nRows: Actual class labels.\nColumns: Predicted class labels.\nFrom the table:\n\nTN: 9644 non-defaulters, correctly predicted ‚Äúno default‚Äù.\nFP: 23 non-defaulters, incorrectly predicted ‚Äúdefault‚Äù.\nFN: 252 defaulters, incorrectly predicted ‚Äúno default‚Äù.\nTP: 81 defaulters, correctly predicted ‚Äúdefault‚Äù."
  },
  {
    "objectID": "qmd/islp4.html#confusion-matrix-key-insights-and-trade-offs",
    "href": "qmd/islp4.html#confusion-matrix-key-insights-and-trade-offs",
    "title": "Statistical Learning: Classification",
    "section": "Confusion Matrix: Key Insights and Trade-offs",
    "text": "Confusion Matrix: Key Insights and Trade-offs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicted No Default\nPredicted Default\nTotal\n\n\n\n\nActual No Default\n9644\n23\n9667\n\n\nActual Default\n252\n81\n333\n\n\nTotal\n9896\n104\n10000\n\n\n\n\n\n\n\n\nKey Problem: LDA misclassifies 252/333 defaulters (75.7%). Low sensitivity (recall) ‚Äì not good at identifying positive cases (defaulters).\nHowever: LDA correctly classifies 99.8% of non-defaulters (1 - 23/9667). High specificity ‚Äì good at identifying negative cases."
  },
  {
    "objectID": "qmd/islp4.html#modifying-the-threshold-adjusting-sensitivity-and-specificity",
    "href": "qmd/islp4.html#modifying-the-threshold-adjusting-sensitivity-and-specificity",
    "title": "Statistical Learning: Classification",
    "section": "Modifying the Threshold: Adjusting Sensitivity and Specificity",
    "text": "Modifying the Threshold: Adjusting Sensitivity and Specificity\n\nLDA (and Bayes classifier) typically use a 0.5 threshold for posterior probability. If Pr(default = Yes | X) &gt; 0.5, predict ‚Äúdefault‚Äù.\nWe can modify this threshold to change the sensitivity/specificity trade-off.\nLowering the Threshold (e.g., to 0.2):\n\nIncreases sensitivity: Identify more defaulters (more TPs).\nDecreases specificity: Incorrectly classify more non-defaulters as defaulters (more FPs).\n\nRaising the Threshold (e.g., to 0.8):\n\nDecreases sensitivity: Identify fewer defaulters (fewer TPs).\nIncreases specificity: Incorrectly classify fewer non-defaulters (fewer FPs).\n\nBest threshold depends on context and relative costs of errors (FP vs.¬†FN)."
  },
  {
    "objectID": "qmd/islp4.html#roc-curves-visualizing-performance-across-thresholds",
    "href": "qmd/islp4.html#roc-curves-visualizing-performance-across-thresholds",
    "title": "Statistical Learning: Classification",
    "section": "ROC Curves: Visualizing Performance Across Thresholds",
    "text": "ROC Curves: Visualizing Performance Across Thresholds\n\n\n\nError rates as a function of threshold"
  },
  {
    "objectID": "qmd/islp4.html#roc-curves-explanation",
    "href": "qmd/islp4.html#roc-curves-explanation",
    "title": "Statistical Learning: Classification",
    "section": "ROC Curves: Explanation",
    "text": "ROC Curves: Explanation\n\n\n\n\n\n\n\n\n\nError rates as a function of threshold\n\n\n\n\n\n\n\nShows various error rates as threshold changes.\nBlack solid line: Overall error rate.\nBlue dashed line: Fraction of defaulters incorrectly classified (1 - sensitivity).\nOrange dotted line: Fraction of errors among non-defaulters (false positive rate, 1 - specificity).\n\n\n\n\n\nA Receiver Operating Characteristic (ROC) curve visualizes classifier performance across all possible thresholds.\nAxes:\n\nX-axis: False Positive Rate (FPR) = 1 - Specificity\nY-axis: True Positive Rate (TPR) = Sensitivity\n\nEach point on the ROC curve represents a different threshold."
  },
  {
    "objectID": "qmd/islp4.html#roc-curve-interpreting-the-shape",
    "href": "qmd/islp4.html#roc-curve-interpreting-the-shape",
    "title": "Statistical Learning: Classification",
    "section": "ROC Curve: Interpreting the Shape",
    "text": "ROC Curve: Interpreting the Shape\n\n\n\nROC curve for LDA classifier on Default data\n\n\n\nIdeal ROC Curve: Hugs the top-left corner. High sensitivity (TPR) with low FPR.\nWorst-Case ROC Curve: Diagonal line from bottom-left to top-right (dotted line). No better than random guessing.\nArea Under the Curve (AUC): Summarizes performance.\n\nAUC = 1: Perfect classifier.\nAUC = 0.5: No better than random.\nAUC &gt; 0.5: Better than random. Higher AUC is better."
  },
  {
    "objectID": "qmd/islp4.html#quadratic-discriminant-analysis-qda-relaxing-assumptions",
    "href": "qmd/islp4.html#quadratic-discriminant-analysis-qda-relaxing-assumptions",
    "title": "Statistical Learning: Classification",
    "section": "Quadratic Discriminant Analysis (QDA): Relaxing Assumptions",
    "text": "Quadratic Discriminant Analysis (QDA): Relaxing Assumptions\n\nQDA is another generative model, like LDA.\nKey Difference: QDA relaxes the assumption of a common covariance matrix.\nQDA Assumption: Each class has its own covariance matrix, Œ£k. Spread and orientation can differ for each class.\nDiscriminant Function: Becomes quadratic in x:\n\n\\[\n\\delta_k(x) = -\\frac{1}{2}x^T\\Sigma_k^{-1}x + x^T\\Sigma_k^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\Sigma_k^{-1}\\mu_k - \\frac{1}{2}\\log|\\Sigma_k| + \\log \\pi_k\n\\]"
  },
  {
    "objectID": "qmd/islp4.html#qda-discriminant-function-explained",
    "href": "qmd/islp4.html#qda-discriminant-function-explained",
    "title": "Statistical Learning: Classification",
    "section": "QDA Discriminant Function: Explained",
    "text": "QDA Discriminant Function: Explained\n\nSimilar terms to LDA, but with Œ£k (covariance matrix for class k) instead of Œ£.\nKey: Quadratic in x. Decision boundaries are curves (ellipses, parabolas, hyperbolas), not lines.\nWe estimate parameters (Œºk, Œ£k, œÄk) from training data."
  },
  {
    "objectID": "qmd/islp4.html#lda-vs.-qda-the-bias-variance-trade-off",
    "href": "qmd/islp4.html#lda-vs.-qda-the-bias-variance-trade-off",
    "title": "Statistical Learning: Classification",
    "section": "LDA vs.¬†QDA: The Bias-Variance Trade-Off",
    "text": "LDA vs.¬†QDA: The Bias-Variance Trade-Off\n\nQDA is more flexible than LDA. More parameters to estimate (own covariance matrix for each class).\nQDA: Higher variance, potentially lower bias. More flexible, so can fit data better (lower bias), but more prone to overfitting (higher variance).\nLDA: Lower variance, potentially higher bias. Stronger assumption (common covariance), so less flexible (higher bias), but less likely to overfit (lower variance).\nWhich is better? Depends on data and bias-variance trade-off.\n\nLDA better when:\n\nFewer training observations. Reducing variance is crucial.\nCommon covariance assumption is reasonable.\n\nQDA recommended when:\n\nTraining set is large. Variance less of a concern.\nCommon covariance assumption is clearly not tenable (classes have very different spreads/orientations)."
  },
  {
    "objectID": "qmd/islp4.html#lda-vs.-qda-a-visual-comparison",
    "href": "qmd/islp4.html#lda-vs.-qda-a-visual-comparison",
    "title": "Statistical Learning: Classification",
    "section": "LDA vs.¬†QDA: A Visual Comparison",
    "text": "LDA vs.¬†QDA: A Visual Comparison\n\n\n\nLDA and QDA decision boundaries"
  },
  {
    "objectID": "qmd/islp4.html#lda-vs.-qda-example-left-panel---common-correlation",
    "href": "qmd/islp4.html#lda-vs.-qda-example-left-panel---common-correlation",
    "title": "Statistical Learning: Classification",
    "section": "LDA vs.¬†QDA Example: Left Panel - Common Correlation",
    "text": "LDA vs.¬†QDA Example: Left Panel - Common Correlation\n\n\n\n\n\n\n\n\n\nLDA and QDA decision boundaries\n\n\n\n\n\n\nLeft Panel (Common Correlation):\n\nClasses have same correlation between predictors.\nBayes boundary (dashed): Linear.\nLDA boundary (dotted): Linear, approximates Bayes well.\nQDA boundary (solid): Quadratic, worse than LDA here. QDA overfits."
  },
  {
    "objectID": "qmd/islp4.html#lda-vs.-qda-example-right-panel---different-correlations",
    "href": "qmd/islp4.html#lda-vs.-qda-example-right-panel---different-correlations",
    "title": "Statistical Learning: Classification",
    "section": "LDA vs.¬†QDA Example: Right Panel - Different Correlations",
    "text": "LDA vs.¬†QDA Example: Right Panel - Different Correlations\n\n\n\n\n\n\n\n\n\nLDA and QDA decision boundaries\n\n\n\n\n\n\nRight Panel (Different Correlations):\n\nClasses have different correlations.\nBayes boundary (dashed): Quadratic.\nQDA boundary (solid): Quadratic, approximates Bayes well.\nLDA boundary (dotted): Linear, performs poorly. Cannot capture the quadratic relationship."
  },
  {
    "objectID": "qmd/islp4.html#naive-bayes-the-naive-assumption",
    "href": "qmd/islp4.html#naive-bayes-the-naive-assumption",
    "title": "Statistical Learning: Classification",
    "section": "Naive Bayes: The ‚ÄúNaive‚Äù Assumption",
    "text": "Naive Bayes: The ‚ÄúNaive‚Äù Assumption\n\nNaive Bayes is another generative model. It makes a very strong simplifying assumption.\nAssumption: Within each class, the p predictors are independent. Knowing the class, the value of one predictor gives no info about others.\nMathematically: fk(x) = fk1(x‚ÇÅ) √ó fk2(x‚ÇÇ) √ó ‚Ä¶ √ó fkp(xp)\n\nfkj(xj): Density function of jth predictor within class k."
  },
  {
    "objectID": "qmd/islp4.html#naive-bayes-explained",
    "href": "qmd/islp4.html#naive-bayes-explained",
    "title": "Statistical Learning: Classification",
    "section": "Naive Bayes: Explained",
    "text": "Naive Bayes: Explained\n\nIndependence assumption simplifies things dramatically. Instead of estimating a complex p-dimensional joint density fk(x), we estimate p one-dimensional densities fkj(xj).\nPlug into Bayes‚Äô theorem:\n\n\\[\n\\Pr(Y = k|X = x) = \\frac{\\pi_k \\times f_{k1}(x_1) \\times f_{k2}(x_2) \\times \\dots \\times f_{kp}(x_p)}{\\sum_{l=1}^K \\pi_l \\times f_{l1}(x_1) \\times f_{l2}(x_2) \\times \\dots \\times f_{lp}(x_p)}\n\\]"
  },
  {
    "objectID": "qmd/islp4.html#naive-bayes-estimating-one-dimensional-densities",
    "href": "qmd/islp4.html#naive-bayes-estimating-one-dimensional-densities",
    "title": "Statistical Learning: Classification",
    "section": "Naive Bayes: Estimating One-Dimensional Densities",
    "text": "Naive Bayes: Estimating One-Dimensional Densities\n\nKey is estimating fkj(xj). Depends on whether Xj is quantitative or qualitative.\nIf Xj is quantitative:\n\nOption 1 (Parametric): Assume Xj | Y = k ~ N(Œºjk, œÉj¬≤). Estimate mean (Œºjk) and variance (œÉj¬≤) for each predictor j within each class k. Different variance for each predictor and class (unlike LDA).\nOption 2 (Non-parametric):\n\nHistogram: Divide range of Xj into bins, count observations in each bin for each class.\nKernel Density Estimation (KDE): More sophisticated, produces smooth density estimate.\n\n\nIf Xj is qualitative:\n\nCount proportion of training observations for each level of predictor within each class. Direct probability estimate."
  },
  {
    "objectID": "qmd/islp4.html#naive-bayes-example-illustrative",
    "href": "qmd/islp4.html#naive-bayes-example-illustrative",
    "title": "Statistical Learning: Classification",
    "section": "Naive Bayes: Example (Illustrative)",
    "text": "Naive Bayes: Example (Illustrative)\n\nTwo classes (K=2), three predictors (p=3). The first two predictors are quantitative, and the third predictor is qualitative with three levels.\nShows estimated one-dimensional density functions fkj.\n\n\n\n\n\n\n\n\n\n\nf11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nf12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nf13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nf21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nf22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nf23"
  },
  {
    "objectID": "qmd/islp4.html#naive-bayes-surprisingly-good-performance",
    "href": "qmd/islp4.html#naive-bayes-surprisingly-good-performance",
    "title": "Statistical Learning: Classification",
    "section": "Naive Bayes: Surprisingly Good Performance!",
    "text": "Naive Bayes: Surprisingly Good Performance!\n\nDespite the strong (and often unrealistic) independence assumption, Naive Bayes often works well.\nWhy? Reduces variance. Fewer parameters to estimate, so less prone to overfitting, especially with small data relative to predictors.\nBias-Variance Trade-off: Naive Bayes has high bias (independence assumption) but low variance. Can be a good trade-off."
  },
  {
    "objectID": "qmd/islp4.html#comparing-classification-methods-an-analytical-view",
    "href": "qmd/islp4.html#comparing-classification-methods-an-analytical-view",
    "title": "Statistical Learning: Classification",
    "section": "Comparing Classification Methods: An Analytical View",
    "text": "Comparing Classification Methods: An Analytical View\n\nLogistic regression, LDA, QDA, Naive Bayes: All can be expressed in terms of maximizing Pr(Y = k | X), or log-odds relative to baseline (K):\n\n\\[\n\\log\\left(\\frac{\\Pr(Y = k|X = x)}{\\Pr(Y = K|X = x)}\\right)\n\\]\n\nThe form of this log-odds expression distinguishes the methods.\n\n\n\n\n\n\n\n\nMethod\nLog-Odds Form\n\n\n\n\nLogistic Regression\nŒ≤k0 + Œ£j=1p Œ≤kjxj (linear)\n\n\nLDA\nak + Œ£j=1p bkjxj (linear)\n\n\nQDA\nak + Œ£j=1p bkjxj + Œ£j=1p Œ£l=1p ckj,lxjxl (quadratic)\n\n\nNaive Bayes\nak + Œ£j=1p gkj(xj) (additive)"
  },
  {
    "objectID": "qmd/islp4.html#comparison-explained",
    "href": "qmd/islp4.html#comparison-explained",
    "title": "Statistical Learning: Classification",
    "section": "Comparison: Explained",
    "text": "Comparison: Explained\n\nLogistic Regression: Log-odds are linear in predictors. Coefficients (Œ≤kj) estimated by MLE.\nLDA: Log-odds also linear. Coefficients (ak, bkj) from multivariate normality and common covariance assumptions.\nQDA: Log-odds are quadratic. Allows for more complex boundaries.\nNaive Bayes: Log-odds are additive. Each predictor contributes independently through gkj(xj). Reflects independence.\nRelationships:\n\nLDA is special case of QDA (equal covariance matrices).\nAny classifier with linear boundary is special case of Naive Bayes.\nLogistic regression has same linear form as LDA, but coefficients estimated differently (MLE vs.¬†normal assumptions).\n\nNone of these methods universally dominate the others. The best choice depends on the characteristics of data."
  },
  {
    "objectID": "qmd/islp4.html#a-comparison-of-classification-methodsan-empirical-comparison",
    "href": "qmd/islp4.html#a-comparison-of-classification-methodsan-empirical-comparison",
    "title": "Statistical Learning: Classification",
    "section": "A comparison of classification methodsÔºöAn empirical comparison",
    "text": "A comparison of classification methodsÔºöAn empirical comparison\n\nTest error rates of different methods are compared through six different scenarios.\n\n\n\n\n\n\n\n\n\n\n\nFIGURE 4.11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFIGURE 4.12"
  },
  {
    "objectID": "qmd/islp4.html#empirical-comparison-details",
    "href": "qmd/islp4.html#empirical-comparison-details",
    "title": "Statistical Learning: Classification",
    "section": "Empirical Comparison: Details",
    "text": "Empirical Comparison: Details\n\n\n\n\n\n\n\n\n\nFIGURE 4.11\n\n\n\n\n\n\n\n\n\nFIGURE 4.12\n\n\n\n\n\n\nSix scenarios. Scenarios 1-3: Bayes boundaries are linear. Scenarios 4-6: Bayes boundaries are non-linear.\np=2 quantitative predictors in each scenario.\nFor each scenario, 100 random training sets generated.\nScenario Details:\n\nScenario 1: Two classes, predictors correlation 0, n = 100\nScenario 2: Two classes, predictors correlation -0.5, n = 100\nScenario 3: Two classes, predictors correlation 0.5, n = 100, modified predictors (nonlinear boundary).\nScenario 4: Two classes, independent standard normals, response from QDA (quadratic boundary).\nScenario 5: Same as 4, but response from more complex quadratic.\nScenario 6: Two classes, independent predictors, complex response (highly non-linear boundary). Compare with K-nearest neighbors (KNN).\n\nResults:\n\nLinear boundaries (1 and 2): LDA and logistic regression do well.\nModerately non-linear (3): QDA outperforms LDA, but logistic regression still good.\nMore complex (4, 5, 6): QDA can be better than LDA and logistic.\nVery complex (6): Non-parametric KNN can be superior, but smoothness (K) must be chosen carefully."
  },
  {
    "objectID": "qmd/islp4.html#generalized-linear-models-glms-beyond-normality",
    "href": "qmd/islp4.html#generalized-linear-models-glms-beyond-normality",
    "title": "Statistical Learning: Classification",
    "section": "Generalized Linear Models (GLMs): Beyond Normality",
    "text": "Generalized Linear Models (GLMs): Beyond Normality\n\nSo far:\n\nQuantitative responses: Least squares linear regression.\nQualitative responses: Classification methods (logistic, LDA, QDA, Naive Bayes).\n\nWhat if Y is neither?\nExample: Predicting hourly users of a bike-sharing program.\nCharacteristics:\n\nNon-negative integers: Can‚Äôt have negative or fractional users.\nCounts: Number of events (rentals) in a time period."
  },
  {
    "objectID": "qmd/islp4.html#why-not-linear-regression-for-counts",
    "href": "qmd/islp4.html#why-not-linear-regression-for-counts",
    "title": "Statistical Learning: Classification",
    "section": "Why Not Linear Regression for Counts?",
    "text": "Why Not Linear Regression for Counts?\n\nProblems with using linear regression:\n\nNegative Predictions: Linear regression can predict negative values (nonsensical for counts).\nMean-Variance Relationship: For counts, variance often increases with the mean. Linear regression assumes constant variance.\nNon-Continuous: Response is discrete. Linear regression assumes continuous.\n\nSolution: Poisson regression. A more natural approach for count data."
  },
  {
    "objectID": "qmd/islp4.html#poisson-regression-modeling-counts",
    "href": "qmd/islp4.html#poisson-regression-modeling-counts",
    "title": "Statistical Learning: Classification",
    "section": "Poisson Regression: Modeling Counts",
    "text": "Poisson Regression: Modeling Counts\n\nPoisson Distribution: Often used to model counts.\nProbability Mass Function: If Y is Poisson, probability of k events is:\n\n\\[\n\\Pr(Y = k) = \\frac{e^{-\\lambda}\\lambda^k}{k!}  \\quad \\text{for } k = 0, 1, 2, \\dots\n\\]\n\nk: Number of events (non-negative integer).\nk!: k factorial (k! = k √ó (k-1) √ó ‚Ä¶ √ó 2 √ó 1). 0! = 1.\nŒª: Parameter (Œª &gt; 0). Expected value (mean) and variance of Y.\nKey: Mean and variance are equal. As mean (Œª) increases, variance increases. Suitable for count data."
  },
  {
    "objectID": "qmd/islp4.html#poisson-regression-model-connecting-mean-to-predictors",
    "href": "qmd/islp4.html#poisson-regression-model-connecting-mean-to-predictors",
    "title": "Statistical Learning: Classification",
    "section": "Poisson Regression Model: Connecting Mean to Predictors",
    "text": "Poisson Regression Model: Connecting Mean to Predictors\n\nModel the mean (Œª) as a function of predictors.\n\n\\[\n\\log(\\lambda(X_1, \\dots, X_p)) = \\beta_0 + \\beta_1X_1 + \\dots + \\beta_pX_p\n\\]\nor equivalently\n\\[\n\\lambda(X_1, \\dots, X_p) = e^{\\beta_0 + \\beta_1X_1 + \\dots + \\beta_pX_p}\n\\]"
  },
  {
    "objectID": "qmd/islp4.html#poisson-regression-model-explained",
    "href": "qmd/islp4.html#poisson-regression-model-explained",
    "title": "Statistical Learning: Classification",
    "section": "Poisson Regression Model: Explained",
    "text": "Poisson Regression Model: Explained\n\nŒª(X‚ÇÅ, X‚ÇÇ, ‚Ä¶, Xp): Mean (and variance) of Poisson, depends on predictors.\nŒ≤‚ÇÄ, Œ≤‚ÇÅ, ‚Ä¶, Œ≤p: Coefficients estimated from data.\nLink Function: Log link. Logarithm of mean is linear in predictors.\nWhy log link?\n\nNon-negativity: Ensures predicted mean (Œª) is non-negative. Exponential function (ex) is always positive.\nLinearity: Models relationship between predictors and log of mean as linear.\n\nEstimation: Use MLE to estimate coefficients."
  },
  {
    "objectID": "qmd/islp4.html#comparing-poisson-regression-model-to-the-linear-regression-model",
    "href": "qmd/islp4.html#comparing-poisson-regression-model-to-the-linear-regression-model",
    "title": "Statistical Learning: Classification",
    "section": "Comparing Poisson Regression Model to the linear regression model",
    "text": "Comparing Poisson Regression Model to the linear regression model\n\n\n\n\n\n\n\nInterpretation of the coefficients\n\nIn linear regression, coefficients represent linear increase.\nIn Poisson regression, coefficients represents the change in the log-mean for a one-unit increase in X, or increase the expected count by a factor of eŒ≤.\n\nMean-variance relationship\n\nIn linear regression, variance doesn‚Äôt change when mean changes.\nIn Poisson regression, variance is equal to mean. The larger the mean, the larger the variance.\n\nNonnegative fitted values\n\nIn linear regression, fitted values could be negative.\nIn Poisson regression, fitted values will always be positive.\n\nPlease read text material for details."
  },
  {
    "objectID": "qmd/islp4.html#generalized-linear-models-glms-the-big-picture",
    "href": "qmd/islp4.html#generalized-linear-models-glms-the-big-picture",
    "title": "Statistical Learning: Classification",
    "section": "Generalized Linear Models (GLMs): The Big Picture",
    "text": "Generalized Linear Models (GLMs): The Big Picture\n\nThree regression models:\n\nLinear Regression: Quantitative responses (normal distribution, constant variance).\nLogistic Regression: Binary responses (Bernoulli/binomial distribution).\nPoisson Regression: Count responses (Poisson distribution).\n\nCommon characteristics:\n\nPredictors and Response: Predictors (X‚ÇÅ, X‚ÇÇ, ‚Ä¶, Xp) predict response (Y).\nConditional Distribution: Given predictors, Y belongs to a family of distributions.\nModeling the Mean: Model mean of Y, conditional on predictors.\nLink Function: Link function (Œ∑) relates mean of Y to linear combination of predictors.\n\nGeneral Form (GLM):\n\n\\[\n\\eta(E(Y|X_1, \\dots, X_p)) = \\beta_0 + \\beta_1X_1 + \\dots + \\beta_pX_p\n\\]\n\nComponents:\n\nResponse (Y) and its distribution.\nPredictors (X1, X2, ‚Ä¶, Xp).\nLink Function (Œ∑).\nLinear Predictor: Œ≤‚ÇÄ + Œ≤‚ÇÅX‚ÇÅ + ‚Ä¶ + Œ≤pXp"
  },
  {
    "objectID": "qmd/islp4.html#summary",
    "href": "qmd/islp4.html#summary",
    "title": "Statistical Learning: Classification",
    "section": "Summary",
    "text": "Summary\n\nClassification: Predicting qualitative responses.\nKey methods:\n\nLogistic Regression: Models probability using logistic function. Directly models Pr(Y = k | X).\nLDA: Generative. Assumes multivariate Gaussian predictors, common covariance.\nQDA: Like LDA, but own covariance matrix per class.\nNaive Bayes: Generative. Assumes independence of predictors within each class.\n\nChoosing: Depends on data and bias-variance trade-off.\n\nLDA/logistic: Linear boundaries.\nQDA: More flexible, non-linear, but higher variance.\nNaive Bayes: Simple, surprisingly good, despite independence.\n\nROC Curves: Evaluate performance across thresholds. AUC summarizes.\nGeneralized linear models handle responses from non-normal distributions, and Poisson regression model is an example for count data."
  },
  {
    "objectID": "qmd/islp4.html#thoughts-and-discussion",
    "href": "qmd/islp4.html#thoughts-and-discussion",
    "title": "Statistical Learning: Classification",
    "section": "Thoughts and Discussion ü§î",
    "text": "Thoughts and Discussion ü§î\n\n\n\n\n\n\n\nReal-world scenarios for each method?\n\nLogistic: Credit scoring, medical diagnosis, marketing (churn).\nLDA: Predictors approximately normal, similar covariance. Some image recognition/signal processing.\nQDA: Normal predictors, different covariances. More complex problems.\nNaive Bayes: Text classification (spam filtering), document categorization, sentiment analysis.\n\nChoosing the ‚Äúbest‚Äù method?\n\nUnderstand Data: Explore. Linear/nonlinear? Normal predictors?\nAssumptions: Violations?\nBias-Variance: Flexible (high variance, low bias) or constrained (low variance, high bias)? Depends on dataset size/complexity.\nMetrics: Cross-validation for test error.\n\nAccuracy\nSensitivity (Recall)\nSpecificity\nPrecision\nF1-score\nAUC\n\nExperiment: Try different methods, compare with cross-validation.\n\nLimitations? When might they fail?\n\nLogistic: Assumes linear log-odds. Unstable with severe separation.\nLDA: Multivariate normality, common covariance. Outliers.\nQDA: Multivariate normality. Overfitting with small data.\nNaive Bayes: Independence often violated.\n\nHow does the bias-variance trade-off play a role in choosing a classification method?\n\nHigh Bias, Low Variance: Simpler models (e.g., LDA, Naive Bayes) tend to have higher bias but lower variance. They are less likely to overfit, but they might miss complex relationships in the data.\nLow Bias, High Variance: More complex models (e.g., QDA) tend to have lower bias but higher variance. They can fit the data more closely, but they are more prone to overfitting, especially with small datasets.\nThe optimal balance depends on the specific dataset and the goals of the analysis.\n\nCan we apply the knowledge of mean-variance relationship and fitted values to choose suitable regression model?\n\nYes.\nIf variance increases as the mean increase and the response is count data, Poisson regression could be considered.\nIf the fitted values are probabilities, which must be between 0 and 1, then models like logistic regression could be considered.\nUnderstanding the characteristics of data will guide us to select suitable models."
  },
  {
    "objectID": "qmd/islp1.html",
    "href": "qmd/islp1.html",
    "title": "Statistical Learning: An Introduction",
    "section": "",
    "text": "Note\n\n\n\nWelcome to Statistical Learning! üöÄ\n\n\n\nThis chapter provides a gentle introduction to the exciting world of statistical learning, a powerful toolkit for understanding data. We will explore the core concepts, see real-world examples, and start our journey toward building predictive models."
  },
  {
    "objectID": "qmd/islp1.html#welcome",
    "href": "qmd/islp1.html#welcome",
    "title": "Statistical Learning: An Introduction",
    "section": "",
    "text": "Note\n\n\n\nWelcome to Statistical Learning! üöÄ\n\n\n\nThis chapter provides a gentle introduction to the exciting world of statistical learning, a powerful toolkit for understanding data. We will explore the core concepts, see real-world examples, and start our journey toward building predictive models."
  },
  {
    "objectID": "qmd/islp1.html#what-is-statistical-learning",
    "href": "qmd/islp1.html#what-is-statistical-learning",
    "title": "Statistical Learning: An Introduction",
    "section": "What is Statistical Learning?",
    "text": "What is Statistical Learning?\nStatistical learning refers to a vast set of tools for understanding data. It‚Äôs about extracting meaningful insights and making predictions from data. These tools can be broadly classified into two categories: Supervised and Unsupervised Learning.\nThink of it like using data to teach a computer to:\n\nPredict: Estimate a future outcome (e.g., will a customer click on an ad?).\nInfer: Understand relationships between variables (e.g., what factors influence house prices?).\nDiscover: Find hidden patterns and structures (e.g., what groups of customers have similar buying habits?)."
  },
  {
    "objectID": "qmd/islp1.html#supervised-learning",
    "href": "qmd/islp1.html#supervised-learning",
    "title": "Statistical Learning: An Introduction",
    "section": "Supervised Learning",
    "text": "Supervised Learning\nSupervised Learning:\n\nBuilding a model to predict or estimate an output based on one or more inputs (also known as predictors, features, or independent variables).\nThink of it like teaching a computer to learn from examples where you provide both the questions (inputs) and the answers (outputs). The algorithm learns the relationship between them.\nExample: Predicting a house price based on its size, location, and number of bedrooms. We know the house prices (the ‚Äúanswers‚Äù) for our training data."
  },
  {
    "objectID": "qmd/islp1.html#unsupervised-learning",
    "href": "qmd/islp1.html#unsupervised-learning",
    "title": "Statistical Learning: An Introduction",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\nUnsupervised Learning:\n\nDiscovering relationships and structure in data without a predefined output variable. There‚Äôs no ‚Äúanswer key.‚Äù\nHere, you‚Äôre letting the computer explore the data and find patterns on its own. The algorithm identifies groupings or patterns without being told what to look for.\nExample: Grouping customers into different segments based on their purchasing behavior, without knowing in advance what those segments should be. We don‚Äôt have pre-labeled groups."
  },
  {
    "objectID": "qmd/islp1.html#supervised-vs.-unsupervised-a-visual-analogy",
    "href": "qmd/islp1.html#supervised-vs.-unsupervised-a-visual-analogy",
    "title": "Statistical Learning: An Introduction",
    "section": "Supervised vs.¬†Unsupervised: A Visual Analogy",
    "text": "Supervised vs.¬†Unsupervised: A Visual Analogy\nImagine you have a basket of fruits üçéüçåüçä.\n\nSupervised Learning: You tell a child, ‚ÄúThis is an apple, this is a banana, this is an orange.‚Äù Then you show them a new fruit and ask, ‚ÄúWhat is this?‚Äù You‚Äôre providing labeled examples. The child learns to classify the fruits based on your labels."
  },
  {
    "objectID": "qmd/islp1.html#supervised-vs.-unsupervised-a-visual-analogy-cont.",
    "href": "qmd/islp1.html#supervised-vs.-unsupervised-a-visual-analogy-cont.",
    "title": "Statistical Learning: An Introduction",
    "section": "Supervised vs.¬†Unsupervised: A Visual Analogy (Cont.)",
    "text": "Supervised vs.¬†Unsupervised: A Visual Analogy (Cont.)\n\nUnsupervised Learning: You give the child the basket and say, ‚ÄúSort these fruits into groups however you think is best.‚Äù The child might group them by color, shape, or size, discovering inherent patterns without being told what to look for. The child discovers the categories."
  },
  {
    "objectID": "qmd/islp1.html#data-mining-machine-learning-and-statistical-learning",
    "href": "qmd/islp1.html#data-mining-machine-learning-and-statistical-learning",
    "title": "Statistical Learning: An Introduction",
    "section": "Data Mining, Machine Learning, and Statistical Learning",
    "text": "Data Mining, Machine Learning, and Statistical Learning\nThese terms are often used interchangeably, but there are subtle differences. This diagram helps visualize their relationships:\n\n\n\n\n\ngraph LR\n    A[Data Mining] --&gt; C(Common Ground)\n    B[Machine Learning] --&gt; C\n    D[Statistical Learning] --&gt; C\n    C --&gt; E[Insights & Predictions]"
  },
  {
    "objectID": "qmd/islp1.html#data-mining-machine-learning-and-statistical-learning-cont.",
    "href": "qmd/islp1.html#data-mining-machine-learning-and-statistical-learning-cont.",
    "title": "Statistical Learning: An Introduction",
    "section": "Data Mining, Machine Learning, and Statistical Learning (Cont.)",
    "text": "Data Mining, Machine Learning, and Statistical Learning (Cont.)\n\nData Mining: Focuses on discovering patterns and extracting knowledge from large datasets, often using techniques from database management and computer science. Think ‚Äúbig data‚Äù and finding hidden patterns. It‚Äôs often about exploratory analysis in very large datasets.\nMachine Learning: Primarily concerned with building algorithms that can learn from and make predictions on data. Emphasizes predictive accuracy and computational efficiency. The focus is on the algorithm learning to make good predictions. It‚Äôs more focused on the algorithms themselves and their performance."
  },
  {
    "objectID": "qmd/islp1.html#data-mining-machine-learning-and-statistical-learning-cont.-1",
    "href": "qmd/islp1.html#data-mining-machine-learning-and-statistical-learning-cont.-1",
    "title": "Statistical Learning: An Introduction",
    "section": "Data Mining, Machine Learning, and Statistical Learning (Cont.)",
    "text": "Data Mining, Machine Learning, and Statistical Learning (Cont.)\n\nStatistical Learning: A subfield of statistics that emphasizes model interpretability and understanding the uncertainty associated with predictions. Provides a rigorous statistical framework for machine learning. It combines statistical rigor with machine learning techniques. We want to understand why the model makes the predictions it does, and how confident we are in those predictions."
  },
  {
    "objectID": "qmd/islp1.html#real-world-applications",
    "href": "qmd/islp1.html#real-world-applications",
    "title": "Statistical Learning: An Introduction",
    "section": "Real-World Applications",
    "text": "Real-World Applications\nLet‚Äôs explore some real-world data sets that we will use throughout the course:\n\nWage Data: Analyzing factors that influence a person‚Äôs wage. (Supervised - Regression)\nStock Market Data: Predicting stock market movements. (Supervised - Classification)\nGene Expression Data: Identifying groups of genes with similar expression patterns. (Unsupervised - Clustering)"
  },
  {
    "objectID": "qmd/islp1.html#wage-data-understanding-income",
    "href": "qmd/islp1.html#wage-data-understanding-income",
    "title": "Statistical Learning: An Introduction",
    "section": "Wage Data: Understanding Income",
    "text": "Wage Data: Understanding Income\nWe want to understand how a person‚Äôs wage is related to their:\n\nAge\nEducation\nYear (calendar year)\n\n\n\n\n\n\n\nNote\n\n\n\nThe goal is to build a model that can predict a person‚Äôs wage based on these factors. This is a supervised learning problem because we have wage data (the ‚Äúoutput‚Äù). Specifically, it‚Äôs a regression problem because the output (wage) is a continuous variable."
  },
  {
    "objectID": "qmd/islp1.html#wage-data-wage-vs.-age",
    "href": "qmd/islp1.html#wage-data-wage-vs.-age",
    "title": "Statistical Learning: An Introduction",
    "section": "Wage Data: Wage vs.¬†Age",
    "text": "Wage Data: Wage vs.¬†Age\n\n\n\nWage as a function of age"
  },
  {
    "objectID": "qmd/islp1.html#wage-data-wage-vs.-age-analysis---1",
    "href": "qmd/islp1.html#wage-data-wage-vs.-age-analysis---1",
    "title": "Statistical Learning: An Introduction",
    "section": "Wage Data: Wage vs.¬†Age (Analysis - 1)",
    "text": "Wage Data: Wage vs.¬†Age (Analysis - 1)\n\n\n\nWage as a function of age\n\n\n\nLeft Panel: This scatterplot shows individual wages plotted against age. Each point represents a person."
  },
  {
    "objectID": "qmd/islp1.html#wage-data-wage-vs.-age-analysis---2",
    "href": "qmd/islp1.html#wage-data-wage-vs.-age-analysis---2",
    "title": "Statistical Learning: An Introduction",
    "section": "Wage Data: Wage vs.¬†Age (Analysis - 2)",
    "text": "Wage Data: Wage vs.¬†Age (Analysis - 2)\n\n\n\nWage as a function of age\n\n\n\nThe blue line represents the average wage for a given age."
  },
  {
    "objectID": "qmd/islp1.html#wage-data-wage-vs.-age-analysis---3",
    "href": "qmd/islp1.html#wage-data-wage-vs.-age-analysis---3",
    "title": "Statistical Learning: An Introduction",
    "section": "Wage Data: Wage vs.¬†Age (Analysis - 3)",
    "text": "Wage Data: Wage vs.¬†Age (Analysis - 3)\n\n\n\nWage as a function of age\n\n\n\nTrend: Wage generally increases with age until around 60, then decreases. This suggests a non-linear relationship. It‚Äôs not a straight line!"
  },
  {
    "objectID": "qmd/islp1.html#wage-data-wage-vs.-age-analysis---4",
    "href": "qmd/islp1.html#wage-data-wage-vs.-age-analysis---4",
    "title": "Statistical Learning: An Introduction",
    "section": "Wage Data: Wage vs.¬†Age (Analysis - 4)",
    "text": "Wage Data: Wage vs.¬†Age (Analysis - 4)\n\n\n\nWage as a function of age\n\n\n\nVariability: There‚Äôs significant spread around the average, meaning age alone isn‚Äôt a perfect predictor. Many other factors influence wage."
  },
  {
    "objectID": "qmd/islp1.html#wage-data-wage-vs.-year",
    "href": "qmd/islp1.html#wage-data-wage-vs.-year",
    "title": "Statistical Learning: An Introduction",
    "section": "Wage Data: Wage vs.¬†Year",
    "text": "Wage Data: Wage vs.¬†Year\n\n\n\nWage as a function of year"
  },
  {
    "objectID": "qmd/islp1.html#wage-data-wage-vs.-year-analysis---1",
    "href": "qmd/islp1.html#wage-data-wage-vs.-year-analysis---1",
    "title": "Statistical Learning: An Introduction",
    "section": "Wage Data: Wage vs.¬†Year (Analysis - 1)",
    "text": "Wage Data: Wage vs.¬†Year (Analysis - 1)\n\n\n\nWage as a function of year\n\n\n\nCenter Panel: Shows wage versus year. Each point represents a person."
  },
  {
    "objectID": "qmd/islp1.html#wage-data-wage-vs.-year-analysis---2",
    "href": "qmd/islp1.html#wage-data-wage-vs.-year-analysis---2",
    "title": "Statistical Learning: An Introduction",
    "section": "Wage Data: Wage vs.¬†Year (Analysis - 2)",
    "text": "Wage Data: Wage vs.¬†Year (Analysis - 2)\n\n\n\nWage as a function of year\n\n\n\nthe line shows the trend."
  },
  {
    "objectID": "qmd/islp1.html#wage-data-wage-vs.-year-analysis---3",
    "href": "qmd/islp1.html#wage-data-wage-vs.-year-analysis---3",
    "title": "Statistical Learning: An Introduction",
    "section": "Wage Data: Wage vs.¬†Year (Analysis - 3)",
    "text": "Wage Data: Wage vs.¬†Year (Analysis - 3)\n\n\n\nWage as a function of year\n\n\n\nObservation: There‚Äôs a gradual increase in average wage over time (2003-2009). This could be due to inflation, economic growth, or other factors."
  },
  {
    "objectID": "qmd/islp1.html#wage-data-wage-vs.-education",
    "href": "qmd/islp1.html#wage-data-wage-vs.-education",
    "title": "Statistical Learning: An Introduction",
    "section": "Wage Data: Wage vs.¬†Education",
    "text": "Wage Data: Wage vs.¬†Education\n\n\n\nWage as a function of education"
  },
  {
    "objectID": "qmd/islp1.html#wage-data-wage-vs.-education-analysis---1",
    "href": "qmd/islp1.html#wage-data-wage-vs.-education-analysis---1",
    "title": "Statistical Learning: An Introduction",
    "section": "Wage Data: Wage vs.¬†Education (Analysis - 1)",
    "text": "Wage Data: Wage vs.¬†Education (Analysis - 1)\n\n\n\nWage as a function of education\n\n\n\nRight Panel: Boxplots of wage for different education levels (1 = lowest, 5 = highest)."
  },
  {
    "objectID": "qmd/islp1.html#wage-data-wage-vs.-education-analysis---2",
    "href": "qmd/islp1.html#wage-data-wage-vs.-education-analysis---2",
    "title": "Statistical Learning: An Introduction",
    "section": "Wage Data: Wage vs.¬†Education (Analysis - 2)",
    "text": "Wage Data: Wage vs.¬†Education (Analysis - 2)\n\n\n\nWage as a function of education\n\n\n\nBoxplots show the distribution of wages within each education group. The box represents the middle 50% of the data, the line in the box is the median, and the ‚Äúwhiskers‚Äù extend to the most extreme data points within 1.5 times the interquartile range (IQR) from the box. Outliers are shown as individual points."
  },
  {
    "objectID": "qmd/islp1.html#wage-data-wage-vs.-education-analysis---3",
    "href": "qmd/islp1.html#wage-data-wage-vs.-education-analysis---3",
    "title": "Statistical Learning: An Introduction",
    "section": "Wage Data: Wage vs.¬†Education (Analysis - 3)",
    "text": "Wage Data: Wage vs.¬†Education (Analysis - 3)\n\n\n\nWage as a function of education\n\n\n\nObservation: Higher education generally leads to higher wages. The boxes (representing the middle 50% of wages) are generally higher for higher education levels."
  },
  {
    "objectID": "qmd/islp1.html#wage-data-wage-vs.-education-analysis---4",
    "href": "qmd/islp1.html#wage-data-wage-vs.-education-analysis---4",
    "title": "Statistical Learning: An Introduction",
    "section": "Wage Data: Wage vs.¬†Education (Analysis - 4)",
    "text": "Wage Data: Wage vs.¬†Education (Analysis - 4)\n\n\n\nWage as a function of education\n\n\n\nConclusion: Combining age, year, and education will likely provide the most accurate wage prediction. We need to consider multiple factors."
  },
  {
    "objectID": "qmd/islp1.html#stock-market-data-predicting-direction",
    "href": "qmd/islp1.html#stock-market-data-predicting-direction",
    "title": "Statistical Learning: An Introduction",
    "section": "Stock Market Data: Predicting Direction",
    "text": "Stock Market Data: Predicting Direction\n\nGoal: Predict whether the S&P 500 stock index will increase or decrease on a given day.\nInput: Percentage changes in the index over the previous 5 days.\nOutput: Categorical (qualitative) ‚Äì either ‚ÄúUp‚Äù or ‚ÄúDown‚Äù. This is a classification problem (predicting a category, not a number)."
  },
  {
    "objectID": "qmd/islp1.html#stock-market-data-previous-days-change",
    "href": "qmd/islp1.html#stock-market-data-previous-days-change",
    "title": "Statistical Learning: An Introduction",
    "section": "Stock Market Data: Previous Day‚Äôs Change",
    "text": "Stock Market Data: Previous Day‚Äôs Change\n\n\n\nBoxplots of previous day‚Äôs change"
  },
  {
    "objectID": "qmd/islp1.html#stock-market-data-previous-days-change-analysis---1",
    "href": "qmd/islp1.html#stock-market-data-previous-days-change-analysis---1",
    "title": "Statistical Learning: An Introduction",
    "section": "Stock Market Data: Previous Day‚Äôs Change (Analysis - 1)",
    "text": "Stock Market Data: Previous Day‚Äôs Change (Analysis - 1)\n\n\n\nBoxplots of previous day‚Äôs change\n\n\n\nLeft Panel: Boxplots show the distribution of the previous day‚Äôs percentage change."
  },
  {
    "objectID": "qmd/islp1.html#stock-market-data-previous-days-change-analysis---2",
    "href": "qmd/islp1.html#stock-market-data-previous-days-change-analysis---2",
    "title": "Statistical Learning: An Introduction",
    "section": "Stock Market Data: Previous Day‚Äôs Change (Analysis - 2)",
    "text": "Stock Market Data: Previous Day‚Äôs Change (Analysis - 2)\n\n\n\nBoxplots of previous day‚Äôs change\n\n\n\nThe boxplots are separated by whether the market went ‚ÄúUp‚Äù or ‚ÄúDown‚Äù today."
  },
  {
    "objectID": "qmd/islp1.html#stock-market-data-previous-days-change-analysis---3",
    "href": "qmd/islp1.html#stock-market-data-previous-days-change-analysis---3",
    "title": "Statistical Learning: An Introduction",
    "section": "Stock Market Data: Previous Day‚Äôs Change (Analysis - 3)",
    "text": "Stock Market Data: Previous Day‚Äôs Change (Analysis - 3)\n\n\n\nBoxplots of previous day‚Äôs change\n\n\n\nObservation: The two boxplots are very similar. This means the distributions of yesterday‚Äôs returns are almost the same whether the market goes up or down today."
  },
  {
    "objectID": "qmd/islp1.html#stock-market-data-previous-days-change-analysis---4",
    "href": "qmd/islp1.html#stock-market-data-previous-days-change-analysis---4",
    "title": "Statistical Learning: An Introduction",
    "section": "Stock Market Data: Previous Day‚Äôs Change (Analysis - 4)",
    "text": "Stock Market Data: Previous Day‚Äôs Change (Analysis - 4)\n\n\n\nBoxplots of previous day‚Äôs change\n\n\n\nConclusion: This suggests that yesterday‚Äôs performance is not a strong predictor of today‚Äôs direction."
  },
  {
    "objectID": "qmd/islp1.html#stock-market-data-two-days-previous",
    "href": "qmd/islp1.html#stock-market-data-two-days-previous",
    "title": "Statistical Learning: An Introduction",
    "section": "Stock Market Data: Two Day‚Äôs Previous",
    "text": "Stock Market Data: Two Day‚Äôs Previous\n\n\n\nBoxplots of two day‚Äôs previous"
  },
  {
    "objectID": "qmd/islp1.html#stock-market-data-two-days-previous-analysis",
    "href": "qmd/islp1.html#stock-market-data-two-days-previous-analysis",
    "title": "Statistical Learning: An Introduction",
    "section": "Stock Market Data: Two Day‚Äôs Previous (Analysis)",
    "text": "Stock Market Data: Two Day‚Äôs Previous (Analysis)\n\n\n\nBoxplots of two day‚Äôs previous\n\n\n\nCenter Panel: Boxplots show the distribution of the 2 day‚Äôs previous percentage change, separated by whether the market went ‚ÄúUp‚Äù or ‚ÄúDown‚Äù today.\nObservation: The two boxplots are very similar."
  },
  {
    "objectID": "qmd/islp1.html#stock-market-data-three-days-previous",
    "href": "qmd/islp1.html#stock-market-data-three-days-previous",
    "title": "Statistical Learning: An Introduction",
    "section": "Stock Market Data: Three Day‚Äôs Previous",
    "text": "Stock Market Data: Three Day‚Äôs Previous\n\n\n\nBoxplots of three day‚Äôs previous"
  },
  {
    "objectID": "qmd/islp1.html#stock-market-data-three-days-previous-analysis",
    "href": "qmd/islp1.html#stock-market-data-three-days-previous-analysis",
    "title": "Statistical Learning: An Introduction",
    "section": "Stock Market Data: Three Day‚Äôs Previous (Analysis)",
    "text": "Stock Market Data: Three Day‚Äôs Previous (Analysis)\n\n\n\nBoxplots of three day‚Äôs previous\n\n\n\nRight Panel: Boxplots show the distribution of the 3 day‚Äôs previous percentage change, separated by whether the market went ‚ÄúUp‚Äù or ‚ÄúDown‚Äù today.\nObservation: The two boxplots are very similar."
  },
  {
    "objectID": "qmd/islp1.html#stock-market-data-predicting-with-qda",
    "href": "qmd/islp1.html#stock-market-data-predicting-with-qda",
    "title": "Statistical Learning: An Introduction",
    "section": "Stock Market Data: Predicting with QDA",
    "text": "Stock Market Data: Predicting with QDA\n\n\n\nQuadratic Discriminant Analysis"
  },
  {
    "objectID": "qmd/islp1.html#stock-market-data-predicting-with-qda-analysis---1",
    "href": "qmd/islp1.html#stock-market-data-predicting-with-qda-analysis---1",
    "title": "Statistical Learning: An Introduction",
    "section": "Stock Market Data: Predicting with QDA (Analysis - 1)",
    "text": "Stock Market Data: Predicting with QDA (Analysis - 1)\n\n\n\nQuadratic Discriminant Analysis\n\n\n\nA statistical learning method called quadratic discriminant analysis (QDA) was used to predict market direction. QDA is a classification method. It assumes that the data within each class (‚ÄúUp‚Äù or ‚ÄúDown‚Äù) follows a Gaussian (normal) distribution, but with different covariance matrices for each class."
  },
  {
    "objectID": "qmd/islp1.html#stock-market-data-predicting-with-qda-analysis---2",
    "href": "qmd/islp1.html#stock-market-data-predicting-with-qda-analysis---2",
    "title": "Statistical Learning: An Introduction",
    "section": "Stock Market Data: Predicting with QDA (Analysis - 2)",
    "text": "Stock Market Data: Predicting with QDA (Analysis - 2)\n\n\n\nQuadratic Discriminant Analysis\n\n\n\nResult: The model correctly predicted the direction about 60% of the time. This is better than random guessing (50%), but still far from perfect. It suggests weak trends might exist, but predicting the stock market is hard!"
  },
  {
    "objectID": "qmd/islp1.html#gene-expression-data-clustering",
    "href": "qmd/islp1.html#gene-expression-data-clustering",
    "title": "Statistical Learning: An Introduction",
    "section": "Gene Expression Data: Clustering",
    "text": "Gene Expression Data: Clustering\n\nGoal: Identify groups (clusters) of cancer cell lines based on their gene expression measurements. We want to see if cell lines naturally group together based on how their genes are expressed.\nInput: 6,830 gene expression measurements for each of 64 cancer cell lines. Each measurement represents the activity level of a gene.\nOutput: None. This is an unsupervised learning problem (specifically, a clustering problem). We don‚Äôt have pre-defined groups; we want to discover them."
  },
  {
    "objectID": "qmd/islp1.html#gene-expression-data-principal-components",
    "href": "qmd/islp1.html#gene-expression-data-principal-components",
    "title": "Statistical Learning: An Introduction",
    "section": "Gene Expression Data: Principal Components",
    "text": "Gene Expression Data: Principal Components\n\n\n\nPrincipal Components Analysis"
  },
  {
    "objectID": "qmd/islp1.html#gene-expression-data-principal-components-analysis---1",
    "href": "qmd/islp1.html#gene-expression-data-principal-components-analysis---1",
    "title": "Statistical Learning: An Introduction",
    "section": "Gene Expression Data: Principal Components (Analysis - 1)",
    "text": "Gene Expression Data: Principal Components (Analysis - 1)\n\n\n\nPrincipal Components Analysis\n\n\n\nChallenge: Visualizing 6,830 dimensions is impossible! We can‚Äôt directly ‚Äúlook‚Äù at the data."
  },
  {
    "objectID": "qmd/islp1.html#gene-expression-data-principal-components-analysis---2",
    "href": "qmd/islp1.html#gene-expression-data-principal-components-analysis---2",
    "title": "Statistical Learning: An Introduction",
    "section": "Gene Expression Data: Principal Components (Analysis - 2)",
    "text": "Gene Expression Data: Principal Components (Analysis - 2)\n\n\n\nPrincipal Components Analysis\n\n\n\nSolution: Principal Component Analysis (PCA) reduces the data to two dimensions (Z1 and Z2) that capture the most important information. PCA finds the directions of greatest variation in the data. These directions are called principal components."
  },
  {
    "objectID": "qmd/islp1.html#gene-expression-data-principal-components-analysis---3",
    "href": "qmd/islp1.html#gene-expression-data-principal-components-analysis---3",
    "title": "Statistical Learning: An Introduction",
    "section": "Gene Expression Data: Principal Components (Analysis - 3)",
    "text": "Gene Expression Data: Principal Components (Analysis - 3)\n\n\n\nPrincipal Components Analysis\n\n\n\nLeft Panel: Each point represents a cell line, colored by a suggested cluster. At least four groups seem clear. This shows the result of the unsupervised clustering. A clustering algorithm (like k-means) was likely applied to the data after PCA was used to reduce the dimensionality."
  },
  {
    "objectID": "qmd/islp1.html#gene-expression-data-principal-components-analysis---4",
    "href": "qmd/islp1.html#gene-expression-data-principal-components-analysis---4",
    "title": "Statistical Learning: An Introduction",
    "section": "Gene Expression Data: Principal Components (Analysis - 4)",
    "text": "Gene Expression Data: Principal Components (Analysis - 4)\n\n\n\nPrincipal Components Analysis\n\n\n\nRight Panel: Same plot as the left panel (using the same PCA dimensions), but points are colored by the actual cancer type (14 types). This is the ‚Äúground truth‚Äù that we didn‚Äôt use for clustering."
  },
  {
    "objectID": "qmd/islp1.html#gene-expression-data-principal-components-analysis---5",
    "href": "qmd/islp1.html#gene-expression-data-principal-components-analysis---5",
    "title": "Statistical Learning: An Introduction",
    "section": "Gene Expression Data: Principal Components (Analysis - 5)",
    "text": "Gene Expression Data: Principal Components (Analysis - 5)\n\n\n\nPrincipal Components Analysis\n\n\n\nObservation: Cell lines with the same cancer type tend to cluster together, validating the unsupervised clustering. This means that even though we didn‚Äôt tell the algorithm the cancer types, it was able to discover them (to some extent) from the gene expression data. This is the power of unsupervised learning!"
  },
  {
    "objectID": "qmd/islp1.html#a-brief-history-of-statistical-learning",
    "href": "qmd/islp1.html#a-brief-history-of-statistical-learning",
    "title": "Statistical Learning: An Introduction",
    "section": "A Brief History of Statistical Learning",
    "text": "A Brief History of Statistical Learning\n\nEarly Beginnings (19th Century): Least squares (linear regression) was developed by Legendre and Gauss. This is the foundation of many statistical learning methods. The goal is to find the line that minimizes the sum of squared differences between observed and predicted values.\nMid-20th Century (1930s-1970s): Linear discriminant analysis (LDA) was developed by Fisher(1936). Logistic regression was developed. These expanded the toolkit for classification and modeling different types of data. Generalized linear models (GLMs) were introduced, providing a unified framework for linear regression, logistic regression, and other models."
  },
  {
    "objectID": "qmd/islp1.html#a-brief-history-of-statistical-learning-cont.",
    "href": "qmd/islp1.html#a-brief-history-of-statistical-learning-cont.",
    "title": "Statistical Learning: An Introduction",
    "section": "A Brief History of Statistical Learning (Cont.)",
    "text": "A Brief History of Statistical Learning (Cont.)\n\nComputational Revolution (1980s onwards): Non-linear methods become feasible (e.g., trees, generalized additive models). Neural Networks and Support Vector Machines were proposed. Computers became powerful enough to handle more complex calculations.\nModern Era: Statistical learning emerges as a distinct field, fueled by powerful software (like R and Python) and increasing data availability. The combination of data, algorithms, and computing power drives the field forward."
  },
  {
    "objectID": "qmd/islp1.html#notation",
    "href": "qmd/islp1.html#notation",
    "title": "Statistical Learning: An Introduction",
    "section": "Notation",
    "text": "Notation\nIt‚Äôs important to understand the notation we‚Äôll use throughout the course:\n\nn: Number of observations (data points). Think of this as the number of rows in your data table. Each observation is a single instance of what you‚Äôre studying (e.g., a person, a day, a cell line).\np: Number of variables (features, predictors). Think of this as the number of columns in your data table excluding the outcome variable (if there is one). Each variable is a characteristic you‚Äôre measuring (e.g., age, education, stock price change)."
  },
  {
    "objectID": "qmd/islp1.html#notation-cont.",
    "href": "qmd/islp1.html#notation-cont.",
    "title": "Statistical Learning: An Introduction",
    "section": "Notation (Cont.)",
    "text": "Notation (Cont.)\n\nxij: Value of the jth variable for the ith observation. This is a specific entry in your data table. For example, x35 would be the value of the 5th variable for the 3rd observation.\nX: A matrix (n x p) representing the data. Think of it as a spreadsheet where rows are observations and columns are variables.\nyi: the ith observation of the variable on which we wish to make predictions"
  },
  {
    "objectID": "qmd/islp1.html#notation-cont.-1",
    "href": "qmd/islp1.html#notation-cont.-1",
    "title": "Statistical Learning: An Introduction",
    "section": "Notation (Cont.)",
    "text": "Notation (Cont.)\n\nBold lowercase (e.g., a): A vector of length n. This typically represents all the values of a single variable across all observations. For example, y might represent a vector of all the wages in the Wage dataset.\nNormal lowercase (e.g., a): A scalar (single number) or a vector not of length n.\nBold capitals (e.g., A): A matrix."
  },
  {
    "objectID": "qmd/islp1.html#notation-example-wage-data",
    "href": "qmd/islp1.html#notation-example-wage-data",
    "title": "Statistical Learning: An Introduction",
    "section": "Notation: Example (Wage Data)",
    "text": "Notation: Example (Wage Data)\nLet‚Äôs apply the notation to the Wage data example:\n\nn = 3000 (3000 people - our observations)\np = 11 (variables like year, age, education, etc. - our predictors)\nx23 would be the value of the 3rd variable (e.g., education) for the 2nd person.\nX is a 3000 x 11 matrix (our data table).\ny1: the first person‚Äôs wage."
  },
  {
    "objectID": "qmd/islp1.html#summary",
    "href": "qmd/islp1.html#summary",
    "title": "Statistical Learning: An Introduction",
    "section": "Summary",
    "text": "Summary\n\nStatistical learning provides tools to understand data, both with (supervised) and without (unsupervised) a specific outcome to predict.\nSupervised learning aims to predict an output, while unsupervised learning explores data structure.\nData mining, machine learning, and statistical learning are related but have different emphases.\nReal-world applications include predicting wages, stock market movements, and clustering gene expression data.\nUnderstanding notation is crucial for following the rest of the course."
  },
  {
    "objectID": "qmd/islp1.html#thoughts-and-discussion",
    "href": "qmd/islp1.html#thoughts-and-discussion",
    "title": "Statistical Learning: An Introduction",
    "section": "Thoughts and Discussion",
    "text": "Thoughts and Discussion\n\nCan you think of other examples of supervised and unsupervised learning problems in your field of interest?\nWhy is it important to understand the limitations of statistical learning models, even when they achieve good predictive accuracy? (Hint: Consider issues like overfitting, bias, and the fact that correlation doesn‚Äôt imply causation.)\nWhat are the potential ethical implications of using statistical learning models in areas like hiring, loan applications, or criminal justice? (Hint: Consider issues like fairness, transparency, and accountability.)\nHow do you imagine the combination of increasing data availability, more powerful hardware, and user-friendly software will further transform the field of statistical learning, making more sophisticated machine learning available to even more researchers?"
  },
  {
    "objectID": "qmd/islp10.html",
    "href": "qmd/islp10.html",
    "title": "Introduction to Deep Learning",
    "section": "",
    "text": "Very Active Research Area: üöÄ Deep learning is at the forefront of artificial intelligence research, with new discoveries and advancements happening constantly.\nSubfield of Machine Learning: It‚Äôs a specialized branch of machine learning, focusing on algorithms inspired by the structure and function of the human brain.\nNeural Networks are Key: The core building block of deep learning is the neural network, a complex structure of interconnected nodes (called ‚Äúneurons‚Äù) that can learn intricate patterns from data."
  },
  {
    "objectID": "qmd/islp10.html#what-is-deep-learning",
    "href": "qmd/islp10.html#what-is-deep-learning",
    "title": "Introduction to Deep Learning",
    "section": "",
    "text": "Very Active Research Area: üöÄ Deep learning is at the forefront of artificial intelligence research, with new discoveries and advancements happening constantly.\nSubfield of Machine Learning: It‚Äôs a specialized branch of machine learning, focusing on algorithms inspired by the structure and function of the human brain.\nNeural Networks are Key: The core building block of deep learning is the neural network, a complex structure of interconnected nodes (called ‚Äúneurons‚Äù) that can learn intricate patterns from data."
  },
  {
    "objectID": "qmd/islp10.html#what-is-machine-learning",
    "href": "qmd/islp10.html#what-is-machine-learning",
    "title": "Introduction to Deep Learning",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\n\n\n\n\n\n\n\nMachine learning is a field of study that gives computers the ability to learn without being explicitly programmed. - Arthur Samuel, 1959\nA computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E. - Tom M. Mitchell, 1997\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nMachine learning focuses on the development of algorithms that can learn from and make predictions on data. These algorithms operate by building a model from example inputs in order to make data-driven predictions or decisions, rather than following strictly static program instructions."
  },
  {
    "objectID": "qmd/islp10.html#what-is-statistical-learning",
    "href": "qmd/islp10.html#what-is-statistical-learning",
    "title": "Introduction to Deep Learning",
    "section": "What is Statistical Learning?",
    "text": "What is Statistical Learning?\n\nStatistical learning is a framework for understanding data based on statistics. It involves building a statistical model for inference or prediction.\nThe goal is often to estimate a function \\(f\\) such that \\(Y = f(X) + \\epsilon\\), where \\(X\\) represents the input variables, \\(Y\\) represents the output variable, and \\(\\epsilon\\) is the random error term.\nEmphasis: Statistical learning emphasizes models and their interpretability and precision, offering tools to assess the model (such as confidence intervals).\n\n\n\n\n\n\n\nNote\n\n\n\nStatistical learning focuses on drawing inferences from data using statistical models. Unlike machine learning, which often prioritizes predictive accuracy, statistical learning also focuses on the interpretability of models and understanding the underlying data generation process."
  },
  {
    "objectID": "qmd/islp10.html#deep-learning-in-action",
    "href": "qmd/islp10.html#deep-learning-in-action",
    "title": "Introduction to Deep Learning",
    "section": "Deep Learning in Action",
    "text": "Deep Learning in Action\n\n\n\nA simple neural network.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis image visually represents a simple neural network. The circles represent neurons, and the lines connecting them represent the connections between neurons. The input layer receives data, the hidden layers process it, and the output layer produces the result."
  },
  {
    "objectID": "qmd/islp10.html#defining-the-landscape-data-mining-machine-learning-and-statistical-learning",
    "href": "qmd/islp10.html#defining-the-landscape-data-mining-machine-learning-and-statistical-learning",
    "title": "Introduction to Deep Learning",
    "section": "Defining the Landscape: Data Mining, Machine Learning, and Statistical Learning",
    "text": "Defining the Landscape: Data Mining, Machine Learning, and Statistical Learning\nLet‚Äôs clarify these related terms, which are often used interchangeably, but have important distinctions:\n\nData Mining: üîç The process of discovering patterns, anomalies, and insights from large datasets. It‚Äôs like ‚Äúunearthing‚Äù valuable information hidden within a mountain of data.\nMachine Learning: ü§ñ Algorithms that can learn from data and improve their performance without being explicitly programmed. Focuses on making accurate predictions or decisions.\nStatistical Learning: üìä A subfield of statistics focused on statistical models for understanding data. Bridges statistics and machine learning, often emphasizing interpretability."
  },
  {
    "objectID": "qmd/islp10.html#relationships-between-concepts",
    "href": "qmd/islp10.html#relationships-between-concepts",
    "title": "Introduction to Deep Learning",
    "section": "Relationships Between Concepts",
    "text": "Relationships Between Concepts\n\n\n\n\n\ngraph LR\n    A[Data Mining] --&gt; C(Common Ground)\n    B[Machine Learning] --&gt; C\n    D[Statistical Learning] --&gt; C\n    C --&gt; E[Insights & Predictions]\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis diagram illustrates the overlapping nature of these fields. They all share the common goal of extracting insights and making predictions from data, but they approach this goal with different tools and emphases. Data mining is often applied to very large datasets, machine learning focuses on prediction, and statistical learning emphasizes inference and model understanding."
  },
  {
    "objectID": "qmd/islp10.html#a-bit-of-history",
    "href": "qmd/islp10.html#a-bit-of-history",
    "title": "Introduction to Deep Learning",
    "section": "A Bit of History",
    "text": "A Bit of History\n\nEarly Days (Late 1980s): Neural networks first gained attention, sparking initial excitement.\nStabilization: The initial hype subsided as researchers explored the practical challenges of training and applying these networks.\nDecline in Popularity: Other machine learning methods, like Support Vector Machines (SVMs), boosting, and random forests, became more popular. These were considered more ‚Äúautomatic.‚Äù\nThe Resurgence (Post-2010): Neural networks made a spectacular comeback, rebranded as ‚ÄúDeep Learning.‚Äù"
  },
  {
    "objectID": "qmd/islp10.html#drivers-of-the-deep-learning-renaissance",
    "href": "qmd/islp10.html#drivers-of-the-deep-learning-renaissance",
    "title": "Introduction to Deep Learning",
    "section": "Drivers of the Deep Learning Renaissance",
    "text": "Drivers of the Deep Learning Renaissance\nWhat fueled this resurgence? Three key factors:\n\nNew Architectures: Researchers developed more sophisticated and effective neural network architectures.\nLarger Datasets: The explosion of digital data provided the massive datasets needed to train complex models.\nMore Computing Power: Advances in hardware, especially GPUs (Graphics Processing Units), made training large models feasible."
  },
  {
    "objectID": "qmd/islp10.html#neural-networks-the-building-blocks",
    "href": "qmd/islp10.html#neural-networks-the-building-blocks",
    "title": "Introduction to Deep Learning",
    "section": "Neural Networks: The Building Blocks",
    "text": "Neural Networks: The Building Blocks\n\nBrain Inspiration: üß† Neural networks are inspired by the human brain, mimicking how neurons connect and process information.\nInterconnected Nodes: Composed of interconnected nodes (‚Äúneurons‚Äù), organized in layers.\nLearning Complex Relationships: Can learn intricate, non-linear relationships between inputs (data) and outputs (predictions).\nFoundation of Deep Learning: Form the foundation for most deep learning models."
  },
  {
    "objectID": "qmd/islp10.html#single-layer-neural-networks-the-basics",
    "href": "qmd/islp10.html#single-layer-neural-networks-the-basics",
    "title": "Introduction to Deep Learning",
    "section": "Single Layer Neural Networks: The Basics",
    "text": "Single Layer Neural Networks: The Basics\n\nInput: It starts with an input vector \\(\\mathbf{X} = (X_1, X_2, \\dots, X_p)\\), representing your data (e.g., pixel values of an image).\nGoal: Build a non-linear function \\(f(\\mathbf{X})\\) to predict a response \\(Y\\) (e.g., the category of an image)."
  },
  {
    "objectID": "qmd/islp10.html#single-layer-neural-network-structure",
    "href": "qmd/islp10.html#single-layer-neural-network-structure",
    "title": "Introduction to Deep Learning",
    "section": "Single Layer Neural Network Structure",
    "text": "Single Layer Neural Network Structure\n\nInput Layer: Receives the input features \\(X_1, \\dots, X_p\\).\nHidden Layer: Computes activations \\(A_k = h_k(\\mathbf{X})\\). These are non-linear transformations of linear combinations of the inputs. Think of them as ‚Äúhidden features.‚Äù\nOutput Layer: A linear model that uses the activations from the hidden layer, producing the final output \\(f(\\mathbf{X})\\)."
  },
  {
    "objectID": "qmd/islp10.html#learning-the-functions",
    "href": "qmd/islp10.html#learning-the-functions",
    "title": "Introduction to Deep Learning",
    "section": "Learning the Functions",
    "text": "Learning the Functions\n\nThe functions \\(h_k(\\cdot)\\) in the hidden layer are not pre-defined; they are learned during training. This allows the network to adapt to the data."
  },
  {
    "objectID": "qmd/islp10.html#visualizing-a-single-layer-network",
    "href": "qmd/islp10.html#visualizing-a-single-layer-network",
    "title": "Introduction to Deep Learning",
    "section": "Visualizing a Single Layer Network",
    "text": "Visualizing a Single Layer Network\n\n\n\nA single-layer neural network.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis diagram shows a single-layer neural network. The input layer (left) receives the data. The hidden layer (middle) performs the non-linear transformation. The output layer (right) produces the prediction. The arrows represent the weights that connect the neurons."
  },
  {
    "objectID": "qmd/islp10.html#single-layer-network-the-math",
    "href": "qmd/islp10.html#single-layer-network-the-math",
    "title": "Introduction to Deep Learning",
    "section": "Single Layer Network: The Math",
    "text": "Single Layer Network: The Math\nThe neural network model can be expressed mathematically as:\n\\[\nf(\\mathbf{X}) = \\beta_0 + \\sum_{k=1}^K \\beta_k h_k(\\mathbf{X})\n\\]\nwhere:\n\\[\nA_k = h_k(\\mathbf{X}) = g\\left(w_{k0} + \\sum_{j=1}^p w_{kj}X_j\\right)\n\\]"
  },
  {
    "objectID": "qmd/islp10.html#breaking-down-the-equation",
    "href": "qmd/islp10.html#breaking-down-the-equation",
    "title": "Introduction to Deep Learning",
    "section": "Breaking Down the Equation",
    "text": "Breaking Down the Equation\n\n\\(K\\): The number of hidden units (neurons in the hidden layer).\n\\(g(\\cdot)\\): The activation function (introduces non-linearity).\n\\(w_{kj}\\) and \\(\\beta_k\\): The weights or parameters of the network (learned during training).\n\\(A_k\\): called activations."
  },
  {
    "objectID": "qmd/islp10.html#activation-functions-introducing-non-linearity",
    "href": "qmd/islp10.html#activation-functions-introducing-non-linearity",
    "title": "Introduction to Deep Learning",
    "section": "Activation Functions: Introducing Non-Linearity",
    "text": "Activation Functions: Introducing Non-Linearity\n\nWhy Non-Linearity? Without it, the network would collapse into a simple linear model. Activation functions allow learning complex, non-linear relationships.\nCommon Choices:\n\nSigmoid: \\(g(z) = \\frac{1}{1 + e^{-z}}\\) (Squashes values between 0 and 1. A ‚Äúsmooth‚Äù on/off switch.)\nReLU (Rectified Linear Unit): \\(g(z) = \\max(0, z)\\) (Computationally efficient. Zero for negative inputs, the input itself for positive inputs.)"
  },
  {
    "objectID": "qmd/islp10.html#visualizing-activation-functions",
    "href": "qmd/islp10.html#visualizing-activation-functions",
    "title": "Introduction to Deep Learning",
    "section": "Visualizing Activation Functions",
    "text": "Visualizing Activation Functions\n\n\n\nSigmoid (left) and ReLU (right) activation functions.\n\n\n\nLeft (Sigmoid): Notice the smooth, S-shaped curve. It ‚Äúsquashes‚Äù any input value into the range between 0 and 1. This was historically popular but can lead to problems during training (vanishing gradients)."
  },
  {
    "objectID": "qmd/islp10.html#visualizing-activation-functions-1",
    "href": "qmd/islp10.html#visualizing-activation-functions-1",
    "title": "Introduction to Deep Learning",
    "section": "Visualizing Activation Functions",
    "text": "Visualizing Activation Functions\n\n\n\nSigmoid (left) and ReLU (right) activation functions.\n\n\n\nRight (ReLU): Notice how it‚Äôs simply zero for negative inputs and a straight line for positive inputs. This simple form is computationally efficient and helps with training deeper networks. It‚Äôs a very popular choice in modern deep learning."
  },
  {
    "objectID": "qmd/islp10.html#why-non-linearity-matters-capturing-complexity",
    "href": "qmd/islp10.html#why-non-linearity-matters-capturing-complexity",
    "title": "Introduction to Deep Learning",
    "section": "Why Non-linearity Matters: Capturing Complexity",
    "text": "Why Non-linearity Matters: Capturing Complexity\n\nReal-World Complexity: Relationships in the real world are rarely simple straight lines.\nInteractions: Non-linearities enable the model to capture interactions between input variables. The effect of one variable might depend on another."
  },
  {
    "objectID": "qmd/islp10.html#example-modeling-interactions",
    "href": "qmd/islp10.html#example-modeling-interactions",
    "title": "Introduction to Deep Learning",
    "section": "Example: Modeling Interactions",
    "text": "Example: Modeling Interactions\n\nConsider:\n\n\\(p=2\\) inputs (\\(X_1\\) and \\(X_2\\)).\n\\(K=2\\) hidden units.\n\\(g(z) = z^2\\) (a simple non-linear activation function).\n\nWith appropriate weights, this can model the interaction \\(X_1X_2\\):\n\\[\nf(\\mathbf{X}) = X_1X_2\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nBy choosing the weights \\(w_{10} = 0, w_{11} = 1, w_{12} = 1, w_{20} = 0, w_{21} = 1, w_{22} = -1\\) and activation function \\(g(z)=z^2\\), the activations can be formed as: \\(A_1 = (X_1 + X_2)^2, A_2 = (X_1 - X_2)^2\\). If we further set \\(\\beta_0 = 0, \\beta_1=0.25, \\beta_2 = -0.25\\), the output becomes: \\(f(\\mathbf{X}) = 0.25(X_1 + X_2)^2 - 0.25(X_1 - X_2)^2 = X_1X_2\\). This illustrates how non-linear activation functions, combined with appropriate weights, allow neural networks to capture interaction effects."
  },
  {
    "objectID": "qmd/islp10.html#multilayer-neural-networks-going-deeper",
    "href": "qmd/islp10.html#multilayer-neural-networks-going-deeper",
    "title": "Introduction to Deep Learning",
    "section": "Multilayer Neural Networks: Going Deeper",
    "text": "Multilayer Neural Networks: Going Deeper\n\nMultiple Hidden Layers: Modern neural networks often have multiple hidden layers, stacked one after another (‚Äúdeep‚Äù learning).\nHierarchical Representations: Each layer builds upon the previous, creating increasingly complex and abstract representations."
  },
  {
    "objectID": "qmd/islp10.html#mnist-digit-classification",
    "href": "qmd/islp10.html#mnist-digit-classification",
    "title": "Introduction to Deep Learning",
    "section": "MNIST Digit Classification",
    "text": "MNIST Digit Classification\n\nMNIST: A classic dataset: images of handwritten digits (0-9).\nImage Format: Each image is 28x28 pixels, grayscale values (0-255). 784 (28*28) input features."
  },
  {
    "objectID": "qmd/islp10.html#visualizing-a-multilayer-network-mnist",
    "href": "qmd/islp10.html#visualizing-a-multilayer-network-mnist",
    "title": "Introduction to Deep Learning",
    "section": "Visualizing a Multilayer Network (MNIST)",
    "text": "Visualizing a Multilayer Network (MNIST)\n\n\n\nA multilayer neural network for MNIST digit classification.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis diagram shows a multilayer neural network for classifying handwritten digits from the MNIST dataset. The input layer is large (784 pixels). Subsequent layers progressively reduce the number of units. The output layer has 10 units (one for each digit)."
  },
  {
    "objectID": "qmd/islp10.html#the-mnist-dataset-details",
    "href": "qmd/islp10.html#the-mnist-dataset-details",
    "title": "Introduction to Deep Learning",
    "section": "The MNIST Dataset: Details",
    "text": "The MNIST Dataset: Details\n\nGoal: Classify each image into its correct digit category (0-9).\nOne-Hot Encoding: Output: a vector \\(\\mathbf{Y} = (Y_0, Y_1, \\dots, Y_9)\\), where \\(Y_i = 1\\) if the image represents digit \\(i\\), and 0 otherwise.\nDataset Size: 60,000 training images and 10,000 test images."
  },
  {
    "objectID": "qmd/islp10.html#mnist-examples",
    "href": "qmd/islp10.html#mnist-examples",
    "title": "Introduction to Deep Learning",
    "section": "MNIST Examples",
    "text": "MNIST Examples\n\n\n\nExamples of handwritten digits from the MNIST dataset.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis figure shows examples of handwritten digits from the MNIST dataset. The goal is to train a neural network to correctly classify these images. Notice the variations in handwriting style, making this a non-trivial task."
  },
  {
    "objectID": "qmd/islp10.html#multilayer-networks-key-differences",
    "href": "qmd/islp10.html#multilayer-networks-key-differences",
    "title": "Introduction to Deep Learning",
    "section": "Multilayer Networks: Key Differences",
    "text": "Multilayer Networks: Key Differences\n\nMultiple Hidden Layers: Layers like \\(L_1\\) (e.g., 256 units), \\(L_2\\) (e.g., 128 units). Each layer learns increasingly complex features.\nMultiple Outputs: Output layer can have multiple units (e.g., 10 for MNIST).\nMultitask Learning: A single network can predict multiple responses simultaneously.\nLoss Function: Choice depends on the task. For multi-class classification (like MNIST), use cross-entropy loss."
  },
  {
    "objectID": "qmd/islp10.html#multilayer-networks-mathematical-formulation-13",
    "href": "qmd/islp10.html#multilayer-networks-mathematical-formulation-13",
    "title": "Introduction to Deep Learning",
    "section": "Multilayer Networks: Mathematical Formulation (1/3)",
    "text": "Multilayer Networks: Mathematical Formulation (1/3)\n\nFirst Hidden Layer: Similar to the single-layer, but with a superscript (1) for the layer:\n\\[\nA_k^{(1)} = h_k^{(1)}(\\mathbf{X}) = g\\left(w_{k0}^{(1)} + \\sum_{j=1}^p w_{kj}^{(1)} X_j\\right)\n\\]"
  },
  {
    "objectID": "qmd/islp10.html#multilayer-networks-mathematical-formulation-23",
    "href": "qmd/islp10.html#multilayer-networks-mathematical-formulation-23",
    "title": "Introduction to Deep Learning",
    "section": "Multilayer Networks: Mathematical Formulation (2/3)",
    "text": "Multilayer Networks: Mathematical Formulation (2/3)\n\nSecond Hidden Layer: Takes activations from the first hidden layer as input:\n\\[\nA_l^{(2)} = h_l^{(2)}(\\mathbf{X}) = g\\left(w_{l0}^{(2)} + \\sum_{k=1}^{K_1} w_{lk}^{(2)} A_k^{(1)}\\right)\n\\]\n\nNotice how \\(A_k^{(1)}\\) (output of the first layer) is now the input."
  },
  {
    "objectID": "qmd/islp10.html#multilayer-networks-mathematical-formulation-33",
    "href": "qmd/islp10.html#multilayer-networks-mathematical-formulation-33",
    "title": "Introduction to Deep Learning",
    "section": "Multilayer Networks: Mathematical Formulation (3/3)",
    "text": "Multilayer Networks: Mathematical Formulation (3/3)\n\nOutput Layer (multi-class classification): Use softmax for probabilities:\n\\[\nf_m(\\mathbf{X}) = \\Pr(Y = m | \\mathbf{X}) = \\frac{e^{Z_m}}{\\sum_{m'=0}^9 e^{Z_{m'}}}\n\\]\nwhere \\(Z_m = \\beta_{m0} + \\sum_{l=1}^{K_2} \\beta_{ml} A_l^{(2)}\\).\n\nSoftmax ensures outputs are probabilities (between 0 and 1, sum to 1)."
  },
  {
    "objectID": "qmd/islp10.html#loss-function-and-optimization",
    "href": "qmd/islp10.html#loss-function-and-optimization",
    "title": "Introduction to Deep Learning",
    "section": "Loss Function and Optimization",
    "text": "Loss Function and Optimization\n\nCross-Entropy Loss (multi-class classification):\n\\[\n-\\sum_{i=1}^n \\sum_{m=0}^9 y_{im} \\log(f_m(\\mathbf{x}_i))\n\\]\n\nMeasures the difference between predicted probabilities and true labels (one-hot encoded).\n\nGoal: Find weights (all \\(w\\) and \\(\\beta\\) values) that minimize this loss.\nOptimization: Use gradient descent (and variants).\nWeights: refers to all trainable parameters, including the coefficients and bias terms."
  },
  {
    "objectID": "qmd/islp10.html#comparison-with-linear-models-mnist",
    "href": "qmd/islp10.html#comparison-with-linear-models-mnist",
    "title": "Introduction to Deep Learning",
    "section": "Comparison with Linear Models (MNIST)",
    "text": "Comparison with Linear Models (MNIST)\n\n\n\nMethod\nTest Error\n\n\n\n\nNeural Network + Ridge\n2.3%\n\n\nNeural Network + Dropout\n1.8%\n\n\nMultinomial Logistic Regression\n7.2%\n\n\nLinear Discriminant Analysis\n12.7%\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis table shows that neural networks significantly outperform traditional linear methods (like multinomial logistic regression and linear discriminant analysis) on the MNIST digit classification task. This highlights the power of deep learning for complex pattern recognition. Ridge and Dropout are regularization techniques."
  },
  {
    "objectID": "qmd/islp10.html#convolutional-neural-networks-cnns-specialized-for-images",
    "href": "qmd/islp10.html#convolutional-neural-networks-cnns-specialized-for-images",
    "title": "Introduction to Deep Learning",
    "section": "Convolutional Neural Networks (CNNs): Specialized for Images",
    "text": "Convolutional Neural Networks (CNNs): Specialized for Images\n\nImage Classification (and more): CNNs are designed for images (and data with spatial structure, like audio).\nHuman Vision Inspiration: Inspired by how humans process visual information, focusing on local patterns.\nKey Idea: Learn local features (edges, corners) and combine them for global patterns (objects)."
  },
  {
    "objectID": "qmd/islp10.html#cnn-architecture-key-components",
    "href": "qmd/islp10.html#cnn-architecture-key-components",
    "title": "Introduction to Deep Learning",
    "section": "CNN Architecture: Key Components",
    "text": "CNN Architecture: Key Components\n\nConvolutional Layers:\n\nApply convolution filters (small templates/kernels) to the image.\nDetect local features (edges, textures). Filters are learned.\nWeight sharing: Same filter applied across the entire image (efficient).\n\nPooling Layers:\n\nDownsample feature maps.\nReduce dimensionality, provide translation invariance (small shifts don‚Äôt change output much).\nMax pooling: Takes the maximum value in a region.\n\nFlatten Layer: convert multi-dimension feature maps to vector.\nFully Connected Layers: Standard neural network layers for classification."
  },
  {
    "objectID": "qmd/islp10.html#cnn-example-tiger-classification",
    "href": "qmd/islp10.html#cnn-example-tiger-classification",
    "title": "Introduction to Deep Learning",
    "section": "CNN Example: Tiger Classification üêÖ",
    "text": "CNN Example: Tiger Classification üêÖ\n\n\n\n\n\n\nInput: An image of a tiger.\nConvolutional Layers: Early layers find edges, stripes.\nPooling Layers: Downsample, provide invariance.\nHigher Layers: Combine features (eyes, ears).\nOutput: Probability of a tiger."
  },
  {
    "objectID": "qmd/islp10.html#convolution-operation-a-closer-look",
    "href": "qmd/islp10.html#convolution-operation-a-closer-look",
    "title": "Introduction to Deep Learning",
    "section": "Convolution Operation: A Closer Look",
    "text": "Convolution Operation: A Closer Look\n\nConvolution Filter: A small matrix (e.g., 3x3) of numbers (a kernel).\nSliding the Filter: ‚ÄúSlid‚Äù across the image, one pixel at a time (or larger stride).\nElement-wise Multiplication and Summation: At each position, element-wise multiplication between filter and image, then sum.\nConvolved Image: Produces a single value in the convolved image (feature map).\nDifferent Filters, Different Features: Different filters detect different features."
  },
  {
    "objectID": "qmd/islp10.html#visualizing-convolution",
    "href": "qmd/islp10.html#visualizing-convolution",
    "title": "Introduction to Deep Learning",
    "section": "Visualizing Convolution",
    "text": "Visualizing Convolution\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis figure illustrates the convolution operation. The filter (small square) is moved across the input image (large square). At each position, the element-wise product of the filter and the corresponding part of the image is computed, and the results are summed to produce a single value in the convolved image."
  },
  {
    "objectID": "qmd/islp10.html#convolution-operation-example",
    "href": "qmd/islp10.html#convolution-operation-example",
    "title": "Introduction to Deep Learning",
    "section": "Convolution Operation: Example",
    "text": "Convolution Operation: Example\n\nInput Image: \\[\nOriginal Image = \\begin{bmatrix}\na & b & c\\\\\nd & e & f\\\\\ng & h & i\\\\\nj & k & l\n\\end{bmatrix}\n\\]\nFilter: \\[\nConvolutionFilter = \\begin{bmatrix}\n\\alpha & \\beta\\\\\n\\gamma & \\delta\n\\end{bmatrix}\n\\]\nConvolved Image: \\[\nConvolved Image = \\begin{bmatrix}\naa + b\\beta + d\\gamma + e\\delta & ba + c\\beta + e\\gamma + f\\delta\\\\\nda + e\\beta + g\\gamma + h\\delta & ea + f\\beta + h\\gamma + i\\delta\\\\\nga + h\\beta + j\\gamma + k\\delta & ha + i\\beta + k\\gamma + l\\delta\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "qmd/islp10.html#convolutional-layer-details",
    "href": "qmd/islp10.html#convolutional-layer-details",
    "title": "Introduction to Deep Learning",
    "section": "Convolutional Layer: Details",
    "text": "Convolutional Layer: Details\n\nMultiple Channels: Color images: 3 channels (Red, Green, Blue). Filters match input channels.\nMultiple Filters: A layer uses many filters (e.g., 32, 64) for various features.\nReLU Activation: Usually, ReLU after convolution.\nDetector Layer: Convolution + ReLU is a detector layer."
  },
  {
    "objectID": "qmd/islp10.html#pooling-layer-downsampling",
    "href": "qmd/islp10.html#pooling-layer-downsampling",
    "title": "Introduction to Deep Learning",
    "section": "Pooling Layer: Downsampling",
    "text": "Pooling Layer: Downsampling\n\nPurpose: Reduce spatial dimensions (width, height).\nMax Pooling: Takes the maximum in a non-overlapping region (e.g., 2x2).\nBenefits:\n\nReduces Computation: Smaller feature maps, fewer computations.\nTranslation Invariance: Robust to small shifts."
  },
  {
    "objectID": "qmd/islp10.html#max-pooling-example",
    "href": "qmd/islp10.html#max-pooling-example",
    "title": "Introduction to Deep Learning",
    "section": "Max Pooling: Example",
    "text": "Max Pooling: Example\nInput:\n\\[\n\\begin{bmatrix}\n1 & 2 & 5 & 3 \\\\\n3 & 0 & 1 & 2 \\\\\n2 & 1 & 3 & 4 \\\\\n1 & 1 & 2 & 0 \\\\\n\\end{bmatrix}\n\\]\nOutput (2x2 max pooling):\n\\[\n\\begin{bmatrix}\n3 & 5 \\\\\n2 & 4\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "qmd/islp10.html#putting-it-all-together-a-complete-cnn-architecture",
    "href": "qmd/islp10.html#putting-it-all-together-a-complete-cnn-architecture",
    "title": "Introduction to Deep Learning",
    "section": "Putting it All Together: A Complete CNN Architecture",
    "text": "Putting it All Together: A Complete CNN Architecture\n\n\n\n\n\n\nSequence of Layers: Convolutional and pooling layers.\nFeature Extraction: Convolutional layers extract.\nDownsampling: Pooling layers downsample."
  },
  {
    "objectID": "qmd/islp10.html#putting-it-all-together-a-complete-cnn-architecture-1",
    "href": "qmd/islp10.html#putting-it-all-together-a-complete-cnn-architecture-1",
    "title": "Introduction to Deep Learning",
    "section": "Putting it All Together: A Complete CNN Architecture",
    "text": "Putting it All Together: A Complete CNN Architecture\n\n\n\n\n\n\nFlatten Layer: Converts 3D feature maps to 1D vector.\nFully Connected Layers: Classification.\nSoftmax Output Layer: Probabilities for each class."
  },
  {
    "objectID": "qmd/islp10.html#data-augmentation-expanding-the-training-set",
    "href": "qmd/islp10.html#data-augmentation-expanding-the-training-set",
    "title": "Introduction to Deep Learning",
    "section": "Data Augmentation: Expanding the Training Set",
    "text": "Data Augmentation: Expanding the Training Set\n\nPurpose: Artificially increase training set size.\nMethod: Random transformations:\n\nRotation üìê\nZoom üîç\nShift ‚ÜîÔ∏éÔ∏è\nFlip üîÑ\n\nBenefits:\n\nReduces Overfitting: Model sees more variations.\nImproves Generalization: More robust.\nRegularization: Acts as regularization."
  },
  {
    "objectID": "qmd/islp10.html#data-augmentation-examples",
    "href": "qmd/islp10.html#data-augmentation-examples",
    "title": "Introduction to Deep Learning",
    "section": "Data Augmentation: Examples",
    "text": "Data Augmentation: Examples\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis figure shows examples of data augmentation. A single image of a digit is transformed in various ways (rotated, shifted, zoomed) to create multiple training examples. This helps the model generalize better to unseen data and reduces overfitting."
  },
  {
    "objectID": "qmd/islp10.html#pretrained-classifiers-leveraging-existing-knowledge",
    "href": "qmd/islp10.html#pretrained-classifiers-leveraging-existing-knowledge",
    "title": "Introduction to Deep Learning",
    "section": "Pretrained Classifiers: Leveraging Existing Knowledge",
    "text": "Pretrained Classifiers: Leveraging Existing Knowledge\n\nLeverage Pre-trained Models: Use models trained on massive datasets (e.g., ImageNet).\nExample: ResNet50: Popular pre-trained CNN.\nWeight Freezing: Use convolutional layers as feature extractors. Freeze weights, train only final layers. Effective for small datasets."
  },
  {
    "objectID": "qmd/islp10.html#pretrained-classifier-illustration",
    "href": "qmd/islp10.html#pretrained-classifier-illustration",
    "title": "Introduction to Deep Learning",
    "section": "Pretrained Classifier: Illustration",
    "text": "Pretrained Classifier: Illustration\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis figure shows how a pretrained classifier (ResNet50) can be used. The convolutional layers (pretrained on ImageNet) are frozen, and only the final layers are trained on a new dataset. This leverages the knowledge learned from a large dataset, even if your own dataset is small."
  },
  {
    "objectID": "qmd/islp10.html#pretrained-classifiers-example-predictions-table-10.10",
    "href": "qmd/islp10.html#pretrained-classifiers-example-predictions-table-10.10",
    "title": "Introduction to Deep Learning",
    "section": "Pretrained Classifiers: Example Predictions (Table 10.10)",
    "text": "Pretrained Classifiers: Example Predictions (Table 10.10)\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nTrue Label\nPrediction 1\nProb 1\nPrediction 2\nProb 2\nPrediction 3\nProb 3\n\n\n\n\n(Flamingo)\nFlamingo\nFlamingo\n0.83\nSpoonbill\n0.17\nWhite stork\n0.00\n\n\n(Cooper‚Äôs Hawk)\nCooper‚Äôs Hawk\nKite\n0.60\nGreat grey owl\n0.09\nNail\n0.12\n\n\n(Cooper‚Äôs Hawk)\nCooper‚Äôs Hawk\nFountain\n0.35\nnail\n0.12\nhook\n0.07\n\n\n(Lhasa Apso)\nLhasa Apso\nTibetan terrier\n0.56\nLhasa\n0.32\ncocker spaniel\n0.03\n\n\n(Cat)\nCat\nOld English sheepdog\n0.82\nShih-Tzu\n0.04\nPersian cat\n0.04\n\n\n(Cape weaver)\nCape weaver\njacamar\n0.28\nmacaw\n0.12\nrobin\n0.12\n\n\n\n\nPredictions and Probabilities: The table displays the top 3 predictions and associated probabilities from ResNet50 for different images."
  },
  {
    "objectID": "qmd/islp10.html#pretrained-classifiers-example-predictions-table-10.10-1",
    "href": "qmd/islp10.html#pretrained-classifiers-example-predictions-table-10.10-1",
    "title": "Introduction to Deep Learning",
    "section": "Pretrained Classifiers: Example Predictions (Table 10.10)",
    "text": "Pretrained Classifiers: Example Predictions (Table 10.10)\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nTrue Label\nPrediction 1\nProb 1\nPrediction 2\nProb 2\nPrediction 3\nProb 3\n\n\n\n\n(Flamingo)\nFlamingo\nFlamingo\n0.83\nSpoonbill\n0.17\nWhite stork\n0.00\n\n\n(Cooper‚Äôs Hawk)\nCooper‚Äôs Hawk\nKite\n0.60\nGreat grey owl\n0.09\nNail\n0.12\n\n\n(Cooper‚Äôs Hawk)\nCooper‚Äôs Hawk\nFountain\n0.35\nnail\n0.12\nhook\n0.07\n\n\n(Lhasa Apso)\nLhasa Apso\nTibetan terrier\n0.56\nLhasa\n0.32\ncocker spaniel\n0.03\n\n\n(Cat)\nCat\nOld English sheepdog\n0.82\nShih-Tzu\n0.04\nPersian cat\n0.04\n\n\n(Cape weaver)\nCape weaver\njacamar\n0.28\nmacaw\n0.12\nrobin\n0.12\n\n\n\n\nCorrect Predictions: In several cases (Flamingo, Lhasa Apso), the model correctly predicts the true label with high probability.\nReasonable Alternatives: Even when incorrect, the model often provides reasonable alternative predictions (e.g., Spoonbill and White stork for Flamingo).\nUncertainty: The probabilities show the model‚Äôs uncertainty. Lower probabilities indicate less confidence.\nLimitation: Pretrained models are limited by the data they were trained on. They might misclassify objects not well-represented in the original training data."
  },
  {
    "objectID": "qmd/islp10.html#document-classification-beyond-images",
    "href": "qmd/islp10.html#document-classification-beyond-images",
    "title": "Introduction to Deep Learning",
    "section": "Document Classification: Beyond Images",
    "text": "Document Classification: Beyond Images\n\nAnother Key Application: Deep learning is effective for document classification.\nGoal: Predict document attributes:\n\nSentiment (positive, negative)\nTopic (sports, politics)\nAuthor\nSpam detection\n\nFeaturization: Converting text to numerical representation.\nExample: Sentiment analysis of IMDb movie reviews."
  },
  {
    "objectID": "qmd/islp10.html#bag-of-words-model-a-simple-featurization",
    "href": "qmd/islp10.html#bag-of-words-model-a-simple-featurization",
    "title": "Introduction to Deep Learning",
    "section": "Bag-of-Words Model: A Simple Featurization",
    "text": "Bag-of-Words Model: A Simple Featurization\n\nSimplest Approach: Surprisingly effective.\nHow it Works:\n\nCreate a dictionary of all unique words.\nRepresent each document as a vector: presence (1) or absence (0) of each word. (Or use word counts).\n\nIgnores Word Order: ‚ÄúBag‚Äù of words ‚Äì order lost.\nSparse Representation: Vectors are mostly zeros.\nBag-of-n-grams: Consider sequences of n words (captures some context)."
  },
  {
    "objectID": "qmd/islp10.html#recurrent-neural-networks-rnns-designed-for-sequences",
    "href": "qmd/islp10.html#recurrent-neural-networks-rnns-designed-for-sequences",
    "title": "Introduction to Deep Learning",
    "section": "Recurrent Neural Networks (RNNs): Designed for Sequences",
    "text": "Recurrent Neural Networks (RNNs): Designed for Sequences\n\nSequential Data: RNNs for:\n\nText (sequences of words)\nTime series (measurements over time)\nAudio\nDNA\n\nKey Idea: Process input one element at a time, maintaining a ‚Äúhidden state‚Äù (memory) of previous elements.\nUnrolling the RNN: Visualizing it ‚Äúunrolled‚Äù in time."
  },
  {
    "objectID": "qmd/islp10.html#rnn-architecture-visualized",
    "href": "qmd/islp10.html#rnn-architecture-visualized",
    "title": "Introduction to Deep Learning",
    "section": "RNN Architecture: Visualized",
    "text": "RNN Architecture: Visualized\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis figure shows the architecture of a recurrent neural network (RNN). The input sequence (X) is processed one element at a time. The hidden state (A) is updated at each step, carrying information from previous steps. The output (O) can be produced at each step or only at the final step. Crucially, the same weights (W, U, B) are used at each step (weight sharing). This allows the RNN to handle sequences of varying lengths."
  },
  {
    "objectID": "qmd/islp10.html#rnn-mathematical-formulation",
    "href": "qmd/islp10.html#rnn-mathematical-formulation",
    "title": "Introduction to Deep Learning",
    "section": "RNN: Mathematical Formulation",
    "text": "RNN: Mathematical Formulation\n\nHidden State Update:\n\\[\n\\mathbf{A}_{lk} = g\\left(w_{k0} + \\sum_{j=1}^p w_{kj} \\mathbf{X}_{lj} + \\sum_{s=1}^K u_{ks} \\mathbf{A}_{l-1,s}\\right)\n\\]\nOutput:\n\\[\n\\mathbf{O}_l = \\beta_0 + \\sum_{k=1}^K \\beta_k \\mathbf{A}_{lk}\n\\]\n\\(g(\\cdot)\\): activation function (e.g., ReLU).\n\\(\\mathbf{W, U, B}\\): shared weight matrices."
  },
  {
    "objectID": "qmd/islp10.html#explaining-the-rnn-equations",
    "href": "qmd/islp10.html#explaining-the-rnn-equations",
    "title": "Introduction to Deep Learning",
    "section": "Explaining the RNN Equations",
    "text": "Explaining the RNN Equations\n\nHidden State Update: New state (\\(\\mathbf{A}_{lk}\\)) depends on:\n\nCurrent input (\\(\\mathbf{X}_{lj}\\)).\nPrevious hidden state (\\(\\mathbf{A}_{l-1,s}\\)) ‚Äì the memory.\nWeights (\\(\\mathbf{W, U}\\)).\n\nOutput: Output (\\(\\mathbf{O}_l\\)) is a linear combination of the hidden state.\nWeight Sharing: Same weight matrices (\\(\\mathbf{W, U, B}\\)) at every time step."
  },
  {
    "objectID": "qmd/islp10.html#rnns-for-document-classification",
    "href": "qmd/islp10.html#rnns-for-document-classification",
    "title": "Introduction to Deep Learning",
    "section": "RNNs for Document Classification",
    "text": "RNNs for Document Classification\n\nBeyond Bag-of-Words: Use the sequence of words.\nWord Embeddings: Represent each word as a dense vector (word embedding). Captures semantic relationships (e.g., ‚Äúking,‚Äù ‚Äúqueen‚Äù). word2vec, GloVe.\nEmbedding Layer: Maps each word (one-hot encoded) to its embedding.\nProcessing the Sequence: RNN processes embeddings, updates state, produces output (e.g., sentiment)."
  },
  {
    "objectID": "qmd/islp10.html#word-embeddings-example",
    "href": "qmd/islp10.html#word-embeddings-example",
    "title": "Introduction to Deep Learning",
    "section": "Word Embeddings: Example",
    "text": "Word Embeddings: Example\n\n\n\n\n\n\n\nNote\n\n\n\n\nOne hot encoding vector length is the vocabulary size.\nWord embeddings provide dense representation, where semantically similar words have similar vectors. For example, ‚Äúking‚Äù and ‚Äúqueen‚Äù would be closer to each other than ‚Äúking‚Äù and ‚Äúapple‚Äù."
  },
  {
    "objectID": "qmd/islp10.html#rnns-for-time-series-forecasting",
    "href": "qmd/islp10.html#rnns-for-time-series-forecasting",
    "title": "Introduction to Deep Learning",
    "section": "RNNs for Time Series Forecasting",
    "text": "RNNs for Time Series Forecasting\n\nTime Series Prediction: RNNs are useful.\nExample: Stock market trading volume.\nInput: Past values (volume, return, volatility).\nOutput: Predicted volume for next day.\nAutocorrelation: Values correlated with past (today‚Äôs volume similar to yesterday‚Äôs). RNNs capture this."
  },
  {
    "objectID": "qmd/islp10.html#time-series-data-example",
    "href": "qmd/islp10.html#time-series-data-example",
    "title": "Introduction to Deep Learning",
    "section": "Time Series Data: Example",
    "text": "Time Series Data: Example\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis figure shows an example of time series data: daily trading statistics from the New York Stock Exchange (NYSE). The data includes log trading volume (top), the Dow Jones Industrial Average (DJIA) return (middle), and log volatility (bottom). Notice the patterns and fluctuations over time. Predicting future values of such series is a common task in finance."
  },
  {
    "objectID": "qmd/islp10.html#autocorrelation-function-acf",
    "href": "qmd/islp10.html#autocorrelation-function-acf",
    "title": "Introduction to Deep Learning",
    "section": "Autocorrelation Function (ACF)",
    "text": "Autocorrelation Function (ACF)\n\n\n\n\n\n\nMeasures Correlation with Past: The ACF measures correlation with lagged values (past).\nInterpreting the ACF: Shows how strongly related values are at different lags. High ACF at lag 1: today‚Äôs value correlated with yesterday‚Äôs."
  },
  {
    "objectID": "qmd/islp10.html#rnn-forecaster-setting-up-the-data",
    "href": "qmd/islp10.html#rnn-forecaster-setting-up-the-data",
    "title": "Introduction to Deep Learning",
    "section": "RNN Forecaster: Setting up the Data",
    "text": "RNN Forecaster: Setting up the Data\n\nInput Sequence: \\(L\\) past observations (e.g., \\(L=5\\) days): \\[\n\\mathbf{X}_1 = \\begin{pmatrix} v_{t-L} \\\\ r_{t-L} \\\\ z_{t-L} \\end{pmatrix}, \\mathbf{X}_2 = \\begin{pmatrix} v_{t-L+1} \\\\ r_{t-L+1} \\\\ z_{t-L+1} \\end{pmatrix}, \\dots, \\mathbf{X}_L = \\begin{pmatrix} v_{t-1} \\\\ r_{t-1} \\\\ z_{t-1} \\end{pmatrix}\n\\]\n\\(v_t\\): trading volume on day \\(t\\).\n\n\\(r_t\\): return on day \\(t\\).\n\\(z_t\\): volatility on day \\(t\\).\n\nOutput: \\(Y = v_t\\) (trading volume on day \\(t\\), to predict).\nCreating Training Examples: Many \\((\\mathbf{X}, Y)\\) pairs from historical data. Sequence of past, corresponding future value."
  },
  {
    "objectID": "qmd/islp10.html#rnn-forecasting-results",
    "href": "qmd/islp10.html#rnn-forecasting-results",
    "title": "Introduction to Deep Learning",
    "section": "RNN Forecasting Results",
    "text": "RNN Forecasting Results\n\n\n\n\n\n\nPerformance: RNN achieves \\(R^2\\) of 0.42 on test data. Explains 42% of variance in volume.\nComparison to Baseline: Outperforms a baseline using yesterday‚Äôs volume."
  },
  {
    "objectID": "qmd/islp10.html#autoregression-ar-model-a-traditional-approach",
    "href": "qmd/islp10.html#autoregression-ar-model-a-traditional-approach",
    "title": "Introduction to Deep Learning",
    "section": "Autoregression (AR) Model: A Traditional Approach",
    "text": "Autoregression (AR) Model: A Traditional Approach\n\nTraditional Time Series Model: Autoregression (AR) is classic.\nLinear Prediction: Predicts based on linear combination of past.\nAR(L) Model: Uses \\(L\\) previous values:\n\\[\n\\hat{v}_t = \\beta_0 + \\beta_1 v_{t-1} + \\beta_2 v_{t-2} + \\dots + \\beta_L v_{t-L}\n\\]\nIncluding Other Variables: Can include lagged values of other variables (return, volatility).\nRNN as a Non-Linear Extension: RNN is a non-linear extension of AR. RNN: complex, non-linear relationships. AR: linear."
  },
  {
    "objectID": "qmd/islp10.html#long-short-term-memory-lstm-a-more-powerful-rnn",
    "href": "qmd/islp10.html#long-short-term-memory-lstm-a-more-powerful-rnn",
    "title": "Introduction to Deep Learning",
    "section": "Long Short-Term Memory (LSTM): A More Powerful RNN",
    "text": "Long Short-Term Memory (LSTM): A More Powerful RNN\n\nAddressing Vanishing Gradients: LSTMs address ‚Äúvanishing gradient‚Äù problem (in standard RNNs with long sequences).\nTwo Hidden States:\n\nShort-term memory: Like standard RNN.\nLong-term memory: Retains information longer.\n\nImproved Performance: LSTMs often better, especially for long sequences."
  },
  {
    "objectID": "qmd/islp10.html#fitting-neural-networks-the-optimization-challenge",
    "href": "qmd/islp10.html#fitting-neural-networks-the-optimization-challenge",
    "title": "Introduction to Deep Learning",
    "section": "Fitting Neural Networks: The Optimization Challenge",
    "text": "Fitting Neural Networks: The Optimization Challenge\n\nComplex Optimization: Fitting (finding optimal weights) is non-convex.\nLocal Minima: Loss function has many local minima.\nKey Techniques:\n\nGradient Descent: Iteratively adjust weights ‚Äúdownhill.‚Äù\nBackpropagation: Efficient gradient computation.\nRegularization: Prevent overfitting (ridge, lasso, dropout).\nStochastic Gradient Descent (SGD): Small batches for faster, robust optimization.\nEarly Stopping: Stop training when validation error starts to increase."
  },
  {
    "objectID": "qmd/islp10.html#gradient-descent-illustration",
    "href": "qmd/islp10.html#gradient-descent-illustration",
    "title": "Introduction to Deep Learning",
    "section": "Gradient Descent: Illustration",
    "text": "Gradient Descent: Illustration\n\n\n\n\n\n\n\nNote\n\n\n\nGradient descent is like rolling a ball down a hill. The ball will eventually settle at the lowest point (the minimum). The learning rate controls how big of a step the ball takes at each iteration. A large learning rate might cause the ball to overshoot, while a small learning rate might make the descent slow."
  },
  {
    "objectID": "qmd/islp10.html#gradient-descent-the-algorithm",
    "href": "qmd/islp10.html#gradient-descent-the-algorithm",
    "title": "Introduction to Deep Learning",
    "section": "Gradient Descent: The Algorithm",
    "text": "Gradient Descent: The Algorithm\n\nIterative Algorithm: Gradient descent is iterative.\nGoal: Find parameters (\\(\\theta\\)) that minimize loss \\(R(\\theta)\\).\nSteps:\n\nInitialize \\(\\theta\\) (often randomly).\nRepeatedly update \\(\\theta\\) opposite the gradient:\n\\[\n\\theta^{(m+1)} \\leftarrow \\theta^{(m)} - \\rho \\nabla R(\\theta^{(m)})\n\\]\n\\(\\rho\\): learning rate (step size). Smaller: smaller steps (slower). Larger: larger steps (might overshoot)."
  },
  {
    "objectID": "qmd/islp10.html#backpropagation-efficient-gradient-computation",
    "href": "qmd/islp10.html#backpropagation-efficient-gradient-computation",
    "title": "Introduction to Deep Learning",
    "section": "Backpropagation: Efficient Gradient Computation",
    "text": "Backpropagation: Efficient Gradient Computation\n\nEfficient Gradient Calculation: Backpropagation is efficient.\nChain Rule: Uses chain rule to ‚Äúpropagate‚Äù error backward.\nEssential for Training: Essential; without it, training is slow."
  },
  {
    "objectID": "qmd/islp10.html#regularization-preventing-overfitting",
    "href": "qmd/islp10.html#regularization-preventing-overfitting",
    "title": "Introduction to Deep Learning",
    "section": "Regularization: Preventing Overfitting",
    "text": "Regularization: Preventing Overfitting\n\nPurpose: Prevent overfitting (model learns training data too well, performs poorly on unseen data).\nMethods:\n\nRidge/Lasso: Penalty for large weights.\nDropout: Randomly ‚Äúdrop out‚Äù units during training (more robust features).\nEarly Stopping: Monitor validation set performance. Stop when validation error increases (even if training error improves)."
  },
  {
    "objectID": "qmd/islp10.html#stochastic-gradient-descent-sgd-speeding-up-training",
    "href": "qmd/islp10.html#stochastic-gradient-descent-sgd-speeding-up-training",
    "title": "Introduction to Deep Learning",
    "section": "Stochastic Gradient Descent (SGD): Speeding Up Training",
    "text": "Stochastic Gradient Descent (SGD): Speeding Up Training\n\nMinibatches: Use small batches of data (not entire dataset) for gradient.\nFaster Updates: Much faster weight updates.\nRandomness: Randomness helps escape local minima.\nStandard Approach: SGD (and variants) is standard."
  },
  {
    "objectID": "qmd/islp10.html#training-and-validation-errors",
    "href": "qmd/islp10.html#training-and-validation-errors",
    "title": "Introduction to Deep Learning",
    "section": "Training and Validation Errors",
    "text": "Training and Validation Errors\n\n\n\n\n\n\n\nNote\n\n\n\nIt‚Äôs essential to monitor both training and validation error during training. The training error typically decreases as the model learns. The validation error initially decreases, but if it starts to increase, it indicates overfitting. Early stopping is a form of regularization that stops the training process before overfitting becomes significant."
  },
  {
    "objectID": "qmd/islp10.html#dropout-learning-illustration",
    "href": "qmd/islp10.html#dropout-learning-illustration",
    "title": "Introduction to Deep Learning",
    "section": "Dropout Learning: Illustration",
    "text": "Dropout Learning: Illustration\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDropout randomly removes units (neurons) during training. This forces the network to learn more robust features and prevents it from relying too heavily on any single neuron. It‚Äôs like training multiple different networks and averaging their predictions."
  },
  {
    "objectID": "qmd/islp10.html#network-tuning-finding-the-right-architecture",
    "href": "qmd/islp10.html#network-tuning-finding-the-right-architecture",
    "title": "Introduction to Deep Learning",
    "section": "Network Tuning: Finding the Right Architecture",
    "text": "Network Tuning: Finding the Right Architecture\n\nArchitecture and Hyperparameters: Choosing architecture (layers, units, activations) and hyperparameters (learning rate, batch size, regularization) is crucial.\nKey Considerations:\n\nNumber of hidden layers.\nUnits per layer.\nRegularization (dropout, ridge/lasso).\nLearning rate.\nBatch size.\nEpochs (passes through training data).\n\nTrial and Error: Finding optimal settings: trial and error (time-consuming). Systematic: grid search, random search. Advanced: Bayesian optimization."
  },
  {
    "objectID": "qmd/islp10.html#interpolation-and-double-descent-a-surprising-phenomenon",
    "href": "qmd/islp10.html#interpolation-and-double-descent-a-surprising-phenomenon",
    "title": "Introduction to Deep Learning",
    "section": "Interpolation and Double Descent: A Surprising Phenomenon",
    "text": "Interpolation and Double Descent: A Surprising Phenomenon\n\n\nInterpolation: Model perfectly fits training data (zero training error).\nDouble Descent: Test error decreases again after increasing (as complexity increases beyond interpolation).\nNot a Contradiction: Does not contradict bias-variance tradeoff."
  },
  {
    "objectID": "qmd/islp10.html#double-descent-explanation",
    "href": "qmd/islp10.html#double-descent-explanation",
    "title": "Introduction to Deep Learning",
    "section": "Double Descent: Explanation",
    "text": "Double Descent: Explanation\n\nTest error shows the U-shape at first, then decreases as model complexity further increases.\nRegularization is helpful to reduce test error.\nDoes not contradict the bias-variance tradeoff. The classical bias-variance tradeoff applies within a fixed model class. Double descent occurs when we consider a sequence of increasingly complex models."
  },
  {
    "objectID": "qmd/islp10.html#when-to-use-deep-learning-choosing-the-right-tool",
    "href": "qmd/islp10.html#when-to-use-deep-learning-choosing-the-right-tool",
    "title": "Introduction to Deep Learning",
    "section": "When to Use Deep Learning: Choosing the Right Tool",
    "text": "When to Use Deep Learning: Choosing the Right Tool\n\nLarge Datasets: Deep learning shines with lots of data. Small datasets: simpler models might be better (interpretable).\nComplex Relationships: Highly non-linear, complex: deep learning effective.\nDifficult Feature Engineering: Deep learning automates feature learning.\nInterpretability is Less Important: Deep learning: ‚Äúblack boxes.‚Äù Interpretability crucial: simpler models.\nComputational Resources: Training: computationally expensive (GPUs)."
  },
  {
    "objectID": "qmd/islp10.html#summary",
    "href": "qmd/islp10.html#summary",
    "title": "Introduction to Deep Learning",
    "section": "Summary",
    "text": "Summary\n\nDeep Learning Power: Powerful techniques (neural networks), learning complex patterns.\nCNNs and RNNs: CNNs: images. RNNs: sequential data.\nOptimization Challenges: Complex, but software simplifies.\nRegularization is Key: Prevent overfitting, improve generalization.\nData and Complexity: Excels with large data, complex relationships.\nConsider Simpler Models: Simpler model, good performance, interpretable: might be better."
  },
  {
    "objectID": "qmd/islp10.html#thoughts-and-discussion",
    "href": "qmd/islp10.html#thoughts-and-discussion",
    "title": "Introduction to Deep Learning",
    "section": "Thoughts and Discussion",
    "text": "Thoughts and Discussion\n\nEthical Implications: Ethics of deep learning (facial recognition, loans, hiring)? Fairness, avoid bias?\nInterpretability: How to make models more interpretable? Why is it important?\nLimitations: Limitations of deep learning? When are other methods (decision trees, SVMs) better?\nFuture of Deep Learning: Future evolution? New architectures, applications?\nModel Complexity: How to decide model complexity? What will happen if we use a too simple or too complex model?\nData Quality: What are the actions to ensure data quality, and remove bias in data sets?"
  }
]